1,"School of Computer Science Probabilistic Graphical Models Introduction to GM and Directed GMs: Bayesian Networks Eric Xing Lecture 1, January 13, 2014 © Eric Xing @ CMU, 2005-2014 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 Reading: see class homepage 1 Class webpage:  http://www.cs.cmu.edu/~epxing/Class/10708/ Logistics © Eric Xing @ CMU, 2005-2014 2 Logistics  Text books:  Daphne Koller and Nir Friedman, Probabilistic Graphical Models  M. I. Jordan, An Introduction to Probabilistic Graphical Models  Mailing Lists:  To contact the instructors: instructor-10708@cs.cmu.edu  Class announcements list: 10708-students@cs.cmu.edu.  TA:  Willie Neiswanger, GHC 8011, Office hours: TBA  Micol Marchetti-Bowick, GHC 8003, Office hours: TBA  Dai Wei, GHC 8011, Office hours: TBA  Guest Lecturers:  TBA  Class Assistant:  Michael Martins, GHC 8001, x8-5527  Instruction aids: Canvas © Eric Xing @ CMU, 2005-2014 3 Logistics  5 homework assignments: 40% of grade  Theory exercises, Implementation exercises  Scribe duties: 10% (~once to twice for the whole semester)  Short reading summary: 10% (due at the beginning of every lecture)  Final project: 40% of grade  Applying PGM to the development of a real, substantial ML system  Design and Implement a (rocord-breaking) distributed Deep Network on Petuum and apply to ImageNet and/or other data  Build a web-scale topic or story line tracking system for news media, or a paper recommendation system for conference review matching  An online car or people or event detector for web-images and webcam  An automatic “what’s up here?” or “photo album” service on iPhone  Theoretical and/or algorithmic work  a more efficient approximate inference or optimization algorithm, e.g., based on stochastic approximation  a distributed sampling scheme with convergence guarantee  3-member team to be formed in the first two weeks, proposal, mid-way presentation, poster & demo, final report, peer review possibly conference submission ! © Eric Xing @ CMU, 2005-2014 4 Past projects:  We will have a prize for the best project(s) …  Winner of the 2005 project: J. Yang, Y. Liu, E. P. Xing and A. Hauptmann, Harmonium-Based Models for Semantic Video Representation and Classification , Proceedings of The Seventh SIAM International Conference on Data Mining (SDM 2007). (Recipient of the BEST PAPER Award)  Other projects: Andreas Krause, Jure Leskovec and Carlos Guestrin, Data Association for Topic Intensity Tracking, 23rd International Conference on Machine Learning (ICML 2006). M. Sachan, A. Dubey, S. Srivastava, E. P. Xing and Eduard Hovy, Spatial Compactness meets Topical Consistency: Jointly modeling Links and Content for Community Detection , Proceedings of The 7th ACM International Conference on Web Search and Data Mining (WSDM 2014). © Eric Xing @ CMU, 2005-2014 5 What Are Graphical Models? © Eric Xing @ CMU, 2005-2014 6 Graph Model M Data D ´ fX(i) 1 ; X(i) 2 ; :::; X(i) m gN i=1 Reasoning under uncertainty! © Eric Xing @ CMU, 2005-2014 Speech recognition Information retrieval Computer vision Robotic control Planning Games Evolution Pedigree 7 The Fundamental Questions Representation  How to capture/model uncertainties in possible worlds?  How to encode our domain knowledge/assumptions/constraints? Inference  How do I answers questions/queries according to my model and/or based given data? Learning  What model is ""right"" for my data? © Eric Xing @ CMU, 2005-2014 ? ? ? ? X1 X2 X3 X4 X5 X6 X7 X8 X9 ) | ( : e.g. D i X P ) ; ( max arg : e.g. M M M D F M   8  Representation: what is the joint probability dist. on multiple variables?  How many state configurations in total? --- 28  Are they all needed to be represented?  Do we get any scientific/medical insight? Learning: where do we get all this probabilities?  Maximal-likelihood estimation? but how many data do we need?  Are there other est. principles?  Where do we put domain knowledge in terms of plausible relationships between variables, and plausible values of the probabilities?  Inference: If not all variables are observable, how to compute the conditional distribution of latent variables given evidence?  Computing p(H|A) would require summing over all 26 configurations of the unobserved variables ) , , , , , , , ( 8 7 6 5 4 3 2 1 X X X X X X X X P Recap of Basic Prob. Concepts © Eric Xing @ CMU, 2005-2014 A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B 9 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 What is a Graphical Model? --- Multivariate Distribution in High-D Space A possible world for cellular signal transduction: © Eric Xing @ CMU, 2005-2014 10 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B Membrane Cytosol X1 X2 X3 X4 X5 X6 X7 X8 GM: Structure Simplifies Representation Dependencies among variables © Eric Xing @ CMU, 2005-2014 11 If Xi's are conditionally independent (as described by a PGM), the joint can be factored to a product of simpler terms, e.g., Why we may favor a PGM? Incorporation of domain knowledge and causal (logical) structures P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) Probabilistic Graphical Models © Eric Xing @ CMU, 2005-2014 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 1+1+2+2+2+4+2+4=18, a 16-fold reduction from 28 in representation cost ! Stay tune for what are these independencies! 12 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 GM: Data Integration © Eric Xing @ CMU, 2005-2014 13 More Data Integration Text + Image + Network Holistic Social Media Genome + Proteome + Transcritome + Phenome + …  PanOmic Biology © Eric Xing @ CMU, 2005-2014 14 If Xi's are conditionally independent (as described by a PGM), the joint can be factored to a product of simpler terms, e.g., Why we may favor a PGM? Incorporation of domain knowledge and causal (logical) structures Modular combination of heterogeneous parts – data fusion Probabilistic Graphical Models © Eric Xing @ CMU, 2005-2014 2+2+4+4+4+8+4+8=36, an 8-fold reduction from 28 in representation cost ! Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X X1 1 X X2 2 X X3 3 X X4 4 X X5 5 X X6 6 X X7 7 X X8 8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X X1 1 X X2 2 X X3 3 X X4 4 X X5 5 X X6 6 X X7 7 X X8 8 X X1 1 X X2 2 X X3 3 X X4 4 X X5 5 X X6 6 X X7 7 X X8 8 P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X2) P(X4| X2) P(X5| X2) P(X1) P(X3| X1) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) 15       H h h p h d p h p h d p d h p ) ( ) | ( ) ( ) | ( ) | ( Posterior probability Likelihood Prior probability Sum over space of hypotheses Rational Statistical Inference  This allows us to capture uncertainty about the model in a principled way  But how can we specify and represent a complicated model?  Typically the number of genes need to be modeled are in the order of thousands! © Eric Xing @ CMU, 2005-2014 h d The Bayes Theorem: 16 GM: MLE and Bayesian Learning  Probabilistic statements of is conditioned on the values of the observed variables Aobs and prior p( |) © Eric Xing @ CMU, 2005-2014 (A,B,C,D,E,…)=(T,F,F,T,F,…) A= (A,B,C,D,E,…)=(T,F,T,T,F,…) …….. (A,B,C,D,E,…)=(F,T,T,T,F,…) A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B 0.9 0.1 c d c 0.2 0.8 0.01 0.99 0.9 0.1 d c d d c D C P(F | C,D) 0.9 0.1 c d c 0.2 0.8 0.01 0.99 0.9 0.1 d c d d c D C P(F | C,D) p() ) ; ( ) | ( ) ; | (   Θ Θ Θ p p p A A  posterior likelihood prior Θ Θ Θ Θ d p Bayes   ) , | (  A 17 If Xi's are conditionally independent (as described by a PGM), the joint can be factored to a product of simpler terms, e.g., Why we may favor a PGM? Incorporation of domain knowledge and causal (logical) structures Modular combination of heterogeneous parts – data fusion Bayesian Philosophy Knowledge meets data Probabilistic Graphical Models © Eric Xing @ CMU, 2005-2014 2+2+4+4+4+8+4+8=36, an 8-fold reduction from 28 in representation cost !     P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 18 So What is a Graphical Model? © Eric Xing @ CMU, 2005-2014 In a nutshell: GM = Multivariate Statistics + Structure 19 What is a Graphical Model? The informal blurb:  It is a smart way to write/specify/compose/design exponentially-large probability distributions without paying an exponential cost, and at the same time endow the distributions with structured semantics A more formal description:  It refers to a family of distributions on a set of random variables that are compatible with all the probabilistic independence propositions encoded by a graph that connects these variables © Eric Xing @ CMU, 2005-2014 A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B ) ( 8 7 6 5 4 3 2 1 ,X ,X ,X ,X ,X ,X ,X X P ) , ( ) ( ) , ( ) | ( ) | ( ) | ( ) ( ) ( ) ( : 6 5 8 6 7 4 3 6 2 5 2 4 2 1 3 2 1 8 1 X X X P X X P X X X P X X P X X P X X X P X P X P X P  20 Directed edges give causality relationships (Bayesian Network or Directed Graphical Model): Undirected edges simply give correlations between variables (Markov Random Field or Undirected Graphical model): Two types of GMs © Eric Xing @ CMU, 2005-2014 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) P(X1, X2, X3, X4, X5, X6, X7, X8) = 1/Z exp{E(X1)+E(X2)+E(X3, X1)+E(X4, X2)+E(X5, X2) + E(X6, X3, X4)+E(X7, X6)+E(X8, X5, X6)} 21 Structure: DAG • Meaning: a node is conditionally independent of every other node in the network outside its Markov blanket • Local conditional distributions (CPD) and the DAG completely determine the joint dist. • Give causality relationships, and facilitate a generative process X Y1 Y2 Descendent Ancestor Parent Children's co-parent Children's co-parent Child Bayesian Networks © Eric Xing @ CMU, 2005-2014 22 Structure: undirected graph • Meaning: a node is conditionally independent of every other node in the network given its Directed neighbors • Local contingency functions (potentials) and the cliques in the graph completely determine the joint dist. • Give correlations between variables, but no explicit way to generate samples X Y1 Y2 Markov Random Fields © Eric Xing @ CMU, 2005-2014 23 Towards structural specification of probability distribution Separation properties in the graph imply independence properties about the associated variables For the graph to be useful, any conditional independence properties we can derive from the graph should hold for the probability distribution that the graph represents The Equivalence Theorem For a graph G, Let D1 denote the family of all distributions that satisfy I(G), Let D2 denote the family of all distributions that factor according to G, Then D1≡D2. © Eric Xing @ CMU, 2005-2014 24 Density estimation Regression Classification Parametric and nonparametric methods Linear, conditional mixture, nonparametric Generative and discriminative approach Q X Q X X Y m,s X X GMs are your old friends © Eric Xing @ CMU, 2005-2014 Clustering 25 (Picture by Zoubin Ghahramani and Sam Roweis) © Eric Xing @ CMU, 2005-2014 An (incomplete) genealogy of graphical models 26 Fancier GMs: reinforcement learning Partially observed Markov decision processes (POMDP) © Eric Xing @ CMU, 2005-2014 27 Fancier GMs: machine translation © Eric Xing @ CMU, 2005-2014 SMT The HM-BiTAM model (B. Zhao and E.P Xing, ACL 2006) 28 Fancier GMs: genetic pedigree © Eric Xing @ CMU, 2005-2014 A0 A1 Ag B0 B1 Bg M 0 M 1 F0 F1 Fg C 0 C 1 C g Sg An allele network 29 Fancier GMs: solid state physics © Eric Xing @ CMU, 2005-2014 Ising/Potts model 30 Application of GMs  Machine Learning  Computational statistics  Computer vision and graphics  Natural language processing  Informational retrieval  Robotic control  Decision making under uncertainty  Error-control codes  Computational biology  Genetics and medical diagnosis/prognosis  Finance and economics  Etc. © Eric Xing @ CMU, 2005-2014 31 Why graphical models  A language for communication  A language for computation  A language for development Origins:  Wright 1920’s  Independently developed by Spiegelhalter and Lauritzen in statistics and Pearl in computer science in the late 1980’s © Eric Xing @ CMU, 2005-2014 32  Probability theory provides the glue whereby the parts are combined, ensuring that the system as a whole is consistent, and providing ways to interface models to data.  The graph theoretic side of graphical models provides both an intuitively appealing interface by which humans can model highly-interacting sets of variables as well as a data structure that lends itself naturally to the design of efficient general-purpose algorithms.  Many of the classical multivariate probabilistic systems studied in fields such as statistics, systems engineering, information theory, pattern recognition and statistical mechanics are special cases of the general graphical model formalism  The graphical model framework provides a way to view all of these systems as instances of a common underlying formalism. --- M. Jordan Why graphical models © Eric Xing @ CMU, 2005-2014 33 A few myths about graphical models They require a localist semantics for the nodes They require a causal semantics for the edges They are necessarily Bayesian They are intractable © Eric Xing @ CMU, 2005-2014     34 Plan for the Class  Fundamentals of Graphical Models:  Bayesian Network and Markov Random Fields  Discrete, Continuous and Hybrid models, exponential family, GLIM  Basic representation, inference, and learning  Case studies: Popular Bayesian networks and MRFs  Multivariate Gaussian Models  Hidden Markov Models  Mixed-membership, aka, Topic models  …  Advanced topics and latest developments  Approximate inference  Monte Carlo algorithms  Vatiational methods and theories  Stochastic algorithms  Nonparametric and spectral graphical models, where GM meets kernels and matrix algebra  “Infinite” GMs: nonparametric Bayesian models  Structured sparsity  Margin-based learning of GMs: where GM meets SVM  Regularized Bayes: where GM meets SVM, and meets Bayesian, and meets NB …  Applications © Eric Xing @ CMU, 2005-2014 35 "
10,"School of Computer Science Probabilistic Graphical Models Gaussian graphical models and Ising models: modeling networks Eric Xing Lecture 10, February 17, 2014 Reading: See class website © Eric Xing @ CMU, 2005-2014 1 Where do networks come from? The Jesus network © Eric Xing @ CMU, 2005-2014 2 Evolving networks March 2005 January 2006 August 2006 Can I get his vote? Corporativity, Antagonism, Cliques, … over time? © Eric Xing @ CMU, 2005-2014 3 … t=1 2 3 T Evolving networks © Eric Xing @ CMU, 2005-2014 4 Recall Multivariate Gaussian Multivariate Gaussian density: WOLG: let We can view this as a continuous Markov Random Field with potentials defined on every node and edge:   ) - ( ) - ( - exp ) ( ) , | ( / /     x x x 1 2 1 2 1 2 2 1      T n p              j i j i ij i i ii n p x x q x q Q Q x x x p 2 2 1 2 / 2 / 1 2 1 - exp ) 2 ( ) , 0 | , , , (    © Eric Xing @ CMU, 2005-2014 5 Cell type Microarray samples Encodes dependencies among genes Gaussian Graphical Model © Eric Xing @ CMU, 2005-2014 6 Edge corresponds to non- zero precision matrix element Precision Matrix Encodes Non-Zero Edges in Gaussian Graphical Modela © Eric Xing @ CMU, 2005-2014 7 Correlation network is based on Covariance Matrix A GGM is a Markov Network based on Precision Matrix Conditional Independence/Partial Correlation Coefficients are a more sophisticated dependence measure With small sample size, empirical covariance matrix cannot be inverted Markov versus Correlation Network © Eric Xing @ CMU, 2005-2014 8 Sparsity One common assumption to make: sparsity Makes empirical sense: Genes are only assumed to interface with small groups of other genes. Makes statistical sense: Learning is now feasible in high dimensions with small sample size sparse © Eric Xing @ CMU, 2005-2014 9 Network Learning with the LASSO Assume network is a Gaussian Graphical Model Perform LASSO regression of all nodes to a target node © Eric Xing @ CMU, 2005-2014 10 Network Learning with the LASSO LASSO can select the neighborhood of each node © Eric Xing @ CMU, 2005-2014 11 L1 Regularization (LASSO) A convex relaxation. Enforces sparsity! Constrained Form Lagrangian Form  y x1 x2 x3 xn-1 xn 1 2 3 n-1 n © Eric Xing @ CMU, 2005-2014 12 Theoretical Guarantees Assumptions  Dependency Condition: Relevant Covariates are not overly dependent  Incoherence Condition: Large number of irrelevant covariates cannot be too correlated with relevant covariates  Strong concentration bounds: Sample quantities converge to expected values quickly If these are assumptions are met, LASSO will asymptotically recover correct subset of covariates that relevant. © Eric Xing @ CMU, 2005-2014 13 Network Learning with the LASSO Repeat this for every node Form the total edge set © Eric Xing @ CMU, 2005-2014 14 If Then with high probability, Consistent Structure Recovery [Meinshausen and Buhlmann 2006, Wainwright 2009] © Eric Xing @ CMU, 2005-2014 15 Why this algorithm work? What is the intuition behind graphical regression?  Continuous nodal attributes  Discrete nodal attributes Are there other algorihtms? More general scenarios: non-iid sample and evolving networks Case study © Eric Xing @ CMU, 2005-2014 16 Multivariate Gaussian Multivariate Gaussian density: A joint Gaussian: How to write down p(x2), p(x1|x2) or p(x2|x1) using the block elements in and ?  Formulas to remember:   ) - ( ) - ( - exp ) ( ) , | ( / /     x x x 1 2 1 2 1 2 2 1      T n p ) , ( ) , | (                               22 21 12 11 2 1 2 1 2 1    x x x x N p 21 1 22 12 11 2 1 2 2 1 22 12 1 2 1 2 1 2 1 1 2 1               | | | | ) ( ) , | ( ) ( V x m V m x x x   N p 22 2 2 2 2 2 2 2     m m m m p V m V m x x  ) , | ( ) ( N © Eric Xing @ CMU, 2005-2014 17 The matrix inverse lemma Consider a block-partitioned matrix: First we diagonalize M  Schur complement: Then we inverse, using this formula: Matrix inverse lemma        H G F E M                          H G E-FH I G -H I H G F E I -FH I - - - 0 0 0 0 1 1 1 X ZW Y W XYZ - - 1 1    G E-FH M/H -1                                                                                1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 M/E GE M/E - M/E F -E GE M/E F E E FH M/H G H H M/H G -H FH M/H - M/H I -FH I H M/H I G -H I H G F E M - - - - - -     1 1 1 1 1 1 1 - - - GE F H-GE F E E G E-FH       © Eric Xing @ CMU, 2005-2014 18 The covariance and the precision matrices                         1 1 1 1 1 1 1 1 1 1 - - FH M/H G H H M/H G -H FH M/H - M/H M                               1 1 1 11 1 1 1 1 11 1 1 1 1 1 11 1 1 1 11 11 Q q q q q I -q -q q Q T T T                    1 1 1 11     T   © Eric Xing @ CMU, 2005-2014 19 Single-node Conditional The conditional dist. of a single node i given the rest of the nodes can be written as: WOLG: let 21 1 22 12 11 2 1 2 2 1 22 12 1 2 1 2 1 2 1 1 2 1               | | | | ) ( ) , | ( ) ( V x m V m x x x   N p                               1 1 1 11 1 1 1 1 11 1 1 1 1 1 11 1 1 1 11 11 Q q q q q I -q -q q Q T T T           © Eric Xing @ CMU, 2005-2014 20 Conditional auto-regression From We can write the following conditional auto-regression function for each node: Neighborhood est. based on auto-regression coefficient © Eric Xing @ CMU, 2005-2014 21 Conditional independence From Given an estimate of the neighborhood si, we have: Thus the neighborhood si defines the Markov blanket of node i © Eric Xing @ CMU, 2005-2014 22 Recent trends in GGM:  Covariance selection (classical method)  Dempster [1972]:  Sequentially pruning smallest elements in precision matrix  Drton and Perlman [2008]:  Improved statistical tests for pruning  L1-regularization based method (hot !)  Meinshausen and Bühlmann [Ann. Stat. 06]:  Used LASSO regression for neighborhood selection  Banerjee [JMLR 08]:  Block sub-gradient algorithm for finding precision matrix  Friedman et al. [Biostatistics 08]:  Efficient fixed-point equations based on a sub-gradient algorithm  … Serious limitations in practice: breaks down when covariance matrix is not invertible Structure learning is possible even when # variables ＞# samples © Eric Xing @ CMU, 2005-2014 23 The Meinshausen-Bühlmann (MB) algorithm: Step 1: Pick up one variable Step 2: Think of it as “y”, and the rest as “z” Step 3: Solve Lasso regression problem between y and z Step 4: Connect the k-th node to those having nonzero weight in w Solving separated Lasso for every single variables: The resulting coefficient does not correspond to the Q value-wise © Eric Xing @ CMU, 2005-2014 24 L1-regularized maximum likelihood learning Input: Sample covariance matrix S  Assumes standardized data (mean=0, variance=1)  S is generally rank-deficient  Thus the inverse does not exist Output: Sparse precision matrix Q  Originally, Q is defined as the inverse of S, but not directly invertible  Need to find a sparse matrix that can be thought as of as an inverse of S Approach: Solve an L1-regularized maximum likelihood equation log likelihood regularizer © Eric Xing @ CMU, 2005-2014 25 From matrix opt. to vector opt.: coupled Lasso for every single Var.  Focus only on one row (column), keeping the others constant  Optimization problem for blue vector is shown to be Lasso (L1- regularized quadratic programming)  Difference from MB’s: Resulting Lasso problems are coupled  The gray part is actually not constant; changes after solving one Lasso problem (because it is the opt of the entire Q that optimize a single loss function, whereas in MB each lasso has its own loss function..  This coupling is essential for stability under noise © Eric Xing @ CMU, 2005-2014 26 Learning Ising Model (i.e. pairwise MRF) Assuming the nodes are discrete (e.g., voting outcome of a person), and edges are weighted, then for a sample x, we have It can be shown the pseudo-conditional likelihood for node k is © Eric Xing @ CMU, 2005-2014 27 New Problem: Evolving Social Networks March 2005 January 2006 August 2006 Can I get his vote? Corporativity, Antagonism, Cliques, … over time? © Eric Xing @ CMU, 2005-2014 28 T0 TN … Drosophila development t* n=1 or some small # Reverse engineering time- specific ""rewiring"" networks © Eric Xing @ CMU, 2005-2014 29 Inference I  KELLER: Kernel Weighted L1-regularized Logistic Regression  Constrained convex optimization  Estimate time-specific nets one by one, based on ""virtual iid"" samples  Could scale to ~104 genes, but under stronger smoothness assumptions [Song, Kolar and Xing, Bioinformatics 09] Lasso: © Eric Xing @ CMU, 2005-2014 30 Conditional likelihood  Neighborhood Selection: Time-specific graph regression:  Estimate at Where and Algorithm – nonparametric neighborhood selection © Eric Xing @ CMU, 2005-2014 31 Structural consistency of KELLER Assumptions Define: A1: Dependency Condition A2: Incoherence Condition A3: Smoothness Condition A4: Bounded Kernel © Eric Xing @ CMU, 2005-2014 32 Theorem [Kolar and Xing, 09] © Eric Xing @ CMU, 2005-2014 33  TESLA: Temporally Smoothed L1-regularized logistic regression  Constrained convex optimization  Scale to ~5000 nodes, does not need smoothness assumption, can accommodate abrupt changes. Inference II [Amr and Xing, PNAS 2009, AOAS 2009] © Eric Xing @ CMU, 2005-2014 34 Temporally Smoothed Graph Regression TESLA: … © Eric Xing @ CMU, 2005-2014 35 Modified estimation procedure estimate block partition on which the coefficient functions are constant estimate the coefficient functions on each block of the partition (*) (**) © Eric Xing @ CMU, 2005-2014 36 Structural Consistency of TESLA I. It can be shown that, by applying the results for model selection of the Lasso on a temporal difference transformation of (*), the block are estimated consistently II. Then it can be further shown that, by applying Lasso on (**), the neighborhood of each node on each of the estimated blocks consistently  Further advantages of the two step procedure  choosing parameters easier  faster optimization procedure [Kolar, and Xing, 2009] © Eric Xing @ CMU, 2005-2014 37 Senate network – 109th congress Voting records from 109th congress (2005 - 2006) There are 100 senators whose votes were recorded on the 542 bills, each vote is a binary outcome © Eric Xing @ CMU, 2005-2014 38 Senate network – 109th congress March 2005 January 2006 August 2006 © Eric Xing @ CMU, 2005-2014 39 Senator Chafee © Eric Xing @ CMU, 2005-2014 40 Senator Ben Nelson T=0.2 T=0.8 © Eric Xing @ CMU, 2005-2014 41 S1 (normal) Progression and Reversion of Breast Cancer cells T4 (malignant) T4 revert 1 T4 revert 2 T4 revert 3 © Eric Xing @ CMU, 2005-2014 42 T4 S1 T4R1 T4R2 T4R3 Estimate Neighborhoods Jointly Across All Cell Types How to share information across the cell types? © Eric Xing @ CMU, 2005-2014 43 Penalize differences between networks of adjacent cell types T4 S1 T4R1 T4R2 T4R3 Sparsity of Difference © Eric Xing @ CMU, 2005-2014 44 RSS for all cell types sparsity Sparsity of difference Tree-Guided Graphical Lasso (Treegl) © Eric Xing @ CMU, 2005-2014 45 S1 T4 EGFR-ITGB1 PI3K-MAPKK MMP Network Overview © Eric Xing @ CMU, 2005-2014 46 S1 cells T4 cells: Increased Cell Proliferation, Growth, Signaling, Locomotion Interactions – Biological Processes © Eric Xing @ CMU, 2005-2014 47 MMP-T4R cells: Significantly reduced interactions T4 cells Interactions – Biological Processes © Eric Xing @ CMU, 2005-2014 48 PI3K-MAPKK-T4R: Reduced Growth, Locomotion and Signaling T4 cells Interactions – Biological Processes © Eric Xing @ CMU, 2005-2014 49 Summary  Graphical Gaussian Model  The precision matrix encode structure  Not estimatable when p >> n  Neighborhood selection:  Conditional dist under GGM/MRF  Graphical lasso  Sparsistency  Time-varying Markov networks  Kernel reweighting est.  Total variation est. © Eric Xing @ CMU, 2005-2014 50 "
11,"School of Computer Science Probabilistic Graphical Models Factor Analysis and State Space Models Eric Xing Lecture 11, February 19o, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 A road map to more complex dynamic models A X Y A X Y A X Y discrete discrete discrete continuous continuous continuous Mixture model e.g., mixture of multinomials Mixture model e.g., mixture of Gaussians Factor analysis A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... HMM (for discrete sequential data, e.g., text) HMM (for continuous sequential data, e.g., speech signal) State space model ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... Factorial HMM Switching SSM 2 © Eric Xing @ CMU, 2005-2014 Recall multivariate Gaussian Multivariate Gaussian density: A joint Gaussian: How to write down p(x1), p(x1|x2) or p(x2|x1) using the block elements in and ?  Formulas to remember:   ) - ( ) - ( - exp ) ( ) , | ( / /     x x x 1 2 1 2 1 2 2 1      T n p ) , ( ) , | (                               22 21 12 11 2 1 2 1 2 1    x x x x N p 21 1 22 12 11 2 1 2 2 1 22 12 1 2 1 2 1 2 1 1 2 1               | | | | ) ( ) , | ( ) ( V x m V m x x x   N p 22 2 2 2 2 2 2 2     m m m m p V m V m x x  ) , | ( ) ( N 3 © Eric Xing @ CMU, 2005-2014 Review: The matrix inverse lemma Consider a block-partitioned matrix: First we diagonalize M  Schur complement: Then we inverse, using this formula: Matrix inverse lemma        H G F E M                          H G E-FH I G -H I H G F E I -FH I - - - 0 0 0 0 1 1 1 X ZW Y W XYZ - - 1 1    G E-FH M/H -1                                                                                1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 M/E GE M/E - M/E F -E GE M/E F E E FH M/H G H H M/H G -H FH M/H - M/H I -FH I H M/H I G -H I H G F E M - - - - - -     1 1 1 1 1 1 1 - - - GE F H-GE F E E G E-FH       4 © Eric Xing @ CMU, 2005-2014 Review: Some matrix algebra Trace and derivatives  Cyclical permutations  Derivatives Determinants and derivatives   i ii a def tr A   T B BA A   tr     T T T xx A xx A Ax x A       tr tr       BCA CAB ABC tr tr tr   1 log     A A A 5 © Eric Xing @ CMU, 2005-2014 Factor analysis An unsupervised linear regression model Geometric interpretation  To generate data, first generate a point within the manifold then add noise. Coordinates of point are components of latent variable. A Y X where is called a factor loading matrix, and is diagonal. ) , ; ( ) ( ) , ; ( ) (      x y x y x x  N N p p I 0 6 © Eric Xing @ CMU, 2005-2014 Marginal data distribution A marginal Gaussian (e.g., p(x)) times a conditional Gaussian (e.g., p(y|x)) is a joint Gaussian Any marginal (e.g., p(y) of a joint Gaussian (e.g., p(x,y)) is also a Gaussian  Since the marginal is Gaussian, we can determine it by just computing its mean and variance. (Assume noise uncorrelated with data.)                                                                 T T T T T T T E E E E E Var E E E E WW XX W X W X W X W X Y Y Y W X W W X Y           0 0 0 ) , ( ~ here w N A Y X 7 © Eric Xing @ CMU, 2005-2014 FA = Constrained-Covariance Gaussian Marginal density for factor analysis (y is p-dim, x is k-dim): So the effective covariance is the low-rank outer product of two long skinny matrices plus a diagonal matrix: In other words, factor analysis is just a constrained Gaussian model. (If were not diagonal then we could model any Gaussian and it would be pointless.) ) , ; ( ) | (     T   y y N p 8 © Eric Xing @ CMU, 2005-2014 FA joint distribution Model Covariance between x and y Hence the joint distribution of x and y: Assume noise is uncorrelated with data or latent variables. ) , ; ( ) ( ) , ; ( ) (      x y x y x x  N N p p I 0              T T T T T T E E E Cov              XW XX W X X Y X Y X,    0 ) , ( ) (                               T T I  0 y x y x N p 9 © Eric Xing @ CMU, 2005-2014 Inference in Factor Analysis Apply the Gaussian conditioning formulas to the joint distribution we derived above, where we can now derive the posterior of the latent variable x given observation y, , where Applying the matrix inversion lemma  Here we only need to invert a matrix of size |x||x|, instead of |y||y|.   ) ( ) ( |                 y y m 1 2 1 22 12 1 2 1 T T               T T T I 22 12 12 11 ) , | ( ) ( | | 2 1 2 1 V m x y x N  p      1 1 1 1 1 1 1 - - - GE F H-GE F E E G E-FH                       1 21 1 22 12 11 2 1 T T I | V   1 1 2 1       T I | V ) ( | |      y V m 1 2 1 2 1 T                                              1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 M/E GE M/E - M/E F -E GE M/E F E E FH M/H G H H M/H G -H FH M/H - M/H - - - - 10 © Eric Xing @ CMU, 2005-2014 Geometric interpretation: inference is linear projection The posterior is: Posterior covariance does not depend on observed data y! Computing the posterior mean is just a linear operation: ) , ; ( ) ( | | 2 1 2 1 V m x y x N  p   1 1 2 1       T I | V ) ( | |      y V m 1 2 1 2 1 T 11 © Eric Xing @ CMU, 2005-2014 Learning FA Now, assume that we are given {yn} (the observation on high- dimensional data) only We have derived how to estimate xn from P(X|Y) How can we learning the model?  Loading matrix   Manifold center   Variance  12 © Eric Xing @ CMU, 2005-2014 EM for Factor Analysis Incomplete data log likelihood function (marginal density of y)  Estimating is trivial:  Parameters and are coupled nonlinearly in log-likelihood Complete log likelihood       T n n n n T n n μ y μ y N y y N D ) ( ) ( where , tr log ) ( ) ( log ) , (                            S S 1 1 2 1 2 2 1 2 T T T T    l   n n N ML y 1  ˆ     T n n n n n n T n n n n T n n n n n T n n n n n n n n x y x y N N x x N x y x y N x x N x y p x p y x p D ) ( ) ( where , tr tr log ) ( ) ( log log ) | ( log ) ( log ) , ( log ) , (                                  1 2 2 1 2 2 1 2 2 1 2 1 1 S S I  c l 13 © Eric Xing @ CMU, 2005-2014 E-step for Factor Analysis Compute  Recall that we have derived:     tr tr log ) , ( 1 2 2 1 2         S N X X N D n T n n  c l ) | ( ) , ( y x p D  c l ) ( T n T n n T n T n T T n n T n n X X y X X y y y N          1 S      T n n n n n n T n n y X y X y X X X | | | E E Var     n n n y X X | E    1 1 2 1       T I | V ) ( | |      y V m 1 2 1 2 1 T ) ( | |        n y x n y X n n 1 2 1 T V m  T y x y x T n n n n n n X X | | | and m m V   2 1 14 © Eric Xing @ CMU, 2005-2014 M-step for Factor Analysis Take the derivates of the expected complete log likelihood wrt. parameters.  Using the trace and determinant derivative rules:     S S 2 2 2 2 1 2 1 1 1 N N N X X N n T n n                          tr tr log c l                                                              n T n n n T n n T n T n n T n T n T T n n T n n n T n n X X X y X X y X X y y y N N N N X X N 1 1 1 1 1 1 2 2 2 2 1 2 ) ( tr tr log S S c l  S  1 t  1 1                   n T n n n T n n t X X X y 15 © Eric Xing @ CMU, 2005-2014 Model Invariance and Identifiability There is degeneracy in the FA model. Since only appears as outer product , the model is invariant to rotation and axis flips of the latent space. We can replace with Q for any orthonormal matrix Q and the model remains the same: (Q)(Q)=(QQ)=. This means that there is no “one best” setting of the parameters. An infinite number of parameters all give the ML score! Such models are called un-identifiable since two people both fitting ML parameters to the identical data will not be guaranteed to identify the same parameters. 16 © Eric Xing @ CMU, 2005-2014 A road map to more complex dynamic models A X Y A X Y A X Y discrete discrete discrete continuous continuous continuous Mixture model e.g., mixture of multinomials Mixture model e.g., mixture of Gaussians Factor analysis A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... A A A A x2 x3 x1 xN y2 y3 y1 yN ... ... HMM (for discrete sequential data, e.g., text) HMM (for continuous sequential data, e.g., speech signal) State space model ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... Factorial HMM Switching SSM 17 © Eric Xing @ CMU, 2005-2014 State space models (SSM) A sequential FA or a continuous state HMM In general, A A A A Y2 Y3 Y1 YN X2 X3 X1 XN ... ... ), ; ( ~ ) ; ( ~ ), ; ( ~ 0 R Q C G A        0 0 0 0 1 1 N N N x x y x x t t t t t t t t v w v w This is a linear dynamic system. t t t t t t v w       ) ( ) ( 1 1 x y x x g G f where f is an (arbitrary) dynamic model, and g is an (arbitrary) observation model 18 © Eric Xing @ CMU, 2005-2014 LDS for 2D tracking Dynamics: new position = old position + velocity + noise (constant velocity model, Gaussian noise) Observation: project out first two components (we observe Cartesian position of object - linear!) noise 1 0 0 0 0 1 0 0 0 1 0 0 0 1 2 1 1 1 2 1 1 1 2 1 2 1                                                   t t t t t t t t x x x x x x x x     noise                                 2 1 2 1 2 1 0 0 1 0 0 0 0 1 t t t t t t x x x x y y   19 © Eric Xing @ CMU, 2005-2014 The inference problem 1 j t t j t t t i t t t j i p i p i p 1 1 1           ) X | X ( ) X | y ( ) | X ( : y A Y2 Y3 Y1 Yt X2 X3 X1 Xt ... ... ) | ( : 1t t x P y 0 α 1 α 2 α t α Filtering given y1, …, yt, estimate xt:  The Kalman filter is a way to perform exact online inference (sequential Bayesian updating) in an LDS.  It is the Gaussian analog of the forward algorithm for HMMs: 20 © Eric Xing @ CMU, 2005-2014 The inference problem 2 Smoothing given y1, …, yT, estimate xt (t<T)  The Rauch-Tung-Strievel smoother is a way to perform exact off-line inference in an LDS. It is the Gaussian analog of the forwards-backwards (alpha-gamma) algorithm:       j j t j i j t i t i t T t X X P y i p 1 1 : 1 ) | ( ) | X (    A Y2 Y3 Y1 Yt X2 X3 X1 Xt ... ... 0 α 1 α 2 α t α 0 γ 1 γ 2 γ t γ 21 © Eric Xing @ CMU, 2005-2014 2D tracking X1 X1 22 © Eric Xing @ CMU, 2005-2014 filtered Kalman filtering in the brain? 23 © Eric Xing @ CMU, 2005-2014 Kalman filtering derivation Since all CPDs are linear Gaussian, the system defines a large multivariate Gaussian.  Hence all marginals are Gaussian.  Hence we can represent the belief state p(Xt|y1:t) as a Gaussian with mean and covariance .  It is common to work with the inverse covariance (precision) matrix ; this is called information form. Kalman filtering is a recursive procedure to update the belief state:  Predict step: compute p(Xt+1|y1:t) from prior belief p(Xt|y1:t) and dynamical model p(Xt+1|Xt) --- time update  Update step: compute new belief p(Xt+1|y1:t+1) from prediction p(Xt+1|y1:t), observation yt+1 and observation model p(yt+1|Xt+1) --- measurement update A A Yt Y1 Xt Xt+1 X1 ... A A A Yt Yt+1 Y1 Xt Xt+1 X1 ... 24 © Eric Xing @ CMU, 2005-2014 Kalman filtering derivation Kalman filtering is a recursive procedure to update the belief state:  Predict step: compute p(Xt+1|y1:t) from prior belief p(Xt|y1:t) and dynamical model p(Xt+1|Xt) --- time update  Update step: compute new belief p(Xt+1|y1:t+1) from prediction p(Xt+1|y1:t), observation yt+1 and observation model p(yt+1|Xt+1) --- measurement update A A Yt Y1 Xt Xt+1 X1 ... A A A Yt Yt+1 Y1 Xt Xt+1 X1 ... 25 © Eric Xing @ CMU, 2005-2014 Predict step Dynamical Model:  One step ahead prediction of state: Observation model:  One step ahead prediction of observation: ) ; ( ~ , Q G A 0 1 N t t t t w w    x x ) ; 0 ( ~ , R v v C t t t t N   x y A A Yt Y1 Xt Xt+1 X1 ... A A A Yt Yt+1 Y1 Xt Xt+1 X1 ... 26 © Eric Xing @ CMU, 2005-2014 Predict step Dynamical Model:  One step ahead prediction of state: Observation model:  One step ahead prediction of observation: ) ; ( ~ , Q G A 0 1 N t t t t w w    x x ) ; 0 ( ~ , R v v C t t t t N   x y t t t t t t | | ˆ ) , , | X ( ˆ x y y x A E      1 1 1 T t t t T t t t t t t t t t T t t t t t t t t GQG A AP w G A w G A E E P                  | 1 | 1 | 1 1 | 1 1 | 1 1 | 1 ) , , | ) ˆ X )( ˆ X ( ) , , | ) ˆ )(X ˆ X ( y y x x y y x x   t t t t t t t v | ˆ ) , , | X ( ) , , | Y ( 1 1 1 1 1 1        x y y y y C C E E   R ) , , | ) ˆ )(Y ˆ Y ( | 1 1 | 1 1 | 1 1          T t t t T t t t t t t C CP E y y y y  t t t T t t t t t t CP E | 1 1 | 1 1 | 1 1 ) , , | ) ˆ )(X ˆ Y (         y y x y  A A Yt Y1 Xt Xt+1 X1 ... A A A Yt Yt+1 Y1 Xt Xt+1 X1 ... 27 © Eric Xing @ CMU, 2005-2014 Update step Summarizing results from previous slide, we have p(Xt+1,Yt+1|y1:t) ~ N(mt+1, Vt+1), where Remember the formulas for conditional Gaussian distributions: ) , ( ) , | (                               22 21 12 11 2 1 2 1 2 1    x x x x N p 21 1 22 12 11 2 1 2 2 1 22 12 1 2 1 2 1 2 1 1 2 1               | | | | ) ( ) , | ( ) ( V x m V m x x x   N p 22 2 2 2 2 2 2 2     m m m m p V m V m x x  ) , | ( ) ( N , ˆ ˆ | |             t t t t t x C x m 1 1 1 , | | | |                R C CP CP C P P V T t t t t T t t t t t 1 1 1 1 1 28 © Eric Xing @ CMU, 2005-2014 Kalman Filter Measurement updates:  where Kt+1 is the Kalman gain matrix Time updates: Kt can be pre-computed (since it is independent of the data). t t t t | | ˆ ˆ x x A  1 T GQG A AP P    t t t t | | 1 ) x ˆ C (y ˆ ˆ t | 1 t 1 t | |          1 1 1 1 t t t t t K x x t t t t t t | | | 1 1 1 1       KCP P P -1 T T R C CP C P K ) ( | |      t t t t t 1 1 1 29 © Eric Xing @ CMU, 2005-2014 Example of KF in 1D Consider noisy observations of a 1D particle doing a random walk: KF equations: ) , ( ~ , | x t t t w w x x  0 1 1 N     ) , ( ~ , z t t v v x z  0 N   t|t t|t t t x x x ˆ ˆ ˆ |    A 1 , | | x t t t t t       T GQG A AP P 1   z x t t t z t x t t t t t t x z x z x x                    | t | 1 t 1 t | | ˆ ) ˆ - ( ˆ ˆ C K 1 1 1 1   z x t z x t t t t t t t                | | | 1 1 1 1 KCP - P P ) )( ( ) ( | | z x t x t t t t t t               -1 T T R C CP C P K 1 1 1 30 © Eric Xing @ CMU, 2005-2014 KF intuition The KF update of the mean is  the term is called the innovation New belief is convex combination of updates from prior and observation, weighted by Kalman Gain matrix: If the observation is unreliable, z (i.e., R) is large so Kt+1 is small, so we pay more attention to the prediction. If the old prior is unreliable (large t) or the process is very unpredictable (large x), we pay more attention to the observation.   z x t t t z t x t t t t t t x z x z x x                    | t | 1 t 1 t | | ˆ ) ˆ - ( ˆ ˆ C K 1 1 1 1 ) ˆ ( t | 1 t 1 t   x z C -1 T T R C CP C P K ) ( | |      t t t t t 1 1 1 31 © Eric Xing @ CMU, 2005-2014 Complexity of one KF step Let and , Computing takes O(Nx 2) time, assuming dense P and dense A. Computing takes O(Ny 3) time. So overall time is, in general, max {Nx 2,Ny 3} x N t X R  y N t Y R  T GQG A AP P    t t t t | | 1 -1 T T R C CP C P K ) ( | |      t t t t t 1 1 1 32 © Eric Xing @ CMU, 2005-2014 The inference problem 2 Smoothing given y1, …, yT, estimate xt (t<T)  The Rauch-Tung-Strievel smoother is a way to perform exact off-line inference in an LDS. It is the Gaussian analog of the forwards-backwards (alpha-gamma) algorithm:       j j t j i j t i t i t T t X X P y i p 1 1 : 1 ) | ( ) | X (    A Y2 Y3 Y1 Yt X2 X3 X1 Xt ... ... 0 α 1 α 2 α t α 0 γ 1 γ 2 γ t γ 33 © Eric Xing @ CMU, 2005-2014 Rauch-Tung-Strievel smoother  General structure: KF results + the difference of the ""smoothed"" and predicted results of the next step  Backward computation: Pretend to know things at t+1 –- such conditioning makes things simple and we can remove this condition finally The difficulty: The trick: A A Yt Yt+1 Xt Xt+1 ) ˆ - ˆ ( ˆ ˆ | | | | t t T t t t t T t 1 1     x x x x L   T t t T t t t T t t t L P P L P P | | | | 1 1      1 1    t t t t t | | P A P L T             t t t T t t t T T t t T t T t y y X X y y y y X X y y y y X X y y X x , , , | , , | , , , | , , | , , , | , , | ˆ def |       1 1 1 1 1 1 1 1 1        E E E E E E       Z Z Y X Z X | , | | E E E  T t y y X , , |  1           Z Z Y X Z Z Y X Z X | , | | , | | Var E E Var Var   (Hw!) Same for Pt|T 34 © Eric Xing @ CMU, 2005-2014 RTS derivation Following the results from previous slide, we need to derive p(Xt+1,Xt|y1:t) ~ N(m, V), where  all the quantities here are available after a forward KF pass Remember the formulas for conditional Gaussian distributions: The RTS smoother , ) , ( ) , | (                               22 21 12 11 2 1 2 1 2 1    x x x x N p 21 1 22 12 11 2 1 2 2 1 22 12 1 2 1 2 1 2 1 1 2 1               | | | | ) ( ) , | ( ) ( V x m V m x x x   N p 22 2 2 2 2 2 2 2     m m m m p V m V m x x  ) , | ( ) ( N , ˆ ˆ | |          t t t t x x m 1 , | | | |          t t t t T t t t t P AP A P P V 1   ) ˆ - ˆ ( ˆ , , , | ˆ | | | | t t T t t t t t t t T t y y X X x 1 1 1 1       x x x L E          T t t T t t t T t t t T T t T t y y X X y x t t L P P L P Var E Var P | | | : : : | def | | , | | ˆ 1 1 1 1 1 1         35 © Eric Xing @ CMU, 2005-2014 Learning SSMs Complete log likelihood EM  E-step: compute these quantities can be inferred via KF and RTS filters, etc., e,g.,  M-step: MLE using c.f., M-step in factor analysis         R C G Q A , ; : , , , ; : , , ) ; ( ) | ( log ) | ( log ) ( log ) , ( log ) , ( , , , , t X X X f t X X X X X f X f x y p x x p x p y x p D t T t t t T t t T t t n t t n t n n t t n t n n n n n                 3 1 2 0 1 1 1 1  c l T t T t t T t t y y X X X X X  , , , 1 1  2 2 T t T t t T t t T t t x P X X X X X | | ˆ ) ( E ) var(               R C G Q A , ; : , , , ; : , , ; ) , ( t X X X f t X X X X X f X f D t T t t t T t t T t t        3 1 2 0 1 1  c l 36 © Eric Xing @ CMU, 2005-2014 Nonlinear systems  In robotics and other problems, the motion model and the observation model are often nonlinear:  An optimal closed form solution to the filtering problem is no longer possible.  The nonlinear functions f and g are sometimes represented by neural networks (multi-layer perceptrons or radial basis function networks).  The parameters of f and g may be learned offline using EM, where we do gradient descent (back propagation) in the M step, c.f. learning a MRF/CRF with hidden nodes.  Or we may learn the parameters online by adding them to the state space: xt'= (xt, ). This makes the problem even more nonlinear. ) ( , ) ( t t t t t t v x g y w x f x     1 37 © Eric Xing @ CMU, 2005-2014 Extended Kalman Filter (EKF) The basic idea of the EKF is to linearize f and g using a second order Taylor expansion, and then apply the standard KF.  i.e., we approximate a stationary nonlinear system with a non-stationary linear system. where and and The noise covariance (Q and R) is not changed, i.e., the additional error due to linearization is not modeled. ) ˆ ( ) ˆ ( ) ˆ ( ) ˆ ( | ˆ | | ˆ | | | t t t t x t t t t t t t x t t t v x x x g y w x x x f x t t t t                   1 1 1 1 1 1 1 1 1 1 C A ) ˆ ( ˆ | | 1 1 1    t t t t x f x x x x f ˆ def ˆ    A x x x g ˆ def ˆ    C 38 © Eric Xing @ CMU, 2005-2014 Online vs offline inference 39 © Eric Xing @ CMU, 2005-2014 The KF update of the mean is Consider the special case where the hidden state is a constant, xt =, but the “observation matrix” C is a time- varying vector, C = xt T.  Hence the observation model at each time slide, , is a linear regression We can estimate recursively using the Kalman filter: This is called the recursive least squares (RLS) algorithm. We can approximate by a scalar constant. This is called the least mean squares (LMS) algorithm. We can adapt t online using stochastic approximation theory. ) ˆ - ( ˆ ˆ t | 1 t 1 t | |        x y x x t t t t t C K A 1 1 1 t T t t v x y    t t T t t t t x x y ) ˆ ( ˆ ˆ 1 t           1 1 1 R P 1 1 1     t t  R P KF, RLS and LMS 40 © Eric Xing @ CMU, 2005-2014 "
12,"School of Computer Science Probabilistic Graphical Models Conditional Random Fields & Case study I: image segmentation Eric Xing Lecture 11, February 21, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 Hidden Markov Model revisit  Transition probabilities between any two states or  Start probabilities  Emission probabilities associated with each state or in general: A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... , ) | ( , j i i t j t a y y p     1 1 1   . , , , , l Multinomia ~ ) | ( , , , I     i a a a y y p M i i i i t t  2 1 1 1  . , , , l Multinomia ~ ) ( M y p     2 1 1   . , , , , l Multinomia ~ ) | ( , , , I    i b b b y x p K i i i i t t  2 1 1   . , | f ~ ) | ( I     i y x p i i t t  1 2 © Eric Xing @ CMU, 2005-2014 Inference (review) ) | ,..., ( ) ( def 1 1 1       k t T t t t k t y x x P k    Forward algorithm  Backward algorithm ) , , ,..., ( ) ( def 1 1 1 1       k t t t t t k t y x x x P k   ) | ( ) | ( ) 1 ( ) 1 ( ) , 1 , 1 ( 1 1 1 1 1 1 : 1 1 def , t t t t j t t t i t t t T j t i t j i t y y p y x p y y x y y p                       i k i i t k t t k t a y x p , ) | ( 1 1   i t i t t i i k k t y x p a 1 1 1 , ) 1 | (        ) | ( , , 1 1 1 1      i t t j i j t i t j i t y x p a         j j i t i t i t T i t i t x y p , : 1 def ) | 1 (     ) | ( ) , ( ) | ( ) ( def def 1 1 1 1       i t j t i t t t y y p j i y x p i A B         t t t t t t t t t t t t t                         . . . . . A B B A B A T T 1 1 1 1 1 The matrix-vector form: 3 © Eric Xing @ CMU, 2005-2014 Learning HMM  Supervised learning: estimation when the “right answer” is known  Examples: GIVEN: a genomic region x = x1…x1,000,000 where we have good (experimental) annotations of the CpG islands GIVEN: the casino player allows us to observe him one evening, as he changes dice and produces 10,000 rolls  Unsupervised learning: estimation when the “right answer” is unknown  Examples: GIVEN: the porcupine genome; we don’t know how frequent are the CpG islands there, neither do we know their composition GIVEN: 10,000 rolls of the casino player, but we don’t see when he changes dice  QUESTION: Update the parameters of the model to maximize P(x|) - -- Maximal likelihood (ML) estimation 4 © Eric Xing @ CMU, 2005-2014 Learning HMM: two scenarios Supervised learning: if only we knew the true state path then ML parameter estimation would be trivial  E.g., recall that for complete observed tabular BN:  What if y is continuous? We can treat as NT observations of, e.g., a GLIM, and apply learning rules for GLIM … Unsupervised learning: when the true state path is unknown, we can fill in the missing values using inference recursions.  The Baum Welch algorithm (i.e., EM)  Guaranteed to increase the log likelihood of the model after each iteration  Converges to local optimum, depending on initial conditions   k j i k ij ijk ML ijk n n , ' , '             n T t i t n j t n n T t i t n ML ij y y y i j i a 2 1 2 1 , , , ) ( # ) ( #          n T t i t n k t n n T t i t n ML ik y x y i k i b 1 1 , , , ) ( # ) ( #     N n T t y x t n t n : , : : , , , 1 1   5 © Eric Xing @ CMU, 2005-2014 The Baum Welch algorithm The complete log likelihood The expected complete log likelihood EM  The E step  The M step (""symbolically"" identical to MLE)                 n T t t n t n T t t n t n n c x x p y y p y p p 1 2 1 1 ) | ( ) | ( ) ( log ) , ( log ) , ; ( , , , , , y x y x θ l                             n T t k i y p i t n k t n n T t j i y y p j t n i t n n i y p i n c b y x a y y y n t n n t n t n n n 1 2 1 1 1 1 , ) | ( , , , ) | , ( , , ) | ( , log log log ) , ; ( , , , , x x x y x θ  l ) | ( , , , n i t n i t n i t n y p y x 1     ) | , ( , , , , , , n j t n i t n j t n i t n j i t n y y p y y x 1 1 1 1              n T t i t n n T t j i t n ML ij a 1 1 2 , , ,         n T t i t n k t n n T t i t n ML ik x b 1 1 1 , , ,   N n i n ML i   1 ,   6 © Eric Xing @ CMU, 2005-2014 Shortcomings of Hidden Markov Model (1): locality of features  HMM models capture dependences between each state and only its corresponding observation  NLP example: In a sentence segmentation task, each segmental state may depend not just on a single word (and the adjacent segmental stages), but also on the (non-local) features of the whole line such as line length, indentation, amount of white space, etc.  Mismatch between learning objective function and prediction objective function  HMM learns a joint distribution of states and observations P(Y, X), but in a prediction task, we need the conditional probability P(Y|X) Y1 Y2 … … … Yn X1 X2 … … … Xn 7 © Eric Xing @ CMU, 2005-2014 Solution: Maximum Entropy Markov Model (MEMM) Models dependence between each state and the full observation sequence explicitly  More expressive than HMMs Discriminative model  Completely ignores modeling P(X): saves modeling effort  Learning objective function consistent with predictive function: P(Y|X) Y1 Y2 … … … Yn X1:n 8 © Eric Xing @ CMU, 2005-2014 Then, shortcomings of MEMM (and HMM) (2): the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 What the local transition probabilities say: • State 1 almost always prefers to go to state 2 • State 2 almost always prefer to stay in state 2 9 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Probability of path 1-> 1-> 1-> 1: • 0.4 x 0.45 x 0.5 = 0.09 10 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Probability of path 2->2->2->2 : • 0.2 X 0.3 X 0.3 = 0.018 Other paths: 1-> 1-> 1-> 1: 0.09 11 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Probability of path 1->2->1->2: • 0.6 X 0.2 X 0.5 = 0.06 Other paths: 1->1->1->1: 0.09 2->2->2->2: 0.018 12 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Probability of path 1->1->2->2: • 0.4 X 0.55 X 0.3 = 0.066 Other paths: 1->1->1->1: 0.09 2->2->2->2: 0.018 1->2->1->2: 0.06 13 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Most Likely Path: 1-> 1-> 1-> 1 • Although locally it seems state 1 wants to go to state 2 and state 2 wants to remain in state 2. • why? 14 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Most Likely Path: 1-> 1-> 1-> 1 • State 1 has only two transitions but state 2 has 5: • Average transition probability from state 2 is lower 15 © Eric Xing @ CMU, 2005-2014 MEMM: the Label bias problem State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 Label bias problem in MEMM: • Preference of states with lower number of transitions over others 16 © Eric Xing @ CMU, 2005-2014 Solution: Do not normalize probabilities locally State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 From local probabilities …. 17 © Eric Xing @ CMU, 2005-2014 Solution: Do not normalize probabilities locally State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 20 30 10 20 10 20 20 30 20 20 30 10 10 30 5 5 10 30 20 20 20 From local probabilities to local potentials • States with lower transitions do not have an unfair advantage! 18 © Eric Xing @ CMU, 2005-2014 From MEMM …. Y1 Y2 … … … Yn X1:n 19 © Eric Xing @ CMU, 2005-2014 CRF is a partially directed model  Discriminative model like MEMM  Usage of global normalizer Z(x) overcomes the label bias problem of MEMM  Models the dependence between each state and the entire observation sequence (like MEMM) From MEMM to CRF Y1 Y2 … … … Yn x1:n 20 © Eric Xing @ CMU, 2005-2014 Conditional Random Fields General parametric form: Y1 Y2 … … … Yn x1:n 21 © Eric Xing @ CMU, 2005-2014 CRFs: Inference  Given CRF parameters and , find the y* that maximizes P(y|x)  Can ignore Z(x) because it is not a function of y  Run the max-product algorithm on the junction-tree of CRF: Y1 Y2 … … … Yn x1:n Y1,Y2 Y2,Y3 ……. Yn-2,Yn-1 Yn-1,Yn Y2 Y3 Yn-2 Yn-1 Same as Viterbi decoding used in HMMs! 22 © Eric Xing @ CMU, 2005-2014 CRF learning Given {(xd, yd)}d=1 N, find *, * such that Computing the gradient w.r.t  Gradient of the log-partition function in an exponential family is the expectation of the sufficient statistics. 23 © Eric Xing @ CMU, 2005-2014 CRF learning  Computing the model expectations:  Requires exponentially large number of summations: Is it intractable?  Tractable!  Can compute marginals using the sum-product algorithm on the chain Expectation of f over the corresponding marginal probability of neighboring nodes!! 24 © Eric Xing @ CMU, 2005-2014 CRF learning Computing marginals using junction-tree calibration: Junction Tree Initialization: After calibration: Y1 Y2 … … … Yn x1:n Y1,Y2 Y2,Y3 ……. Yn-2,Yn-1 Yn-1,Yn Y2 Y3 Yn-2 Yn-1 Also called forward-backward algorithm 25 © Eric Xing @ CMU, 2005-2014 CRF learning Computing feature expectations using calibrated potentials: Now we know how to compute rL(,): Learning can now be done using gradient ascent: 26 © Eric Xing @ CMU, 2005-2014 CRF learning In practice, we use a Gaussian Regularizer for the parameter vector to improve generalizability In practice, gradient ascent has very slow convergence  Alternatives:  Conjugate Gradient method  Limited Memory Quasi-Newton Methods 27 © Eric Xing @ CMU, 2005-2014 CRFs: some empirical results Comparison of error rates on synthetic data CRF error HMM error HMM error MEMM error MEMM error CRF error Data is increasingly higher order in the direction of arrow CRFs achieve the lowest error rate for higher order data 28 © Eric Xing @ CMU, 2005-2014 CRFs: some empirical results Parts of Speech tagging  Using same set of features: HMM >=< CRF > MEMM  Using additional overlapping features: CRF+ > MEMM+ >> HMM 29 © Eric Xing @ CMU, 2005-2014 Other CRFs  So far we have discussed only 1- dimensional chain CRFs  Inference and learning: exact  We could also have CRFs for arbitrary graph structure  E.g: Grid CRFs  Inference and learning no longer tractable  Approximate techniques used  MCMC Sampling  Variational Inference  Loopy Belief Propagation  We will discuss these techniques soon 30 © Eric Xing @ CMU, 2005-2014 Image Segmentation  Image segmentation (FG/BG) by modeling of interactions btw RVs  Images are noisy.  Objects occupy continuous regions in an image. Input image Pixel-wise separate optimal labeling Locally-consistent joint optimal labeling [Nowozin,Lampert 2012] Y* argmax y{0,1}n Vi(yi, X) Vi, j(yi, yj) jNi  iS  iS          . Y: labels X: data (features) S: pixels Ni: neighbors of pixel i Unary Term Pairwise Term 31 © Eric Xing @ CMU, 2005-2014 Undirected Graphical Models (with an Image Labeling Example)  Image can be represented by 4-connected 2D grid.  MRF / CRF with image labeling problem  X={xi}iS: observed data of an image.  xi: data at i-th site (pixel or block) of the image set S  Y={yi}iS: (hidden) labels at i-th site. yi {1,…, L}.  Object: maximize the conditional probability Y*=argmaxY P(Y|X) xi si xj sj yi= 0 (BG) si yi= 1 (FG) sj Y* 32 © Eric Xing @ CMU, 2005-2014 MRF (Markov Random Field)  Definition: Y={yi}iS is called Markov Random Field on the set S, with respect to neighborhood system N, iff for all i S,  The posterior probability is  (1) Very strict independence assumptions for tractability: Label of each site is a function of data only at that site.  (2) P(Y) is modeled as a MRF P(yi|yS-{i}) = P(yi|yNi). yj yi xi P(Y | X) P(X,Y) P(X) P(X |Y)P(Y)  P(xi | yi) iS  P(Y) (1) (2) P(Y) 1 Z c(yc) cC  33 © Eric Xing @ CMU, 2005-2014 CRF  Definition: Let G = (S, E), then (X, Y) is said to be a Conditional Random Field (CRF) if, when conditioned on X, the random variables yi obey the Markov property with respect to the graph  Globally conditioned on the observation X yj yi xi P(yi|yS-{i}) = P(yi|yNi) MRF: P(yi|X,yS-{i}) = P(yi|X,yNi) CRF yj yi xi MRF 34 © Eric Xing @ CMU, 2005-2014 CRF vs MRF  MRF: two-step generative model  Infer likelihood P(X|Y) and prior P(Y)  Use Bayes theorem to determine posterior P(Y|X)  CRF: one-step discriminative model  Directly Infer posterior P(Y|X)  Popular Formulation P(Y | X) P(X,Y) P(X) P(X |Y)P(Y)  P(xi | yi) iS  1 Z c(yc) cC  P(Y | X) 1 Z exp( log p(xi | yi) V2(yi, yi') i'Ni  iS  iS  ) MRF P(Y | X) 1 Z exp( V 1(yi | X) V2(yi, yi' | X) i'Ni  iS  iS  ) CRF Potts model for P(Y) with only pairwise potential Only up to pairwise clique potentials Assumption 35 © Eric Xing @ CMU, 2005-2014 Example of CRF – DRF  A special type of CRF  The unary and pairwise potentials are designed using local discriminative classifiers.  Posterior  Association Potential  Local discriminative model for site i: using logistic link with GLM.  Interaction Potential  Measure of how likely site i and j have the same label given X Ai(yi, X) logP(yi | fi(X)) P(Y | X) 1 Z exp( Ai(yi, X) Iij(yi, yj, X) jNi  iS  iS  ) S. Kumar and M. Hebert. Discriminative Random Fields. IJCV, 2006. Association Interaction P(yi 1| fi(X))  1 1exp((wT fi(X))) (wT fi(X)) Iij(yi, yj, X) kyiyj (1k)(2(yiyjij(X))1)) (1) Data-independent smoothing term (2) Data-dependent pairwise logistic function 36 © Eric Xing @ CMU, 2005-2014 Example of CRF – DRF Results  Task: Detecting man-made structure in natural scenes.  Each image is divided in non-overlapping 16x16 tile blocks.  An example  Logistic: No smoothness in the labels  MRF: Smoothed False positive. Lack of neighborhood interaction of the data S. Kumar and M. Hebert. Discriminative Random Fields. IJCV, 2006. Input image Logistic MRF DRF 37 © Eric Xing @ CMU, 2005-2014 Example of CRF –Body Pose Estimation  Task: Estimate a body pose.  Need to detect parts of human body  Appearance + Geometric configuration.  A large number of DOFs  Use CRF to model a human body  Nodes: Parts (head, torso, upper/ lower left/right arms). L=(l1,…, l6), li = [xi, yi, θi].  Edges: Pairwise linkage between parts  Tree vs. Graph V. Ferrari et al. Progressive search space reduction for human pose estimation. CVPR 2008. D. Ramanan. Learning to Parse Images of Articulated Bodies."" NIPS 2006. [Zisserman 2010] 38 © Eric Xing @ CMU, 2005-2014 Example of CRF –Body Pose Estimation  Posterior of configuration  ψ(li,lj): relative position with geometric constraints  ϕ(li): local image evidence for a part in a particular location  If E is a tree, exact inference is efficiently performed by BP.  Example of unary and pairwise terms  Unary term: appearance feature P(L | I)exp( (li) (li,lj (i, j)E  i  )) HOG of lower arm template (learned) HOG of image L2 Distance  Pairwise term: kinematic layout li lj Truncated quadratic [Zisserman 2010] 39 © Eric Xing @ CMU, 2005-2014 Example of CRF – Results of Body Pose Estimation  Examples of results  Datasets and codes are available.  http://www.ics.uci.edu/~dramanan/papers/parse/  http://www.robots.ox.ac.uk/~vgg/research/pose_estimation/ [Ferrari et al. 2008] [Ramanan 2006] 40 © Eric Xing @ CMU, 2005-2014 Summary  Conditional Random Fields are partially directed discriminative models  They overcome the label bias problem of MEMMs by using a global normalizer  Inference for 1-D chain CRFs is exact  Same as Max-product or Viterbi decoding  Learning also is exact  globally optimum parameters can be learned  Requires using sum-product or forward-backward algorithm  CRFs involving arbitrary graph structure are intractable in general  E.g.: Grid CRFs  Inference and learning require approximation techniques  MCMC sampling  Variational methods  Loopy BP 41 © Eric Xing @ CMU, 2005-2014 "
124,"CIS 700Advanced Machine Learning Structured Machine Learning:  Theory and Applications in Natural Language ProcessingDan Roth Department of Computer and Information Science University of Pennsylvania Page 1 What’s the class about  Motivation  How I plan to teach it  RequirementsA testLinda read text with 2000 words  How many words had 7 characters and ended with an “ing”? Write down a number A = …..   How many words had 7 characters and had an “n” in the 6th position? Write down a number B = ……  Check if A > B  (Yes/No)  This has nothing to do with the class Related to Tversky & Kahneman Theory of Representatives and Small Sample decision making. Can we use it as a basis for better NLP?     Before IntroductionWho are you?  ML Background NLP Background AI Background Programming BackgroundA Story Page 4(ENGLAND, June, 1989) - Christopher Robin is alive and well.  He lives in England.  He is the same person that you read about in the book, Winnie the Pooh. As a boy, Chris lived in a pretty home called Cotchfield Farm.  When Chris was three years old, his father wrote a poem about him.  The poem was printed in a magazine for others to read.  Mr. Robin then wrote a book.  He made up a fairy tale land where Chris lived.  His friends were animals.  There was a bear called Winnie the Pooh.  There was also an owl and a young pig, called a piglet.  All the animals were stuffed toys that Chris owned.  Mr. Robin made them come to life with his words.  The places in the story were all near Cotchfield Farm. Winnie the Pooh was written in 1925.  Children still love to read about Christopher Robin and his animal friends.  Most people don't know he is a real person who is grown now.  He has written two books of his own.  They tell what it is like to be famous.1. Christopher Robin was born in England.      2.  Winnie the Pooh is a title of a book.   3. Christopher Robin’s dad was a magician.     4. Christopher Robin must be at least 65 now. This is an Inference ProblemTextual EntailmentPage 5Eyeing the huge market potential, currently led by Google, Yahoo took over search company  Overture Services Inc. last yearYahoo acquired OvertureIs it true that…? (Textual Entailment)Overture is a search company Google is a search  company ……….Google owns OvertureA key problem in natural language understanding is to abstract  over the inherent syntactic and semantic variability in natural language.Why is it Difficult?Page 6MeaningLanguageMidas: I hope that everything I touch becomes gold. One cannot simply map natural language to a representation that gives rise to reasoning Ambiguity 7It’s a version of Chicago – the standard classic Macintosh menu font, with that distinctive thick diagonal in the ”N”.Chicago was used by default for Mac menus through MacOS 7.6, and OS 8 was released mid-1997..Chicago VIII was one of the early 70s-era Chicago albums to catch my ear, along with Chicago II.Variability in Natural Language ExpressionsDetermine if Jim Carpenter works for the government  Jim Carpenter works for the U.S. Government. The American government employed Jim Carpenter. Jim Carpenter was fired by the US Government. Jim Carpenter worked in a number of important positions.  ….  As a press liaison for the IRS, he made contacts in the white house.  Russian interior minister Yevgeny Topolov met yesterday with his US counterpart, Jim Carpenter. Former US Secretary of Defense Jim Carpenter spoke today…Page 8Machine Learning is needed to support abstraction over the raw text, and deal with:   Identifying/Understanding Relations, Entities and Semantic Classes  Acquiring knowledge from external resources; representing knowledge  Identifying, disambiguating  & tracking  entities, events, etc.   Time, quantities, processes…Conventional programming techniques cannot deal with the variability of expressing meaning nor with the  ambiguity of interpretationLearningThe process of Abstraction has to be driven by statistical learning methods.  Over the last two decades or so it became clear that machine learning methods are necessary in order to support this process of abstraction.  But—  Page 9Classification: Ambiguity Resolution10Illinois’ bored of education                              [board] Nissan Car and truck plant;             plant and animal kingdom (This  Art) (can N) (will MD) (rust V)               V,N,N  The dog bit  the kid. He was taken to a veterinarian;   a hospital                                       Tiger was in Washington for the PGA Tour                                                                    Finance; Banking; World News; Sports  Important or not important; love or hate ClassificationThe goal is to learn a function f: X Y that maps observations in a domain to one of several categories.  Task: Decide which of {board ,bored } is more  likely in  the  given  context:  X: some representation of:                              The Illinois’ _______ of education met yesterday…  Y: {board ,bored }  Typical learning protocol:  Observe a collection of labeled examples (x,y) 2 X £ Y Use it to learn a function f:XY that is consistent with the observed examples, and (hopefully) performs well on new, previously unobserved examples.11Classification is Well UnderstoodTheoretically: generalization bounds (at least for linear models) How many example does one need to see in order to guarantee good behavior on previously unobserved examples. Algorithmically: good learning algorithms for linear representations. Can deal with very high dimensionality (106 features)  Very efficient in terms of computation and # of examples. On-line. Key issues remaining: Learning protocols: how to minimize interaction (supervision); how to map domain/task information to supervision; semi-supervised learning; active learning; ranking. What are the features? No good theoretical understanding here. Is it sufficient for making progress in NLP?12A Story Page 13(ENGLAND, June, 1989) - Christopher Robin is alive and well.  He lives in England.  He is the same person that you read about in the book, Winnie the Pooh. As a boy, Chris lived in a pretty home called Cotchfield Farm.  When Chris was three years old, his father wrote a poem about him.  The poem was printed in a magazine for others to read.  Mr. Robin then wrote a book.  He made up a fairy tale land where Chris lived.  His friends were animals.  There was a bear called Winnie the Pooh.  There was also an owl and a young pig, called a piglet.  All the animals were stuffed toys that Chris owned.  Mr. Robin made them come to life with his words.  The places in the story were all near Cotchfield Farm. Winnie the Pooh was written in 1925.  Children still love to read about Christopher Robin and his animal friends.  Most people don't know he is a real person who is grown now.  He has written two books of his own.  They tell what it is like to be famous.1. Christopher Robin was born in England.      2.  Winnie the Pooh is a title of a book.   3. Christopher Robin’s dad was a magician.     4. Christopher Robin must be at least 65 now. This is an Inference ProblemJoint Inference with General Constraint StructureEntities and Relations Models could be learned separately/jointly; constraints may come up only at decision time. Key Questions:  How to learn the model(s)?  What is the source of the knowledge? How to guide the global inference? Joint inference gives good improvement Not all learning is from examples; communication-driven learning is essential. Expectation is a knowledge intensive componentNatural Language UnderstandingNatural language understanding decisions are global decisions that require  Making (local) predictions driven by different models trained in different ways, at different times/conditions/scenarios The ability to put these predictions together coherently Knowledge, that guides the decisions so they satisfy our expectations     Multiple forms of Inference; many boil down to determining best assignment to a collection of variables of interest But not all…         Page 15Natural Language Interpretation is an Inference Process that is best thought of as a knowledge constrained optimization problem, done on top of multiple statistically learned models. Semantic Parsing       Successful interpretation involves multiple decisions What entities appear in the interpretation? “New York” refers to a state or a city?  How to compose fragments together?  state(next_to()) >< next_to(state()) Page 16Y: largest( state( next_to( state(NY) AND next_to (state(MD))))Coherency in Semantic Role LabelingPage 17Predicate-arguments generated should be consistent across phenomenaThe touchdown scored by Brady cemented  the victory of the Patriots.Linguistic Constraints:  A0: the Patriots  Sense(of): 11(6) A0: Brady  Sense(by): 1(1)Traveling by bus book by Hemmingway his son by his third wifeson of a friend it was kind of you construction of the library South of the siteLearning and Inference Natural Language Decisions are Structured  Global decisions in which several local decisions play a role  but there are mutual dependencies on their outcome. It is essential to make coherent decisions in a way that takes the interdependencies into account. Joint, Global Inference.  Unlike “standard” classification problems, in most interesting NLP problems there is a need to predict values for multiple interdependent variables.  These are typically called Structured Output Problems – and will be the focus of this class. Page 18Statistics or Linguistics?Statistical approaches were very successful in NLP  But, it has become clear that there is a need to move from strictly Data Driven approaches to Knowledge Driven approaches  Knowledge: Linguistics, Background world knowledge   How to incorporate Knowledge into Learning & Decision Making?  In many respects Structured Prediction addresses this question.  This also distinguishes it from the “standard” study of probabilistic models.Page 19Hard Co-reference ProblemsPage 20Requires knowledge Acquisition The bee landed on the flower because it had/wanted pollen.  Lexical knowledge  John Doe robbed Jim Roy. He was arrested by the police.  The Subj of “rob” is more likely than the Obj of “rob” to be the Obj of “arrest” Requires an inference framework that can make use of this knowledge  NL interpretation is an inference problem best modelled as a knowledge constrained optimization problem over multiple statistically learned models.       Knowledge representation ?This ClassProblems  that will motivate us  Perspectives we’ll develop it’s not only the learning algorithm…  What we’ll do and howPage 21Some ExamplesPart of Speech Tagging This is a sequence labeling problem  The simplest example where (it seems that) the decision with respect to one word depends on the decision with respect to others. Named Entity Recognition This is a sequence segmentation problem Not all segmentations are possible There are dependencies among assignments of values to different segments.  Relation Extraction Works_for (Jim, US-government) ; co-reference resolution Semantic Role Labeling Decisions here build on previous decisions (Pipeline Process) Clear constraints among decisions  Page 22Semantic Role Labeling I left my pearls to my daughter in my will . [I]A0 left [my pearls]A1 [to my daughter]A2 [in my will]AM-LOC .  A0	Leaver A1	Things left A2	Benefactor AM-LOC	Location        I left my pearls to my daughter in my will .Page 23Overlapping arguments  If A2 is present, A1 must also be present.  Who did what to whom, when, where, why,…How to express the constraints on the decisions?  How to “enforce” them? Algorithmic ApproachPage 24Identify argument candidates Pruning  [Xue&Palmer, EMNLP’04] Argument Identifier  Binary classification (A-Perc) Classify argument candidates Argument Classifier  Multi-class classification (A-Perc) Inference Use the estimated probability distributions given by the argument classifier Use structural and linguistic constraints Infer the optimal global output I left my nice pearls to hercandidate argumentsUse the pipeline architecture’s simplicity while maintaining uncertainty:  keep probability distributions over decisions & use global inference at decision time.Page 25A Lot More ProblemsPOS/Shallow Parsing/NER  SRL  Parsing/Dependency Parsing  Information Extraction/Relation Extraction   Co-Reference Resolution  Transliteration  Textual Entailment  Computational IssuesPage 26Difficulty of Annotating DataDecouple?  Joint Learning  vs.  Joint InferenceThe Inference Problem  How to solve/make decisions ? The Learning Problem   How to train the model ?  Indirect Supervision  Constraints Driven Learning Semi-supervised Learning  Constraints Driven Learning A MODEL PerspectiveModels Generative/ Descriminative Training Supervised, Semi-Supervised, indirect supervision Knowledge Features Models Structure Declarative Information   Approach Unify treatment Demistify Survey key ML techniques used in Structured NLP Page 27Structure (Some) Lectures by me.  Flipped Structure You will read stuff Group discussions: of teaching material and projects Assignments Project: A group project; reports and presentations  Presentations: group paper presentations/tutorials (1-2 papers each)   4 critical surveys Assignment 0 – Machine Learning Preparation. Expectations This is an advanced course. I view my role as guiding you through the material and helping you in your first steps as an researcher. I expect that your participation in class, reading assignments and presentations will reflect independence, mathematical rigor and critical thinkingPage 28Questions?This CourseOrganization of MaterialDimension I: Models On Line, Perceptron Based Exponential Models (Logistic Regression, HMMs, CRFs)  SVMs Constrained Conditional Models (ILP based formulations) Neural Networks  (Markov Logic Networks?) Dimension II: Learning & Inference Tasks Basic sequential and Structured Prediction Training Paradigms (Unsupervised & Semi Supervised; Indirect Supervision)  Latent Representations Inference and Approximate Inference Dimension III: Applications There is a lot of overlaps, several things can belong to several categories. It will make it more interesting. Page 29Projects Project: Chosen by you We can provide ideas Small groups (2-3)  4 steps:  proposal,  intermediary report #1,  Intermediary report #2,  Final report and presentation. Page 30Presentations/Tutorial Groups (Tentative)EXP Models/CRF Structured SVM MLN Optimization Features CCM Structured Perceptron Neural Networks Inference Latent Representations     Page 31SummaryClass will be taught (mostly) as a flipped seminar + a project Tues/Thurs meetings might be replaced by Friday morning meetings, say.   Critical readings of papers + Projects + Presentation Attendance is mandatory  If you want to drop the class, do it quickly – not to affect the projects.  Web site with all the information is already available .   On Thursday, I will hand you a Homework 0:  A ML prerequisites  Exam (take home: 3 hours).Page 32"
125,"Page 1CIS 700Advanced Machine Learning for NLPReview 1: Supervised Learning, Binary ClassifiersDan Roth Department of Computer and Information Science University of PennsylvaniaAugmented and modified by Vivek SrikumarSupervised learning: General settingGiven: Training examples of the form <x, f(x)> The function f is an unknown function The input x is represented in a feature space Typically x 2 {0,1}n or x 2 <n For a training example x, f(x) is called the label Goal: Find a good approximation for f The label Binary classification: f(x) 2 {-1,1} Multiclass classification: f(x) 2 {1, 2, 3, , K} Regression: f(x) 2 <  2Nature of applications Humans can perform a task, but can’t describe how they do it Eg: Object detection in images Context Sensitive Spelling  The desired function is hard to obtain in closed form Eg: Stock market The subject argument of a verb 3Binary classificationSpam filtering  Is an email spam or not? Recommendation systems Given user’s movie preferences, will she like a new movie? Malware detection Is an Android app malicious? Time series prediction Will the future value of a stock increase or decrease with respect to its current value?  4The fundamental problem: Machine learning is ill-posed!5Can you learn this function?  What is it?Is learning possible at all?There are 216 = 65536 possible Boolean functions over 4 inputs Why? There are 16 possible outputs. Each way to fill these 16 slots is a different function, giving 216 functions. We have seen only 7 outputs We cannot know what the rest are without seeing them Think of an adversary filling in the labels every time you make a guess at the function6Solution: Restricted hypothesis spaceA hypothesis space is the set of possible functions we consider We were looking at the space of all Boolean functions  Choose a hypothesis space that is smaller than the space of all functions Only simple conjunctions (with four variables, there are only 16 conjunctions without negations) m-of-n rules: Pick a set of n variables. At least m of them must be true Linear functions How do we pick a hypothesis space?  Using some prior knowledge (or by guessing) What if the hypothesis space is so small that nothing in it agrees with the data? We need a hypothesis space that is flexible enough7Where are we?Supervised learning: The general setting Linear classifiers The Perceptron algorithm Learning as optimization Support vector machines Logistic Regression8Linear ClassifiersInput is a n dimensional vector x Output is a label y 2 {-1, 1}  Linear threshold units classify an example x as  Classification rule= sgn(b+ wTx) = sgn(b +wi xi) b + wTx ¸ 0 ) Predict y = 1 b + wTx < 0 ) Predict y = -1  9For nowLinear ClassifiersInput is a n dimensional vector x Output is a label y 2 {-1, 1}  Linear threshold units classify an example x as  Classification rule= sgn(b+ wTx) = sgn(b +wi xi) b + wTx ¸ 0 ) Predict y = 1 b + wTx < 0 ) Predict y = -1  10For nowThe geometry of a linear classifier11sgn(b +w1 x1 + w2x2)x1x2b +w1 x1 + w2x2=0In n dimensions, a linear classifier  represents a hyperplane that separates the space into two half-spacesWe only care about the sign, not the magnitude[w1 w2]Linear classifiers are an expressive hypothesis classMany functions are linear Conjunctions y = x1 Æ x2 Æ x3           y = sgn(-3 + x1 + x2 + x3)            W = (1,1,1); b = -3 At least m-of-n functions We will see later in the class that many structured predictors are linear functions too Often a good guess for a hypothesis space Some functions are not linear The XOR function Non-trivial Boolean functions12XOR is not linearly separable13x1x2No line can be drawn to separate the two classesEven these functions can be made linear      The trick: Change the representation14These points are not separable in 1-dimension by a line  What is a one-dimensional line, by the way?Even these functions can be made linearThe trick: Use feature conjunctions15Transform points: Represent each point x in 2 dimensions by (x, x2)Now the data is linearly separable in this space! Exercise: How do you apply this for the XOR case?Almost linearly separable data16sgn(b +w1 x1 + w2x2)x1x2b +w1 x1 + w2x2=0Training data is almost separable, except for some noise  How much noise do we allow for?Training a linear classifierThree cases to consider: Training data is linearly separable Simple conjunctions, general linear functions, etc Training data is linearly inseparable XOR, etc Training data is almost linearly separable Noise in the data We could allow some the classifier to make some mistakes on the training data to account for these   17Simplifying notationWe can stop writing b at each step because of the following notational sugar: 	The prediction function is sgn(b + wTx) 	Rewrite x as [1, x]  ! x’ 	Rewrite w as [b, w] ! w’ 		increases dimensionality by one 	But we can write the prediction as sgn(w’Tx’)  We will not show b, and instead fold the bias term into the input by adding an extra feature But remember that it is there18Where are we?Supervised learning: The general setting Linear classifiers The Perceptron algorithm Learning as optimization Support vector machines Logistic Regression19The Perceptron algorithmRosenblatt 1958  The goal is to find a separating hyperplane For separable data, guaranteed to find one  An online algorithm Processes one example at a time  Several variants exist (will discuss briefly at towards the end) 20The algorithmGiven a training set D = {(x,y)}, x 2 <n, y 2 {-1,1} Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Predict y’ = sgn(wTx) If y ≠ y’, update w Ã w + y x Return w  Prediction: sgn(wTx)21The algorithmGiven a training set D = {(x,y)}	 Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Predict y’ = sgn(wTx) If y ≠ y’, update w Ã w + y x Return w  Prediction: sgn(wTx)22In practice, good to shuffle D before the inner loopGeometry of the perceptron update23wold(x, +1)(x, +1)wneww Ã w + y xExercise: Verify for yourself that a similar picture can be drawn for negative examples too(x, +1)PredictUpdateAfterConvergenceConvergence theorem If there exist a set of weights that are consistent with the data (i.e. the data is linearly separable), the perceptron algorithm will converge. How long?  Cycling theorem If the training data is not linearly separable, then the learning algorithm will eventually repeat the same set of weights and enter an infinite loop24MarginThe margin of a hyperplane for a dataset is the distance between the hyperplane and the data point nearest to it.     25Margin with respect to this hyperplaneMarginThe margin of a hyperplane for a dataset is the distance between the hyperplane and the data point nearest to it. The margin of a data set (°) is the maximum margin possible for that dataset using any weight vector.     26Margin of the dataThe mistake bound theorem [Novikoff 1962]Let D={(xi, yi)} be a labeled dataset that is separable  Let ||   xi||   · R for all examples.  Let ° be the margin of the dataset D.  Then, the perceptron algorithm will make at most R2/°2 mistakes on the data.  Proof idea: We know that there is some true weight vector w*. Each perceptron update reduces the angle between w and w*. Note: The theorem doesn’t depend on the number of examples we have. 27Beyond the separable caseThe good news Perceptron makes no assumption about data distribution Even adversarial After a fixed number of mistakes, you are done. Don’t even need to see any more data The bad news: Real world is not linearly separable Can’t expect to never make mistakes again What can we do: more features, try to be linearly separable if you can28Variants of the algorithmSo far: We return the final weight vector Averaged perceptron Remember every weight vector in your sequence of updates. Weigh each weight vector as a function of the number of examples that survived this weight vector.  Make a prediction with the weighted average of the weight vectors. Comes with strong theoretical guarantees about generalization.  Need to be smart about implementation. 29Next stepsWhat is the perceptron doing? Error-bound exists, but over training data Is it minimizing a loss function? Can we say something about future errors?  More on different loss functions Support vector machine A look at regularization Logistic Regression 30"
126,"Page 1CIS 700Advanced Machine Learning for NLPReview 2: Loss minimization, SVM and Logistic RegressionDan Roth Department of Computer and Information Science University of PennsylvaniaAugmented and modified by Vivek SrikumarPerceptron algorithmGiven a training set D = {(x,y)}, x 2 <n, y 2 {-1,1} Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Predict y’ = sgn(wTx) If y ≠ y’, update w Ã w + y x Return w  Prediction: sgn(wTx)2Or equivalently, If y wTx · 0, update w Ã w + y xWhere are we?Supervised learning: The general setting Linear classifiers The Perceptron algorithm Support vector machines Learning as optimization Logistic Regression3What is the Perceptron algorithm doing?Mistake-bound on the training set  What about future examples? Can we say something about them?  Can we say anything about the future?4Recall: MarginThe margin of a hyperplane for a dataset is the distance between the hyperplane and the data point nearest to it.     5Margin with respect to this hyperplaneWhich line is a better choice? Why?6h1h2Which line is a better choice? Why?7h2++A new example,  not from the  training set might be misclassified if the margin is smallerh1Maximal margin and generalizationLarger margin gives better generalization  Note: learning is done on a training set You can minimize your performance on the training data But, you care about your performance on previousely unseen data  The notion of a margin is related to the notion of the expressivity of the hypothesis space In this case, a hypothesis space of linear functions Maximizing margin => fewer errors on future examples This idea forms the basis of many learning algorithms SVM, averaged perceptron, AdaBoost,…   8Maximizing marginMargin = distance of the closest point from the hyperplane   We want maxw °9Recall: The geometry of a linear classifier10sgn(b +w1 x1 + w2x2)x1x2b +w1 x1 + w2x2=0We only care about the sign, not the magnitude[w1 w2]Maximizing marginMargin = distance of the closest point from the hyperplane  We want maxw ° We only care about the sign of w in the end and not the magnitude Set the activation of the closest point to be 1 and allow w to adjust itself Sometimes called the functional margin 	maxw ° is equivalent to minw ||w||in this setting11Max-margin classifiersLearning a classifier: min ||w|| such that the activation of the closest point is 1  Learning problem:    This is called the “hard” Support Vector Machine We will look at solving this optimization problem later12What if the data is not separable?Hard SVM   13What if the data is not separable?Hard SVM   This is a constrained optimization problem  If the data is not separable, there is no w that will classify the data  Infeasible problem, no solution!14Dealing with non-separable dataKey idea: Allow some examples to “break into the margin”  15+This separator has a large enough margin that it should generalize well. So, while computing margin, ignore the examples that make the margin smaller or the data inseparable.-Soft SVMHard SVM:   Introduce one slack variable »i per example and require yiwTxi ¸ 1 - »i and »i ¸ 0 New optimization problem for learning 16Maximize marginEvery example has an functional margin of at least 1Soft SVMHard SVM:   Introduce one slack variable »i per example and require yiwTxi ¸ 1 - »i and »i ¸ 0  Soft SVM learning: 17Maximize marginMaximize marginEvery example is at least at a distance 1 from the hyperplaneMinimize total slack (i.e allow as few examples as possible to violate the margin)Tradeoff between the two termsSoft SVM     Eliminate the slack variables to rewrite this as   This form is more interpretable18Maximize marginMinimize total slack (i.e allow as few examples as possible to violate the margin)Tradeoff between the two termsMaximizing margin and minimizing loss   Three cases An example is correctly classified: penalty = 0 An example is incorrectly classified: penalty = 1 – yi wTxi An example is correctly classified but within the margin: penalty = 1 – yi wTxi  This is the hinge loss function 19Maximize marginPenalty for the prediction according to the weight vectorThe Hinge Loss20SVM objective function 21Regularization term:  Maximize the margin Imposes a preference over the hypothesis space and pushes for better generalization Can be replaced with other regularization terms which impose other preferencesEmpirical Loss:  Hinge loss  Penalizes weight vectors that make mistakes   Can be replaced with other loss functions which impose other preferencesA hyper-parameter that controls the tradeoff between a large margin and a small hinge-lossWhere are we?Supervised learning: The general setting Linear classifiers The Perceptron algorithm Support vector machines Learning as optimization Logistic Regression22Computational Learning TheoryStudies theoretical issues about the importance of representation, sample complexity (how many examples are enough), computational complexity (how fast can I learn) Led to algorithms such as support vector machine and boosting   No assumptions about the distribution of examples But assume that both training and test examples come from the same distribution  Provides bounds that depend on the size of the hypothesis class23Learning as loss minimizationCollect some annotated data. More is generally better Pick a hypothesis class  Eg: binominal, linear classifiers Also, decide on how to impose a preference over hypotheses Choose a loss function Eg: negative log-likelihood Decide on how to penalize incorrect decisions Minimize the loss Eg: Set derivative to zero, more complex algorithm 24Learning as loss minimization: The setupExamples <x, y> are created from some unknown distribution P Identify a hypothesis class H Define penalty for incorrect predictions: The loss function L Learning: Pick a function f 2 H to minimize expected loss 					minH EP[L] Use samples from P to estimate expectation:  The training set D = {<x, y>} “Empirical risk minimization” minf 2 H D L(y, f(x))  25Regularized loss minimization: Logistic regressionLearning:  With linear classifiers:  What is a loss function? Loss functions should penalize mistakes We are minimizing average loss over the training data  What is the ideal loss function for classification?26The 0-1 lossPenalize classification mistakes between true label y and prediction y’    For linear classifiers, the prediction y’ = sgn(wTx)  Mistake if y wTx · 0   Minimizing 0-1 loss is intractable. Need surrogates	27Loss functions28Typically plotted as a function of ywTxSVMSupport Vector Machines: SummarySVM = linear classifier + regularization Recall that perceptron did not have regularization Ideally, we would like to minimize 0-1 loss, but can not SVM minimizes hinge loss Variants exist Will not cover Dual formulation, support vectors, kernels  29Solving the SVM optimization problem  This function is convex in w30Convex functionsA function f is convex if for every u, v in the domain, and for every a 2 [0,1] we have f(a u + (1-a) v) · a f(u) + (1-a) f(v)     The necessary condition for w* to be a minimum for a function f: df(w*)/dw = 0  For convex functions, this is both necessary and sufficient  31Solving the SVM optimization problem  This function is convex in w This is a quadratic optimization problem because the objective is quadratic Earlier methods: Used techniques from Quadratic Programming Very slow No constraints, can use gradient descent Still very slow!32Gradient descent for SVMGradient descent algorithm to minimize a function f(w): Initialize solution w Repeat Find the gradient of f at w: r f Set w Ã w – r r f  Gradient of the SVM objective requires summing over the entire training set Slow Does not really scale 33Stochastic gradient descent for SVMGiven a training set D = {(x,y)}, x 2 <n, y 2 {-1,1} Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Treat (x,y) as a full dataset and take the derivative of the SVM objective at the current w to be r w Ã w – r r Return w  What is the gradient of the hinge loss with respect to w? (The hinge loss is not a differentiable function!)34Stochastic sub-gradient descent for SVMGiven a training set D = {(x,y)}, x 2 <n, y 2 {-1,1} Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: 	If y wTx · 1,  		then w Ã (1-r)w + r C y x  		else w Ã (1-r) w Return w Prediction: sgn(wTx) Compare to the perceptron algorithm!35Perceptron update: If y wTx · 0, update w Ã w + r y xr: learning rate, many tweaks possibleSVM summary from optimization perspectiveMinimize regularized hinge loss Solve using stochastic gradient descent Very fast, run time does not depend on number of examples Compare with Perceptron algorithm: Perceptron does not maximize margin width Perceptron variants can force a margin Convergence criterion is an issue; can be too aggressive in the beginning and get to a reasonably good solution fast; but convergence is slow for very accurate weight vector Another successful optimization algorithm: Dual coordinate descent, implemented in liblinear36Questions?Where are we?Supervised learning: The general setting Linear classifiers The Perceptron algorithm Support vector machine Learning as optimization Logistic Regression37Regularized loss minimization: Logistic regressionLearning:  With linear classifiers:  SVM uses the hinge loss  Another loss function: The logistic loss 38Loss functions39Typically plotted as a function of ywTxSVMLogistic regressionSmooth, differentiableThe probabilistic interpretationSuppose we believe that the labels are generated using the following probability distribution:    Predict label = 1 if P(1 | x, w) > P(-1 | x, w) Equivalent to predicting 1 if wTx ¸ 0 Why? 40The probabilistic interpretationSuppose we believe that the labels are generated using the following probability distribution:    What is the log-likelihood of seeing a dataset D = {<xi, yi>} given a weight vector w? 41Prior distribution over the weight vectorsA prior balances the tradeoff between the likelihood of the data and existing belief about the parameters Suppose each weight wi is drawn independently from the normal distribution centered at zero with variance ¾2 Bias towards smaller weights   Probability of the entire weight vector:    42Source: WikipediaRegularized logistic regressionWhat is the probability of seeing a dataset D = {<xi, yi>} and a weight vector w? 		P(w | D) / P(D, w) = P(D | w) P(w)  Learning: Find weight vector by maximizing the  posterior distribution P(w | D)  		   Once again, regularized loss minimization! This is the Bayesian interpretation of regularization43Exercise: Derive the stochastic gradient descent algorithm for logistic regression. Regularized loss minimizationLearning objective for both SVM, logistic regression:  loss over training data + regularizer  Different loss functions Hinge loss vs. logistic loss Same regularizer, but different interpretation Margin vs prior Hyper-parameter controls tradeoff between the loss and regularizer Other regularizers/loss functions also possible44Questions?Review of binary classificationSupervised learning: The general setting Linear classifiers The Perceptron algorithm Support vector machine Learning as optimization Logistic Regression45Questions?"
128,"Dan Roth University of Illinois, Urbana-Champaign danr@cs.uiuc.edu http://L2R.cs.uiuc.edu/~danr7 Sequential Models Tutorial onMachine Learning in Natural Language Processingand Information ExtractionSequential Inference Problems  HMM  Advanced Models2OutlineShallow Parsing What it is Why we need it Shallow Parsing (Learning) Models Learning Sequences Hidden Markov Model (HMM) Discriminative Approaches HMM with Classifiers PMM Learning and Inference CSCL Related Approaches3ParsingParsing is a task to analyze for syntactical structure of inputs An output is a “parse tree”4Parsing5ParsingParsing is an important task toward understanding natural languages It is not an easy task Many NLP tasks do not require all information from parse trees6Shallow ParsingShallow Parsing is a generic term for a task identifies only a subset of parse trees7Shallow Parsing8Not all the parse tree information is needed It should do better than full parsing in those specific parts It can be done more robuslty More adaptive to data from new domains (Li & Roth’ CoNLL’01)  Shallow parsing as an intermediate step toward full parsing  It is a generic chunking task – applicable to many other segmentation tasks such as named entity recognition. [LBJ generic segmentation implementation]Reasons for Studying Shallow Parsing9By shallow parsing we mean: identifying non-overlapping, non-embedding phrases.      Shallow Parsing = Text ChunkingShallow Parsing [Today]10Usually text chunking without any further specification follows the definition from CoNLL-2000 shared task [Demo]     Text Chunking11   Text Chunking12Modeling Shallow ParsingModel shallow parsing as a sequence prediction problem For each word predict one of these : B – Beginning of a chunk I – Inside a chunk but not a beginning O – Outside a chunk  13Modeling Shallow Parsing14Similar ProblemsThis model applies to many other problems NLP Named entity recognition Verb-argument identification Other domains Identifying genes (splice sites, Chuang&Roth’01) 15OutlineShallow Parsing What it is Why we need it Shallow Parsing (Learning) Models Learning Sequences Hidden Markov Model (HMM) Discriminative Approaches HMM with Classifiers PMM Learning and Inference CSCL Related Approaches16Hidden Markov Model (HMM)HMM is a probabilistic generative model It models how an observed sequence is generated Let’s call each position in a sequence a time step At each time step, there are two variables Current state (hidden) Observation 17HMM     Elements Initial state probability P(s1) Transition probability P(st|st-1)  Observation probability P(ot|st) 18HMM for Shallow ParsingStates: {B, I, O} Observations: Actual words and/or part-of-speech tags 19HMM for Shallow Parsing       Given a sentence, we can ask what the most likely state sequence isInitial state probability: P(s1=B),P(s1=I),P(s1=O)Transition probabilty: P(st=B|st-1=B),P(st=I|st-1=B),P(st=O|st-1=B), P(st=B|st-1=I),P(st=I|st-1=I),P(st=O|st-1=I), …Observation Probability: P(ot=‘Mr.’|st=B),P(ot=‘Brown’|st=B),…, P(ot=‘Mr.’|st=I),P(ot=‘Brown’|st=I),…, …20Finding most likely state sequence in HMM (1)21Finding most likely state sequence in HMM (2)22Finding most likely state sequence in HMM (3)23Finding most likely state sequence in HMM (4)     Viterbi’s Algorithm Dynamic Programming24Learning the ModelEstimate Initial state probability P (s1) Transition probability P(st|st-1)  Observation probability P(ot|st) Unsupervised Learning EM Algorithm Supervised Learning Estimate each element directly from data25Experimental ResultsNP Prediction (POS Tags Only) 89.08 Recall 86.62 Precision 87.83 F126NoteExperiments use a different representation  27OutlineShallow Parsing What it is Why we need it Shallow Parsing (Learning) Models Learning Sequences Hidden Markov Model (HMM) Discriminative Approaches HMM with Classifiers PMM Learning and Inference CSCL Related Approaches28Problems with HMMLong-term dependencies are hard to incorporate HMM is trained to maximize the likelihood of the data, not to maximize the predictions Maximize P(S,O), not P(S|O)  What metric do we care about?29Discriminative ApproachesModel the predictions directly Shallow Parsing Goal: predict BIO sequences given a sentence S* = argmax P(S|O) Generative approaches model P(S,O) Discriminative approaches model P(S|O) directly30Discriminative ApproachesUse classifiers to predict the labels based on the context of the inputs       Good: larger context can be incorporatedA classifier31Problems with using ClassifiersOutputs may be inconsistentA classifier32ConstraintsAn ‘I’ cannot follow an ‘O’ The sequence has to begin with a ‘B’ or an ‘O’      How to maintain constraints?BIO33HMM revisitedHMM does not have this problem. These constraints are taken care for by the transition probabilities. Specifically, zero transition probabilities ensure that outputs always satisfy constraints34HMM with ClassifiersHMM requires observation probability P(ot|st) Classifiers give us P(st|ot)  We can compute P(ot|st) by P(ot|st) = P(st|ot)P(ot)/P(st) See details here  35Experimental ResultsNP Prediction36Projection based Markov Model (PMM)Classifiers use previous prediction as part of inputsA classifier37PMMEach classifier outputs P(st|st-1,O)         How do you Train/Test in this case?38Experimental ResultsNP Prediction39OutlineShallow Parsing What it is Why we need it Shallow Parsing (Learning) Models Learning Sequences Hidden Markov Model (HMM) Discriminative Approaches HMM with Classifiers PMM Learning and Inference CSCL Related Approaches40Learning and InferenceLearning: estimate parameters from data, and makes predictions for new data Inference: ensure that outputs satisfy constraints41InferenceAssign values to the variables of interest (output; states) in a way that maximizes your objective function and satisfies constraints42InferenceBoolean Constraint Satisfaction Problem V – set of variables Cost – c: V  [0,1] Clauses – model constraints Satisfying assignment – : V  {0,1}  Find the solution  that minimize the cost C() = i=1..n (vi)c(vi)43Inference for Shallow ParsingDefine a variable vi for each possible chunk Cost of each chunk = -P(vi is a chunk) For any two overlapping chunks: (vi  vj)  Find the solution  that minimizes the cost C() = i=1..n (vi)c(vi)  This is a good objective function – it maximizes the expected number of correct chucks. CSP in general is hard Structure of the constraints yields a problem that can be solved by a shortest path algorithm44Inference for Shallow Parsingo1     o2     o3     o4     o5     o6     o7         [         [                  [                           ]                    ]       ] 45Experimental ResultsNP Prediction46Other Approaches to Shallow ParsingMEMM CRF Global Perceptron (Collins’) Others (e.g., global SVM)47Maximum Entropy Markov Model (MEMM)Similar to PMM      Each term is learned with maximum entropy model  48Conditional Random Field (CRF)Similar to PMM, but based on Markov random field theory (undirected graph)       49Collins’ Perceptron Training for HMMSimilar to HMM, but use Perceptron algorithm to learn      50Maximum Entropy Markov Model (MEMM)51Maximum Entropy Markov Model (MEMM)52Conditional Random Field (CRF)     53ConclusionsAll approaches use linear representation The differences are Features How to learn weights Training Paradigms:  Global Training (HMM, CRF, Global Perceptron) Modular Training (PMM, MEMM, CSCL)  These approaches are easier to train, but may requires additional mechanisms to enforce global constraints. "
129,"Page 1CIS 700Advanced Machine Learning for NLPMulticlass classification: Local and Global ViewsDan Roth Department of Computer and Information Science University of PennsylvaniaAugmented and modified by Vivek SrikumarAdministrationCritical Reviews: Some reviews are missing Please follow the schedule on the web Projects: NN 17 CCM 7 SVM 6 Perc 6 Exp 5 Groups: 10 groups, two focused on each technical direction. Software: Neural Networks: Software – on your own Structured SVMs: use Illinois-SL  Structured Perceptron: use Illinois-SL  CCMs: use LBJava or Illinois-SL  Exp: Software – on your own. Readers will be given; feel free to use the Illinois NLP Pipeline and/or any other tool.  Content and Requirements:2OutlineA high level view of Structured Prediction Sequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences3Inference: given input x (a document, a sentence),                           predict the best structure y = {y1,y2,…,yn} 2 Y  (entities & relations) Assign values to the y1,y2,…,yn, accounting for dependencies among yis Inference is expressed as a maximization of a scoring function                                     y’ = argmaxy 2 Y wT Á (x,y)    Inference requires, in principle, touching all y 2 Y at decision time, when we are given x 2 X and attempt to determine the best y 2 Y for it, given w  For some structures, inference is computationally easy.  Eg: Using the Viterbi algorithm  In general, NP-hard (can be formulated as an ILP) Structured Prediction: InferenceJoint features on inputs and outputsFeature Weights (estimated during learning)Set of allowed structuresPlacing in context: a crash course in structured predictionPage 4Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss. Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):    Page 5Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss. Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):     We call these conditions the learning constraints.  In most learning algorithms used today, the update of the weight vector w is done in an on-line fashion,  Think about it as Perceptron; this procedure applies to Structured Perceptron, CRFs, Linear Structured SVM W.l.o.g. (almost) we can thus write the generic structured learning algorithm as follows:Score of annotated structureScore of any other structurePenalty for predicting other structure8 yPage 6In the structured case, the prediction (inference) step is often intractable and needs to be done many timesStructured Prediction: Learning AlgorithmFor each example (xi, yi)   Do: (with the current weight vector w) Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y wT Á ( xi ,y) Check the learning constraints Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndForPage 7Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution I: decompose the scoring function to EASY and HARD partsEASY: could be feature functions that correspond to an HMM, a linear CRF,   or even ÁEASY (x,y) = Á(x), omiting dependence on y, corresponding to classifiers. May not be enough if the HARD part is still part of each inference step.Page 8Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution II: Disregard some of the dependencies: assume a simple model.Page 9Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)   This is the most commonly used solution in NLP todaySolution III: Disregard some of the dependencies during learning; take into account at decision timePage 10OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences11SequencesSequences of states Text is a sequence of words or even letters A video is a sequence of frames  If there are K unique states, the set of unique state sequences is infinite  Our goal (for now): Define probability distributions over sequences  If x1, x2, , xn is a sequence that has n tokens, we want to be able to define  …for all values of n  12A history-based model  Each token is dependent on all the tokens that came before it Simple conditioning Each P(xi | …) is a multinomial probability distribution over the tokens13It was a bright cold day in April. Example: A Language model 14A history-based model  Each token is dependent on all the tokens that came before it Simple conditioning Each P(xi | …) is a multinomial probability distribution over the tokens What is the problem here? How many parameters do we have?  Grows with the size of the sequence!15Solution: Lose the historyA system can be in one of K states at a time State at time t is xt First-order Markov assumption  The state of the system at any time is independent of the full sequence history given the previous state   Defined by two sets of probabilities:  Initial state distribution: P(x1 = Sj) State transition probabilities: P(xi = Sj | xi-1 = Sk)16Discrete Markov ProcessExample: Another language modelIt was a bright cold day in April17If there are K tokens/states, how many parameters do we need? O(K2)Example: The weatherThree states: rain, cloudy, sunny    Observations are Markov chains: Eg: cloudy sunny sunny rain Probability of the sequence =  P(cloudy) P(sunny|cloudy) P(sunny | sunny) P(rain | sunny) 18State transitions:Initial probabilityTransition probabilitiesThese probabilities define the model; can find P(any sequence)mth order Markov ModelA generalization of the first order Markov Model  Each state is only dependent on m previous states  More parameters But still less than storing entire history19Questions?OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences20Hidden Markov ModelDiscrete Markov Model:  States follow a Markov chain Each state is an observation  Hidden Markov Model: States follow a Markov chain States are not observed Each state stochastically emits an observation  21Toy part-of-speech exampleThe Fed raises interest rates22TheDeterminerFedNounraisesVerbinterestNounratesNounstartEmissionsP(The | Determiner) = 0.5 P(A | Determiner) = 0.3 P(An | Determiner) = 0.1 P(Fed | Determiner) = 0 …P(Fed| Noun) = 0.001 P(raises| Noun) = 0.04 P(interest| Noun) = 0.07 P(The| Noun) = 0 …Joint model over states and observationsNotation Number of states = K, Number of observations = M ¼: Initial probability over states  (K dimensional vector) A: Transition probabilities (K£K matrix) B: Emission probabilities (K£M matrix)  Probability of states and observations Denote states by y1, y2,  and observations by x1, x2, 23Example: Named Entity RecognitionGoal: To identify persons, locations and organizations in text  B-org     O   B-per I-per       O         O   Facebook CEO Mark  Zuckerberg announced new  O        O      O O   O          O B-loc I-loc  privacy features in the conference in San   Francisco24ObservationsOther applicationsSpeech recognition Input: Speech signal Output: Sequence of words NLP applications Information extraction Text chunking  Computational biology Aligning protein sequences Labeling nucleotides in a sequence as exons, introns, etc.25Questions?Three questions for HMMsGiven an observation sequence, x1, x2,  xn and a model (¼, A, B), how to efficiently calculate the probability of the observation?  Given an observation sequence, x1, x2, , xn and a model (¼, A, B), how to efficiently calculate the most probable state sequence?  How to calculate (¼, A, B) from observations?26[Rabiner 1999]OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences27Input: A hidden Markov model (¼, A, B) An observation sequence x = (x1, x2, , xn)  Output: A state sequence y = (y1, y2, , yn) that corresponds to Maximum a posteriori inference (MAP inference)  Computationally: combinatorial optimization  Most likely state sequence28Some slides based on Noah Smith’s slidesMAP inferenceWe want  We have defined   But, P(y | x, ¼, A, B) / P(x, y |¼, A, B)  And we don’t care about P(x) we are maximizing over y  So, 29How many possible sequences? 30In this simple case, 16 sequences (1¢2¢2¢2¢2)List of allowed tags for each wordHow many possible sequences?31List of allowed states for each observationObservationsOutput: One state per observation yi = sj Kn possible sequences to consider inNaïve approachesTry out every sequence Score the sequence y as P(y|x, ¼, A, B) Return the highest scoring one What is the problem? Correct, but slow, O(Kn) Greedy search Construct the output left to right For each i, elect the best yi using yi-1 and xi What is the problem? Incorrect but fast, O(n)  32Solution: Use the independence assumptionsRecall: The first order Markov assumption The state at token i is only influenced by the previous state, the next state and the token itself  Given the adjacent labels, the others do not matter  Suggests a recursive algorithm33Deriving the recursive algorithm34Deriving the recursive algorithm35Deriving the recursive algorithm36Abstract away the score for all decisions till here into scoreDeriving the recursive algorithm37Deriving the recursive algorithm38Abstract away the score for all decisions till here into scoreDeriving the recursive algorithm39Abstract away the score for all decisions till here into scoreDeriving the recursive algorithm40Viterbi algorithmInitial: For each state s, calculate  Recurrence: For i = 2 to n, for every state s, calculate   Final state: calculate   41This only calculates the max. To get final answer (argmax),  keep track of which state corresponds to the max at each step  build the answer using these back pointers¼: Initial probabilities A: Transitions B: EmissionsQuestions?Max-product algorithm for first order sequencesGeneral ideaDynamic programming The best solution for the full problem relies on best solution to sub-problems  Memoize partial computation  Examples Viterbi algorithm Dijkstra’s shortest path algorithm …	42Viterbi algorithm as best path43Goal: To find the highest scoring path in this trellisComplexity of inferenceComplexity parameters Input sequence length: n Number of states: K Memory Storing the table: nK (scores for all states at each position) Runtime At each step, go over pairs of states O(nK2)44Questions?OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences45Learning HMM parametersAssume we know the number of states in the HMM Two possible scenarios We are given a data set D = {<xi, yi>} of sequences labeled with states And we have to learn the parameters of the HMM (¼, A, B)   We are given only a collection of sequences D = {xi}		 And we have to learn the parameters of the HMM (¼, A, B)         EM algorithm: We will look at this setting in a subsequent lecture 46Supervised learning with complete dataUnsupervised learning, with incomplete dataSupervised learning of HMMWe are given a dataset D = {<xi, yi>} Each xi is a sequence of observations and yi is a sequence of states that correspond to xi Goal: Learn initial, transition, emission distributions (¼, A, B)  How do we learn the parameters of the probability distribution? The maximum likelihood principle47Where have we seen this before?Supervised learning details ¼, A, B can be estimated separately just by counting Makes learning simple and fast [Exercise: Derive the following using derivatives of the log likelihood. Requires Lagrangian multipliers.] 48Initial probabilitiesTransition probabilitiesEmission probabilitiesNumber of examplesNumber of instances where the first state is sPriors and smoothingMaximum likelihood estimation works best with lots of annotated data Never the case  Priors inject information about the probability distributions Dirichlet priors for multinomial distributions  Effectively additive smoothing Add small constants to the counts49Hidden Markov Models summaryPredicting sequences  As many output states as observations  Markov assumption helps decompose the score  Several algorithmic questions Most likely state Learning parameters  Supervised, Unsupervised Probability of an observation sequence Sum over all assignments to states, replace max with sum in Viterbi Probability of state for each observation Sum over all assignments to all other states50Questions?HMM reduxThe independence assumption   Training via maximum likelihood We are optimizing joint likelihood of the input and the output for trainingAt prediction time, we only care about the probability of output given the input. Why not directly optimize this conditional likelihood instead?51Modeling next-state directlyInstead of modeling the joint distribution P(x, y) only focus on P(y|x) Which is what we care about eventually anyway  For sequences, different formulations Maximum Entropy Markov Model [McCallum, et al 2000] Projection-based Markov Model [Punyakanok and Roth, 2001] (other names: discriminative/conditional markov model, …)52Generative models  learn P(x, y) Characterize how the data is generated (both inputs and outputs) Eg: Naïve Bayes, Hidden Markov Model    Discriminative models  learn P(y | x) Directly characterizes the decision boundary only Eg: Logistic Regression, Conditional models (several names)Generative vs Discriminative modelsA generative model tries to characterize the distribution of the inputs, a discriminative model doesn’t careQuestions?53Another independence assumption     This assumption lets us write the conditional probability of the output as  HMMConditional model54Modeling P(yi | yi-1, xi)Different approaches possible Train a maximum entropy classifier  Or, ignore the fact that we are predicting a probability, we only care about maximizing some score. Train any classifier, using say the perceptron algorithm For both cases: Use rich features that depend on input and previous state We can increase the dependency to arbitrary neighboring xi’s Eg. Neighboring words influence this words POS tag55Detour: Log-linear models for multiclassConsider multiclass classification Inputs: x  Output: y 2 {1, 2,  , K} Feature representation: Á(x, y) We have seen this before Define probability of an input x taking a label y as     A generalization of logistic regression to multi-class Interpretation: Score for label, converted to a well-formed probability distribution by exponentiating + normalizing56Training a log-linear modelGiven a data set D = {<xi, yi>} Apply the maximum likelihood principle    Maybe with a regularizer 57Gradient based methods using gradient of Simple approach Initialize w Ã 0 For t = 1, 2, … Update w Ã w + ®t r L(w) Return w   In practice, use more sophisticated methods Off the shelf L-BFGS implementations availableHow to maximize?58Questions?Consider all distributions P such that the empirical counts of the features matches the expected counts   Recall: Entropy of a distribution P(y|x) is    A measure of smoothness	 Without any other information, maximized by the uniform distribution  Maximum entropy learning argmaxp H(p) such that it satisfies this constraint Another training idea: MaxEnt59Maximum Entropy distribution = log-linearTheorem  The maximum entropy distribution among those satisfying the constraint has an exponential form  Among exponential distributions, the maximum entropy distribution is the most likely distribution60Questions?The next-state model     This assumption lets us write the conditional probability of the output as  HMMConditional model61Back to sequencesModeling P(yi | yi-1, xi)Different approaches possible Train a maximum entropy classifier Basically, multinomial logistic regression  Ignore the fact that we are predicting a probability, we only care about maximizing some score. Train any classifier, using say the perceptron algorithm For both cases: Use rich features that depend on input and previous state We can increase the dependency to arbitrary neighboring xi’s Eg. Neighboring words influence this words POS tag62Maximum Entropy Markov Model63Compare to HMM: Only depends on the word and the previous tagQuestions?Goal: Compute P(y | x)Á(x, 0, start, y0)Á(x, 1, y0, y1)Á(x, 2, y1, y2)Á(x, 3, y2, y3)Á(x, 4, y3, y4)Can get very creative hereUsing MEMMTraining Next-state predictor locally as maximum likelihood Similar to any maximum entropy classifier Prediction/decoding Modify the Viterbi algorithm for the new independence assumptions 64HMMConditional Markov modelGeneralization: Any multiclass classifier Viterbi decoding: we only need a score for each decision So far, probabilistic classifiers  In general, use any learning algorithm to build get a score for the label yi given yi-1 and x Multiclass versions of perceptron, SVM Just like MEMM, these allow arbitrary features to be defined  Exercise: Viterbi needs to be re-defined to work with sum of scores rather than the product of probabilities  65Comparison to HMMWhat we gain Rich feature representation for inputs Helps generalize better by thinking about properties of the input tokens rather than the entire tokens Eg: If a word ends with –es, it might be a present tense verb (such as raises). Could be a feature; HMM cannot capture this  Discriminative predictor Model P(y | x) rather than P(y, x) Joint vs conditional 66Questions?But…local classifiers ! Label bias problemRecall: the independence assumption  67Eg: Part-of-speech tagging the sentenceSuppose these are the only state transitions allowedOption 1: P(D | The) ¢               	  P(N | D, robot) ¢ 		  P(N | N, wheels) ¢  		  P(V | N, are) ¢  		  P(A | V, round)Option 2: P(D | The) ¢                    	  P(N | D, robot) ¢ 		  P(V | N, wheels) ¢  		  P(N | V, are) ¢  		  P( R| N, round)“Next-state” classifiers are locally normalizedExample based on [Wallach 2002]But…local classifiers ! Label bias problem68Suppose these are the only state transitions allowedOption 1: P(D | The) ¢               	  P(N | D, robot) ¢ 		  P(N | N, wheels) ¢  		  P(V | N, are) ¢  		  P(A | V, round)Option 2: P(D | The) ¢                    	  P(N | D, robot) ¢ 		  P(V | N, wheels) ¢  		  P(N | V, are) ¢  		  P( R| N, round)But…local classifiers ! Label bias problem69Suppose these are the only state transitions allowedOption 1: P(D | The) ¢               	  P(N | D, robot) ¢ 		  P(N | N, wheels) ¢  		  P(V | N, are) ¢  		  P(A | V, round)Option 2: P(D | The) ¢                    	  P(N | D, robot) ¢ 		  P(V | N, wheels) ¢  		  P(N | V, are) ¢  		  P( R| N, round)P(V | N, Fred) ¢P(N | V, Fred) ¢The path scores are the same  Even if the word Fred is never observed as a verb in the data, it will be predicted as one   The input Fred does not influence the output at allLabel BiasStates with a single outgoing transition effectively ignore their input States with lower-entropy next states are less influenced by observations  Why? Because each the next-state classifiers are locally normalized If a state has fewer next states, each of those will get a higher probability mass …and hence preferred  Side note: Surprisingly doesn’t affect some tasks Eg: POS tagging70Summary: Local models for SequencesConditional models  Use rich features in the mode  Possibly suffer from label bias problem 71OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences72OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences73So far…Hidden Markov models Pros: Decomposition of total probability with tractable Cons: Doesn’t allow use of features for representing inputs Also, joint model  Local, conditional Markov Models  Pros: Conditional model, allows features to be used Cons: Label bias problem 74Global modelsTrain the predictor globally Instead of training local decisions independently  Normalize globally Make each edge in the model undirected Not associated with a probability, but just a “score”  Recall the difference between local vs. global for multiclass75HMM vs. A local model vs. A global model76HMMConditional modelGlobal modelP(yt | yt-1)P(xt | yt)P(yt | yt-1, xt)fT(yt, yt-1)fE(yt, xt)Local: P is locally normalized to add up to one for each tGlobal: The functions fT and fE are scores that are not normalizedGenerativeDiscriminativeConditional Random Field77wTÁ(x, y0, y1)wTÁ(x, y1, y2)wTÁ(x, y2, y3)Each node is a random variable  We observe some nodes and need to assign the rest  Each clique is associated with a scoreArbitrary features, as with local conditional modelsConditional Random Field: Factor graph78Each node is a random variable  We observe some nodes and need to assign the rest  Each factor is associated with a scoreFactorsConditional Random Field: Factor graph79y0y1y2y3xwTÁ(y0, y1)wTÁ( y1, y2)wTÁ(x, y2, y3)Each node is a random variable  We observe some nodes and need to assign the rest  Each clique is associated with a scorewTÁ(y0, x)wTÁ( y1, x)wTÁ( y2, x)wTÁ( y3, x)A different factorization: Recall decomposition of structures into parts. Same ideaConditional Random Field for sequences  80Z: Normalizing constant, sum over all sequencesCRF: A different viewInput: x, Output: y, both sequences (for now)  Define a feature vector for the entire input and output sequence: Á(x, y)  Define a giant log-linear model, P(y | x) parameterized by w   Just like any other log-linear model, except Space of y is the set of all possible sequences of the correct length Normalization constant sums over all sequences81Global featuresThe feature function decomposes over the sequence82PredictionGoal: To predict most probable sequence y an input x   But the score decomposes as  Prediction via Viterbi (with sum instead of product)83Training a chain CRFInput:  Dataset with labeled sequences, D = {<xi, yi>} A definition of the feature function   How do we train? Maximize the (regularized) log-likelihood84Recall: Empirical loss minimizationTraining with inferenceMany methods for training Numerical optimization Use an implementation of the L-BFGS algorithm in practice  Stochastic gradient ascent is often competitive  Simple gradient ascent    Training involves inference!  A different kind than what we have seen so far  Summing over all sequences is just like Viterbi With summation instead of maximization85CRF summaryAn undirected graphical model Decompose the score over the structure into a collection of factors Each factor assigns a score to assignment of the random variables it is connected to  Training and prediction Final prediction via argmax wTÁ(x, y) Train by maximum (regularized) likelihood  Relation to other models Effectively a linear classifier A generalization of logistic regression to structures An instance of Markov Random Field, with some random variables observed We will see this soon86OutlineSequence models  Hidden Markov models  Inference with HMM Learning  Conditional Models and Local Classifiers  Global models Conditional Random Fields  Structured Perceptron for sequences87HMM is also a linear classifierConsider the HMM          Or equivalently   This is a linear function log P terms are the weights; counts and indicators are features Can be written as wTÁ(x, y) and add more features     88Indicators: Iz = 1 if z is true; else 0HMM is a linear classifier 89TheatethedoghomeworkDetVerbDetNounNounlog P(Det ! Noun) £ 2+ log P(Noun ! Verb) £ 1+ log P(Verb ! Det) £ 1log P(The | Det) £ 1+ log P(dog| Noun) £ 1+ log P(ate| Verb) £ 1+ log P(the| Det) £ 1+ log P(homework| Noun) £ 1+ Á(x, y): Properties of this output and the inputw: Parameters of the modelConsider log P(x, y)log P(x, y) = A linear scoring function = wTÁ(x,y)Towards structured PerceptronHMM is a linear classifier Can we treat it as any linear classifier for training? If so, we could add additional features that are global properties As long as the output can be decomposed for easy inference  The Viterbi algorithm calculates max wTÁ(x, y) Viterbi only cares about scores to structures (not necessarily normalized)  We could push the learning algorithm to train for un-normalized scores If we need normalization, we could always normalize by computing exponentiating and dividing by Z That is, the learning algorithm can effectively just focus on the score of y for a particular x Train a discriminative model!  90Structured Perceptron algorithmGiven a training set D = {(x,y)}	 Initialize w = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Predict y’ = argmaxy’ wTÁ(x, y’) If y ≠ y’, update w Ã w + learningRate (Á(x, y) - Á(x, y’)) Return w  Prediction: argmaxy wTÁ(x, y)91In practice, good to shuffle D before the inner loopNotes on structured perceptronMistake bound for separable data, just like perceptron  In practice, use averaging for better generalization Initialize a = 0 After each step, whether there is an update or not, a Ã w + a Note, we still check for mistake using w not a Return a at the end instead of w  Exercise: Optimize this for performance – modify a only on errors  Global update One weight vector for entire sequence  Not for each position Same algorithm can be derived from constraint classification Create a binary classification data set and run perceptron 92Structured Perceptron with averagingGiven a training set D = {(x,y)}	 Initialize w = 0 2 <n, a = 0 2 <n For epoch = 1 … T: For each training example (x, y) 2 D: Predict y’ = argmaxy’ wTÁ(x, y’) If y ≠ y’, update w Ã w + learningRate (Á(x, y) - Á(x, y’)) Set a Ã a + w Return a 93CRF vs. structured perceptronConsider stochastic gradient descent update for CRF For a training example (xi, yi)   Structured perceptron For a training example (xi, yi)94Caveat: Adding regularization will change the CRF update, averaging changes the perceptron updateExpectation vs maxThe lay of the landHMM: A generative model, assigns probabilities to sequences Hidden Markov Models are actually just linear classifiers  Don’t really care whether we are predicting probabilities. We are assigning scores to a full output for a given input (like multiclass)  Generalize algorithms for linear classifiers. Sophisticated models that can use arbitrary features  Structured Perceptron      Structured SVMModel probabilities via logistic functions. Gives us the log-linear representation  Log-probabilities for sequences for a given input   Learn by maximizing likelihood. Sophisticated models that can use arbitrary features  Conditional Random field Applicable beyond sequences Eventually, similar objective minimized with different loss functions95Two roads divergeComing soon…Discriminative/Conditional modelsSequence models: SummaryGoal: Predict an output sequence given input sequence  Hidden Markov Model  Inference Predict via Viterbi algorithm  Conditional models/discriminative models Local approaches (no inference during training) MEMM, conditional Markov model Global approaches (inference during training) CRF, structured perceptron  To think What are the parts in a sequence model? How is each model scoring these parts?96Same dichotomy for more general structuresPrediction is not always tractable for general structures"
13,"School of Computer Science Probabilistic Graphical Models Variational Inference Eric Xing Lecture 13, February 24, 2014 Reading: See class website a 1 © Eric Xing @ CMU, 2005-2014 Inference Problems  Compute the likelihood of observed data  Compute the marginal distribution over a particular subset of nodes  Compute the conditional distribution for disjoint subsets A and B  Compute a mode of the density  Methods we have Brute force Elimination Message Passing (Forward-backward , Max-product /BP, Junction Tree) Sharing intermediate terms Individual computations independent 2 © Eric Xing @ CMU, 2005-2014 Sum-Product Revisited Tree-structured GMs Message Passing on Trees:  On trees, converge to a unique fixed point after a finite number of iterations 3 © Eric Xing @ CMU, 2005-2014 Junction Tree Revisited General Algorithm on Graphs with Cycles  Steps: B C S => Triangularization => Construct JTs => Message Passing on Clique Trees 4 © Eric Xing @ CMU, 2005-2014 Local Consistency Given a set of functions associated with the cliques and separator sets They are locally consistent if: For junction trees, local consistency is equivalent to global consistency! 5 © Eric Xing @ CMU, 2005-2014 An Ising model on 2-D image  Nodes encode hidden information (patch- identity).  They receive local information from the image (brightness, color).  Information is propagated though the graph over its edges.  Edges encode ‘compatibility’ between nodes. ? air or water ? 6 © Eric Xing @ CMU, 2005-2014 Why Approximate Inference? Why can’t we just run junction tree on this graph? If NxN grid, tree width at least N N can be a huge number(~1000s of pixels)  If N~O(1000), we have a clique with 2100 entries            i i i j i j i ij X X X Z X p 0 exp 1 ) (   7 © Eric Xing @ CMU, 2005-2014 Approaches to inference Exact inference algorithms  The elimination algorithm  Message-passing algorithm (sum-product, belief propagation)  The junction tree algorithms Approximate inference techniques  Variational algorithms  Loopy belief propagation  Mean field approximation  Stochastic simulation / sampling methods  Markov chain Monte Carlo methods 8 © Eric Xing @ CMU, 2005-2014 Loopy Belief Propogation 9 © Eric Xing @ CMU, 2005-2014 i k k k k i j k k k Mki      k i i k x i i j i ij j j i x M x x x x M i ) ( ) ( ) , ( ) (   Compatibilities (interactions) external evidence   k k k i i i i x M x x b ) ( ) ( ) (  Recap: Belief Propagation BP Message-update Rules BP on trees always converges to exact marginals (cf. Junction tree algorithm) 10 © Eric Xing @ CMU, 2005-2014 Beliefs and messages in FG i     ) ( ) ( ) ( ) ( i N a i i a i i i i x m x f x b “beliefs” “messages” a          ) ( \ ) ( ) ( ) ( ) ( ) ( ) ( a N i i a i a a a a a i N c i i c i a i x m X f X b x m x m       i a x X i a N j j a j a a i i a x m X f x m \ \ ) ( ) ( ) ( ) ( 11 © Eric Xing @ CMU, 2005-2014 What if the graph is loopy?  12 © Eric Xing @ CMU, 2005-2014 i k k k k i j k k k Mki Belief Propagation on loopy graphs BP Message-update Rules May not converge or converge to a wrong solution      k i i k x i i j i ij j j i x M x x x x M i ) ( ) ( ) , ( ) (   Compatibilities (interactions) external evidence   k k k i i i i x M x x b ) ( ) ( ) (  13 © Eric Xing @ CMU, 2005-2014 A fixed point iteration procedure that tries to minimize Fbethe Start with random initialization of messages and beliefs  While not converged do  At convergence, stationarity properties are guaranteed  However, not guaranteed to converge! Loopy Belief Propagation     ) ( ) ( ) ( i N a i i a i i x m x b     ) ( ) ( ) ( ) ( a N i i a i a a a a x m X f X b      a i N c i i c i new a i x m x m \ ) ( ) ( ) (       i a x X i a N j j a j a a i new i a x m X f x m \ \ ) ( ) ( ) ( ) ( 14 © Eric Xing @ CMU, 2005-2014 Loopy Belief Propagation If BP is used on graphs with loops, messages may circulate indefinitely But let’s run it anyway and hope for the best …  Empirically, a good approximation is still achievable  Stop after fixed # of iterations  Stop when no significant change in beliefs  If solution is not oscillatory but converges, it usually is a good approximation Loopy-belief Propagation for Approximate Inference: An Empirical Study Kevin Murphy, Yair Weiss, and Michael Jordan. UAI '99 (Uncertainty in AI). ] 15 © Eric Xing @ CMU, 2005-2014 So what is going on? Is it a dirty hack that you bet your luck? 16 © Eric Xing @ CMU, 2005-2014 Approximate Inference  Let us call the actual distribution P  We wish to find a distribution Q such that Q is a “good” approximation to P  Recall the definition of KL-divergence  KL(Q1||Q2)>=0  KL(Q1||Q2)=0 iff Q1=Q2  We can therefore use KL as a scoring function to decide a good Q  But, KL(Q1||Q2) KL(Q2||Q1)    F f a a a X f Z X P ) ( / 1 ) ( ) ) ( ) ( log( ) ( ) || ( 2 1 1 2 1 X Q X Q X Q Q Q KL X   17 © Eric Xing @ CMU, 2005-2014 Which KL?  Computing KL(P||Q) requires inference!  But KL(Q||P) can be computed without performing inference on P  Using ) ) ( ) ( log( ) ( ) || ( X P X Q X Q P Q KL X   ) ( log ) ( ) ( log ) ( X P X Q X Q X Q X X     ) ) ( / 1 log( ) ( ) || (      F f a a Q Q a X f Z E X H P Q KL    F f a a a X f Z X P ) ( / 1 ) (       F f a a Q Q a X f E Z X H ) ( log / 1 log ) ( ) ( log ) ( X P E X H Q Q    18 © Eric Xing @ CMU, 2005-2014 Optimization function  We will call the “Free energy” *  =?  F(P,Q) >= F(P,P) Z X f E X H P Q KL F f a a Q Q a log ) ( log ) ( ) || (       ) , ( Q P F ) , ( Q P F *Gibbs Free Energy ) , ( P P F 19 © Eric Xing @ CMU, 2005-2014 The Energy Functional  Let us look at the functional  can be computed if we have marginals over each fa  is harder! Requires summation over all possible values  Computing F, is therefore hard in general.  Approach 1: Approximate with easy to compute      F f a a Q Q a X f E X H Q P F ) ( log ) ( ) , (  F f a a Q a X f E ) ( log    X Q X Q X Q H ) ( log ) ( ) , ( Q P F  ) , ( Q P F 20 © Eric Xing @ CMU, 2005-2014 Tree Energy Functionals  Consider a tree-structured distribution  The probability can be written as:    involves summation over edges and vertices and is therefore easy to compute    i d i i i a a a x b b b     1 x x) (                i a i i i i i i a a a a a tree b b d b b H x x x x x x ln ln 1                 i a i i i i i i a a a a a a a Tree b b d f b b F x x x x x x x ln ln 1 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 7 3 6 2 5 1 78 67 23 12 .. F F F F F F F F F F            21 © Eric Xing @ CMU, 2005-2014 Bethe Approximation to Gibbs Free Energy  For a general graph, choose  Called “Bethe approximation” after the physicist Hans Bethe  Equal to the exact Gibbs free energy when the factor graph is a tree  In general, HBethe is not the same as the H of a tree Betha F Q P F   ) , ( 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 8 6 2 5 1 78 67 23 12 2 2 F F F F F F F F F Fbethe           .. ..             betha a a i i i i i i a a a a a a a Bethe H f b b d f b b F i a          x x x x x x x x ln ln 1                i a i i i i i i a a a a a Bethe b b d b b H x x x x x x ln ln 1 22 © Eric Xing @ CMU, 2005-2014 Bethe Approximation Pros:  Easy to compute, since entropy term involves sum over pairwise and single variables Cons:  may or may not be well connected to  It could, in general, be greater, equal or less than Optimize each b(xa)'s.  For discrete belief, constrained opt. with Lagrangian multiplier  For continuous belief, not yet a general formula  Not always converge bethe F Q P F   ) , ( ) , ( Q P F ) , ( Q P F 23 © Eric Xing @ CMU, 2005-2014 Bethe Free Energy for FG                 i a i i i i i i a a a a a a a Betha b b d f b b F x x x x x x x ln ln 1                i a i i i i i i a a a a a Bethe b b d b b H x x x x x x ln ln 1   betha a a Bethe H f F    x 24 © Eric Xing @ CMU, 2005-2014 Minimizing the Bethe Free Energy   Set derivative to zero                      a x X a a i i a N i x i ai x i i i i Bethe i a i i X b x b x x b F L \ ) ( } ) ( 1 {   25 © Eric Xing @ CMU, 2005-2014                      a x X i i a a a N i x i ai x i i i i Bethe i a i i x b X b x x b F L \ ) ( } 1 ) ( {   0 ) (    i i x b L             ) ( ) ( 1 1 exp ) ( i N a i ai i i i x d x b  0 ) (    a a X b L              ) ( ) ( ) ( exp ) ( a N i i ai a a a a x X E X b  Constrained Minimization of the Bethe Free Energy 26 © Eric Xing @ CMU, 2005-2014 Bethe = BP on FG We had: Identify to obtain BP equations: i     ) ( ) ( ) ( ) ( i N a i i a i i i i x m x f x b “beliefs” “messages” a      ) ( \ ) ( ) ( ) ( ) ( a N i a i N c i i c a a a a x m X f X b The “belief” is the BP approximation of the marginal probability.             ) ( ) ( 1 1 exp ) ( i N a i ai i i i x d x b               ) ( ) ( ) ( log exp ) ( a N i i ai a a a a x X f X b         a i N b i i b i a i i ai x m x m x ) ( ) ( log )) ( log( ) (  27 © Eric Xing @ CMU, 2005-2014 Using , ) ( ) ( \    i a x X a a i i a X b x b we get a i          i a x X i a N j a j N b j j b a a i i a x m X f x m \ \ ) ( \ ) ( ) ( ) ( ) ( i a = BP Message-update Rules ( A sum product algorithm ) 28 © Eric Xing @ CMU, 2005-2014 Summary so far     F f a a a X f Z X P ) ( / 1 ) (      F f a a Q Q a X f E X H Q P F ) ( log ) ( ) , (                  i a i i i i i i a a a a a a a b b d b f b Q P F x x x x x x x log 1 log ) , (             ) ( ) ( 1 1 exp ) ( i N a i ai i i i x d x b               ) ( ) ( ) ( log exp ) ( a N i i ai a a a a x X f X b  a 29 © Eric Xing @ CMU, 2005-2014 For a distribution p(X|) associated with a complex graph, computing the marginal (or conditional) probability of arbitrary random variable(s) is intractable Variational methods  formulating probabilistic inference as an optimization problem:   ) , ( min arg * q p F q Betha q S   on distributi y probabilit ) (tractable a : q The Theory Behind LBP             bethe a a i i i i i i a a a a a a a Bethe H f b b d f b b F i a          x x x x x x x x ln ln 1 30 © Eric Xing @ CMU, 2005-2014  But we do not optimize q(X) explicitly, focus on the set of beliefs  e.g.,  Relax the optimization problem  approximate objective:  relaxed feasible set:  The loopy BP algorithm:  a fixed point iteration procedure that tries to solve b*   ) ( min arg * b F E b b b o   M )} ( ), , ( { , i i j i j i x b x x b b      ) (b F H q  o M M  ) ( M M  o The Theory Behind LBP 31 © Eric Xing @ CMU, 2005-2014  But we do not optimize q(X) explicitly, focus on the set of beliefs  e.g.,  Relax the optimization problem  approximate objective:  relaxed feasible set:  The loopy BP algorithm:  a fixed point iteration procedure that tries to solve b*   ) ( min arg * b F E b b b o   M )} ( ), , ( { , i i j i j i x b x x b b      ) , ( , i j i Betha b b H H          i i x j j i x i o x x x x ) ( ) , ( , ) ( |     1 0 M The Theory Behind LBP 32 © Eric Xing @ CMU, 2005-2014 Mean Field Approximation 33 © Eric Xing @ CMU, 2005-2014 Naïve Mean Field Fully factorized variational distribution 34 © Eric Xing @ CMU, 2005-2014 Naïve Mean Field for Ising Model  Optimization Problem  Update Rule  resembles “message” sent from node to  forms the “mean field” applied to from its neighborhood 35 © Eric Xing @ CMU, 2005-2014  Optimize q(XH) in the space of tractable families  i.e., subgraph of Gp over which exact computation of Hq is feasible Tightening the optimization space  exact objective:  tightened feasible set: q H T Q  q q q H E q    min arg * T ) ( Q T  Mean field methods 36 © Eric Xing @ CMU, 2005-2014 )] ( [ X p G )}] ( [{ c c X q G Exact: Clusters: (intractable) Cluster-based approx. to the Gibbs free energy (Wiegerinck 2001, Xing et al 03,04) 37 © Eric Xing @ CMU, 2005-2014 Mean field approx. to Gibbs free energy Given a disjoint clustering, {C1, … , CI}, of all variables Let Mean-field free energy  Will never equal to the exact Gibbs free energy no matter what clustering is used, but it does always define a lower bound of the likelihood Optimize each qi(xc)'s.  Variational calculus …  Do inference in each qi(xc) using any tractable algorithm ), ( ) ( i i i q q C X X             i C i C i i C i C i i C i i i i C i q q E q G x x x x x x ln ) ( MF            i x i i i i x i j i j i x x j i i i j i x q x q x x q x x x q x q G ln ) ( ) ( e.g., MF   (naïve mean field) 38 © Eric Xing @ CMU, 2005-2014 ) , | ( ) ( , , , , * i j i i i i q MB H C E C H C H i p q   X x X X Theorem: The optimum GMF approximation to the cluster marginal is isomorphic to the cluster posterior of the original distribution given internal evidence and its generalized mean fields: GMF algorithm: Iterate over each qi The Generalized Mean Field theorem 39 © Eric Xing @ CMU, 2005-2014 [xing et al. UAI 2003] A generalized mean field algorithm 40 © Eric Xing @ CMU, 2005-2014 [xing et al. UAI 2003] A generalized mean field algorithm 41 © Eric Xing @ CMU, 2005-2014 Theorem: The GMF algorithm is guaranteed to converge to a local optimum, and provides a lower bound for the likelihood of evidence (or partition function) the model. Convergence theorem 42 © Eric Xing @ CMU, 2005-2014 Gibbs predictive distribution:                 i j i j i ij i i i i A x X X x X p N  0 exp ) | ( }) : { | ( i j i j x X p N   j x j x mean field equation: }) : { | ( exp ) ( i j i j i q j i ij i i i i j X X p A X X X X q j q i j N N                     0 j q j X } : { i j j X j q N    Xi Approximate p(X) by fully factorized q(X)=Piqi(Xi) For Boltzmann distribution p(X)=exp{i < j qijXiXj+qioXi}/Z : Xi xjqj resembles a “message” sent from node j to i {xjqj : j Ni} forms the “mean field” applied to Xi from its neighborhood } : { i q j j X j N    j q j X The naive mean field approximation 43 © Eric Xing @ CMU, 2005-2014 Cluster marginal of a square block Ck:                      k k MBC k k MB j k C i k C k k C j i X q j i ij C i i i j i ij C X X X X X X q , ) ( ' , , ' exp ) (    0 Virtually a reparameterized Ising model of small size. Example 1: Generalized MF approximations to Ising models 44 © Eric Xing @ CMU, 2005-2014 GMF approximation to Ising models GMF2x2 GMF4x4 BP Attractive coupling: positively weighted Repulsive coupling: negatively weighted 45 © Eric Xing @ CMU, 2005-2014 GMFr GMFb BP Example 2: Sigmoid belief network 46 © Eric Xing @ CMU, 2005-2014 Example 3: Factorial HMM 47 © Eric Xing @ CMU, 2005-2014 Automatic Variational Inference  Currently for each new model we have to  derive the variational update equations  write application-specific code to find the solution  Each can be time consuming and error prone  Can we build a general-purpose inference engine which automates these procedures? ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... fHMM Mean field approx. Structured variational approx.  48 © Eric Xing @ CMU, 2005-2014 a general, iterative message passing algorithm clustering completely defines approximation  preserves dependencies  flexible performance/cost trade-off  clustering automatable recovers model-specific structured VI algorithms, including:  fHMM, LDA  variational Bayesian learning algorithms easily provides new structured VI approximations to complex models Cluster-based MF (e.g., GMF) 49 © Eric Xing @ CMU, 2005-2014 "
130,"Page 1CIS 700Advanced Machine Learning for NLPA First Look at Structures Dan Roth Department of Computer and Information Science University of PennsylvaniaAugmented and modified by Vivek SrikumarAdmin Stuff2Next Tuesday we will start with students’ presentation of papers. If you haven’t done so, please choose a paper TODAY  Presentations: 20 Min + 10 Min discussion Please send your presentation to me no later than 2 days before the presentation  Sunday evening/Tuesday Evening (Tuesday/Thursday Presentation) Come to the office hour to discuss it once you get comments You will be asked to grade the presentations of all other students  Critical Reviews First one is due next week, 9/26 < 2p; pdf; make it look like a paper; try to make it interesting.   Grading:  Project 40%, Reviews 30% (12,6,6,6), Presentation 20%, Participation 10%  Project Proposals are due on 9/28 Come to office hours to discuss itSo far…Binary classifiers Output: 0/1  Multiclass classifiers Output: one of a set of labels  Decomposed learning vs. joint/global learning  Winner-take-all prediction for multiclass 3Sequence Tagging   Naïve approach (actually works pretty well for POS tagging) Multiple extensions What are the issues that come up?  The cat sat  on  the mat . DT	  NN VBD	IN	DT	NN	. Brings up two key issues:  1. Large space of outputs Prediction: y* = argmaxy 2 Y wT Á ( x) Since Y  is large, Prediction needs to be more sophisticated. We call it Inference.  And, can’t train a separate weight vector for each possible output as we did in multiclass  2. If you want to decompose the output and deal with a smaller Y   Need to account for (in) consistencies If the various Y = (y1, y2, …, yk) components depend on each other, how would you predict one without knowledge of the others? 	 Prediction: y* = argmaxy 2 Y wT Á ( x, y) This issue will come up both in training and at decision time.  Structured Prediction5Decomposing the outputWe need to produce a graph In general, we cannot enumerate all possible graphs for the argmax  Solution: Think of the graph as combination of many smaller parts The parts should agree with each other in the final output Each part has a score The total score for the graph is the sum of scores of each part  Decomposition of the output into parts also helps generalization Why?6Decomposing the output: Example7Setting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeNote: The output  y is a labeled assignment of the nodes and edges                          ,                            ,                            ,…     The input x not shown hereThe scoring function (via the weight vector) scores outputs  For generalization and ease of inference, break the output into parts and score each part The score for the structure is the sum of the part scores  What is the best way to do this decomposition? Depends….Decomposing the output: Example8One option: Decompose fully. All nodes and edges are independently scoredSetting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeStill need to ensure that the colored edges form a valid output (i.e. a tree)Linear functionsDecomposing the output: Example9One option: Decompose fully. All nodes and edges are independently scoredSetting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeStill need to ensure that the colored edges form a valid output (i.e. a tree)  This is invalid output! Even this simple decomposition requires inference to ensure validityDecomposing the output: Example10Setting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeAnother possibility:  Score each edge and its nodes together And many other edges…  Each patch represents piece that is scored independentlyLinear functionDecomposing the output: Example11Setting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeAnother possibility:  Score each edge and its nodes together And many other edges…  Each patch represents piece that is scored independentlyInference should ensure that  The output is a tree, and  Shared nodes have the same label in all the piecesDecomposing the output: Example12Setting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeAnother possibility:  Score each edge and its nodes together And many other edges…  Each patch represents piece that is scored independentlyInference should ensure that  The output is a tree, and  Shared nodes have the same label in all the partsInvalid! Two parts disagree on the label for this nodeDecomposing the output: Example13Setting Output: Nodes and edges are labeled and the blue and orange edges form a tree  Goal: Find the highest scoring treeWe have seen two examples of decomposition  Many other decompositions possible…InferenceEach part is scored independently Key observation: Number of possible inference outcomes for each part may not be large Even if the number of possible structures might be large  Inference: How to glue together the pieces to build a valid output? Depends on the “shape” of the output  Computational complexity of inference is important Worst case: intractable With assumptions about the output, polynomial algorithms exist.  We may encounter some examples in more detail: Predicting sequence chains: Viterbi algorithm To parse a sentence into a tree: CKY algorithm In general, might have to either live with intractability or approximate 14Questions?Training regimesDecomposition of outputs gives two approaches for training Decomposed training/Learning without inference Learning algorithm does not use the prediction procedure during training  Global training/Joint training/Inference-based training Learning algorithm uses the final prediction procedure during training  Similar to the two strategies we had before with multiclass  Inference complexity often an important consideration in choice of modeling and training Especially so if full inference plays a part during training Ease of training smaller/less complex models could give intermediate training strategies between fully decomposed and fully joint 15Example 1: Semantic Role LabelingBased on the dataset PropBank [Palmer et. al. 05]  Large human-annotated corpus of verb semantic relations The task: To predict arguments of verbs 16Given a sentence, identifies who does what to whom, where and when.The bus was heading for Nairobi in KenyaRelation: Head Mover[A0]: the bus Destination[A1]: Nairobi in KenyaPredicateArgumentsOne important lesson in the following examples is that “consistency” is actually a loaded term. In most interesting cases it would mean “coherency via some knowledge.Predicting verb argumentsIdentify candidate arguments for verb using parse tree Filtered using a binary classifier Classify argument candidates Multi-class classifier (one of multiple labels per candidate) Inference Using probability estimates from argument classifier Must respect structural and linguistic constraints Eg: No overlapping argumentsThe bus was heading for Nairobi in Kenya.17Inference: verb argumentsThe bus was heading for Nairobi in Kenya.18Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 2.019heading (The bus,  		for Nairobi,  		for Nairobi in Kenya)Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 1.920heading (The bus,  		for Nairobi in Kenya)Total: 2.0Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 1.921heading (The bus,  		for Nairobi in Kenya)Structured output is…A data structure with a pre-defined schema Eg: SRL converts raw text into a record in a database    Equivalently, a graph  Often restricted to be a specific family of graphs: chains, trees, etc   22HeadThe busNairobi in KenyaA0A1Questions/comments?Example 2: Object detection23Photo by Andrew Dressel - Own work. Licensed under Creative Commons Attribution-Share Alike 3.0How would you design a predictor that labels all the parts using the tools we have seen so far?A good illustration for why one might want decomposition – transfer. E.g., a wheel classifier might be useful in many other tasks.One approach to build this structure24Photo by Andrew Dressel - Own work. Licensed under Creative Commons Attribution-Share Alike 3.0Final output: Combine the predictions of these individual classifiers (local classifiers)  The predictions interact with each other  Eg: The same box can not be both a left wheel and a right wheel, handle bar does not overlap with seat, etc  Need inference to compose  the outputExample 3: Sequence labelingInput: A sequence of tokens (like words) Output: A sequence of labels of same length as input  Eg: Part-of-speech tagging:  	Given a sentence, find parts-of-speech of all the words 25TheDeterminerFedNounraisesVerbinterestNounratesNounVerb  (I fed the dog)Verb (Poems don’t interest me)Verb (He rates movies online)Other possible tags in different contexts,More on this in next lecturePart-of-speech taggingGiven a word, its label depends on : The identity and characteristics of the word Eg. Raises is a Verb because it ends in –es (among other reasons)  Its grammatical context Fed in “The Fed” is a Noun because it follows a Determiner Fed in “I fed the..” is a Verb because it follows a Pronoun 26Each output label is dependent on its neighbors in addition to the inputOne possible model:More examplesProtein 3D structure prediction    Inferring layout of a room     27Image from [Schwing et al 2013]Structured output is…A graph, possibly labeled and/or directed Possibly from a restricted family, such as chains, trees, etc. A discrete representation of input Eg. A table, the SRL frame output, a sequence of labels etc  A collection of inter-dependent decisions Eg: The sequence of decisions used to construct the output The result of a combinatorial optimization problem argmaxy 2 all outputsscore(x, y) We have seen something similar before in the context of multiclass28RepresentationProceduralThere are a countable number of graphs Question: Why can’t we treat each output as a label and train/predict as multiclass?Challenges with structured outputTwo challenges We cannot train a separate weight vector for each possible inference outcome For multiclass, we could train one weight vector for each label   We cannot enumerate all possible structures for inference Inference for multiclass was easy  Solution Decompose the output into parts that are labeled Define  how the parts interact with each other how labels are scored for each part  an inference algorithm to assign labels to all the parts29Computational issues: Reprise30Different algorithms,  Exact vs approximate inferenceData annotation difficultySemi-supervised/indirectly supervised?Different algorithms,  joint vs decomposed learningTask dependent, of course Training and inference concerns drive this tooSummaryWe saw several examples of structured output  Structures are graphs Sometimes useful to think of them as a sequence of decisions Also useful to think of them as data structures  Multiclass is the simplest type of structure Lessons from multiclass are useful  Modeling outputs as structures Decomposition of the output, inference, training31Questions?Inference: given input x (a document, a sentence),                           predict the best structure y = {y1,y2,…,yn} 2 Y  (entities & relations) Assign values to the y1,y2,…,yn, accounting for dependencies among yis Inference is expressed as a maximization of a scoring function                             y’ = argmaxy 2 Y wT Á (x,y)    Inference requires, in principle, touching all y 2 Y at decision time, when we are given x 2 X and attempt to determine the best y 2 Y for it, given w  For some structures, inference is computationally easy.  Eg: Using the Viterbi algorithm  In general, NP-hard (can be formulated as an ILP) Structured Prediction: InferenceJoint features on inputs and outputsFeature Weights (estimated during learning)Set of allowed structuresPlacing in context: a crash course in structured prediction (skip)Page 32Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss.  Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):    Page 33Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss.  Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):     We call these conditions the learning constraints.  In most learning algorithms used today, the update of the weight vector w is done in an on-line fashion,  Think about it as Perceptron; this procedure applies to Structured Perceptron, CRFs, Linear Structured SVM W.l.o.g. (almost) we can thus write the generic structured learning algorithm as follows:Score of annotated structureScore of any other structurePenalty for predicting other structure8 yPage 34In the structured case, prediction (inference) is often intractable but needs to be done many timesStructured Prediction: Learning AlgorithmFor each example (xi, yi)   Do: (with the current weight vector w) Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y wT Á ( xi ,y) Check the learning constraints Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndForPage 35Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution I: decompose the scoring function to EASY and HARD partsEASY: could be feature functions that correspond to an HMM, a linear CRF,   or even ÁEASY (x,y) = Á(x), omiting dependence on y, corresponding to classifiers. May not be enough if the HARD part is still part of each inference step.Page 36Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution II: Disregard some of the dependencies: assume a simple model.Page 37Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)   This is the most commonly used solution in NLP todaySolution III: Disregard some of the dependencies during learning; take into account at decision timePage 38Next steps…Sequence prediction Markov model Predicting a sequence  Viterbi algorithm Training MEMM, CRF, structured perceptron for sequences After sequences General representation of probabilistic models Bayes Nets and Markov Random Fields Generalization of global training algorithms to arbitrary conditional models Inference techniques More on Conditional models, constraints on inference39"
131,"CIS 700Advanced Machine Learning Structured Machine Learning:  Theory and Applications in Natural Language ProcessingShyam Upadhyay Department of Computer and Information Science University of Pennsylvania Page 1ReminderGoogle form for filling in preferences for paper presentation  Deadline 19th September  Fill in 4 papers, each from a different section.  No class on Thursday (reading assignment on MEMM, CRF) Today’s PlanIngredients of Structured Prediction  Structured Prediction Formulation Multiclass Classification  HMM Seq. Labeling  Dependency Parsing  Structured Perceptron    Ingredients of a Structured Prediction ProblemInput  Output  Feature Extractor  Inference Inference                                                       , also called “Decoding”  Loss  Ingredients of a Structured Prediction ProblemInput IInstance in SL Output IStructure in SL Feature Extractor AbstractFeatureGenerator in SL Inference Inference  Loss  AbstractInferenceSolver in SLMulticlass Classification (Toy)N training examples, and we need to predict the label from M different classes.    Winner takes all. Maintain M different weight vectors. Score a example using each of the weight vectors. Predict the class whose score is highest.  QuestionsWhat is   What is  How to write the score function as                                     ?     How to do inference?SolutionSee it in CodeShort Quiz How will you implement the error correcting code approach to multiclass classification? How does definition of      or        change? How does definition of weight vector change? How does inference change?   Write binary classification as structured prediction Try to avoid redundant weights.Sequence Tagging   Naïve approach  Local inference (actually works pretty well for POS tagging) The	cat	sat	on	the	mat	. DT	NN	VBD	IN	DT	NN	. Sequence TaggingHMM     Roth 1999, Collins 2002    How to write this as a structured prediction problem?  QuestionsWhat is   What is  How to write the score function as                                     ?  Should respect the model    How to do inference?Solution – Weight VectorSolution – Feature VectorInference in HMMGreedy Choose current position’s tag so that is maximizes the score so far.   Viterbi Use Dynamic Programming to incrementally compute,   Sampling MCMC  (HW) HMM with Greedy and MCMC.   See it in CodeShort Quiz – Naïve BayesRecall Naïve Bayes classification,      How to formulate this as structured prediction?  Assume there are only two classes.    (HW) Implement Naïve Bayes in Illinois-SL.Assume there are only two classesDependency ParsingBorrowed from Graham Neubig’s slidesTyped and UntypedBorrowed from Graham Neubig’s slidesBefore we proceed, convince yourself that this is a (directed) tree.Learning ProblemSetup INPUT: A sentence with N words.  OUTPUT: A directed tree representing the dependency relations.  Given input x there are a fixed # of legal candidate trees.  Search Space Find the highest scoring dependency tree, from the space of all dependency trees of N words.  How big is the search space? Exponential # of candidates!QuestionsWhat is   What is  How to write the score function as                                     ?  Unlike Multiclass, cannot learn a different model for each tree    How to do inference?Learn a model to score edge (i,j) of a candidate tree S[i][j] = score of word i having the word j as a parent.    Score of a dependency tree is sum of score of its edges.     Can you think of features for a edge?Decompose the ScoreFinding Highest Scoring TreeCast as a directed maximum spanning tree problem.  Compute a matrix S of edge scores.  Chu-Liu-Edmonds Algorithm (black-box).  Our First Structured Learning AlgorithmSo far, we just identified what ingredients we needed.  How to learn a weight vector?  Structured Perceptron (Collins 2002) Structured Version of Binary Perceptron  Mistake Driven, just like Perceptron  Virtually Identical Binary PerceptronStructured PerceptronShort QuizWhy do we not have a bias term?  Do we see all possible structures during training?  AbstractInferenceSolver Where was this used?  Learning requires inference over structures Such inference can prove costly for large search spaces. Think improvements.Averaged Structured PerceptronRemember we do not want to use only one weight vector. Why?  Naïve way of averaging. Maintain a list of weight vectors seen during training. Maintain counts of how many examples each vector “survived”. Compute weighted average at the end. Drawbacks?   Better way? I only want to maintain O(1) weight vectors and make updates only when necessary.AveragingSay we make the ith update at time ci  Weight vector after ith update is wi  The algorithm stops at time cT and the last mistake was made at time cn What is the weighted average? Averaged Structured PerceptronWhat We Learned TodayIngredients for Structured Prediction  Toy Formulations  Our First Learning Algorithm for Structured PredictionHW for Today’s LectureRequired Reading Ming-Wei Chang’s Thesis Chapter 2 (most of today’s lecture) Hal Daume’s Thesis Chapter 2 (structured perceptron) M. CollinsDiscriminative Training for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms EMNLP 2002. Optional Reading L. Huang, S. Fayong, Y. GuoStructured Perceptron with Inexact SearchNAACL 2012. Try implementation exercises given in the slides.  "
132,"CIS 700Advanced Machine Learning Structured Machine Learning:  Structured Learning with Latent VariablesShyam Upadhyay Department of Computer and Information Science University of Pennsylvania Page 1Ingredients of a Structured Prediction ProblemInput  Output  Feature Extractor  Inference Inference                                                       , also called “Decoding”  Loss  Ingredients of a Structured Prediction Problem (General)Input  Output  Feature Extractor  Inference Inference                                                       , also called “Decoding”  Loss  Loss-sensitive inference   Some tasks have inherent latent structure connecting the input to the output. “Latent”- not provided at training time (expensive to annotate). Predicting a “good” latent structure can improve final prediction       Positing a good latent structure is task-dependent (requires background knowledge)Intuition Behind Latent ModelingPredicting Latent StructuresSome latent structures are invalid. Cannot make local decisions, need to predict the structure jointly.  Cannot always enumerate latent structures (trees etc.). How to predict a good latent structure which helps predict the gold output?Quick Detour To EMSoft E Step   Generate a distribution over latent variables Does not commit to a single latent variable.  M Step    Example Soft K-Means Clustering, Rabiner’s HMM algorithm …Soft EMExample – Soft ClusteringEach point can be “explained” by several clusters. Hard EMHard E Step   Pick only the best latent variable. (Spike Distribution) M Step    M Step actually did not change. Example K-Means ClusteringHard ClusteringEach point is “explained” by exactly one cluster. SSVM and Latent SSVMStructured SVMLatent Structured SVMReformulationThe best explaining hidden variable given the gold output and inputThe best explaining hidden variable given the inputThis is like Hard EMWe have 2 inference problems!Ingredients of a Structured Prediction Problem with Latent VariablesInput	Output		Latent structure  Feature Extractor Inference Hidden-Variable Completion   Joint Inference over output and latent structure (“decoding”)   Loss  Generalized to include latent variable, but can ignore it too. Optimization  Difference of convex functions (or sum of a convex and concave). CCCP (Concave-Convex Procedure)     Matching points on the convex and concave terms which have the same tangent.       Example of CCCPLearning AlgorithmM - StepE - StepInterpreting the UpdatesWhat is added to the weight vector       Note similarity to the Structured Perceptron/SVM update.  Note the update also improves the next E step (better latent representation will chosen).Feature of the latent structure inferred using gold outputFeature of the latent structure inferred jointlyInitializationThe latent SSVM problem is non-convex (convince yourself)  Initialization matters.  How to find a Good Initialization?  How to guide the model better? Discriminative Learning over Constrained Latent Representations  Ming-Wei Chang, Dan Goldwasser, Dan Roth, Vivek Srikumar Structured Output Learning with Indirect Supervision  Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser, Dan RothQuestions?"
133,"Pranking with Ranking Koby Crammer and Yoram Singer Presented by : Soham Dan Content and some ﬁgures borrowed from [Crammer, Koby, and Yoram Singer. Pranking with ranking.NIPS. 2002] and talk slides. Introduction ▶Problem ▶Input : Sequence of instance-rank pairs (x1, y 1)...(xt, y t) ▶Output : A model(essentially a rank prediction rule) which assigns to each instance a rank. ▶Goal: To have the predicted rank as close as possible to the true rank. ▶Note : The ranks need not be unique! Introduction ▶Problem ▶Input : Sequence of instance-rank pairs (x1, y 1)...(xt, y t) ▶Output : A model(essentially a rank prediction rule) which assigns to each instance a rank. ▶Goal: To have the predicted rank as close as possible to the true rank. ▶Note : The ranks need not be unique! ▶Similarity with ▶Classiﬁcation Problems : Assign one of k possible labels to a new instance. ▶Regression Problems : Set of k labels is structured as there is a total order relation between labels. Introduction ▶Problem ▶Input : Sequence of instance-rank pairs (x1, y 1)...(xt, y t) ▶Output : A model(essentially a rank prediction rule) which assigns to each instance a rank. ▶Goal: To have the predicted rank as close as possible to the true rank. ▶Note : The ranks need not be unique! ▶Similarity with ▶Classiﬁcation Problems : Assign one of k possible labels to a new instance. ▶Regression Problems : Set of k labels is structured as there is a total order relation between labels. Natural Settings to rank / rate instances Information Retrieval , Collaborative Filtering Problem Figure 1: Movie rating prediction (Example : Netﬂix challenge) Possible Solutions ▶Cast as a regression or classiﬁcation problem Possible Solutions ▶Cast as a regression or classiﬁcation problem ▶Reduce a total order into a set of preference over pairs. Drawback : Sample size blowup from n to Ø(n2). Also, no easy adaptation for online settings. Possible Solutions ▶Cast as a regression or classiﬁcation problem ▶Reduce a total order into a set of preference over pairs. Drawback : Sample size blowup from n to Ø(n2). Also, no easy adaptation for online settings. ▶PRank Algorithm : Directly maintains totally ordered set by projection of instances into reals, associating ranks with distinct sub-intervals of the reals and adapting the support of each subinterval while learning. Problem Setup ▶Input Stream: Sequence of instance-rank pairs (x1, y1)...(xt, yt) where each instance xt ∈Rn. Corresponding rank yt ∈Y which is a ﬁnite set with a total order relation (structured) . W.l.o.g. Y = 1, 2, 3..., k with > as the order relation. 1 ≺2 ≺... ≺k Problem Setup ▶Input Stream: Sequence of instance-rank pairs (x1, y1)...(xt, yt) where each instance xt ∈Rn. Corresponding rank yt ∈Y which is a ﬁnite set with a total order relation (structured) . W.l.o.g. Y = 1, 2, 3..., k with > as the order relation. 1 ≺2 ≺... ≺k ▶Ranking Rule (H) : Mapping from instances to ranks, Rn →Y. The family of ranking rules considered here : w ∈Rn and k thresholds : b1 ≤b2 ≤... ≤bk = ∞ Problem Setup ▶Input Stream: Sequence of instance-rank pairs (x1, y1)...(xt, yt) where each instance xt ∈Rn. Corresponding rank yt ∈Y which is a ﬁnite set with a total order relation (structured) . W.l.o.g. Y = 1, 2, 3..., k with > as the order relation. 1 ≺2 ≺... ≺k ▶Ranking Rule (H) : Mapping from instances to ranks, Rn →Y. The family of ranking rules considered here : w ∈Rn and k thresholds : b1 ≤b2 ≤... ≤bk = ∞ ▶Given a ranking rule deﬁned by w and b, the predicted rank (ˆ yt) on a new instance x is H(x) = minr∈1,2,..,k {r : w · x −br < 0} Problem Setup ▶Input Stream: Sequence of instance-rank pairs (x1, y1)...(xt, yt) where each instance xt ∈Rn. Corresponding rank yt ∈Y which is a ﬁnite set with a total order relation (structured) . W.l.o.g. Y = 1, 2, 3..., k with > as the order relation. 1 ≺2 ≺... ≺k ▶Ranking Rule (H) : Mapping from instances to ranks, Rn →Y. The family of ranking rules considered here : w ∈Rn and k thresholds : b1 ≤b2 ≤... ≤bk = ∞ ▶Given a ranking rule deﬁned by w and b, the predicted rank (ˆ yt) on a new instance x is H(x) = minr∈1,2,..,k {r : w · x −br < 0} ▶Algorithm makes a mistake on instance xt if ˆ yt ̸= yt and loss on that input is |ˆ yt −yt|. ▶Loss after T rounds is PT t=1 |ˆ yt −yt| Perceptron Recap Overview of Algorithm ▶Online Algorithm Overview of Algorithm ▶Online Algorithm ▶In each round the ranking algorithm ▶Gets an input instance ▶Outputs the rank as prediction ▶Receives the correct rank value ▶If there is an error ▶Computes loss ▶Updates the rank-prediction rule Overview of Algorithm ▶Online Algorithm ▶In each round the ranking algorithm ▶Gets an input instance ▶Outputs the rank as prediction ▶Receives the correct rank value ▶If there is an error ▶Computes loss ▶Updates the rank-prediction rule ▶Conservative or Mistake driven algorithm :The algorithm updates its ranking rule only on rounds on which it made ranking mistakes. Overview of Algorithm ▶Online Algorithm ▶In each round the ranking algorithm ▶Gets an input instance ▶Outputs the rank as prediction ▶Receives the correct rank value ▶If there is an error ▶Computes loss ▶Updates the rank-prediction rule ▶Conservative or Mistake driven algorithm :The algorithm updates its ranking rule only on rounds on which it made ranking mistakes. ▶No statistical assumptions over data.The algorithm should do well irrespectively of speciﬁc sequence of inputs and target labels Algorithm Illustration Algorithm Illustration Algorithm Illustration Algorithm Illustration Algorithm Illustration Algorithm Illustration Algorithm Illustration Algorithm Figure 2: The PRank Algorithm Algorithm Figure 2: The PRank Algorithm ▶Rank y is expanded into k −1 virtual variables y1, .., yk−1, where yr = +1 if w · x > br and yr = −1 otherwise. ▶On mistakes, b and w · x are moved towards each other. Analysis 1. Lemma : Order Preservation 2. Theorem : Mistake Bound Lemma : Order Preservation Can this happen ? Lemma : Order Preservation Can this happen ? NO Lemma : Order Preservation Can this happen ? NO Let wt and bt be the current ranking rule, where bt 1 ≤... ≤bt k−1 and let (xt, yt) be an instance-rank pair fed to PRank on round t. Denote by wt+1 and bt+1 the resulting ranking rule after the update of PRank, then bt+1 1 ≤... ≤bt+1 k−1 Lemma : Order Preservation Let wt and bt be the current ranking rule, where bt 1 ≤... ≤bt k−1 and let (xt, yt) be an instance-rank pair fed to PRank on round t. Denote by wt+1 and bt+1 the resulting ranking rule after the update of PRank, then bt+1 1 ≤... ≤bt+1 k−1 Proof Sketch : ▶bt r are integers for all r and t since for all r we initialize b1 r = 0, and bt+1 r −bt r ∈{−1, 0, +1}. ▶Proof by Induction : Showing bt+1 r+1 ≥bt+1 r is equivalent to proving bt r+1−bt r ≥yt r+1[(wt·xt−bt r+1)yt r+1 ≤0]−yt r [(wt·xt−bt r)yt r ≤0] Lemma : Order Preservation Theorem : Mistake Bound Let (xl, y1), ..., (xT, yT) be an input sequence for PRank where xt ∈Rn and yt ∈l, ..., k. Denote by R2 = maxt ||xt||2. Assume that there is a ranking rule v∗= (w∗, b∗) with b∗ 1 ≤... ≤b∗ k−1 of a unit norm that classiﬁes the entire sequence correctly with margin γ = minr,t (w∗· xt −b∗ r )yt r > 0. Then, the rank loss of the algorithm PT t=1 |ˆ yt −yt|, is at most (k−1)(R2+1) γ2 . Proof of Theorem ▶wt+1 = wt + (P r τ t r )xt and bt+1 r = bt r −τ t r ▶Let nt = |ˆ yt −yt| be diﬀerence between the true rank and the predicted rank. Clearly, nt = P r |τ t r | ▶To prove the theorem we bound P t nt from above by bounding ||vt||2 from above and below. ▶v∗· vt+1 = v∗· vt + Pk−1 r=1 τ t r (w∗xt −b∗ r ) ▶Pk−1 r=1 τ t r (w∗xt −b∗ r ) ≥ntγ = ⇒v∗vT+1 ≥γ P t nt = ⇒ ||vT+1||2 ≥γ2(P t nt)2 ▶To bound the norm of v from above : ▶||vt+1||2 = ||wt||2 + ||bt||2 + 2 P r τ t r (wt · xt −bt r) + (P r τ t r )2||xt||2 + P r(τ t r )2 ▶Since, (P r τ t r )2 ≤(nt)2 and P r(τ t r )2 = nt ▶||vt+1||2 = ||vt||2 + 2 P r τ t r (wt · xt −bt r) + (nt)2||xt||2 + nt ▶P r τ t r (wt ·xt −bt r) = P r[(wt ·xt −bt r) ≤0](wt ·xt −bt r)yr ≤0 ▶Since, ||xt||2 ≤R2 = ⇒||vt+1||2 = ||vt||2 + (nt)2R2 + nt ▶Using the lower bound, we get, P t nt ≤R2[P t(nt)2]/[P t nt]+1 γ2 ▶nt ≤k −1 = ⇒P t(nt)2 ≤(k −1) P t nt = ⇒P t nt ≤ (k−1)(R2+1) γ2 Experiments Experiments Experiments ▶Models ▶Multi-class Generalization of Perceptron (MCP) : kn parameters : under-constrained ▶Widrow HoﬀAlgorithm for Online Regression (WH): n parameters : over-constrained ▶PRank : n + k −1 parameters : accurately constrained Experiments ▶Models ▶Multi-class Generalization of Perceptron (MCP) : kn parameters : under-constrained ▶Widrow HoﬀAlgorithm for Online Regression (WH): n parameters : over-constrained ▶PRank : n + k −1 parameters : accurately constrained ▶Datasets ▶Synthetic dataset ▶EachMovie dataset-used for collaborative ﬁltering tasks ▶Evaluation in batch setting- outperforms multi-class SVM, SVR Experiments ▶Models ▶Multi-class Generalization of Perceptron (MCP) : kn parameters : under-constrained ▶Widrow HoﬀAlgorithm for Online Regression (WH): n parameters : over-constrained ▶PRank : n + k −1 parameters : accurately constrained ▶Datasets ▶Synthetic dataset ▶EachMovie dataset-used for collaborative ﬁltering tasks ▶Evaluation in batch setting- outperforms multi-class SVM, SVR Figure 4: Time-averaged ranking-loss comparison of MCP,WH,PRank on the synthetic dataset, EachMovie-100 and 200 datasets respectively Key takeaways Key takeaways 1. The ranking problem is a structured prediction task because of the total order between the diﬀerent ratings. Key takeaways 1. The ranking problem is a structured prediction task because of the total order between the diﬀerent ratings. 2. Online algorithm for ranking problem via projections and conservative update of the projection’s direction and the threshold values. Key takeaways 1. The ranking problem is a structured prediction task because of the total order between the diﬀerent ratings. 2. Online algorithm for ranking problem via projections and conservative update of the projection’s direction and the threshold values. 3. Experiments indicate this algorithm performs better than regression and classiﬁcation models for ranking tasks. Further Reading Types of Ranking Algorithms: ▶Point-wise Approaches - PRanking ▶Pair-wise Approaches - RankSVM, RankNet, Rankboost ▶List-wise Approaches - SVMmap, AdaRank, SoftRank Further Reading Types of Ranking Algorithms: ▶Point-wise Approaches - PRanking ▶Pair-wise Approaches - RankSVM, RankNet, Rankboost ▶List-wise Approaches - SVMmap, AdaRank, SoftRank References: ▶Liu, Tie-Yan. Learning to rank for information retrieval. Foundations and Trends R ⃝in Information Retrieval 3.3 (2009): 225-331. Further Reading Types of Ranking Algorithms: ▶Point-wise Approaches - PRanking ▶Pair-wise Approaches - RankSVM, RankNet, Rankboost ▶List-wise Approaches - SVMmap, AdaRank, SoftRank References: ▶Liu, Tie-Yan. Learning to rank for information retrieval. Foundations and Trends R ⃝in Information Retrieval 3.3 (2009): 225-331. ▶Agarwal, Shivani, and Partha Niyogi. Generalization bounds for ranking algorithms via algorithmic stability. Journal of Machine Learning Research 10.Feb (2009): 441-474. "
134,"Learning and Inference over Constrained Output Vasin Punyakanok Dan Roth Wen-tau Yih Dav Zimak Presenter：Mingyang Liu（Applied math） Three fundamentally different solutions to learn classifiers over structured output • Local classifiers are learned and used to predict each output component separately (LO) • Learning: Find the hypothesis ℎ: X →Y without constraints/structure in output. Cheaper computationally • Prediction: y= 𝑎𝑟𝑔𝑚𝑎𝑥+ℎ(𝑥) • Searching space is small • Eg. SVM, perceptron, regression 1 Three fundamentally different solutions to learn classifiers over structured output • Learning is decoupled from the task of maintaining structured output(L+I) • Learning step: Find the hypothesis ℎ: X →Y without dependencies among 𝑦/. Cheaper computationally. • Making decision step: predict the best structure y= 𝑦0, … , 𝑦3 with dependencies among 𝑦/ • Searching space is large(NP-hard) • Eg. Conditional models[McCallum et al 2000] • In the learning procedure, we learn single classifer 𝑃(𝑆6 = 𝑠6|𝑆690 = 𝑠690, 𝑂6 = 𝑜6 ), so there is not inference because there we do not build a classifer for the whole structure/sequence. • In the final decision step, put all the estimated parameters in the model and use them in Viterbi, which is a global inference algorithm, to predict the best sequence of states. The structure of the sequence is in this step. So L+I • Incorporating global constraints sometimes is not available, not needed, or just too expensive 1 Three fundamentally different solutions to learn classifiers over structured output • Incorporating dependencies among the variables into the learning process(IBT) • Learning: Find the hypothesis ℎ: X →Y with dependencies among 𝑦/. MakIng learning more difficult • Making decision step: predict the best structure y= 𝑦0, … , 𝑦3 with dependencies among 𝑦/ • Searching space is large • Eg. CRF[Lafferty et al., 2001] • log posterior distribution over weights 2 Lots of choice , constraints L+I v.s IBT in Chunking • Goal: identification of parts of speech • Given 𝑜0, 𝑜<, 𝑜=, 𝑜>, 𝑜?, 𝑜@, 𝑜A, 𝑜B, 𝑜C, 𝑜0D • Classifer 1(start of chunk): [ [ [ [ [ • Classifer 2(end of chunk): ] ] ] ] ] • Inference(constraints): [ ] [ ] learning independent classifiers(LO, L+I) Inference based training(IBT) vs 3 Definition: Structured classification problem • Given an observation of input variable 𝑿= (𝑋0, … , 𝑋G)=(𝑥0, … , 𝑥G) • Find ‘best’ assignment y for 𝒀= 𝑌 0, … , 𝑌 G • y is consistent with structure on 𝒀 • This structure can be thought of as constraining the output space YG to a smaller space C(YG ) ⊆YG 4 Y (𝑥/) Definition: Structure output classifier • Local scoring functions 𝑓 +(𝒙, 𝑡), 𝑓 +: XM× 1, … , n →R • Represent the score for 𝑌 6 = 𝑦∊Y • Global scoring function 𝑓: XM×YG →R • 𝑓𝒙, 𝒚= 𝑓𝒙, 𝑦0, … , 𝑦G = ∑ 𝑓 +T(𝒙, 𝑡) G 6U0 • Eg. Dependency Parsing • Find the highest scoring dependency tree, from the space of all dependency trees of N words. • Learn a model to score edge (i,j) of a candidate tree 𝑠𝑖, 𝑗= 𝑤⋅𝑓(𝑖, 𝑗) • Score of a dependency tree is sum of score of its edges 𝑠𝑥, 𝑦= ∑/,Z ∈+ 𝑠𝑖, 𝑗= ∑/,Z ∈+ 𝑤⋅𝑓(𝑖, 𝑗) • Structured output classifier ℎ: XM →YG • ℎ𝒙= 𝑎𝑟𝑔𝑚𝑎𝑥+\∈] Y^ 𝑓(𝒙, 𝑦′) 5 Definition: Linear representation • Linear local scoring function 𝑓 + 𝒙, 𝑡= 𝛼+ ⋅Φ+(𝒙, 𝑡) • 𝛼+ is weight vector , Φ+ 𝒙, 𝑡is feature vector • Linear global scoring function 𝑓𝒙, 𝒚= 𝛼⋅Φ(𝒙, 𝒚) • 𝛼, Φ 𝑥, 𝑦∈𝑅|Y| • Φ(𝒙, 𝒚)=(Φ0(𝒙, 𝒚),…, Φ|Y|(𝒙, 𝒚)) • Φ+ 𝒙, 𝒚= ∑ Φ+T 𝒙, 𝑡𝐼+TU+ G 6U0 for class y • Structured output classifier ℎ: XM →Y G • ℎ𝒙= 𝑎𝑟𝑔𝑚𝑎𝑥+\∈] Y ^ 𝛼⋅Φ(𝒙, 𝒚′) 6 Online perceptron-style algorithm 7 No global constraints Having global constraints key difference from learning locally is that feedback from the inference process determines which classifiers to modify so that together, the classifiers and the inference procedure yield the desired result Conjectures • When local classification problems are easy: LO>L+I>IBT • Information from Structure is not necessary • When local classification problems are getting harder: L+I>LO>IBT • Structure becomes more important • We also have decent classifiers learned locally • When local classification problems are extremely harder: IBT>L+I>LO • It is unlikely that structure based inference can fix poor classifiers learned locally 8 Definition: Separability and Learnability • A classifier, 𝑓∈𝐻, globally separates a dataset D iff for all examples 𝒙, 𝒚∈ 𝐷, 𝑓𝒙, 𝒚> 𝑓𝒙, 𝒚g for all 𝒚g ∈Y G\𝒚 • All-vs-all • A classifier, 𝑓∈𝐻, locally separates a dataset D iff for all examples 𝒙, 𝒚∈ 𝐷, 𝑓 +T 𝒙, 𝑡> 𝑓 + 𝒙, 𝑡for all y∈Y\y6 and for all t • 1-vs-all • Learning algorithm A, ∶𝐷→𝐻 • D is globally/locally learnable by A, if there exists an 𝑓∈𝐻such that 𝑓 globally/locally separates D 9 Relationships between local and global learning • local separability implies global separability, but the inverse is not true • 𝑓𝒙, 𝒚= ∑ 𝑓 +T(𝒙, 𝑡) G 6U0 >∑ 𝑓 +Tg(𝒙, 𝑡) G 6U0 = 𝑓𝒙, 𝒚′ for at least one t, 𝑦6 g ≠𝑦6 • local separability implies local and global learnability • global separability implies global learnability, but not local learnability 10 Claim • If the local classification tasks are separable, then L+I outperforms IBT • If the task is globally separable, but not locally separable then IBT outperforms L+I only with sufficient examples. 11 Experiments (Synthetic Data ) • Each example x = (𝑥0, 𝑥<, . . . , 𝑥m) ∈𝑅n × . . .× 𝑅n • Binary label y = 𝑦0, . . . , 𝑦m ∈ 0, 1 m from • 𝒚= ℎ𝒙= 𝑎𝑟𝑔𝑚𝑎𝑥𝒚∈] Yp ∑𝑦/𝑓 / 𝑥/ −1 −𝑦/ 𝑓 /(𝑥/) r / • 𝐶Ym is a random constraint on y • Each 𝑓 / corresponds to a local classifier 𝑦/ = 𝑔/ 𝑥/ = 𝐼tu vu wD • The dataset generated from this hypothesis is globally linearly separable • Let f(x, y*)= ∑𝑦/𝑓 / 𝑥/ −1 −𝑦/ 𝑓 /(𝑥/) r / . f(x, y*)> f(x, y’) for all 𝒚g ∈𝐶Ym \y*from argmax. 12 Experiments (vary the difficulty of local classification) • Let fraction κ of the data where ℎ𝒙≠𝑔𝒙= (𝑔0 𝑥0 , … , 𝑔m(𝑥m)) • i.e. 𝑔𝑥∉𝐶Ym because of constraint space • We can regard κ as how many bracket appear in the single classifier but not exist after inference. 13 Black brackets are • chosen by local classifiers • rejected by constraints • Indicating quality of local classifers Performance locally linearly separable L+I IBT not totally locally linearly separable most difficult local classification tasks IBT LO LO In all cases, inference helps 14 Experiments (Real-World Data) • Semantic-Role Labeling • To identify, for each verb in the sentence, all the constituents which fill a semantic role, and determine their argument types. • Structural constraints are necessary to ensure, for example, that no arguments can overlap or embed each other. More features ⇒more separable ⇒local classifiers are easy to learn IBT IBT L+I L+I 15 Experiments (Real-World Data) • Noun Phrase Labeling • identification of phrases or of words that participate in a syntactic relation- ship IBT IBT L+I L+I Similarly, only when the problem becomes difficult IBT > L+I 16 Bound Prediction • When learning globally, it is possible to learn concepts that may be difficult to learn locally, since the global constraints are not available to the local algorithms. • While the global hypothesis space is more expressive, it has a substantially larger representation.(Need more data) 17 Well-known VC-style generalization bound 18 Upper bounds of generalization error for learning locally 19 Improved generalization bound for globally learned classifiers 20 "
135,"Page 1CIS 700Advanced Machine Learning for NLPComments on Structured PredictionDan Roth Department of Computer and Information Science University of PennsylvaniaComments: ILP for Entities & RelationsFormulation So far, sequential structure What to do when we want to enforce more general structure  (remember: structure = knowledge; formally – restricting the size of the huge Y space)  History: Metric Labeling Complexity And practice  Expressivity Every Boolean condition can be expressed as a set of linear inequalities Think about: how was did “model” trained?  2Admin Stuff3Please send me your draft lectures at least two days before your presentation  Please follow the (changing) schedule on the web  Feedback on projects and reviews – hopefully before the end of this week.  Don’t forget to send us your evaluation of the presentations.  Shallow Parsing Example4Phrase Identification ProblemUse classifiers’ outcomes to identify phrases Final outcome determined by optimizing classifiers outcome and constrainsPage 5Input:		o1 o2 o3 o4 o5 o6 o7 o8 o9 o10  Classifier 1: Classifier 2:  Infer:Did this classifier make a mistake? (When are you looking at its prediction?) How to train it?Training regimesDecomposition of outputs gives two approaches for training Decomposed training/Learning without inference Learning algorithm does not use the prediction procedure during training  Global training/Joint training/Inference-based training Learning algorithm uses the final prediction procedure during training  Similar to the two strategies we had before with multiclass  Inference complexity often an important consideration in choice of modeling and training Especially so if full inference plays a part during training Ease of training smaller/less complex models could give intermediate training strategies between fully decomposed and fully joint 6Shallow Parsing Example7L+I: Learning plus InferencePage 8Training w/o ConstraintsTesting: Inference with ConstraintsIBT: Inference-based Trainingf1(x)f2(x)f3(x)f4(x)f5(x)XYWhich one is better?  When and Why?ClaimsWhen the local classification problems are “easy”, L+I outperforms IBT. In many applications, the components are identifiable and easy to learn (e.g., argument, open-close, PER). Only when the local problems become difficult to solve in isolation, IBT outperforms L+I, but needs a larger number of training examples.  Will show  experimental results and theoretical intuition to support our claims.Page 9L+I: cheaper computationally; modular IBT should be better in the limit;  In our domains: never happensCombinations: L+I, and then IBT are possiblePage 10Perceptron-based Global Learningf1(x)f2(x)f3(x)f4(x)f5(x)XYFor this example, assume that we “know” that y2 and y3 have to be different. Example 1: Semantic Role LabelingBased on the dataset PropBank [Palmer et. al. 05]  Large human-annotated corpus of verb semantic relations The task: To predict arguments of verbs 11Given a sentence, identifies who does what to whom, where and when.The bus was heading for Nairobi in KenyaRelation: Head Mover[A0]: the bus Destination[A1]: Nairobi in KenyaPredicateArgumentsOne important lesson in the following examples is that “consistency” is actually a loaded term. In most interesting cases it would mean “coherency via some knowledge.Predicting verb argumentsIdentify candidate arguments for verb using parse tree Filtered using a binary classifier Classify argument candidates Multi-class classifier (one of multiple labels per candidate) Inference Using probability estimates from argument classifier Must respect structural and linguistic constraints Eg: No overlapping argumentsThe bus was heading for Nairobi in Kenya.12Inference: verb argumentsThe bus was heading for Nairobi in Kenya.13Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 2.014heading (The bus,  		for Nairobi,  		for Nairobi in Kenya)Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 1.915heading (The bus,  		for Nairobi in Kenya)Total: 2.0Inference: verb argumentsThe bus was heading for Nairobi in Kenya.Total: 1.916heading (The bus,  		for Nairobi in Kenya)Structured output is…A data structure with a pre-defined schema Eg: SRL converts raw text into a record in a database    Equivalently, a graph  Often restricted to be a specific family of graphs: chains, trees, etc   17HeadThe busNairobi in KenyaA0A1Questions/comments?Example 2: Object detection18Photo by Andrew Dressel - Own work. Licensed under Creative Commons Attribution-Share Alike 3.0How would you design a predictor that labels all the parts using the tools we have seen so far?A good illustration for why one might want decomposition – transfer. E.g., a wheel classifier might be useful in many other tasks.One approach to build this structure19Photo by Andrew Dressel - Own work. Licensed under Creative Commons Attribution-Share Alike 3.0Final output: Combine the predictions of these individual classifiers (local classifiers)  The predictions interact with each other  Eg: The same box can not be both a left wheel and a right wheel, handle bar does not overlap with seat, etc  Need inference to compose  the outputInference: given input x (a document, a sentence),                           predict the best structure y = {y1,y2,…,yn}  Y  (entities & relations) Assign values to the y1,y2,…,yn, accounting for dependencies among yis Inference is expressed as a maximization of a scoring function                                     y’ = argmaxy  Y wT Φ (x,y)    Inference requires, in principle, touching all y  Y at decision time, when we are given x  X and attempt to determine the best y  Y for it, given w  For some structures, inference is computationally easy.  Eg: Using the Viterbi algorithm  In general, NP-hard (can be formulated as an ILP) Structured Prediction: InferenceJoint features on inputs and outputsFeature Weights (estimated during learning)Set of allowed structuresPlacing in context: a crash course in structured prediction (skip)Page 20Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss.  Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):    Page 21Structured Prediction: LearningLearning: given a set of structured examples {(x,y)}                         find a scoring function w that minimizes empirical loss.  Learning is thus driven by the attempt to find a weight vector w such that for each given annotated example (xi, yi):     We call these conditions the learning constraints.  In most learning algorithms used today, the update of the weight vector w is done in an on-line fashion,  Think about it as Perceptron; this procedure applies to Structured Perceptron, CRFs, Linear Structured SVM W.l.o.g. (almost) we can thus write the generic structured learning algorithm as follows:Score of annotated structureScore of any other structurePenalty for predicting other structurePage 22In the structured case, prediction (inference) is often intractable but needs to be done many timesStructured Prediction: Learning AlgorithmPage 23Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution I: decompose the scoring function to EASY and HARD partsEASY: could be feature functions that correspond to an HMM, a linear CRF,   or even ÁEASY (x,y) = Á(x), omiting dependence on y, corresponding to classifiers. May not be enough if the HARD part is still part of each inference step.Page 24Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo Solution II: Disregard some of the dependencies: assume a simple model.Page 25Structured Prediction: Learning AlgorithmFor each example (xi, yi) Do: Predict: perform Inference with the current weight vector  yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)  Check the learning constraint Is the score of the current prediction better than of (xi, yi)? If Yes – a mistaken prediction Update w Otherwise: no need to update w on this example EndDo yi’ = argmaxy 2 Y  wEASYT ÁEASY ( xi ,y) + wHARDT ÁHARD ( xi ,y)   This is the most commonly used solution in NLP todaySolution III: Disregard some of the dependencies during learning; take into account at decision timePage 26Constrained Conditional ModelsTraining:  learning the objective function (w, u) Decouple? Decompose? Force u to model hard constraints?  A way to push the learned model to satisfy our output expectations (or expectations from a latent representation)  [CoDL, Chang et. al (07, 12); Posterior Regularization, Ganchev et. al (10); Unified EM (Samdani et. al (12)] The benefits of thinking about it as an ILP are conceptual and computational. Knowledge component:   (Soft) constraints y = argmaxy 2 Y  wTÁ(x, y) + uTC(x, y)  y = argmaxy  1Á(x,y) wx,y     subject to Constraints C(x,y) Any MAP problem w.r.t. any probabilistic model, can be formulated as an ILP  [Roth+ 04, Taskar 04]Page 27Variables are models  Knowledge/Linguistics Constraints  Cannot have both A states and B states in an output sequence. Knowledge/Linguistics Constraints  If a modifier chosen, include its head If verb is chosen, include its arguments Page 28Examples: CCM FormulationsWhile Á(x, y) and C(x, y)  could be the same; we want C(x, y) to express high level declarative knowledge over the statistical models. Sequential Prediction  HMM/CRF based:                      Argmax  ¸ij xijSentence Compression/Summarization: Language Model based:                      Argmax  ¸ijk xijkConstrained Conditional Models Allow: Decouple complexity of the learned model from that of the desired output Learn a simple model  (multiple; pipelines); reason with a complex one. Accomplished by incorporating constraints to bias/re-rank global decisions to satisfy (minimally violate) expectations.  y = argmaxy 2 Y  wTÁ(x, y) + uTC(x, y) Formulate NLP Problems as ILP problems         (inference may be done otherwise) 	1. Sequence tagging            (HMM/CRF + Global constraints) 	2. Sentence Compression   (Language Model + Global Constraints) 	3. SRL                                      (Independent classifiers + Global Constraints) "
136,"CIS 700Advanced Machine Learning Structured Machine Learning:  Structured Learning with Latent VariablesShyam Upadhyay Department of Computer and Information Science University of Pennsylvania Page 1Ingredients of a Structured Prediction ProblemInput  Output  Feature Extractor  Inference Inference                                                       , also called “Decoding”  Loss  Ingredients of a Structured Prediction Problem (General)Input  Output  Feature Extractor  Inference Inference                                                       , also called “Decoding”  Loss  Loss-sensitive inference   Some tasks have inherent latent structure connecting the input to the output. “Latent”- not provided at training time (expensive to annotate). Predicting a “good” latent structure can improve final prediction       Positing a good latent structure is task-dependent (requires background knowledge)Intuition Behind Latent ModelingPredicting Latent StructuresSome latent structures are invalid. Cannot make local decisions, need to predict the structure jointly.  Cannot always enumerate latent structures (trees etc.). How to predict a good latent structure which helps predict the gold output?Quick Detour To EMSoft E Step   Generate a distribution over latent variables Does not commit to a single latent variable.  M Step    Example Soft K-Means Clustering, Rabiner’s HMM algorithm …Soft EMExample – Soft ClusteringEach point can be “explained” by several clusters. Hard EMHard E Step   Pick only the best latent variable. (Spike Distribution) M Step    M Step actually did not change. Example K-Means ClusteringHard ClusteringEach point is “explained” by exactly one cluster. SSVM and Latent SSVMStructured SVMLatent Structured SVMReformulationThe best explaining hidden variable given the gold output and inputThe best explaining hidden variable given the inputThis is like Hard EMWe have 2 inference problems!Ingredients of a Structured Prediction Problem with Latent VariablesInput	Output		Latent structure  Feature Extractor Inference Hidden-Variable Completion   Joint Inference over output and latent structure (“decoding”)   Loss  Generalized to include latent variable, but can ignore it too. Optimization  Difference of convex functions (or sum of a convex and concave). CCCP (Concave-Convex Procedure)     Matching points on the convex and concave terms which have the same tangent.       Example of CCCPLearning AlgorithmM - StepE - StepInterpreting the UpdatesWhat is added to the weight vector       Note similarity to the Structured Perceptron/SVM update.  Note the update also improves the next E step (better latent representation will chosen).Feature of the latent structure inferred using gold outputFeature of the latent structure inferred jointlyInitializationThe latent SSVM problem is non-convex (convince yourself)  Initialization matters.  How to find a Good Initialization?  How to guide the model better? Discriminative Learning over Constrained Latent Representations  Ming-Wei Chang, Dan Goldwasser, Dan Roth, Vivek Srikumar Structured Output Learning with Indirect Supervision  Ming-Wei Chang, Vivek Srikumar, Dan Goldwasser, Dan RothQuestions?"
137,"CIS 700Advanced Machine Learning for NLPInferenceApplicationsDan Roth Department of Computer and Information Science University of PennsylvaniaAdmin StuffOn Thursday we will discuss the projects Please prepare a 5 minutes PPT presentation Send it to me by 4pm on Thursday.   The goal is to show where you are, highlight problems and need for changes Try to have some quantitative evaluation already! It often allows you to see where you are  Examples are keyOutlineInference Formulations and Algorithms Integer Linear Programs  Applications “Difficulty” of modelling Dependency Parsing Example  Setting1: 3Dependency ParsingRoth & Srikumar: ILP formulations in Natural Language Processing    https://github.com/CogComp/cogcomp-nlp ModelingRoth & Srikumar: ILP formulations in Natural Language ProcessingObjective     Integer Linear Programming Formulation: Maximum Spanning TreeModeling (2)Roth & Srikumar: ILP formulations in Natural Language ProcessingCan add a lot more linguistically motivated constraintsLessonsRoth & Srikumar: ILP formulations in Natural Language ProcessingIssuesRoth & Srikumar: ILP formulations in Natural Language ProcessingRiedel and Clarke (2006): An ILP formulation for dependency parsing: MST + linguistically motivated “hard” constraints to forbid some arc configurations.  Problem: The formulation includes an exponential number of constraints—one for each possible cycle.  Proposed a cutting plane algorithm, invoking constraints selectively only when they are violated by the current solution.  Still slow Martins, Smith,  Xing (2009) A single commodity flow formulation for the (undirected) minimum spanning tree problem (Magnanti and Wolsey (1994)) Requires O(n2 ) variables and constraints. SettingModeling problems as structured prediction problems  Structure =  Assumptions on the output; Knowledge about the output Augmenting Probabilistic Models with declarative constraints  The prediction (Inference) problem involves assigning values to a set of interdependent discrete variables We want to determine the best assignment   If you have a probabilistic model over the variables of interest this problem is often called the MAP problem (Maximum A-Posteriori) OutlineRoth & Srikumar: ILP formulations in Natural Language ProcessingModeling problems as structured prediction problems  Hard and soft constraints to represent prior knowledge Augmenting Probabilistic Models with declarative constraints.  Inference AlgorithmsMAP inference is discrete optimizationRoth & Srikumar: ILP formulations in Natural Language ProcessingA combinatorial problem      Computational complexity depends on The size of the input The factorization of the scores More complex factors generally lead to expensive inference  A generally bad strategy in most but the simplest cases: “Enumerate all possible structures and pick the highest scoring one”-1054.1,….MAP inference is searchRoth & Srikumar: ILP formulations in Natural Language ProcessingWe want a collection of decisions that has highest score 			argmaxy wTÁ(x,y)  No algorithm can find the max without considering every possible structure Why?  How can we solve this computational problem? Exploit the structure of the search space and the cost function That is, exploit decomposition of the scoring function  Approaches for inferenceRoth & Srikumar: ILP formulations in Natural Language ProcessingShould the maximization be performed exactly?  Or is a close-to-highest-scoring structure good enough?  Exact: Search, dynamic programming, integer linear programming  Heuristic (called approximate inference): Gibbs sampling, belief propagation, beam search   Relaxations: linear programming relaxations, Lagrangian delaxation, dual decomposition, AD3  "
138,"Cutting Plane Training of Structural SVM Seth Neel University of Pennsylvania sethneel@wharton.upenn.edu September 28, 2017 Seth Neel (Penn) Short title September 28, 2017 1 / 33 Overview Structural SVMs have shown promise for building accurate structural prediction models in natural language processing and other domains Current training algorithms (while polynomial time) are expensive on large datasets; superlinear Paper proposes a new cutting plane training method that runs in linear time Number of iterations until convergence is independent of the number of training examples Empirical work shows that it is much faster than conventional cutting plane methods Seth Neel (Penn) Short title September 28, 2017 2 / 33 SVM Binary classiﬁcation (primal): min w,ϵi≥0 1 2||w||2 + C n X i ϵi s.t. ∀i ∈{1, . . . n} : yi(wTxi) ≥1 −ϵi Seth Neel (Penn) Short title September 28, 2017 3 / 33 Hinge Loss Hinge loss is a convex upper bound for the 0 −1 loss. The SVM can be represented as unconstrained minimization of the regularized hinge loss. Seth Neel (Penn) Short title September 28, 2017 4 / 33 Hinge Loss Hinge loss is a convex upper bound for the 0 −1 loss. The SVM can be represented as unconstrained minimization of the regularized hinge loss. Seth Neel (Penn) Short title September 28, 2017 4 / 33 Multiclass SVM Recall: multiclass margin is deﬁned as the score diﬀerence between the highest scoring label and the second highest min 1 2 k X i=1 ||wk||2 + C X i ϵi s.t. ∀i, (xi, yi) ∈D, k ̸= yi : wt yixi −wt kxi ≥1 −ϵi, ϵi ≥0 Seth Neel (Penn) Short title September 28, 2017 5 / 33 Structured Prediction Learn a function f : X →Y where Y is a space of multivariate and structured outputs for a given x predict fw(x) = argmaxy∈YwTΨ(x, y) intuitively Ψ measures the compatibility of y and x Seth Neel (Penn) Short title September 28, 2017 6 / 33 Structured SVM Assume that fw(x, y) = wtΨ(x, y) We generalize the SVM optimization problem to train w The true loss function is ∆(y, hw(x)) = 1{y ̸= hw(x)} hinge loss provides a convex upper bound to the true loss function: Seth Neel (Penn) Short title September 28, 2017 7 / 33 Structured SVM Assume that fw(x, y) = wtΨ(x, y) We generalize the SVM optimization problem to train w The true loss function is ∆(y, hw(x)) = 1{y ̸= hw(x)} hinge loss provides a convex upper bound to the true loss function: margin-rescaling: ∆MR(hw(x), y) = maxy ′∈Y{∆(y, y ′) −w tΨ(x, y) + w tΨ(x, y ′)} slack-rescaling: ∆SR(hw(x), y) = maxy ′∈Y{∆(y, y ′) ∗(1 −w tΨ(x, y) + w tΨ(x, y ′))} Conceptually similar; we focus on MR. Seth Neel (Penn) Short title September 28, 2017 7 / 33 n-slack structural svm optimization (MR) min w,ϵ≥0 1 2||w||2 + C n X i ϵi s.t. ∀y′ ∈Y, i ∈[n] : wt[Ψ(xi, yi) −Ψ(xi, y′)] ≥∆(yi, y′) −ϵi Note that if Ψ is the |Y| ∗p dimensional embedding we recover the multiclass formulation, where p is the dimension of xi. Seth Neel (Penn) Short title September 28, 2017 8 / 33 Past Approaches Note the n-slack optimization problem has |Y| ∗n constraints, not obviously eﬃciently solvable In fact it is: a greedily constructed cutting plane model requires only O(n/ϵ2) constraints. other methods: exponentiated gradient methods, Taskar reformulation as a QP, stochastic gradient methods. Seth Neel (Penn) Short title September 28, 2017 9 / 33 Cutting Plane Algorithm 1 Seth Neel (Penn) Short title September 28, 2017 10 / 33 Cutting Plane Algorithm 1 This algorithm is eﬃcient assuming existence of a separation oracle calculating the most violated constraint [line 5] note for natural choices of ∆this is the assumption that we can eﬃciently solve the inference problem Seth Neel (Penn) Short title September 28, 2017 11 / 33 Contribution (this paper): Faster Training! Using a simple reformulation of the optimization problem: 1 constant iteration complexity = ⇒linear runtime in n 2 several orders of magnitude improvement in runtime (worst case analysis) 3 empirical study shows speedup in practice Seth Neel (Penn) Short title September 28, 2017 12 / 33 1-slack formulation Theorem The n-slack formulation is equivalent to the optimization: min w,ϵ≥0 1 2||w||2 + CE s.t. ∀(y′ 1, y′ 2, . . . y′ n) ∈Yn : 1 nwt n X i=1 [Ψ(xi, yi) −Ψ(xi, y′ i )] ≥1 n X i ∆(yi, y′ i ) −E Note now we only have 1 slack variable shared among all constraints, but we actually have exponentially more constraints Seth Neel (Penn) Short title September 28, 2017 13 / 33 What is the point? New reformulation has constraint that binds on linear combination of data points = ⇒support vectors are linear combinations of data points This ﬂexibility gives a much sparser set of non-zero dual variables, which translates into a smaller cutting plane model (constant) High-level: exponentially more constraints, but only a constant number of them matter Seth Neel (Penn) Short title September 28, 2017 14 / 33 1-slack: proof It suﬃces to show that for any ﬁxed w, the smallest feasible E in the 1-slack formulation and smallest feasible 1 n P i ϵi in the n-slack formulation are equal. Seth Neel (Penn) Short title September 28, 2017 15 / 33 1-slack: proof It suﬃces to show that for any ﬁxed w, the smallest feasible E in the 1-slack formulation and smallest feasible 1 n P i ϵi in the n-slack formulation are equal. Proof. In n-slack, for a ﬁxed w, for each i, each constraint gives ϵi ≥∆(y′, yi) −wt[Ψ(xi, yi) −Ψ(xi, y′)], which shows that the smallest feasible is ϵi = max y′ {∆(y′, yi) −wt[Ψ(xi, yi) −Ψ(xi, y′)]} In 1-slack the smallest feasible E is similarly at: max y′ 1,y′ 2,...,y′ n {1 n X i (∆(yi, y′ i ) −wt[Ψ(xi, yi) −Ψ(xi, y′)])} Seth Neel (Penn) Short title September 28, 2017 15 / 33 1-slack: proof ctd Proof. But this function separates over each y′ i and so it is equal to: 1 n P i maxy′ i (∆(yi, y′ i ) −wt[Ψ(xi, yi) −Ψ(xi, y′)]) which as deﬁned in the 1-slack formulation is simply 1 nϵi. Seth Neel (Penn) Short title September 28, 2017 16 / 33 The Main Algorithm Similar to the previous cutting plane algorithm, only adds 1 constraint in each iteration. What is diﬀerent is that only a constant number of constraints are suﬃcient to ﬁnd an ϵ-approximate solution. Seth Neel (Penn) Short title September 28, 2017 17 / 33 Correctness Obvious; when (if) the algorithm terminates: 1 The objective is optimized over a strictly smaller set of constraints, and hence has a smaller value 2 If the algorithm terminates the solution is approximately feasible Seth Neel (Penn) Short title September 28, 2017 18 / 33 Iteration Complexity Let ∆= maxi,y′ ∆(yi, y′). Let R = maxi,y′ ||Ψ(xi, yi) −Ψ(xi, y′)||. Then the iteration complexity is 16R2C ϵ + log2( ∆ 4R2C ) Seth Neel (Penn) Short title September 28, 2017 19 / 33 Iteration Complexity: Outline min w,ϵ≥0 1 2||w||2 + CE s.t. ∀(y′ 1, y′ 2, . . . y′ n) ∈Yn : 1 nwt n X i=1 [Ψ(xi, yi)−Ψ(xi, y′ i )] ≥1 n X i ∆(yi, y′ i )−E The pair (w, E) = (0, ∆) is feasible in the above Thus the optimal value is ≤C∆. This means that the dual program has optimal value ≤C∆ We show each iteration increases the dual objective by some constant amount, forcing constant iteration complexity. Seth Neel (Penn) Short title September 28, 2017 20 / 33 Classic SVM Dual Binary classiﬁcation (primal): min w,ϵi≥0 1 2|w||2 + C n X i ϵi s.t. ∀i ∈{1, . . . n} : yi(wTxi) ≥1 −ϵi Seth Neel (Penn) Short title September 28, 2017 21 / 33 Classic SVM Dual Binary classiﬁcation (primal): min w,ϵi≥0 1 2|w||2 + C n X i ϵi s.t. ∀i ∈{1, . . . n} : yi(wTxi) ≥1 −ϵi Binary classiﬁcation (dual): min α n X i=1 αi −1 n X i,j αiαjyiyjxt i xj s.t. n X i=1 αiyi = 0 ∀i ∈{1, . . . n} : 0 ≤αi ≤C n Seth Neel (Penn) Short title September 28, 2017 21 / 33 The Dual Program The dual program is easy to compute by forming the Lagrangian and taking derivatives. First we deﬁne: Seth Neel (Penn) Short title September 28, 2017 22 / 33 Line Search Lemma Proof Note that the dual objective is a QP of the form Θ(α) = htα −1 2αtHα where h = {∆(¯ y)}¯ y∈Yn, Ha,b = αaαbHMR(ya, yb) Seth Neel (Penn) Short title September 28, 2017 23 / 33 Proof of Line Search Lemma. Proof. Direct calculation yields: Θ(α + βη) −Θ(α) = β[∇Θ(α)Tη −1 2βηTHη] Setting the derivative with respect to β equal to 0 gives β = ∇θ(α)T η ηT Hη . Substituting this value of β gives max β Θ(βη + α) −Θ(α) = 1 2 (∇Θ(α)Tη)2 ηTHη This is the maximum value, unless the value of β at the optimum is > C, in which case the optimum occurs at β = C, by concavity in β. Substituting in β = C gives the desired result. Seth Neel (Penn) Short title September 28, 2017 24 / 33 Iteration Complexity (Sketch) In step 8 we add a constraint. Seth Neel (Penn) Short title September 28, 2017 25 / 33 This adds a variable to the dual program: αˆ y. Note can set αˆ y = 0, and so it only increases the objective value. Seth Neel (Penn) Short title September 28, 2017 26 / 33 Adding a new constraint ˆ y to the model adds a new parameter to the dual problem; increases its objective value Seth Neel (Penn) Short title September 28, 2017 27 / 33 Adding a new constraint ˆ y to the model adds a new parameter to the dual problem; increases its objective value We pick an η : ηˆ y = 1, η′ y = −1 C αy′ such that α + βη is always dual-feasible (since ηT1 = 0) Seth Neel (Penn) Short title September 28, 2017 27 / 33 Adding a new constraint ˆ y to the model adds a new parameter to the dual problem; increases its objective value We pick an η : ηˆ y = 1, η′ y = −1 C αy′ such that α + βη is always dual-feasible (since ηT1 = 0) The increase in objective value is lower bounded by the increase in a line search along η (since we are only searching a feasible region of the dual) Seth Neel (Penn) Short title September 28, 2017 27 / 33 Adding a new constraint ˆ y to the model adds a new parameter to the dual problem; increases its objective value We pick an η : ηˆ y = 1, η′ y = −1 C αy′ such that α + βη is always dual-feasible (since ηT1 = 0) The increase in objective value is lower bounded by the increase in a line search along η (since we are only searching a feasible region of the dual) Line search lemma (with some algebra) lets us lower bound this increase as a function of ϵ, C, R. Seth Neel (Penn) Short title September 28, 2017 27 / 33 Time Complexity Summary O(n) calls to the separation oracle O(n) computation time per iteration Constant number of iterations Each QP is of constant size, and is hence solved in constant time. For non-linear kernel it is O(n2) Seth Neel (Penn) Short title September 28, 2017 28 / 33 Experiments Applied to binary classiﬁcation, multi-class classiﬁcation, linear chain HMMs, and CFG Grammar learning Two Questions: 1 Does the 1-slack algorithm achieve signiﬁcant speedups in practice? 2 Is the w∗value as good a solution? Seth Neel (Penn) Short title September 28, 2017 29 / 33 Speed: 1-slack vs. n-slack Slower speedup with slower separation oracle - still signiﬁcant gains Seth Neel (Penn) Short title September 28, 2017 30 / 33 Speed: 1-slack vs. SVM light on binary classiﬁcation Faster even on binary classiﬁcation than state-of-the-art SVM training algorithms Seth Neel (Penn) Short title September 28, 2017 31 / 33 Accuracy vs. n-slack Very similar for all tasks except HMM where n-slack outperforms This is because the duality gap Cϵ is signiﬁcant part of the objective value since the data was almost linearly separable. Generalization performance for HMM was still comparable. Seth Neel (Penn) Short title September 28, 2017 32 / 33 References Joachims, Finley, Yu (2009) Cutting Plane Training of Structural SVMs Seth Neel (Penn) Short title September 28, 2017 33 / 33 "
139,"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data (John Lafferty et. al.) Presenter: Devanshu Jain (devjain@seas.upenn.edu) 1 Layout • Problem Statement • Generative Models: HMM • Limitations of Generative Models • Discriminative Models: MEMM • Limitations of MEMM • Discriminative Models: CRF • Loss Function convexity • Inference • Parameter Estimation • Experiments • CRF spin-offs 2 Problem Statement • The problem, we wish to solve: Labelling Sequence Data • POS tagging • Named Entity Recognition • Statement: Given an observation sequence x, we want to choose a label sequence y* such that the conditional probability P(y | x) is maximized, that is: 𝒚∗= 𝒂𝒓𝒈 𝒎𝒂𝒙𝒚 𝑷𝒚 𝒙) 3 Generative Models: HMM • HMMs can be used to solve such problems • In NER, each observation (xt) can be the identity of the word at position t and each state (yt) can be the named-entity label, i.e. one of {Person, Organization, Location, Other}. • To be precise, named entities can be multi-tokens. So, BIO-method. • B: Beginning, I: Intermediate, O: Outside. So each label is prefixed with these letters which indicate whether it’s the beginning or continuation of a named entity. • It makes 2 assumptions: 1. Each state depends only on its immediate predecessor, that is, yt ⊥yi given yt-1 such that i={1, 2, …, t-2} 2. Each observation xt depends only on its current state yt 𝑝𝑦, 𝑥= 1 𝑝𝑦2 𝑦234)𝑝(𝑥2|𝑦2) 7 284 • The model is generative because it models the distribution P(y , x) 4 Generative Models: Limitations • Modelling p(x) is difficult because it may consist of many interdependent features • For example: In NER, word’s identity may not be enough evidence, especially in case of ‘Person’ category as many proper nouns may not occur in training data. It may be helpful to identify features like capitalization, neighboring words, suffix, etc. • HMMs make the independence assumption (2) but that is not true above because, suffix and capitalization are highly dependent on the word’s identity. • Generative Models, in general, can be enhanced to model inter- dependencies between such features, but then modelling that becomes highly intractable. • But, in the end, there is a definite mismatch between the desired learning objective and the prediction objective function. • Discriminative models such as MEMM, CRF try to address this issue. 5 Discriminative Models: MEMM • Maximum Entropy Markov Models (MEMMs) are discriminative models, where each state has an exponential model that takes the observation sequence as input and outputs a probability distribution over the next possible states. 𝑃𝒚𝒙) = 1 𝑃(𝑦2|𝑦234, 𝑥2) 7 284 • Each of the 𝑃(𝑦2|𝑦234, 𝑥2), is an exponential model of the form: 𝑃(𝑦2|𝑦234, 𝑥2) = 1 𝑍(𝑥2, 𝑦234) exp (? 𝜆A𝑓 A(𝑥2, 𝑦2) C A ) where Z is a normalization constant and the summation is over all features 6 MEMM: Limitations – Label Bias Problem • MEMM suffers from Label Bias Problem, i.e., the transition probabilities of leaving a given state is normalized for only that state. • Imagine that during the training a state s only saw state s’ as the next state when given observation o, then according to the eq in previous slide: 𝑃𝑠′ 𝑠, 𝑜 = 1 this is because the normalization is done per state and not globally. 7 MEMM: Limitations – Label Bias Problem (example borrowed from Dr. Ramesh Nallapati’s slides: http://www.cs.stanford.edu/~nmramesh/crf) • We can observe in the diagram: • State 1 almost always intend to transit to State 2 • State 2 almost always intend to stay in State 2 State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 8 MEMM: Limitations – Label Bias Problem (example borrowed from Dr. Ramesh Nallapati’s slides: http://www.cs.stanford.edu/~nmramesh/crf) • P(1->1->1->1) = 0.4*0.45*0.5 = 0.090 • P(2->2->2->2) = 0.2*0.3*0.3 = 0.018 • P(1->2->2->2) = 0.6*0.3*0.3 = 0.054 • P(2->1->1->1) = 0.2*0.45*0.5 = 0.450 State 1 State 2 State 3 State 4 State 5 Observation 1 Observation 2 Observation 3 Observation 4 0.4 0.6 0.2 0.2 0.2 0.2 0.2 0.45 0.55 0.2 0.3 0.1 0.1 0.3 0.5 0.5 0.1 0.3 0.2 0.2 0.2 9 Discriminative Models: CRF • Conditional Random Fields (CRFs) overcomes the Label Bias problem • X: random variable over the observation sequence • Y: random variable over the label sequence • Defn : Let G=(V,E) be a graph such that Y = (Yv)v∈V so that Y is indexed by vertices of G. Then (X,Y) is a conditional random field in case, when conditioned on X, the random variables Yv obey the Markov property with respect to the graph: p(Yv | X, Yw, w≠v) = p(Yv | X, Yw, w~v), where w~v means that w and v are neighbors in G. • These are Linear Chain CRF 10 Fundamental Theorem of Random Fields • Given by Hammersley and Clifford, 1971, it states that the probability distribution of x satisfies the Markov property with respect to graph G(V,E) if and only if, it can be factored according to G: 𝑃𝒙= 1 𝑍1 𝜓𝒞 C 𝒞 • where 𝑍is the normalization constant and 𝜓𝒞is the potential function over clique 𝒞. • log (𝜓𝒞) ≜𝝀𝒞 7𝒇(𝒞), where 𝒇(. ) is the feature vector defined over the clique and 𝝀is the corresponding weight vector for those features. 𝑃𝒙= 1 𝑍1 exp (𝝀𝒞 7𝒇(𝒞)) C 𝒞 = 1 𝑍exp (? 𝝀𝒞 7𝒇(𝒞) C 𝒞 ) 11 CRF Notation Legend • In the few of next slides, we use the following notation: • N: Number of training examples • Each training example is denoted by (x,y) • x = <𝑥4, 𝑥S, 𝑥T,…, 𝑥7> • y = <𝑦4, 𝑦S, 𝑦T,…, 𝑦7> • 𝒙2 U: tth term of ith x • 𝒚2 U: label of tth term of ith x • 𝑓(boolean edge feature) and 𝑔(boolean vertex feature) are feature functions. • 𝑙iterates over 𝑓features • 𝑚iterates over 𝑔features • 𝑣is a vertex from vertex set V ; 𝑒is an edge from edge set E • 𝑦|[: components of 𝑦along the edge 𝑒 • 𝑦|\: components of 𝑦along the vertex 𝑣 12 Linear Chain CRF • P(Y) can be factored into the distributions involving the cliques of the graph. • Although the graph contains X and Y, we only define the random field over Y • X are observables • If the graph G forms a tree or simply a chain, then the cliques are the edges and vertices of the graph (G-X), and the probability distribution of Y takes the form: 𝑃𝒚|𝒙∝exp ? 𝜆^𝑓 ^ 𝑒, 𝒚|[, 𝒙 C [_`,^ + ? 𝜇c𝑔c 𝑣, 𝒚|\, 𝒙 C \_d,c ∝ exp ( ? ?(? 𝜆^𝑓 ^ 𝑦2 U , 𝑦234 U , 𝒙2 U + ? 𝜇c𝑔c 𝑦2 U , 𝒙 C c C ^ 7 284 e U84 )) • We can normalize the above equation by the following constant: 𝑍𝒙= ? exp( ? ?(? 𝜆^𝑓 ^(𝑦2 (U), 𝑦234 (U) , 𝒙2 U) + ? 𝜇c𝑔c(𝑦2 (U), 𝒙) C c C ^ 7 284 e U84 )) C 𝒚 • So now, the normalization constant is only a function of observation sequence and not the current state. Hence, it solves the label bias problem. 13 Linear Chain CRF : Loss Function & Inference • The loss function is the log likelihood of the probability distribution: 𝑙𝜃= ? ? ? 𝜆g𝑓 g(𝑦2 (U), 𝑦234 (U) , 𝒙2 U) C g 7 284 e U84 −log (𝑍𝒙U ) • Here, all the features f and g have been written as f for convenience • The loss function is concave , which follows from exponential model of the probability distribution • This means that there is a global optimal point. • Inference: To compute the most likely labelling, we compute y*: 𝒚∗= argmax 𝒚 𝑃(𝒚|𝒙) • Viterbi algorithm can be used to solve this! 14 Parameter Estimation: Gradient Ascent • Looking at the nature of loss function, we can use gradient ascent algorithm to maximize the likelihood function 𝑙𝜃: 𝛻g𝑙(𝜃) = ? ? 𝑓 g(𝑦2 (U), 𝑦234 (U) , 𝑥2 (U)) 7 284 e U84 −1 𝑍? exp (. ) C m n ? ? 𝑓 g 𝑦 n2 U , 𝑦 n234 U , 𝑥2 U 7 284 e U84 = ? ? 𝑓 g(𝑦2 (U), 𝑦234 (U) , 𝑥2 (U)) 7 284 e U84 −? 𝑃𝑦 n|𝑥, 𝜃 C m n ? ? 𝑓 g 𝑦 n2 U , 𝑦 n234 U , 𝑥2 U 7 284 e U84 = 𝐸𝑓 g −𝐸p 𝑓 g • Equating the gradient to zero, we note that the optima is reached when empirical expectation of feature 𝑓 g is equal to the expectation w.r.t. the model. • However, finding the closed form solution for 𝜃is not always possible. 15 Parameter Estimation: Iterative Scaling • The main idea behind Iterative Scaling is that parameters are updated such that the new values are closer to the optima than before. • If 𝜃= 𝜆4, 𝜆S, … , 𝜆^, 𝜇4, 𝜇S, … 𝜇c and ∆𝜃= 𝛿𝜆4, 𝛿𝜆S, … , 𝛿𝜆^, 𝛿𝜇4, 𝛿𝜇S, … 𝛿𝜇c , then 𝜃+ ∆𝜃will result in a model with higher log likelihood. • Authors, (in this paper) provide 2 algorithms to solve this: • Generalized Iterative Scaling (Algorithm S) • Improved Iterative Scaling (Algorithm T) • However, the convergence is really slow. • In the experiments done by Lafferty et. al. on using CRF for POS tagging, they observed that CRF did not converge to the optima value even after 2000 iterations (starting from a uniform distribution) • On the other hand, MEMM was able to converge in ~100 iterations. • They, then tried the CRF parameters with the MEMM-optimal parameters as initial values and observed the algorithm to converge in 1000 iterations. 16 Parameter Estimation: Newton method • Consider a multivariate function 𝑙𝜃, where 𝜃= 𝜆4, 𝜆S, … , 𝜆^ . We want to choose ∆𝜃= 𝛿𝜆4, 𝛿𝜆S, … , 𝛿𝜆^ such that : 𝑙𝜃+ ∆𝜃< 𝑙𝜃 • We can use Taylor expansion to compute the value of 𝑙at points nearby 𝜃u (given that the function 𝑙is twice differentiable): 𝑙𝜽u + ∆𝜽≈𝑙𝜽u + ∆𝜽7𝛻𝑙𝜽u + 1 2 ∆𝜽7(𝛻S𝑙𝜽u )∆𝜽 • Here 𝛻𝑙is the gradient and 𝛻S𝑙is the hessian of the function 𝑙 • We need to choose ∆𝜽 to minimize 𝑙𝜽u + ∆𝜽 • So, differentiating the eqn above w.r.t. ∆𝜽 and setting it to zero, we get: ∆𝜽= −𝑯𝒏 3𝟏𝒈𝒏 where 𝑯𝒏 3𝟏is the inverse of hessian matrix and 𝒈𝒏is the gradient, both evaluated at 𝜽u 17 Parameter Estimation: Quasi-Newton method • Newton method requires the computation of inverse of Hessian matrix. • Its quadratic in size • Many problems use millions of features. Even storing the matrix can be a big issue. • Idea in Quasi-Newton method is that instead of recalculating the hessian matrix at every point in the iteration, we can approximate the hessian. The approximation needs to qualify certain conditions: • Symmetricity: Hessian is a symmetric matrix since the order of differentiation is irrelevant • Secant Condition: 𝐻u(𝜽𝒏−𝜽𝒏3𝟏) = (𝒈𝒏−𝒈𝒏3𝟏), that is, hessian is the ratio of the change in gradients w.r.t. the change in values, which is quite natural of the hessian • Positive semi-definiteness • L-BFGS updates are applied to do an approximation constrained on the above conditions • Advantage: The convergence is much faster. 18 Experiments – Mixed Order Sources • Objective of experiment was to observe the performance of systems if the data is from a mixed order Markov chain. • HMM was used to generate data: set of 5 labels and 26 observation values • Transition Probability: 𝑝} 𝑦U 𝑦U34, 𝑦U3S = 𝛼𝑝𝑦U 𝑦U34, 𝑦U3S + 1 −𝛼𝑝(𝑦U 𝑦U34 • Emission Probability: 𝑝} 𝑥U 𝑦U, 𝑥U34 = 𝛼𝑝𝑥U 𝑦U, 𝑥U34 + 1 −𝛼𝑝𝑥U 𝑦U • They don’t mention the initial probabilities • Many test sets were generated with different values of 𝛼 • Linear chain CRF with Generalized Iterative Scaling, MEMM and HMM was used to train the model 19 Experiments – Mixed Order Sources MEMM vs CRF 20 Experiments – Mixed Order Sources MEMM vs HMM 21 Experiments – Mixed Order Sources CRF vs HMM • Square points represent test datasets that were generated with 𝛼< 0.5 and solid circles represents test sets that were generated with 𝛼> 0.5 22 Regularization • A major thing missing in the Lafferty’s paper was regularization. • As we saw in previous slide, the optimal parameters are reached when the model expectation of a feature becomes equal to the empirical expectation of the feature. • Thus, the model can over-fit to the training data. • We can add a regularization term to the log likelihood equation to remedy this: 𝑙𝜃= ∑ ∑ ∑𝜆g𝑓 g(𝑦2 (U), 𝑦234 (U) , 𝒙2 U) C g 7 284 e U84 −log 𝑍𝒙U −∑ ƒ„ … S†… C g • Here, we have used L2-regularizer. We can also use L1-regularizer or could follow structured sparsity approach by grouping features on a template-basis. 23 General CRF • Instead of assuming the graph G to be linear, we can assume a more general graph. • Doing that, the definition of cliques would change in the slide 13 and we will take components of y corresponding to those cliques • Parameter Estimation: • Gradient Descent and Iterative Scaling methods both require the calculation of P(y|x) • Its an NP-hard problem for a general graph • Approximation algorithms are required. • Inference also suffers from the same problem as above. 24 Skip Chain CRF • Linear Chain CRF make the assumption that labels follow the Markov property given the observation sequence. • In tasks such as Information Extraction, it may be important for dependencies among labels for similar observations. • For example, the same name is mentioned multiple times in a document (non consecutively). We may want to link the states for these observation symbols. 𝑃𝒚𝒙= 1 𝑍[exp ( ? ? ? 𝜆^𝑓 ^ 𝑦2 U , 𝑦234 U , 𝒙2 U C ^ 7 284 e U84 ) + exp (? ? ? 𝜇g𝑔g 𝑦ˆ U , 𝑦\ U , 𝒙U C g C (ˆ,\)_ℒ e U84 )] • The second term corresponds to skip-edges (edges between non consecutive states) • The parameters can be estimated in the same way. However, inference becomes difficult. • Viterbi algorithm can no longer be applied. • Have to use approximation algorithms 25 Semi CRF • In many problems such as NER, observation sequence is segmented such as proper names, locations, etc. and each segment needs to be labelled separately. • Night Watchmen stabbed Jon Snow: {(1,2,I), (3,3,O), (4,5,I)}: Each (t,u,y) means t: starting position ; u: ending position ; y: label for the segment • Consider an observation sequence x and its corresponding segmentation s = <(ti,ui,yi)> • Define a vector 𝒈= < 𝑔4, 𝑔S, … , 𝑔‹ > of K feature functions each of which maps a particular segment j in s to a measurement 𝑔g(𝑗, 𝒙, 𝒔). Then, 𝑃𝒔|𝒙, 𝜽= 4 Ž exp ( 𝜽. 𝑮(𝒙, 𝒔)), where 𝐆𝐱, 𝐬= ∑ 𝒈(𝑗, 𝒙, 𝒔) |𝒔| “84 • Inference can be done by Viterbi Algorithm • The parameters 𝜽can be estimated using Quasi-Newton methods 26 References • J. Lafferty, A. McCallum, F. Pereira Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data ICML 2002 • C. Sutton and A. McCallum Introduction to Conditional Random Fields for Relational Learning In Statistical Relational Learning, 2007 • C. Sutton and A. McCallum Collective segmentation and labeling of distant entities in information extraction University of Massachusetts Amherst Dept. Of Computer Science, 2004. • S. Sarawagi and W. Cohen Semi-Markov Conditional Random Fields for Information Extraction NIPS, 2005 27 Thank You 28 "
14,"School of Computer Science Probabilistic Graphical Models Theory of Variational Inference: Inner and Outer Approximation Eric Xing Lecture 14, March 3, 2014 Reading: W & J Book Chapters © Eric Xing @ CMU, 2005-2014 1 Roadmap Two families of approximate inference algorithms  Loopy belief propagation (sum-product)  Mean-field approximation Are there some connections of these two approaches? We will re-exam them from a unified point of view based on the variational principle:  Loop BP: outer approximation  Mean-field: inner approximation © Eric Xing @ CMU, 2005-2014 2 Variational Methods “Variational”: fancy name for optimization-based formulations  i.e., represent the quantity of interest as the solution to an optimization problem  approximate the desired solution by relaxing/approximating the intractable optimization problem Examples:  Courant-Fischer for eigenvalues:  Linear system of equations:  variational formulation:  for large system, apply conjugate gradient method © Eric Xing @ CMU, 2005-2014 3 Inference Problems in Graphical Models Undirected graphical model (MRF): The quantities of interest:  marginal distributions:  normalization constant (partition function): Question: how to represent these quantities in a variational form?  Use tools from (1) exponential families; (2) convex analysis © Eric Xing @ CMU, 2005-2014 4 Exponential Families Canonical parameterization Log normalization constant: it is a convex function (Prop 3.1) Effective canonical parameters: Canonical Parameters Sufficient Statistics Log partition Function © Eric Xing @ CMU, 2005-2014 5 Graphical Models as Exponential Families Undirected graphical model (MRF): MRF in an exponential form:  can be written in a linear form after some parameterization © Eric Xing @ CMU, 2005-2014 6 Example: Gaussian MRF Consider a zero-mean multivariate Gaussian distribution that respects the Markov property of a graph  Hammersley-Clifford theorem states that the precision matrix also respects the graph structure Gaussian MRF in the exponential form  Sufficient statistics are © Eric Xing @ CMU, 2005-2014 7 Example: Discrete MRF  In exponential form © Eric Xing @ CMU, 2005-2014 8 Why Exponential Families? Computing the expectation of sufficient statistics (mean parameters) given the canonical parameters yields the marginals Computing the normalizer yields the log partition function (or log likelihood function) © Eric Xing @ CMU, 2005-2014 9 Computing Mean Parameter: Bernoulli A single Bernoulli random variable Inference = Computing the mean parameter Want to do it in a variational manner: cast the procedure of computing mean (summation) in an optimization-based formulation © Eric Xing @ CMU, 2005-2014 10 Given any function , its conjugate dual function is: Conjugate dual is always a convex function: point-wise supremum of a class of linear functions Conjugate Dual Function © Eric Xing @ CMU, 2005-2014 11 Dual of the Dual is the Original Under some technical condition on (convex and lower semi-continuous), the dual of dual is itself: For log partition function  The dual variable has a natural interpretation as the mean parameters © Eric Xing @ CMU, 2005-2014 12 Computing Mean Parameter: Bernoulli The conjugate Stationary condition If If We have The variational form: The optimum is achieved at . This is the mean! © Eric Xing @ CMU, 2005-2014 13 Remark The last few identities are not coincidental but rely on a deep theory in general exponential family.  The dual function is the negative entropy function  The mean parameter is restricted  Solving the optimization returns the mean parameter and log partition function Next step: develop this framework for general exponential families/graphical models. However,  Computing the conjugate dual (entropy) is in general intractable  The constrain set of mean parameter is hard to characterize  Hence we need approximation © Eric Xing @ CMU, 2005-2014 14 Computation of Conjugate Dual Given an exponential family The dual function The stationary condition: Derivatives of A yields mean parameters The stationary condition becomes Question: for which does it have a solution ? © Eric Xing @ CMU, 2005-2014 15 Computation of Conjugate Dual Let’s assume there is a solution such that The dual has the form The entropy is defined as So the dual is when there is a solution © Eric Xing @ CMU, 2005-2014 16 Complexity of Computing Conjugate Dual The dual function is implicitly defined:  Solving the inverse mapping for canonical parameters is nontrivial  Evaluating the negative entropy requires high-dimensional integration (summation) Question: for which does it have a solution ? i.e., the domain of .  the ones in marginal polytope! © Eric Xing @ CMU, 2005-2014 17 Marginal Polytope For any distribution and a set of sufficient statistics , define a vector of mean parameters  is not necessarily an exponential family The set of all realizable mean parameters  It is a convex set For discrete exponential families, this is called marginal polytope © Eric Xing @ CMU, 2005-2014 18 Convex Polytope Convex hull representation Half-plane representation  Minkowski-Weyl Theorem: any non-empty convex polytope can be characterized by a finite collection of linear inequality constraints © Eric Xing @ CMU, 2005-2014 19 Example: Two-node Ising Model Sufficient statistics: Mean parameters: Two-node Ising model  Convex hull representation  Half-plane representation conv{(0,0,0),(1,0,0),(0,1,0),(1,1,1)} © Eric Xing @ CMU, 2005-2014 20 Marginal Polytope for General Graphs Still doable for connected binary graphs with 3 nodes: 16 constraints For tree graphical models, the number of half-planes (facet complexity) grows only linearly in the graph size General graphs?  extremely hard to characterize the marginal polytope © Eric Xing @ CMU, 2005-2014 21 Variational Principle (Theorem 3.4) The dual function takes the form  satisfies The log partition function has the variational form For all , the above optimization problem is attained uniquely at that satisfies © Eric Xing @ CMU, 2005-2014 22 Example: Two-node Ising Model The distribution  Sufficient statistics The marginal polytope is characterized by The dual has an explicit form The variational problem The optimum is attained at © Eric Xing @ CMU, 2005-2014 23 Variational Principle Exact variational formulation  : the marginal polytope, difficult to characterize  : the negative entropy function, no explicit form Mean field method: non-convex inner bound and exact form of entropy Bethe approximation and loopy belief propagation: polyhedral outer bound and non-convex Bethe approximation © Eric Xing @ CMU, 2005-2014 24 Mean Field Approximation © Eric Xing @ CMU, 2005-2014 25 For an exponential family with sufficient statistics defined on graph G, the set of realizable mean parameter set Idea: restrict p to a subset of distributions associated with a tractable subgraph Tractable Subgraphs © Eric Xing @ CMU, 2005-2014 26 Mean Field Methods For a given tractable subgraph F, a subset of canonical parameters is Inner approximation Mean field solves the relaxed problem  is the exact dual function restricted to © Eric Xing @ CMU, 2005-2014 27 Example: Naïve Mean Field for Ising Model Ising model in {0,1} representation Mean parameters For fully disconnected graph F, The dual decomposes into sum, one for each node µs = E p[Xs] = P [Xs = 1] for all s V, and µst = E p[XsXt] = P [(Xs,Xt) = (1,1)] for all (s,t) E. © Eric Xing @ CMU, 2005-2014 28 Example: Naïve Mean Field for Ising Model Mean field problem The same objective function as in free energy based approach The naïve mean field update equations Also yields lower bound on log partition function © Eric Xing @ CMU, 2005-2014 29 Geometry of Mean Field Mean field optimization is always non-convex for any exponential family in which the state space is finite Recall the marginal polytope is a convex hull  contains all the extreme points  If it is a strict subset, then it must be non-convex Example: two-node Ising model  It has a parabolic cross section along , hence non-convex © Eric Xing @ CMU, 2005-2014 30 Bethe Approximation and Sum-Product © Eric Xing @ CMU, 2005-2014 31 Sum-Product/Belief Propagation Algorithm Message passing rule: Marginals: Exact for trees, but approximate for loopy graphs (so called loopy belief propagation) Question:  How is the algorithm on trees related to variational principle?  What is the algorithm doing for graphs with cycles? © Eric Xing @ CMU, 2005-2014 32 Tree Graphical Models Discrete variables on a tree Sufficient statistics: Exponential representation of distribution: where Mean parameters are marginal probabilities: p(x;θ) e xp X s V θ s(xs) + X (s,t) E θ st(xs,xt) © Eric Xing @ CMU, 2005-2014 33 Marginal Polytope for Trees Recall marginal polytope for general graphs By junction tree theorem (see Prop. 2.1 & Prop. 4.1) In particular, if , then has the corresponding marginals © Eric Xing @ CMU, 2005-2014 34 Decomposition of Entropy for Trees For trees, the entropy decomposes as The dual function has an explicit form © Eric Xing @ CMU, 2005-2014 35 Exact Variational Principle for Trees Variational formulation Assign Lagrange multiplier for the normalization constraint ; and for each marginalization constraint The Lagrangian has the form © Eric Xing @ CMU, 2005-2014 36 Lagrangian Derivation Taking the derivatives of the Lagrangian w.r.t. and Setting them to zeros yields © Eric Xing @ CMU, 2005-2014 37 Lagrangian Derivation (continued) Adjusting the Lagrange multipliers or messages to enforce yields Conclusion: the message passing updates are a Lagrange method to solve the stationary condition of the variational formulation © Eric Xing @ CMU, 2005-2014 38 BP on Arbitrary Graphs Two main difficulties of the variational formulation  The marginal polytope is hard to characterize, so let’s use the tree- based outer bound These locally consistent vectors are called pseudo-marginals.  Exact entropy lacks explicit form, so let’s approximate it by the exact expression for trees © Eric Xing @ CMU, 2005-2014 39 Bethe Variational Problem (BVP) Combining these two ingredient leads to the Bethe variational problem (BVP):  A simple structured problem (differentiable & constraint set is a simple convex polytope)  Loopy BP can be derived as am iterative method for solving a Lagrangian formulation of the BVP (Theorem 4.2); similar proof as for tree graphs  A set of pseudo-marginals given by Loopy BP fixed point in any graph if and only if they are local stationary points of BVP © Eric Xing @ CMU, 2005-2014 40 Geometry of BP Consider the following assignment of pseudo-marginals  Can easily verify  However, (need a bit more work) Tree-based outer bound  For any graph,  Equality holds if and only if the graph is a tree Question: does solution to the BVP ever fall into the gap?  Yes, for any element of outer bound , it is possible to construct a distribution with it as a BP fixed point (Wainwright et. al. 2003) 3 2 1 © Eric Xing @ CMU, 2005-2014 41 Inexactness of Bethe Entropy Approximation Consider a fully connected graph with  It is globally valid: ; realized by the distribution that places mass 1/2 on each of configuration (0,0,0,0) and (1,1,1,1)   3 2 1 4 © Eric Xing @ CMU, 2005-2014 42 Remark This connection provides a principled basis for applying the sum-product algorithm for loopy graphs However,  Although there is always a fixed point of loopy BP, there is no guarantees on the convergence of the algorithm on loopy graphs  The Bethe variational problem is usually non-convex. Therefore, there are no guarantees on the global optimum  Generally, no guarantees that is a lower bound of Nevertheless,  The connection and understanding suggest a number of avenues for improving upon the ordinary sum-product algorithm, via progressively better approximations to the entropy function and outer bounds on the marginal polytope (Kikuchi clustering) © Eric Xing @ CMU, 2005-2014 43 Summary  Variational methods in general turn inference into an optimization problem via exponential families and convex duality  The exact variational principle is intractable to solve; there are two distinct components for approximations:  Either inner or outer bound to the marginal polytope  Various approximation to the entropy function  Mean field: non-convex inner bound and exact form of entropy  BP: polyhedral outer bound and non-convex Bethe approximation  Kikuchi and variants: tighter polyhedral outer bounds and better entropy approximations (Yedidia et. al. 2002) © Eric Xing @ CMU, 2005-2014 44 "
140,"Guiding Semi-Supervision with Constraint-Driven Learning Ming-Wei Chang1 Lev Ratinov2 Dan Roth3 1Department of Computer Science University of Illinois at Urbana-Champaign Paper presentation by: Drew Stone Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 1 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 2 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 3 / 39 Semi-supervised learning Introduction Question: When labelled data is scarce, how can we take advantage of unlabelled data for training? Intuition: If there exists structure in the underlying distribution of samples, points close/clustered to one another may share labels Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 4 / 39 Semi-supervised learning Framework Given a model M trained on labelled samples from a distribution D and an unlabelled set of examples U ⊆Dm Learn labels for each example in U − →Learn(U, M) and re-use examples (now labelled) to tune M − →M∗ Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 5 / 39 Semi-supervised learning Framework Given a model M trained on labelled samples from a distribution D and an unlabelled set of examples U ⊆Dm Learn labels for each example in U − →Learn(U, M) and re-use examples (now labelled) to tune M − →M∗ Beneﬁt: Access to more training data Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 5 / 39 Semi-supervised learning Framework Given a model M trained on labelled samples from a distribution D and an unlabelled set of examples U ⊆Dm Learn labels for each example in U − →Learn(U, M) and re-use examples (now labelled) to tune M − →M∗ Beneﬁt: Access to more training data Drawback: Learned model might drift from correct classiﬁer if the assumptions on the distribution do not hold. Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 5 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 6 / 39 Constraint-driven learning Introduction Motivation: Keep the learned model simple by using constraints to balance over-simplicity Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 7 / 39 Constraint-driven learning Introduction Motivation: Keep the learned model simple by using constraints to balance over-simplicity Beneﬁts: Simple models (less features) are more computationally eﬃcient Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 7 / 39 Constraint-driven learning Introduction Motivation: Keep the learned model simple by using constraints to balance over-simplicity Beneﬁts: Simple models (less features) are more computationally eﬃcient Intuition: Fix a set of task-speciﬁc constraints to enable the use of a simple machine learning model but encode task-speciﬁc constraints to make both learning easier and more correct. Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 7 / 39 Constraint-driven learning Framework Given an objective function argmaxy λ · F(x, y) Deﬁne the set of linear (non-linear) constraints {Ci}i≤k Ci : X × Y − →{0, 1} Solve the optimization problem given the constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 8 / 39 Constraint-driven learning Framework Given an objective function argmaxy λ · F(x, y) Deﬁne the set of linear (non-linear) constraints {Ci}i≤k Ci : X × Y − →{0, 1} Solve the optimization problem given the constraints Any Boolean rule can be encoded as a set of linear inequalities. Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 8 / 39 Constraint-driven learning Example task Sequence tagging with HMM/CRF + global constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 9 / 39 Constraint-driven learning Sequence tagging constraints Unique label for each word Edges must for a path A verb must exist Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 10 / 39 Constraint-driven learning Example task Semantic role labelling with independent classiﬁers + global constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 11 / 39 Constraint-driven learning Example task Each verb predicate carries with it a new inference problem Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 12 / 39 Constraint-driven learning Example task Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 13 / 39 Constraint-driven learning SRL constraints No duplicate argument classes Unique labels Order constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 14 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 15 / 39 Constraint-drive learning w/ semi-supervision Introduction Intuition: Use constraints to provide better training samples from our unlabelled set of samples Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 16 / 39 Constraint-drive learning w/ semi-supervision Introduction Intuition: Use constraints to provide better training samples from our unlabelled set of samples Beneﬁt: Deviations from simple model only do so towards a more expressive answer, since constraints guide the model Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 16 / 39 Constraint-drive learning w/ semi-supervision Examples Consider the constraint over {0, 1}n of 1∗0∗. For every possible sequence {1∗010∗}, there are 2 good ﬁxes {1∗110∗, 1∗000∗}. What is the best approach for learning? Consider the constraint, state transitions must occur on punctuation marks. There could be many good sequences abiding by this constraint. Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 17 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 18 / 39 Constraint-driven learning w/ semi-supervision Model Consider a data and output domain X, Y and a distribution D deﬁned over X × Y Input/output pairs (x, y) ∈X × Y are given as sequence pairs: x = (x1, · · · , xN), y = (y1, · · · , yM) We wish to ﬁnd a structured output classiﬁer h : X − →Y that uses a structured scoring function f : X × Y − →R to choose the most likely output sequence, where the correct output sequence is given the highest score We are given (or deﬁne) a set of constraints {Ci}i≤K Ci : X × Y − →{0, 1} Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 19 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 20 / 39 Constraint-driven learning w/ semi-supervision Scoring Scoring rule: f (x, y) = M X i=1 λifi(x, y) = λ · F(x, y) ≡w T · φ(x, y) Compatible for any linear discriminative and generative model such as HMMs and CRFs Local feature functions {fi}i≤M allow for tractable inference by capturing contextual structure Space of structured pairs could be huge Locally there are smaller distances to account for Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 21 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 22 / 39 Constraint-driven learning w/ semi-supervision Sample constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 23 / 39 Constraint-driven learning w/ semi-supervision Constraint pipeline (hard) Deﬁne 1C(x) ⊆Y as the set of output sequences that assign the value 1 for a given (x, y) Our objective function then becomes argmaxy∈1C(x) λ · F(x, y) Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 24 / 39 Constraint-driven learning w/ semi-supervision Constraint pipeline (hard) Deﬁne 1C(x) ⊆Y as the set of output sequences that assign the value 1 for a given (x, y) Our objective function then becomes argmaxy∈1C(x) λ · F(x, y) Intuition: Find best output sequence y that maximizes the score, ﬁtting the hard constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 24 / 39 Constraint-driven learning w/ semi-supervision Constraint pipeline (soft) Given a suitable distance metric between an output sequence and the space of outputs respecting a single constraint Given a set of soft constraints {Ci}i≤K with penalties {ρi}i≤K, we get a new objective function: argmaxy λ · F(x, y) − K X i=1 ρi · d(y, 1Ci(x)) Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 25 / 39 Constraint-driven learning w/ semi-supervision Constraint pipeline (soft) Given a suitable distance metric between an output sequence and the space of outputs respecting a single constraint Given a set of soft constraints {Ci}i≤K with penalties {ρi}i≤K, we get a new objective function: argmaxy λ · F(x, y) − K X i=1 ρi · d(y, 1Ci(x)) Goal: Find best output sequence under our model that violates the least amount of constraints Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 25 / 39 Constraint-driven learning w/ semi-supervision Constraint pipeline (soft) Given a suitable distance metric between an output sequence and the space of outputs respecting a single constraint Given a set of soft constraints {Ci}i≤K with penalties {ρi}i≤K, we get a new objective function: argmaxy λ · F(x, y) − K X i=1 ρi · d(y, 1Ci(x)) Goal: Find best output sequence under our model that violates the least amount of constraints Intuition: Given a learned model, λ · F(x, y), we can bias its decisions using the amount by which the output sequence violates each soft constraint Option: Use minimal Hamming distance to a sequence d(y, 1Ci(x)) = miny′∈Ci(x)H(y, y′) Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 25 / 39 Constraint-driven learning w/ semi-supervision Distance example Taken from CCM-NAACL-12-Tutorial d(y, 1Ci(x)) = M X j=1 φCi(yj) − →count violations Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 26 / 39 Constraint-driven learning Recall: sequence tagging Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 27 / 39 Constraint-driven learning Recall: sequence tagging Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 28 / 39 Constraint-driven learning Recall: sequence tagging (a) correct citation parsing (b) HMM predicted citation parsing Adding punctuation state transition constraint returns correct parsing under the same HMM Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 29 / 39 Constraint-driven learning Recall: SRL Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 30 / 39 Constraint-driven learning Recall: SRL Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 31 / 39 Constraint-driven learning Recall: SRL Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 32 / 39 Outline 1 Background Semi-supervised learning Constraint-driven learning 2 Constraint-driven learning with semi-supervision Introduction Model Scoring Constraint pipeline Constraint Driven Learning (CODL) algorithm 3 Experiments Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 33 / 39 Constraint-driven learning w/ semi-supervision CODL algorithm Constraints are used in inference not learning Maximum likelihood estimation of λ (EM algorithm) Semi-supervision occurs on line 7,9 (i.e. for each unlabelled sample, we generate K training examples) All unlabelled samples are re-labelled each training cycle, unlike ”self-training” perspective Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 34 / 39 Constraint-driven learning w/ semi-supervision Expectation Maximization: Top K-hard EM EM procedures typically assign a distribution over all input/output pairs for a given unlabelled x ∈X Instead, we choose top K, y ∈Y maximizing our soft objective function and assign uniform probability to each output Constraints mutate distribution each step Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 35 / 39 Constraint-driven learning w/ semi-supervision Motivation for K > 1 There may be multiple ways to correct constraint-violating samples We have access to more training data Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 36 / 39 Experiments Citations Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 37 / 39 Experiments HMM Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 38 / 39 Pictures taken from: CCM-NAACL-12-Tutorial Chang, M. W., Ratinov, L., & Roth, D. (2007, June). Guiding semi-supervision with constraint-driven learning. In ACL (pp. 280-287). Ming-Wei Chang Lev Ratinov Dan Roth (University of Illinois at Urbana-Champaign) Guiding Semi-Supervision with Constraint-Driven Learning Paper presentation by: Drew Stone 39 / 39 "
141,"Integer Linear Programming Inference for Conditional Random Fields By Dan Roth and Wen-tau Yih, ICLM’05 Ran Chen University of Pennsylvania ran1chen@wharton.upenn.edu October 18, 2017 Ran Chen ILP inference for CRFs October 18, 2017 1 / 27 Overview Incorporating general constraints over the output space is natural and important in many settings Though Viterbi, a dynamic programming algorithm can be used eﬃciently to output the labels that maximize the joint conditional probability given the observation ( in CRF setting), it fails to encode general constraints. Integer linear programming can be used to incorporate a wide range of general constraints Experimentally, the post-training inference incorporating general constraints by integer linear programming dramatically improves the performance of both CRF based methods and local learning algorithms. Ran Chen ILP inference for CRFs October 18, 2017 2 / 27 Overview 1 Review of Linear Chain Conditional Random Field and Viterbi Algorithm Formulation of Linear Chain CRF and Viterbi Algorithm Incorporating Constraints in Viterbi 2 Inference Using Integer Linear Programming Solving Shortest Path Problem - a diﬀerent perspective of Viterbi Incorporate General Constraints with ILP 3 Experiments Experiment Setting Experiment Results 4 Take-Away Points Ran Chen ILP inference for CRFs October 18, 2017 3 / 27 General Linear Chain CRF (Model) Assume there are K feature functions, f 1, · · · , f K, each of them maps a pair of sequence (y, x) and token index i to f k(y, x, i) ∈R. Where y stands for the sequence of label and x stands for the sequence of observation. The global feature vector is deﬁned by F(y, x) = P i < f 1(y, x, i), · · · , f K(y, x, i) > the probability distribution is deﬁned as Prλ(Y|X) = exp(λ · F(Y, X)) Zλ(X) , where Zλ(X) = P Y exp(λ · F(Y, X)) is a normalization factor The goal is to ﬁnd y maximizing the above quantity. Ran Chen ILP inference for CRFs October 18, 2017 4 / 27 Linear Chain CRF and Viterbi Algorithm (Inference) - 1 When f k(y, x, i) is only related to x, yi−1 and yi , deﬁne Mi(y′, y|x) = exp(P j λjfj(y′, y, x, i)) , where y′, y ∈Y The sequence probability is p(y|x, λ) = 1 Zλ(x)Πn i=0Mi(yi−1, yi|x), where y−1, yn are two augmented special nodes before and after the start and end of the sequence. ˆ y = argmaxy 1 Zλ(x)Πn i=0Mi(yi−1, yi|x) = argmaxy Πn i=0Mi(yi−1, yi|x) Ran Chen ILP inference for CRFs October 18, 2017 5 / 27 Linear Chain CRF and Viterbi Algorithm (Inference) - 2 Viterbi Algorithm computes the most likely label sequence (ˆ y) given the observation x. At step i, it records all the optimal sequences ending at label y, ∀y ∈Y, y∗ i (y), and also the corresponding product Pi(y). The recursive function of Viterbi Algorithm 1 P0(y) = M0(start, y|x) and y∗ 0(y) = y 2 For 1 ≤i ≤n, y∗ i (y) = y∗ i−1(ˆ y) · (y) and Pi(y) = maxy ′∈YPi−1(y ′)M(y ′, y|(x)), where ˆ y = argmaxy ′∈YPi−1y ′M(y ′, y|x) and ”.” is the concatenation operator Ran Chen ILP inference for CRFs October 18, 2017 6 / 27 Training of Linear Chain CRFs Training is to estimate the values of the weight vector λ given the training set T = {(xk, yk)}N k=1, where xk and yk are observation sequence and label sequence. ˆ λ = argmax λ Lλ = argmax λ X k log(pλ(yk|xk)) = argmax λ X k [λ · F(yk, xk) −log Zλ(xk)] Training methods: 1 maximum likelihood training Find ˆ λ, e.g. generalized iterative scaling, conjugate-gradient, limited-memory quasi-Newton 2 discriminatively learning reducing the number of error predictions directly (ﬁnd ˜ λ = argmaxλ −|{k : yk ̸= argmaxy log(pλ(y|xk))}| ), e.g. sturctured (Voted) perceptron (Collins 2002) Ran Chen ILP inference for CRFs October 18, 2017 7 / 27 Incorporating Constraints in Viterbi—Natural Constraints on output spaces 1 In many NLP problems (e.g. chunking, semantic role labeling, information extraction), there is a need to identify segments of consecutive words in the sentence and classify them to one of several classes BIO representation: label B- (Begin): the ﬁrst word of a segment, ”-” indicates the phrase type label I- (Inside): the word is part of , but not ﬁrst in the segment label O (Outside): all other words in the sentence Figure: suppose we have types: person, location, time, money, organization Ran Chen ILP inference for CRFs October 18, 2017 8 / 27 Incorporating Constraints in Viterbi—Natural Constraints on output spaces 2 When no consecutive segments share the same type, BIO representation can be simpliﬁed to the IO representation Constraints: ”no duplicate segments” (e.g. semantic role labeling); ”I” does not follow ”O” in BIO representation; a known label of a speciﬁc position or disallow some tokens Ran Chen ILP inference for CRFs October 18, 2017 9 / 27 Incorporating Constrains in Viterbi I label does not immediately follow O label ⇒set Mi(yi−1 = O, yi = I|x) = 0, ∀1 ≤i ≤n −1 Label at position i has to be a ⇒set Mi(yi−1, yi) = 0, ∀yi−1 ∈Y and all yi ∈Y −{a} Global constraints? (e.g. no duplicate segments, relation between distant tokens) ⇒unable to incorporate in Viterbi Modiﬁcation of matrix M is not suﬃcient to incorporate long distant, more general constraints Ran Chen ILP inference for CRFs October 18, 2017 10 / 27 Reformulatin gthi problem in Shortest Path Problem Graph G=(V,E) mn + 2 nodes: start, end, vi,j (vi,j i th point with j th label) 2m + (n −1)m2 edges: (start, v0,j), (vn−1,j, end), (vi,j, vi+1,k) edge weight: edge weight of vi−1,y and vi,y′ is −log(Mi(y, y′|x)) reformulate to shortest path problem: ﬁnd argmaxyΠn−1 i=0 Mi(yi−1yi|x) ⇔ﬁnd the shortest path Ran Chen ILP inference for CRFs October 18, 2017 11 / 27 Reformulating the problem in ILP Setting-1 The shortest path problem could be reformulated in integer linear programming Ran Chen ILP inference for CRFs October 18, 2017 12 / 27 Reformulating the problem in ILP Setting-2 Taking the objective function into the formulation, we have Ran Chen ILP inference for CRFs October 18, 2017 13 / 27 Incorporating global constraints through ILP-examples no duplicate argument labels m(n −1 −i)xi,ab ≤ X 0≤y≤m−1 i+1≤j≤n−1 1 −xj,ya , ∀i, a, b s.t. 1 ≤i ≤n −2, 0 ≤b ≤m −1, a ̸= b at least one Argument At least one segment should not be O: X 0≤i≤n−1 0≤y≤m−1 xi,y0 ≤n −1 Known verb position The verb (at position i) should be labeled O: X 0≤y≤m−1 xi,y0 = 1 Ran Chen ILP inference for CRFs October 18, 2017 14 / 27 Incorporating global constraints through ILP-examples segment A of tokens share the same label Denote vi,y = P 0≤y′≤m−1 xi,y′y, then the constraint is |A|vp,l ≤ X i∈A vi,l ∀p ∈A, ∀l, 0 ≤l ≤m −1 a appears ⇒b appears X 0≤y≤m−1 xj,ya ≤ X 0≤y≤m−1 0≤i≤n−1 xi,yb ∀j, 0 ≤j ≤n −1 Ran Chen ILP inference for CRFs October 18, 2017 15 / 27 Incorporating global constraints in Integer Linear Prgramming Setting-2 Theorem All possible Boolean functions over the variables of interest can be represented as sets of linear (in)equalities (Gueret et al., 2002) Ran Chen ILP inference for CRFs October 18, 2017 16 / 27 Properties of Linear programming Deﬁnition (TU) A matrix A is totally unimodular if the determinant of every square submatrix of A is +1,-1,0. Theorem (Veinott & Dantzig) Let A be an (m,n)-integral (integer) matrix with full row rank m. Then solution to the linear program max(cx : Ax ≤b, x ∈Rn +) is integral (integer) vector b, if and only if A is totally unimodular. Ran Chen ILP inference for CRFs October 18, 2017 17 / 27 Linear programming results on Shortest Path Problem Theorem (Wolsey, 1998) The coeﬃcient matrix of the linear programming for the shortest path problem is totally unimodular ⇒(Computing Complexity) Shortest path problem could be transformed from integer linear programming into a common linear programming (adding constraints xi,uv ≤1, −xi,uv ≤0). Therefore, interior point could be used to solve it, which only takes polynomial time. Ran Chen ILP inference for CRFs October 18, 2017 18 / 27 Experiment Setting Semantic Role Labeling using the deﬁnition of PropBank, taking only the core arguments. Using IO representation. The goal is to assign each chunk with one of the following labels: O, I-A0, I-A1, I-A2,I-A3,I-A4,I-A5 features: state features; word; pos,chunk type of the neighboring chunk; edge; end general constraints are not used in training procedures, they are only use in inference procedure CRFs are trained through maximum log likelihood and discriminative method Ran Chen ILP inference for CRFs October 18, 2017 19 / 27 Experiment Setting-Constraints No duplicate argument labels In the SRL task, a verb in a sentence cannot have two arguments of the same type. Argument candidates Generate a candidate list with high recall but low precision. Each candidate argument is a segment of consecutive chunks. Although not every candidate is an argument of the target verb, each chunk in the candidate has to be assigned the same label. This is an eﬀective constraint that provides argument-level information. At least one argument Each verb in a sentence must have at least one core argument ⇒at least one chunk will be assigned a label other than O. Known verb position the known verbs should be labeled 0. Disallow arguments Given a particular verb, not every argument type is legitimate. The arguments that a verb can take are deﬁned in the frame ﬁles in the PropBank corpus. Ran Chen ILP inference for CRFs October 18, 2017 20 / 27 Experiment Results - CRFs General constraints improve the results signiﬁcantly, for both CRF training algorithm. However, incorporating general constraints to discriminative training method (CRF-D) does not perform satisfactory, which may due to the limited training examples (Punykanok et al., 2005) Ran Chen ILP inference for CRFs October 18, 2017 21 / 27 Experiment Results - Local Learning Systems To totally decoupling learning and inference, the training procedure could be reduced to multi-class classiﬁer. Constraints are only used in the inference procedure. The multi-class classiﬁers used are: regularized versions of perceptron, winnow, voted perceptron and voted winnow. The criteria is F1. The results shows that with the increasing constraints, local learning systems’ performance improve dramatically and even surpass the CRFs when all 5 constraints are encompassed. Ran Chen ILP inference for CRFs October 18, 2017 22 / 27 Experiment Results - Comparison CRF results Local learning system’s result IBT, L+I, the diﬃculty of local training Ran Chen ILP inference for CRFs October 18, 2017 23 / 27 Experiment Results - Run Time Local learning based systems are more eﬃcient (w.r.t runtime). Ran Chen ILP inference for CRFs October 18, 2017 24 / 27 Take-Away Points The shortest path problem solved by Viterbi algorithm can be represented and solved through integer linear programming Integer linear programming can systematically incorporate general constraints that can not be incorporated in Viterbi Experimentally, Incorporating general constraints through integer linear programming indeed dramatically improve the performances of both CRFs and local learning systems. Sometimes, in the presence of structure on output, enforcing the constrains only at the evaluation time results in comparable performance at a much lower cost. Ran Chen ILP inference for CRFs October 18, 2017 25 / 27 References Roth D, Yih W (2005) Integer linear programming inference for conditional random ﬁelds[C] Proceedings of the 22nd international conference on Machine learning ACM, 2005: 736-743. Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2005) Learning and inference over constrained output. Proc. of IJCAI-2005. Collins, M. (2002) Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. Proc. of EMNLP-2002. Ran Chen ILP inference for CRFs October 18, 2017 26 / 27 The End Thanks for your attention! Ran Chen ILP inference for CRFs October 18, 2017 27 / 27 "
142,"CS388: Natural Language Processing Lecture 1: Introduc9on Greg Durrett Administrivia ‣ Course website: hBp://www.cs.utexas.edu/~gdurreB/courses/fa2018/cs388.shtml ‣ Piazza: link on the course website ‣ My oﬃce hours: Wednesday 10am-noon, GDC 3.420 ‣ TA: Jifan Chen; Oﬃce hours: ‣ Monday + Tuesday, 1pm-2pm GDC 1.302 ‣ Lecture: Tuesdays and Thursdays 9:30am - 10:50am Course Requirements ‣ 391L Machine Learning (or equivalent) ‣ 311 or 311H Discrete Math for Computer Science (or equivalent) ‣ Addi9onal prior exposure to probability, linear algebra, op9miza9on, linguis9cs, and NLP useful but not required ‣ Python experience Enrollment ‣ I want everyone to be able to take this class! ‣ Mini1 is out now (due September 11): ‣ If this seems like it’ll be challenging for you, come and talk to me (this is smaller- scale than the projects, which are smaller-scale than the ﬁnal project) ‣ Please look at the assignment well before then What’s the goal of NLP? ‣ Be able to solve problems that require deep understanding of text Siri, what’s the most valuable American company? Apple recognize marketCap is the target value recognize predicate do computa9on Who is its CEO? ‣ Example: dialogue systems resolve references Tim Cook Automa9c Summariza9on … … One of New America’s writers posted a statement cri9cal of Google. Eric Schmidt, Google’s CEO, was displeased. The writer and his team were dismissed. provide missing context paraphrase to provide clarity compress text Machine Transla9on Trump Pope family watch a hundred years a year in the White House balcony People’s Daily, August 30, 2017 NLP Analysis Pipeline Syntac9c parses Coreference resolu9on En9ty disambigua9on Discourse analysis Summarize Extract informa9on Answer ques9ons Iden9fy sen9ment ‣ NLP is about building these pieces! Translate Text Analysis Applica/ons Text Annota/ons ‣ All of these components are modeled with sta9s9cal approaches trained with machine learning How do we represent text? Labels Sequences/tags Trees Text the movie was good + Beyoncé had one of the best videos of all 6me subjec/ve Tom Cruise stars in the new Mission Impossible ﬁlm PERSON WORK_OF_ART I eat cake with icing PP NP S NP VP VBZ NN ﬂights to Miami λx. ﬂight(x) ∧ dest(x)=Miami How do we use these representa9ons? Labels Sequences Trees Text Analysis Text ‣ Main ques9on: What representa9ons do we need for language? What do we want to know about it? ‣ Boils down to: what ambigui9es do we need to resolve? … Applica/ons Tree transducers (for machine transla9on) Extract syntac9c features Tree-structured neural networks end-to-end models … Why is language hard? (and how can we handle that?) Language is Ambiguous! ‣ Hector Levesque (2011): “Winograd schema challenge” (named amer Terry Winograd, the creator of SHRDLU) The city council refused the demonstrators a permit because they ______ violence they feared they advocated ‣ This is so complicated that it’s an AI challenge problem! (AI-complete) ‣ Referen9al/seman9c ambiguity Language is Ambiguous! ‣ Headlines slide credit: Dan Klein ‣ Syntac9c/seman9c ambiguity: parsing needed to resolve these, but need context to ﬁgure out which parse is correct ‣ Teacher Strikes Idle Kids ‣ Hospitals Sued by 7 Foot Doctors ‣ Ban on Nude Dancing on Governor’s Desk ‣ Iraqi Head Seeks Arms ‣ Stolen Pain9ng Found by Tree ‣ Kids Make Nutri9ous Snacks ‣ Local HS Dropouts Cut in Half Language is Really Ambiguous! ‣ There aren’t just one or two possibili9es which are resolved pragma9cally ‣ Combinatorially many possibili9es, many you won’t even register as ambigui9es, but systems s9ll have to resolve them It is really nice out il fait vraiment beau It’s really nice The weather is beau9ful It is really beau9ful outside He makes truly beau9ful It fact actually handsome He makes truly boyfriend ‣ Lots of data! slide credit: Dan Klein What do we need to understand language? What do we need to understand language? ‣ World knowledge: have access to informa9on beyond the training data DOJ greenlights Disney - Fox merger metaphor; “approves” Department of Jus6ce ‣ What is a green light? How do we understand what “green ligh9ng” does? ‣ Grounding: learn what fundamental concepts actually mean in a data-driven way McMahan and Stone (2015) Golland et al. (2010) What do we need to understand language? ‣ Linguis9c structure ‣ …but computers probably won’t understand language the same way humans do ‣ However, linguis9cs tells us what phenomena we need to be able to deal with and gives us hints about how language works Centering Theory Grosz et al. (1995) What do we need to understand language? What techniques do we use? (to combine data, knowledge, linguis9cs, etc.) Unsup: topic models, grammar induc9on Collins vs. Charniak parsers A brief history of (modern) NLP 1980 1990 2000 2010 2018 earliest stat MT work at IBM “AI winter” rule-based, expert systems Penn treebank NP VP S Ratnaparkhi tagger NNP VBZ Sup: SVMs, CRFs, NER, Sen9ment Semi-sup, structured predic9on Neural Structured Predic9on ‣ Supervised techniques work well on very liBle data annota9on (two hours!) unsupervised learning ‣ Even neural nets can do preBy well! “Learning a Part-of-Speech Tagger from Two Hours of Annota9on” GarreBe and Baldridge (2013) beBer system! ‣ All of these techniques are data-driven! Some data is naturally occurring, but may need to label Less Manual Structure? Hall, DurreB, Klein (2014) Klein and Manning (2003) Petrov et al. (2006) ‣ Manually constructed grammars -> EM-induced grammars -> basic grammars + features -> … Sutskever et al. (2015), Bahdanau et al. (2014) Less Manual Structure? LSTM The yield on the benchmark issue rose to 10% from 5% ( S ( NP ( NP ( DT The ) ( NN yield … ‣ No grammars! Bahdanau et al. (2014) DeNero et al. (2008) Less Manual Structure? Does manual structure have a place? ‣ Neural nets don’t always work out of domain! Moosavi and Strube (2017) ‣ Coreference: rule-based systems are s9ll about as good as deep learning out-of-domain ‣ LORELEI: transi9on point below which phrase- based systems are beBer ‣ Why is this? Induc9ve bias! ‣ Can mul9-task learning help? Wikipedia Newswire Trump Pope family watch a hundred years a year in the White House balcony ‣ Maybe manual structure would help… Does manual structure have a place? Where are we? ‣ NLP consists of: analyzing and building representa9ons for text, solving problems involving text ‣ These problems are hard because language is ambiguous, requires drawing on data, knowledge, and linguis9cs to solve ‣ Knowing which techniques use requires understanding dataset size, problem complexity, and a lot of tricks! ‣ NLP encompasses all of these things NLP vs. Computa9onal Linguis9cs ‣ NLP: build systems that deal with language data ‣ CL: use computa9onal tools to study language Hamilton et al. (2016) NLP vs. Computa9onal Linguis9cs ‣ Computa9onal tools for other purposes: literary theory, poli9cal science… Bamman, O’Connor, Smith (2013) Outline of the Course ML and structured predic9on for NLP { Neural nets{ { Syntax/ seman9cs { Applica9ons: MT, IE, summariza9on, dialogue, etc. Course Goals ‣ Cover fundamental machine learning techniques used in NLP ‣ Make you a “producer” rather than a “consumer” of NLP tools ‣ Cover modern NLP problems encountered in the literature: what are the ac9ve research topics in 2018? ‣ The four assignments should teach you what you need to know to understand nearly any system in the literature (e.g.: state-of-the-art NER system = project 1 + mini 2, basic MT system = project 2) ‣ Understand how to look at language data and approach linguis9c phenomena Assignments ‣ Two minis (10% each), two projects (20% each) ‣ Implementa9on-oriented, with an open-ended component to each ‣ Mini 1 (classiﬁca9on) is out NOW ‣ ~2 weeks per assignment, 5 “slip days” for automa9c extensions ‣ Grading: ‣ Minis: 80% for reaching the performance threshold, 20% writeup ‣ Projects: 60% for reaching the performance threshold, 20% writeup, 20% extension These projects require understanding of the concepts, ability to write performant code, and ability to think about how to debug complex systems. They are challenging, so start early! Assignments ‣ Final project (40%) ‣ Groups of 2 preferred, 1 is possible ‣ (Brief!) proposal to be approved by me ‣ WriBen in the style and tone of an ACL paper Survey 1. Fill in: I am a [CS / ____] [PhD / masters / undergrad] in year [1 2 3 4 5+] 2. Which of the following have you learned in a class? 1. Bayes’ Rule 2. SVMs 3. Expecta9on maximiza9on 4. RNNs 3. Which of the following have you used? 1. Python 2. numpy/scipy/scikit-learn 3. Tensorﬂow/(Py)Torch/Theano 4. Fill in: Assuming I can enroll, my probability of taking this class is X% 5. One interes9ng fact about yourself, or what you like to do in your spare 9me "
143,"CS395T: Structured Models for NLP Lecture 2: Binary Classiﬁca>on Greg Durrett Some slides adapted from Vivek Srikumar, University of Utah Administrivia ‣ Course enrollment ‣ OHs this week: Jifan 1pm-2pm Tues (today) in GDC 1.304 TA desk #1 Greg 11am-12pm Weds + 10am-11am Fri in GDC 3.420 ‣ Readings on course website ‣ Mini1 is out, due September 11 ‣ Feel free to extend the code as needed; op>mizers, featuriza>on, etc. isn’t set in stone This Lecture ‣ Linear classiﬁca>on fundamentals ‣ Three discrimina>ve models: logis>c regression, perceptron, SVM ‣ Naive Bayes, maximum likelihood in genera>ve models ‣ Diﬀerent mo>va>ons but very similar update rules / inference! Classiﬁca>on Classiﬁca>on ‣ Embed datapoint in a feature space + ++ + + + + + - - - - - - - - - ‣ Linear decision rule: = [0.5, 1.6, 0.3] [0.5, 1.6, 0.3, 1] x y 2 {0, 1} f(x) 2 Rn ‣ Datapoint with label but in this lecture and are interchangeable x f(x) w>f(x) + b > 0 f(x) ‣ Can delete bias if we augment feature space: w>f(x) > 0 + ++ + + + + + - - - - - - - - - + ++ + + + + + - - - - - - - - - ??? f(x) = [x1, x2, x12, x22, x1x2] x1 x2 + ++ + + + + + - - - - - - - - - + ++ + + + + + - - - - - - - - - x1x2 x1 f(x) = [x1, x2] Linear func>ons are powerful! ‣ “Kernel trick” does this for “free,” but is too expensive to use in NLP applica>ons, training is instead of O(n2) O(n · (num feats)) Classiﬁca>on: Sen>ment Analysis this movie was great! would watch again Nega>ve Posi>ve that ﬁlm was awful, I’ll never watch again ‣ Surface cues can basically tell you what’s going on here: presence or absence of certain words (great, awful) ‣ Steps to classiﬁca>on: ‣ Turn examples like this into feature vectors ‣ Pick a model / learning algorithm ‣ Train weights on data to get our classiﬁer Feature Representa>on this movie was great! would watch again Posi>ve ‣ Convert this example to a vector using bag-of-words features ‣ Requires indexing the features (mapping them to axes) [contains the] [contains a] [contains was] [contains movie] [contains ﬁlm] 0 0 1 1 0 ‣ More sophis>cated feature mappings possible (m-idf), as well as lots of other features: character n-grams, parts of speech, lemmas, … posi>on 0 posi>on 1 posi>on 2 posi>on 3 posi>on 4 ‣ Very large vector space (size of vocabulary), sparse features … f(x) = [ … Naive Bayes Naive Bayes ‣ Data point , label P(y|x) = P(y)P(x|y) P(x) / P(y)P(x|y) constant: irrelevant for ﬁnding the max = P(y) n Y i=1 P(xi|y) Bayes’ Rule “Naive” assump>on: x = (x1, ..., xn) y 2 {0, 1} ‣ Formulate a probabilis>c model that places a distribu>on linear model! P(y|x) y n xi ‣ Compute , predict to classify P(x, y) argmaxyP(y|x) argmaxyP(y|x) = argmaxy log P(y|x) = argmaxy "" log P(y) + n X i=1 log P(xi|y) # Naive Bayes Example P(y|x) /[ ] it was great P(y|x) / P(y) n Y i=1 P(xi|y) = argmaxy log P(y|x) = argmaxy "" log P(y) + n X i=1 log P(xi|y) # Maximum Likelihood Es>ma>on ‣ Data points provided (j indexes over examples) ‣ Find values of that maximize data likelihood (genera>ve): P(y), P(xi|y) (xj, yj) data points (j) features (i) m Y j=1 P(yj, xj) = m Y j=1 P(yj) "" n Y i=1 P(xji|yj) # ith feature of jth example Maximum Likelihood Es>ma>on ‣ Imagine a coin ﬂip which is heads with probability p m X j=1 log P(yj) = 3 log p + log(1 −p) log likelihood p 0 1 P(H) = 0.75 ‣ Maximum likelihood parameters for binomial/ mul>nomial = read counts oﬀ of the data + normalize ‣ Observe (H, H, H, T) and maximize likelihood: m Y j=1 P(yj) = p3(1 −p) ‣ Easier: maximize log likelihood Maximum Likelihood Es>ma>on ‣ Data points provided (j indexes over examples) ‣ Find values of that maximize data likelihood (genera>ve): P(y), P(xi|y) (xj, yj) data points (j) features (i) m Y j=1 P(yj, xj) = m Y j=1 P(yj) "" n Y i=1 P(xji|yj) # ‣ Equivalent to maximizing logarithm of data likelihood: m X j=1 log P(yj, xj) = m X j=1 "" log P(yj) + n X i=1 log P(xji|yj) # ith feature of jth example Maximum Likelihood for Naive Bayes — + this movie was great! would watch again that ﬁlm was awful, I’ll never watch again — I didn’t really like that movie dry and a bit distasteful, it misses the mark — great potenCal but ended up being a ﬂop — + I liked it well enough for an acCon ﬂick I expected a great ﬁlm and leE happy + + brilliant direcCng and stunning visuals P(great|+) = 1 2 P(great|−) = 1 4 P(+) = 1 2 P(−) = 1 2 P(y|x) / P(+)P(great|+) P(−)P(great|−) [ ]= 1/4 1/8 [ ]= 2/3 1/3 [ ] it was great P(great|−) = 1 4 Naive Bayes: Summary ‣ Model y n xi P(x, y) = P(y) n Y i=1 P(xi|y) ‣ Learning: maximize by reading counts oﬀ the data ‣ Inference P(x, y) argmaxy log P(y|x) = argmaxy "" log P(y) + n X i=1 log P(xi|y) # ‣ Alterna>vely: log P(y = +|x) −log P(y = −|x) > 0 , log P(y = +|x) P(y = −|x) + n X i=1 log P(xi|y = +) P(xi|y = −) > 0 Problems with Naive Bayes ‣ Naive Bayes is naive, but another problem is that it’s generaCve: spends capacity modeling P(x,y), when what we care about is P(y|x) ‣ Correlated features compound: beauCful and gorgeous are not independent! the ﬁlm was beauCful, stunning cinematography and gorgeous sets, but boring — P(xbeautiful|+) = 0.1 P(xstunning|+) = 0.1 P(xgorgeous|+) = 0.1 P(xbeautiful|−) = 0.01 P(xstunning|−) = 0.01 P(xgorgeous|−) = 0.01 P(xboring|−) = 0.1 P(xboring|+) = 0.01 ‣ Discrimina>ve models model P(y|x) directly (SVMs, most neural networks, …) Logis>c Regression Logis>c Regression ‣ To learn weights: maximize discrimina>ve log likelihood of data P(y|x) P(y = +|x) = logistic(w>x) P(y = +|x) = exp(Pn i=1 wixi) 1 + exp(Pn i=1 wixi) L(xj, yj = +) = log P(yj = +|xj) = n X i=1 wixji −log 1 + exp n X i=1 wixji !! sum over features Logis>c Regression @L(xj, yj) @wi = xji − @ @wi log 1 + exp n X i=1 wixji !! = xji − 1 1 + exp (Pn i=1 wixji) @ @wi 1 + exp n X i=1 wixji !! = xji − 1 1 + exp (Pn i=1 wixji)xji exp n X i=1 wixji ! deriv of log deriv of exp = xji −xji exp (Pn i=1 wixji) 1 + exp (Pn i=1 wixji) = xji(1 −P(yj = +|xj)) L(xj, yj = +) = log P(yj = +|xj) = n X i=1 wixji −log 1 + exp n X i=1 wixji !! Logis>c Regression If P(+) is close to 1, make very liule update Otherwise make wi look more like xji, which will increase P(+) ‣ Gradient of wi on posi>ve example ‣ Gradient of wi on nega>ve example If P(+) is close to 0, make very liule update Otherwise make wi look less like xji, which will decrease P(+) xj(yj −P(yj = 1|xj)) = xji(−P(yj = +|xj)) = xji(yj −P(yj = +|xj)) ‣ Can combine these gradients as ‣ Recall that yj = 1 for posi>ve instances, yj = 0 for nega>ve instances. Regulariza>on ‣ Regularizing an objec>ve can mean many things, including an L2- norm penalty to the weights: m X j=1 L(xj, yj) −λkwk2 2 ‣ Keeping weights small can prevent overﬁwng ‣ For most of the NLP models we build, explicit regulariza>on isn’t necessary ‣ Early stopping ‣ For neural networks: dropout and gradient clipping ‣ Large numbers of sparse features are hard to overﬁt in a really bad way Logis>c Regression: Summary ‣ Model ‣ Learning: gradient ascent on the (regularized) discrimina>ve log- likelihood ‣ Inference argmaxyP(y|x) fundamentally same as Naive Bayes P(y = 1|x) ≥0.5 , w>x ≥0 P(y = +|x) = exp(Pn i=1 wixi) 1 + exp(Pn i=1 wixi) Perceptron/SVM Perceptron ‣ Simple error-driven learning approach similar to logis>c regression ‣ Decision rule: ‣ Guaranteed to eventually separate the data if the data are separable ‣ If incorrect: if posi>ve, if nega>ve, w w + x w w −x w w −xP(y = 1|x) w w + x(1 −P(y = 1|x)) Logis>c Regression w>x > 0 Support Vector Machines ‣ Many separa>ng hyperplanes — is there a best one? + ++ + + + + + - - - - - - - - - Support Vector Machines ‣ Many separa>ng hyperplanes — is there a best one? + ++ + + + + + - - - - - - - - - margin Support Vector Machines ‣ Constraint formula>on: ﬁnd w via following quadra>c program: Minimize s.t. As a single constraint: minimizing norm with ﬁxed margin <=> maximizing margin kwk2 2 8j w>xj ≥1 if yj = 1 w>xj −1 if yj = 0 8j (2yj −1)(w>xj) ≥1 ‣ Generally no solu>on (data is generally non-separable) — need slack! N-Slack SVMs Minimize s.t. 8j (2yj −1)(w>xj) ≥1 −⇠j 8j ⇠j ≥0 ‣ The are a “fudge factor” to make all constraints sa>sﬁed ⇠j λkwk2 2 + m X j=1 ⇠j ‣ Take the gradient of the objec>ve: @ @wi ⇠j = 0 if ⇠j = 0 @ @wi ⇠j = (2yj −1)xji if ⇠j > 0 = xji if yj = 1, −xji if yj = 0 ‣ Looks like the perceptron! But updates more frequently Gradients on Posi>ve Examples Logis>c regression Perceptron ) = x(1 −logistic(w>x)) x if w>x < 0, else 0 SVM (ignoring regularizer) Hinge (SVM) Logis>c Perceptron 0-1 Loss w>x *gradients are for maximizing things, which is why they are ﬂipped x if w>x < 1, else 0 Comparing Gradient Updates (Reference) x(y −P(y = 1|x)) x(y −logistic(w>x)) Perceptron if classiﬁed incorrectly 0 else SVM if not classiﬁed correctly with margin of 1 0 else (2y −1)x (2y −1)x = y = 1 for pos, 0 for neg Logis>c regression (unregularized) Op>miza>on — next >me… ‣ Range of techniques from simple gradient descent (works preuy well) to more complex methods (can work beuer) ‣ Most methods boil down to: take a gradient and a step size, apply the gradient update >mes step size, incorporate es>mated curvature informa>on to make the update more eﬀec>ve Sen>ment Analysis Bo Pang, Lillian Lee, Shivakumar Vaithyanathan (2002) the movie was gross and overwrought, but I liked it this movie was great! would watch again ‣ Bag-of-words doesn’t seem suﬃcient (discourse structure, nega>on) this movie was not really very enjoyable ‣ There are some ways around this: extract bigram feature for “not X” for all X following the not + + — Sen>ment Analysis ‣ Simple feature sets can do preuy well! Bo Pang, Lillian Lee, Shivakumar Vaithyanathan (2002) Sen>ment Analysis Wang and Manning (2012) Before neural nets had taken oﬀ — results weren’t that great Naive Bayes is doing well! Ng and Jordan (2002) — NB can be beuer for small data 81.5 89.5 Kim (2014) CNNs Recap ‣ Logis>c regression: P(y = 1|x) = exp (Pn i=1 wixi) (1 + exp (Pn i=1 wixi)) Gradient (unregularized): ‣ SVM: Decision rule: Decision rule: w>x ≥0 P(y = 1|x) ≥0.5 , w>x ≥0 (Sub)gradient (unregularized): 0 if correct with margin of 1, else x(y −P(y = 1|x)) x(2y −1) Recap ‣ Logis>c regression, SVM, and perceptron are closely related ‣ SVM and perceptron inference require taking maxes, logis>c regression has a similar update but is “so}er” due to its probabilis>c nature ‣ All gradient updates: “make it look more like the right thing and less like the wrong thing” "
144,"CS388: Natural Language Processing Lecture 3: Mul7class Classiﬁca7on Greg Durrett Some slides adapted from Vivek Srikumar, University of Utah Administrivia ‣ Course enrollment ‣ All materials on the course website (linked from my homepage) ‣ Mini 1 due Tuesday at 5pm! Recall: Binary Classiﬁca7on ‣ Logis7c regression: P(y = 1|x) = exp (Pn i=1 wixi) (1 + exp (Pn i=1 wixi)) Gradient (unregularized): ‣ SVM: quadra7c program to minimize weight vector norm w/slack Decision rule: Decision rule: w>x ≥0 P(y = 1|x) ≥0.5 , w>x ≥0 (Sub)gradient (unregularized): 0 if correct with margin of 1, else x(y −P(y = 1|x)) x(2y −1) Loss Func7ons Hinge (SVM) Logis7c Perceptron 0-1 (ideal) w>x Loss This Lecture ‣ Mul7class fundamentals ‣ Mul7class logis7c regression ‣ Mul7class SVM ‣ Feature extrac7on ‣ Op7miza7on Mul7class Fundamentals Text Classiﬁca7on ~20 classes Sports Health Image Classiﬁca7on ‣ Thousands of classes (ImageNet) Car Dog En7ty Linking ‣ 4,500,000 classes (all ar7cles in Wikipedia) Although he originally won the event, the United States An7- Doping Agency announced in August 2012 that they had disqualiﬁed Armstrong from his seven consecu7ve Tour de France wins from 1999–2005. Lance Edward Armstrong is an American former professional road cyclist Armstrong County is a county in Pennsylvania… ? ? Reading Comprehension ‣ Mul7ple choice ques7ons, 4 classes (but classes change per example) Richardson (2013) Binary Classiﬁca7on ‣ Binary classiﬁca7on: one weight vector deﬁnes posi7ve and nega7ve classes + ++ + + + + + - - - - - - - - - Mul7class Classiﬁca7on 1 1 1 1 1 1 2 22 2 3 3 3 3 ‣ Can we just use binary classiﬁers here? Mul7class Classiﬁca7on ‣ One-vs-all: train k classiﬁers, one to dis7nguish each class from all the rest 1 1 1 1 1 1 2 22 2 3 3 3 3 1 1 1 1 1 1 2 22 2 3 3 3 3 ‣ How do we reconcile mul7ple posi7ve predic7ons? Highest score? Mul7class Classiﬁca7on ‣ Not all classes may even be separable using this approach 1 1 1 1 1 1 2 2 22 2 2 3 3 3 3 3 3 ‣ Can separate 1 from 2+3 and 2 from 1+3 but not 3 from the others (with these features) Mul7class Classiﬁca7on ‣ All-vs-all: train n(n-1)/2 classiﬁers to diﬀeren7ate each pair of classes 1 1 1 1 1 1 3 3 3 3 1 1 1 1 1 1 2 22 2 ‣ Again, how to reconcile? Mul7class Classiﬁca7on + ++ + + + + + - - - - - - - - - 1 1 1 1 1 1 2 22 2 3 3 3 3 ‣ Binary classiﬁca7on: one weight vector deﬁnes both classes ‣ Mul7class classiﬁca7on: diﬀerent weights and/or features per class Mul7class Classiﬁca7on ‣ Decision rule: ‣ Can also have one weight vector per class: ‣ Formally: instead of two labels, we have an output space containing a number of possible classes Y ‣ Same machinery that we’ll use later for exponen7ally large output spaces, including sequences and trees argmaxy2Yw> y f(x) argmaxy2Yw>f(x, y) ‣ Single-weight-vector is going to be beier for reasons we’ll come back to ‣ Mul7ple feature vectors, one weight vector features depend on choice of label now! note: this isn’t the gold label Feature Extrac7on Block Feature Vectors ‣ Decision rule: argmaxy2Yw>f(x, y) too many drug trials, too few pa5ents Health Sports Science f(x)= I[contains drug], I[contains pa5ents], I[contains baseball] = [1, 1, 0] [1, 1, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0, 0] f(x, y = ) = Health f(x, y = ) = Sports ‣ Equivalent to having three weight vectors in this case feature vector blocks for each label ‣ Base feature func7on: I[contains drug & label = Health] Making Decisions f(x) = I[contains drug], I[contains pa5ents], I[contains baseball] w = [+2.1, +2.3, -5, -2.1, -3.8, +5.2, +1.1, -1.7, -1.3] = Health: +4.4 Sports: -5.9 Science: -1.9 argmax too many drug trials, too few pa5ents Health Sports Science [1, 1, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0, 0] f(x, y = ) = Health f(x, y = ) = Sports “word drug in Science ar7cle” = +1.1 w>f(x, y) Another example: POS tagging blocks NNS VBZ NN DT … f(x, y=VBZ) = I[curr_word=blocks & tag = VBZ], I[prev_word=router & tag = VBZ] I[next_word=the & tag = VBZ] I[curr_suﬃx=s & tag = VBZ] ‣ Classify blocks as one of 36 POS tags ‣ Next two lectures: sequence labeling! ‣ Example x: sentence with a word (in this case, blocks) highlighted ‣ Extract features with respect to this word: not saying that the is tagged as VBZ! saying that the follows the VBZ word the router the packets Mul7class Logis7c Regression Mul7class Logis7c Regression ‣ Compare to binary: nega7ve class implicitly had f(x, y=0) = the zero vector sum over output space to normalize ‣ Training: maximize = n X j=1 w>f(xj, y⇤ j ) −log X y exp(w>f(xj, y)) ! L(x, y) = n X j=1 log P(y⇤ j |xj) P(y = 1|x) = exp(w>f(x)) 1 + exp(w>f(x)) Pw(y|x) = exp ! w>f(x, y) "" P y02Y exp (w>f(x, y0)) Training ‣ Mul7class logis7c regression ‣ Likelihood L(xj, y⇤ j ) = w>f(xj, y⇤ j ) −log X y exp(w>f(xj, y)) @ @wi L(xj, y⇤ j ) = fi(xj, y⇤ j ) − P y fi(xj, y) exp(w>f(xj, y)) P y exp(w>f(xj, y)) @ @wi L(xj, y⇤ j ) = fi(xj, y⇤ j ) −Ey[fi(xj, y)] gold feature value model’s expecta7on of feature value Pw(y|x) = exp ! w>f(x, y) "" P y02Y exp (w>f(x, y0)) @ @wi L(xj, y⇤ j ) = fi(xj, y⇤ j ) − X y fi(xj, y)Pw(y|xj) Training too many drug trials, too few pa5ents [1, 1, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0, 0] f(x, y = ) = Health f(x, y = ) = Sports y* = Health Pw(y|x) = [0.2, 0.5, 0.3] [1, 1, 0, 0, 0, 0, 0, 0, 0] — 0.2 [1, 1, 0, 0, 0, 0, 0, 0, 0] — 0.5 [0, 0, 0, 1, 1, 0, 0, 0, 0] — 0.3 [0, 0, 0, 0, 0, 0, 1, 1, 0] = [0.8, 0.8, 0, -0.5, -0.5, 0, -0.3, -0.3, 0] gradient: (made up values) @ @wi L(xj, y⇤ j ) = fi(xj, y⇤ j ) − X y fi(xj, y)Pw(y|xj) Logis7c Regression: Summary ‣ Model: ‣ Learning: gradient ascent on the discrimina7ve log-likelihood ‣ Inference: “towards gold feature value, away from expecta7on of feature value” f(x, y⇤) −Ey[f(x, y)] = f(x, y⇤) − X y [Pw(y|x)f(x, y)] Pw(y|x) = exp ! w>f(x, y) "" P y02Y exp (w>f(x, y0)) argmaxyPw(y|x) Training ‣ Are all decisions equally costly? ‣ We can deﬁne a loss func7on `(y, y⇤) too many drug trials, too few pa5ents Health Sports Sports Science Sports Science Predicted Predicted : not so bad : bad error `( , ) = Health Sports Health Science `( , ) = 3 1 Mul7class SVM Mul7class SVM Correct predic7on now has to beat every other class Minimize s.t. 8j (2yj −1)(w>xj) ≥1 −⇠j 8j ⇠j ≥0 λkwk2 2 + m X j=1 ⇠j 8j8y 2 Y w>f(xj, y⇤ j ) ≥w>f(xj, y) + `(y, y⇤ j ) −⇠j The 1 that was here is replaced by a loss func7on Score comparison is more explicit now slack variables > 0 iﬀ example is support vector Mul7class SVM Health Science Sports 2.4+0 1.3+3 1.8+1 Y ‣ Does gold beat every label + loss? No! ‣ = 4.3 - 2.4 = 1.9 ⇠j ‣ Most violated constraint is Sports; what is ? 8j8y 2 Y w>f(xj, y⇤ j ) ≥w>f(xj, y) + `(y, y⇤ j ) −⇠j w>f(x, y) + `(y, y⇤) ‣ Perceptron would make no update here ⇠j Mul7class SVM Minimize s.t. 8j ⇠j ≥0 λkwk2 2 + m X j=1 ⇠j 8j8y 2 Y w>f(xj, y⇤ j ) ≥w>f(xj, y) + `(y, y⇤ j ) −⇠j ‣ One slack variable per example, so it’s set to be whatever the most violated constraint is for that example ⇠j = max y2Y w>f(xj, y) + `(y, y⇤ j ) −w>f(xj, y⇤ j ) ‣ Plug in the gold y and you get 0, so slack is always nonnega7ve! Compu7ng the Subgradient ‣ If , the example is not a support vector, gradient is zero ⇠j = 0 ‣ Otherwise, (update looks backwards — we’re minimizing here!) @ @wi ⇠j = fi(xj, ymax) −fi(xj, y⇤ j ) ‣ Perceptron-like, but we update away from *loss-augmented* predic7on Minimize s.t. 8j ⇠j ≥0 λkwk2 2 + m X j=1 ⇠j 8j8y 2 Y w>f(xj, y⇤ j ) ≥w>f(xj, y) + `(y, y⇤ j ) −⇠j ⇠j = max y2Y w>f(xj, y) + `(y, y⇤ j ) −w>f(xj, y⇤ j ) Puzng it Together Minimize s.t. 8j ⇠j ≥0 λkwk2 2 + m X j=1 ⇠j 8j8y 2 Y w>f(xj, y⇤ j ) ≥w>f(xj, y) + `(y, y⇤ j ) −⇠j ‣ (Unregularized) gradients: ‣ SVM: f(x, y⇤) −Ey[f(x, y)] = f(x, y⇤) − X y [Pw(y|x)f(x, y)] ‣ Log reg: f(x, y⇤) −f(x, ymax) (loss-augmented max) ‣ SVM: max over ys to compute gradient. LR: need to sum over ys Op7miza7on Structured Predic7on ‣ Four elements of a structured machine learning method: ‣ Model: probabilis7c, max-margin, deep neural network ‣ Objec7ve ‣ Inference: just maxes and simple expecta7ons so far, but will get harder ‣ Training: gradient descent? Op7miza7on ‣ Stochas7c gradient *ascent* ‣ Very simple to code up ‣ “First-order” technique: only relies on having gradient ‣ Newton’s method ‣ Second-order technique Inverse Hessian: n x n mat, expensive! ‣ Op7mizes quadra7c instantly ‣ Quasi-Newton methods: L-BFGS, etc. approximate inverse Hessian ‣ Sezng step size is hard (decrease when held-out performance worsens?) w w + ↵g, g = @ @wL w w + ✓@2 @w2 L ◆−1 g AdaGrad Duchi et al. (2011) ‣ Op7mized for problems with sparse features ‣ Per-parameter learning rate: smaller updates are made to parameters that get updated frequently (smoothed) sum of squared gradients from all updates ‣ Generally more robust than SGD, requires less tuning of learning rate ‣ Other techniques for op7mizing deep models — more later! wi wi + ↵ 1 q ✏+ Pt ⌧=1 g2 ⌧,i gti Summary ‣ You’ve now seen everything you need to implement mul7-class classiﬁca7on models ‣ Next 7me: HMMs (POS tagging) ‣ In 2 lectures: CRFs (NER) "
145,"CS388: Natural Language Processing Lecture 4: Sequence Models I Greg Durrett Parts of this lecture adapted from Dan Klein, UC Berkeley and Vivek Srikumar, University of Utah Administrivia ‣ Project 1 out today, due September 27 ‣ This class will cover what you need to get started on it, the next class will cover everything you need to complete it ‣ Viterbi algorithm, CRF NER system, extension ‣ Extension should be substanUal: don’t just try one addiUonal feature (see syllabus/spec for discussion, samples on website) ‣ Mini 1 due today Recall: MulUclass ClassiﬁcaUon ‣ LogisUc regression: Gradient (unregularized): ‣ SVM: deﬁned by quadraUc program (minimizaUon, so gradients are ﬂipped) Loss-augmented decode P(y|x) = exp ! w>f(x, y) "" P y02Y exp (w>f(x, y0)) @ @wi L(xj, y⇤ j ) = fi(xj, y⇤ j ) −Ey[fi(xj, y)] ⇠j = max y2Y w>f(xj, y) + `(y, y⇤ j ) −w>f(xj, y⇤ j ) Subgradient (unregularized) on jth example = fi(xj, ymax) −fi(xj, y⇤ j ) Recall: OpUmizaUon ‣ StochasUc gradient *ascent* w w + ↵g, g = @ @wL wi wi + ↵ 1 q ✏+ Pt ⌧=1 g2 ⌧,i gt,i ‣ Adagrad: ‣ SGD/AdaGrad have a batch size parameter ‣ Large batches (>50 examples): can parallelize within batch ‣ …but bigger batches oden means more epochs required because you make fewer parameter updates ‣ Shuﬄing: online methods are sensiUve to dataset order, shuﬄing helps! This Lecture ‣ Sequence modeling ‣ HMMs for POS tagging ‣ Viterbi, forward-backward ‣ HMM parameter esUmaUon LinguisUc Structures ‣ Language is tree-structured I ate the spagheh with chopsUcks I ate the spagheh with meatballs ‣ Understanding syntax fundamentally requires trees — the sentences have the same shallow analysis I ate the spagheh with chopsUcks I ate the spagheh with meatballs PRP VBZ DT NN IN NNS PRP VBZ DT NN IN NNS LinguisUc Structures ‣ Language is sequenUally structured: interpreted in an online way Tanenhaus et al. (1995) POS Tagging Ghana ’s ambassador should have set up the big mee6ng in DC yesterday . ‣ What tags are out there? NNP POS NN MD VB VBN RP DT JJ NN IN NNP NN . POS Tagging Slide credit: Dan Klein POS Tagging Fed raises interest rates 0.5 percent VBD VBN NNP VBZ NNS VB VBP NN VBZ NNS CD NN I’m 0.5% interested in the Fed’s raises! I hereby increase interest rates 0.5% Fed raises interest rates 0.5 percent VBD VBN NNP VBZ NNS VB VBP NN VBZ NNS CD NN ‣ Other paths are also plausible but even more semanUcally weird… ‣ What governs the correct choice? Word + context ‣ Word idenUty: most words have <=2 tags, many have one (percent, the) ‣ Context: nouns start sentences, nouns follow verbs, etc. What is this good for? ‣ Text-to-speech: record, lead ‣ Preprocessing step for syntacUc parsers ‣ Domain-independent disambiguaUon for other tasks ‣ (Very) shallow informaUon extracUon Sequence Models ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output ‣ POS tagging: x is a sequence of words, y is a sequence of tags ‣ Today: generaUve models P(x, y); discriminaUve models next Ume Hidden Markov Models y = (y1, ..., yn) Output ‣ Input x = (x1, ..., xn) ‣ Model the sequence of y as a Markov process (dynamics model) y1 y2 ‣ Markov property: future is condiUonally independent of the past given the present ‣ If y are tags, this roughly corresponds to assuming that the next tag only depends on the current tag, not anything before y3 P(y3|y1, y2) = P(y3|y2) ‣ Lots of mathemaUcal theory about how Markov chains behave Hidden Markov Models ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output y1 y2 yn x1 x2 xn … P(y, x) = P(y1) n Y i=2 P(yi|yi−1) n Y i=1 P(xi|yi) IniUal distribuUon TransiUon probabiliUes Emission probabiliUes } } } ‣ P(x|y) is a distribuUon over all words in the vocabulary — not a distribuUon over features (but could be!) ‣ MulUnomials: tag x tag transiUons, tag x word emissions ‣ ObservaUon (x) depends only on current state (y) TransiUons in POS Tagging ‣ Dynamics model Fed raises interest rates 0.5 percent VBD VBN NNP VBZ NNS VB VBP NN VBZ NNS CD NN ‣ likely because start of sentence ‣ likely because verb oden follows noun ‣ direct object follows verb, other verb rarely follows past tense verb (main verbs can follow modals though!) P(y1 = NNP) P(y2 = VBZ|y1 = NNP) P(y3 = NN|y2 = VBZ) P(y1) n Y i=2 P(yi|yi−1) EsUmaUng TransiUons ‣ Similar to Naive Bayes esUmaUon: maximum likelihood soluUon = normalized counts (with smoothing) read oﬀ supervised data Fed raises interest rates 0.5 percent . NNP VBZ NN NNS CD NN ‣ How to smooth? ‣ One method: smooth with unigram distribuUon over tags ‣ P(tag | NN) P(tag|tag−1) = (1 −λ) ˆ P(tag|tag−1) + λ ˆ P(tag) = empirical distribuUon (read oﬀ from data) ˆ P . = (0.5 ., 0.5 NNS) ‣ Emissions P(x | y) capture the distribuUon of words occurring with a given tag Emissions in POS Tagging ‣ P(word | NN) = (0.05 person, 0.04 oﬃcial, 0.03 interest, 0.03 percent …) Fed raises interest rates 0.5 percent NNP VBZ NN NNS CD NN ‣ When you compute the posterior for a given word’s tags, the distribuUon favors tags that are more likely to generate that word ‣ How should we smooth this? Inference in HMMs ‣ Inference problem: ‣ ExponenUally many possible y here! ‣ SoluUon: dynamic programming (possible because of Markov structure!) ‣ Many neural sequence models depend on enUre previous tag sequence, need to use approximaUons like beam search ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output y1 y2 yn x1 x2 xn … P(y, x) = P(y1) n Y i=2 P(yi|yi−1) n Y i=1 P(xi|yi) argmaxyP(y|x) = argmaxy P(y, x) P(x) Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Vivek Srikumar ‣ Best (parUal) score for a sequence ending in state s Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Dan Klein ‣ “Think about” all possible immediate prior state values. Everything before that has already been accounted for by earlier stages. Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Vivek Srikumar Viterbi Algorithm slide credit: Vivek Srikumar Forward-Backward Algorithm ‣ In addiUon to ﬁnding the best path, we may want to compute marginal probabiliUes of paths P(yi = s|x) P(yi = s|x) = X y1,...,yi−1,yi+1,...,yn P(y|x) ‣ What did Viterbi compute? P(ymax|x) = max y1,...,yn P(y|x) ‣ Can compute marginals with dynamic programming as well using an algorithm called forward-backward Forward-Backward Algorithm P(y3 = 2|x) = sum of all paths through state 2 at time 3 sum of all paths Forward-Backward Algorithm slide credit: Dan Klein P(y3 = 2|x) = sum of all paths through state 2 at time 3 sum of all paths = ‣ Easiest and most ﬂexible to do one pass to compute and one to compute Forward-Backward Algorithm ↵1(s) = P(s)P(x1|s) ↵t(st) = X st−1 ↵t−1(st−1)P(st|st−1)P(xt|st) ‣ IniUal: ‣ Recurrence: ‣ Same as Viterbi but summing instead of maxing! ‣ These quanUUes get very small! Store everything as log probabiliUes Forward-Backward Algorithm ‣ IniUal: βn(s) = 1 βt(st) = X st+1 βt+1(st+1)P(st+1|st)P(xt+1|st+1) ‣ Recurrence: ‣ Big diﬀerences: count emission for the next Umestep (not current one) Forward-Backward Algorithm ↵1(s) = P(s)P(x1|s) ↵t(st) = X st−1 ↵t−1(st−1)P(st|st−1)P(xt|st) βn(s) = 1 βt(st) = X st+1 βt+1(st+1)P(st+1|st)P(xt+1|st+1) P(s3 = 2|x) = ↵3(2)β3(2) P i ↵3(i)β3(i) ‣ What is the denominator here? ‣ Does this explain why beta is what it is? P(x) HMM POS Tagging ‣ Baseline: assign each word its most frequent tag: ~90% accuracy ‣ Trigram HMM: ~95% accuracy / 55% on unknown words Slide credit: Dan Klein Trigram Taggers ‣ Trigram model: y1 = (<S>, NNP), y2 = (NNP, VBZ), … ‣ P((VBZ, NN) | (NNP, VBZ)) — more context! Noun-verb-noun S-V-O Fed raises interest rates 0.5 percent NNP VBZ NN NNS CD NN ‣ Tradeoﬀ between model capacity and data size — trigrams are a “sweet spot” for POS tagging HMM POS Tagging ‣ Baseline: assign each word its most frequent tag: ~90% accuracy ‣ Trigram HMM: ~95% accuracy / 55% on unknown words ‣ TnT tagger (Brants 1998, tuned HMM): 96.2% accuracy / 86.0% on unks Slide credit: Dan Klein ‣ State-of-the-art (BiLSTM-CRFs): 97.5% / 89%+ Errors oﬃcial knowledge made up the story recently sold shares JJ/NN NN VBD RP/IN DT NN RB VBD/VBN NNS Slide credit: Dan Klein / Toutanova + Manning (2000) (NN NN: tax cut, art gallery, …) Remaining Errors ‣ Underspeciﬁed / unclear, gold standard inconsistent / wrong: 58% ‣ Lexicon gap (word not seen with that tag in training) 4.5% ‣ Unknown word: 4.5% ‣ Could get right: 16% (many of these involve parsing!) ‣ Diﬃcult linguisUcs: 20% They set up absurd situa6ons, detached from reality VBD / VBP? (past or present?) a $ 10 million fourth-quarter charge against discon6nued opera6ons adjecUve or verbal parUciple? JJ / VBN? Manning 2011 “Part-of-Speech Tagging from 97% to 100%: Is It Time for Some LinguisUcs?” Other Languages ‣ Universal POS tagset (~12 tags), cross-lingual model works as well as tuned CRF using external resources Gillick et al. 2016 Next Time ‣ CRFs: feature-based discriminaUve models ‣ Structured SVM for sequences ‣ Named enUty recogniUon "
146,"CS388: Natural Language Processing Lecture 5: Sequence Models II Greg Durrett Administrivia ‣ Project 1 is out, sample writeups on website ‣ Mini 1 graded by next lecture Recall: HMMs ‣ Inference problem: ‣ Viterbi: ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output y1 y2 yn x1 x2 xn … P(y, x) = P(y1) n Y i=2 P(yi|yi−1) n Y i=1 P(xi|yi) argmaxyP(y|x) = argmaxy P(y, x) P(x) ‣ Training: maximum likelihood esQmaQon (with smoothing) scorei(s) = max yi−1 P(s|yi−1)P(xi|s)scorei−1(yi−1) This Lecture ‣ (if Qme) Beam search ‣ CRFs: model (+features for NER), inference, learning ‣ Named enQty recogniQon (NER) Named EnQty RecogniQon Barack Obama will travel to Hangzhou today for the G20 mee=ng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O ‣ BIO tagset: begin, inside, outside ‣ Why might an HMM not do so well here? ‣ Lots of O’s, so tags aren’t as informaQve about context ‣ Sequence of tags — should we use an HMM? ‣ Insuﬃcient features/capacity with mulQnomials (especially for unks) CRFs CondiQonal Random Fields ‣ HMMs are expressible as Bayes nets (factor graphs) y1 y2 yn x1 x2 xn … ‣ This reﬂects the following decomposiQon: ‣ Locally normalized model: each factor is a probability distribuQon that normalizes P(y, x) = P(y1)P(x1|y1)P(y2|y1)P(x2|y2) . . . CondiQonal Random Fields any real-valued scoring funcQon of its arguments ‣ How do we max over y? Intractable in general — can we ﬁx this? ‣ CRFs: discriminaQve models with the following globally-normalized form: ‣ HMMs: ‣ Naive Bayes : logisQc regression :: HMMs : CRFs local vs. global normalizaQon <-> generaQve vs. discriminaQve P(y|x) = 1 Z Y k exp(φk(x, y)) normalizer P(y, x) = P(y1)P(x1|y1)P(y2|y1)P(x2|y2) . . . ‣ Locally normalized discriminaQve models do exist (MEMMs) SequenQal CRFs y1 y2 yn x1 x2 xn … P(y|x) / Y k exp(φk(x, y)) y1 y2 yn x1 x2 xn … φt φe φo P(y|x) / exp(φo(y1)) n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(xi, yi)) ‣ HMMs: ‣ CRFs: P(y, x) = P(y1)P(x1|y1)P(y2|y1)P(x2|y2) . . . SequenQal CRFs y1 y2 yn x1 x2 xn … φt φe φo y1 y2 yn … φt φe φo x P(y|x) / exp(φo(y1)) n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(xi, yi)) ‣ We condiQon on x, so every factor can depend on all of x (including transiQons, but we won’t do this) n Y i=1 exp(φe(yi, i, x)) ‣ y can’t depend arbitrarily on x in a generaQve model token index — lets us look at current word SequenQal CRFs ‣ NotaQon: omit x from the factor graph enQrely (implicit) y1 y2 yn … φt φe φo x y1 y2 yn … φt φe φo ‣ Don’t include iniQal distribuQon, can bake into other factors P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) SequenQal CRFs: Feature FuncQons y1 y2 yn … φe φt ‣ Phis can be almost anything! Here we use linear funcQons of sparse features ‣ Looks like our single weight vector mulQclass logisQc regression model φt(yi−1, yi) = w>ft(yi−1, yi) P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # φe(yi, i, x) = w>fe(yi, i, x) P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) Basic Features for NER Barack Obama will travel to Hangzhou today for the G20 mee=ng . O B-LOC O TransiQons: Emissions: Ind[B-LOC & Current word = Hangzhou] Ind[B-LOC & Prev word = to] ft(yi−1, yi) = Ind[yi−1 & yi] fe(y6, 6, x) = P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # = Ind[O — B-LOC] Features for NER Leicestershire is a nice place to visit… I took a vaca=on to Boston Apple released a new version… According to the New York Times… ORG ORG LOC LOC Texas governor Greg AbboI said Leonardo DiCaprio won an award… PER PER LOC φe(yi, i, x) Features for NER ‣ Context features (can’t use in HMM!) ‣ Words before/ajer ‣ Tags before/ajer ‣ Word features (can use in HMM) ‣ CapitalizaQon ‣ Word shape ‣ Preﬁxes/suﬃxes ‣ Lexical indicators ‣ Gazekeers ‣ Word clusters Leicestershire Boston Apple released a new version… According to the New York Times… CRFs Outline ‣ Model: P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) ‣ Inference ‣ Learning P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # CompuQng (arg)maxes y1 y2 yn … φe φt ‣ : can use Viterbi exactly as in HMM case ‣ and play the role of the Ps now, same dynamic program exp(φt(yi−1, yi)) exp(φe(yi, i, x)) P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) argmaxyP(y|x) { max y1,...,yn eφt(yn−1,yn)eφe(yn,n,x) · · · eφe(y2,2,x)eφt(y1,y2)eφe(y1,1,x) = max y2,...,yn eφt(yn−1,yn)eφe(yn,n,x) · · · eφe(y2,2,x) max y1 eφt(y1,y2)eφe(y1,1,x) = max y3,...,yn eφt(yn−1,yn)eφe(yn,n,x) · · · max y2 eφt(y2,y3)eφe(y2,2,x) max y1 eφt(y1,y2)score1(y1) { Inference in General CRFs y1 y2 yn … φe φt ‣ Can do inference in any tree-structured CRF ‣ Max-product algorithm: generalizaQon of Viterbi to arbitrary tree- structured graphs (sum-product is generalizaQon of forward-backward) CRFs Outline ‣ Model: P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) ‣ Inference: argmax P(y|x) from Viterbi ‣ Learning P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # Training CRFs ‣ Gradient is completely analogous to logisQc regression: P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # P(y|x) / exp w>f(x, y) ‣ LogisQc regression: ‣ Maximize L(y⇤, x) = log P(y⇤|x) intractable! @ @wL(y⇤, x) = n X i=2 ft(y⇤ i−1, y⇤ i ) + n X i=1 fe(y⇤ i , i, x) −Ey "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # Training CRFs ‣ Let’s focus on emission feature expectaQon @ @wL(y⇤, x) = n X i=2 ft(y⇤ i−1, y⇤ i ) + n X i=1 fe(y⇤ i , i, x) −Ey "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # Ey "" n X i=1 fe(yi, i, x) # = X y2Y P(y|x) "" n X i=1 fe(yi, i, x) # = n X i=1 X y2Y P(y|x)fe(yi, i, x) = n X i=1 X s P(yi = s|x)fe(s, i, x) CompuQng Marginals y1 y2 yn … φe φt P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) Z = X y n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) ‣ For both HMMs and CRFs: ‣ Normalizing constant P(yi = s|x) = forwardi(s)backwardi(s) P s0 forwardi(s0)backwardi(s0) Z for CRFs, P(x) for HMMs ‣ Analogous to P(x) for HMMs Posteriors vs. ProbabiliQes P(yi = s|x) = forwardi(s)backwardi(s) P s0 forwardi(s0)backwardi(s0) ‣ Posterior is derived from the parameters and the data (condiQoned on x!) HMM CRF Model parameter (usually mulQnomial distribuQon) Inferred quanQty from forward-backward Inferred quanQty from forward-backward Undeﬁned (model is by deﬁniQon condiQoned on x) P(xi|yi), P(yi|yi−1) P(yi|x), P(yi−1, yi|x) Training CRFs ‣ TransiQon features: need to compute ‣ …but you can build a preky good system without transiQon features P(yi = s1, yi+1 = s2|x) using forward-backward as well ‣ For emission features: gold features — expected features under model @ @wL(y⇤, x) = n X i=1 fe(y⇤ i , i, x) − n X i=1 X s P(yi = s|x)fe(s, i, x) CRFs Outline ‣ Model: P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) ‣ Inference: argmax P(y|x) from Viterbi ‣ Learning: run forward-backward to compute posterior probabiliQes; then P(y|x) / exp w> "" n X i=2 ft(yi−1, yi) + n X i=1 fe(yi, i, x) # @ @wL(y⇤, x) = n X i=1 fe(y⇤ i , i, x) − n X i=1 X s P(yi = s|x)fe(s, i, x) Pseudocode for each epoch for each example extract features on each emission and transiQon (look up in cache) compute marginal probabiliQes with forward-backward compute potenQals phi based on features + weights accumulate gradient over all emissions and transiQons ImplementaQon Tips for CRFs ‣ Caching is your friend! Cache feature vectors especially ‣ Try to reduce redundant computaQon, e.g. if you compute both the gradient and the objecQve value, don’t rerun the dynamic program ‣ If things are too slow, run a proﬁler and see where Qme is being spent. Forward-backward should take most of the Qme ‣ Exploit sparsity in feature vectors where possible, especially in feature vectors and gradients ‣ Do all dynamic program computaQon in log space to avoid underﬂow Debugging Tips for CRFs ‣ Hard to know whether inference, learning, or the model is broken! ‣ Compute the objecQve — is opQmizaQon working? ‣ Learning: is the objecQve going down? Can you ﬁt a small training set? Are you applying the gradient correctly? ‣ Inference: check gradient computaQon (most likely place for bugs) ‣ Is the same for all i? ‣ Do probabiliQes normalize correctly + look “reasonable”? (Nearly uniform when untrained, then slowly converging to the right thing) ‣ If objecQve is going down but model performance is bad: ‣ Inference: check performance if you decode the training set X s forwardi(s)backwardi(s) NER NER ‣ CRF with lexical features can get around 85 F1 on this problem ‣ Other pieces of informaQon that many systems capture ‣ World knowledge: The delegaQon met the president at the airport, Tanjug said. ORG? PER? Nonlocal Features The delegaQon met the president at the airport, Tanjug said. The news agency Tanjug reported on the outcome of the meeQng. ‣ More complex factor graph structures can let you capture this, or just decode sentences in order and use features on previous sentences Finkel and Manning (2008), RaQnov and Roth (2009) Semi-Markov Models Barack Obama will travel to Hangzhou today for the G20 mee=ng . ‣ Chunk-level predicQon rather than token-level BIO ‣ y is a set of touching spans of the sentence ‣ Cons: there’s an extra factor of n in the dynamic programs { { { { { { PER O LOC ORG O O ‣ Pros: features can look at whole span at once Sarawagi and Cohen (2004) EvaluaQng NER ‣ PredicQon of all Os sQll gets 66% accuracy on this example! Barack Obama will travel to Hangzhou today for the G20 mee=ng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O ‣ What we really want to know: how many named enQty chunk predicQons did we get right? ‣ Precision: of the ones we predicted, how many are right? ‣ Recall: of the gold named enQQes, how many did we ﬁnd? ‣ F-measure: harmonic mean of these two How well do NER systems do? RaQnov and Roth (2009) Lample et al. (2016) BiLSTM-CRF + ELMo Peters et al. (2018) 92.2 Beam Search Viterbi Time Complexity Fed raises interest rates 0.5 percent VBD VBN NNP VBZ NNS VB VBP NN VBZ NNS CD NN ‣ n word sentence, s tags to consider — what is the Qme complexity? tags sentence ‣ O(ns2) — s is ~40 for POS, n is ~20 Viterbi Time Complexity ‣ Many tags are totally implausible ‣ Can any of these be: ‣ Determiners? ‣ PreposiQons? ‣ AdjecQves? ‣ Features quickly eliminate many outcomes from consideraQon — don’t need to consider these going forward Fed raises interest rates 0.5 percent VBD VBN NNP VBZ NNS VB VBP NN VBZ NNS CD NN Beam Search ‣ Maintain a beam of k plausible states at the current Qmestep ‣ Expand all states, only keep k top hypotheses at new Qmestep Fed VBD VBN NNP raises +1.2 +0.9 +0.7 NN +0.3 VBZ +1.2 VBZ -2.0 NNS-1.0 Not expanded … VBZ DT NNS +1.2 -1.0 -5.3 … … PRP -5.8 Not expanded ‣ Beam size of k, Qme complexity -2.0 O(nks log(ks)) ‣ Maintain priority queue to eﬃciently add things How good is beam search? ‣ k=1: greedy search ‣ Choosing beam size: ‣ 2 is usually beker than 1 ‣ Usually don’t use larger than 50 ‣ Depends on problem structure ‣ If beam search is much faster than compuQng full sums, can use structured SVM instead of CRFs, but we won’t discuss that here Next Time ‣ Neural networks "
147,"CS388: Natural Language Processing Lecture 6: Neural Networks Greg Durrett Administrivia ‣ Mini 1 graded, posted on Canvas ‣ Project 1 due in 9 days ‣ Xi Ye (88.0 F1), Quang Duong (87.3 F1), Uday KusupaQ (87.2 F1) 6 students in the 86 range, rest are 85 or below ‣ Test F1s << dev F1 ‣ Changing thresholds / imbalanced classiﬁcaQon ‣ POS/chunk features ‣ Small bug ﬁxed in BadNerModel (no impact on the code you write) ‣ Someone got 86.3 with only 7 features total, classiﬁer is a dicQonary This Lecture ‣ Feedforward neural networks + backpropagaQon ‣ Neural network basics ‣ ApplicaQons ‣ Neural network history ‣ Beam search: in a few lectures ‣ ImplemenQng neural networks (if Qme) History: NN “dark ages” ‣ Convnets: applied to MNIST by LeCun in 1998 ‣ LSTMs: Hochreiter and Schmidhuber (1997) ‣ Henderson (2003): neural shic-reduce parser, not SOTA 2008-2013: A glimmer of light… ‣ Collobert and Weston 2011: “NLP (almost) from scratch” ‣ Feedforward neural nets induce features for sequenQal CRFs (“neural CRF”) ‣ 2008 version was marred by bad experiments, claimed SOTA but wasn’t, 2011 version Qed SOTA ‣ Socher 2011-2014: tree-structured RNNs working okay ‣ Krizhevskey et al. (2012): AlexNet for vision 2014: Stuﬀ starts working ‣ Sutskever et al. + Bahdanau et al.: seq2seq for neural MT (LSTMs work for NLP?) ‣ Kim (2014) + Kalchbrenner et al. (2014): sentence classiﬁcaQon / senQment (convnets work for NLP?) ‣ 2015: explosion of neural nets for everything under the sun ‣ Chen and Manning transiQon-based dependency parser (even feedforward networks work well for NLP?) Why didn’t they work before? ‣ Datasets too small: for MT, not really bener unQl you have 1M+ parallel sentences (and really need a lot more) ‣ Op,miza,on not well understood: good iniQalizaQon, per-feature scaling + momentum (Adagrad / Adadelta / Adam) work best out-of-the-box ‣ Regulariza,on: dropout is preny helpful ‣ Inputs: need word representaQons to have the right conQnuous semanQcs ‣ Computers not big enough: can’t run for enough iteraQons Neural Net Basics Neural Networks ‣ How can we do nonlinear classiﬁcaQon? Kernels are too slow… ‣ Want to learn intermediate conjuncQve features of the input argmaxyw>f(x, y) ‣ Linear classiﬁcaQon: the movie was not all that good I[contains not & contains good] Neural Networks: XOR x1 x2 x1 x2 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 x1, x2 (generally x = (x1, . . . , xm)) y (generally y = (y1, . . . , yn)) y = x1 XOR x2 ‣ Let’s see how we can use neural nets to learn a simple nonlinear funcQon ‣ Inputs ‣ Output Neural Networks: XOR x1 x2 x1 x2 x1 XOR x2 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 “or” y = a1x1 + a2x2 X y = a1x1 + a2x2 + a3 tanh(x1 + x2) (looks like action potential in neuron) Neural Networks: XOR y = a1x1 + a2x2 x1 x2 x1 x2 x1 XOR x2 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 X y = a1x1 + a2x2 + a3 tanh(x1 + x2) x2 x1 “or” y = −x1 −x2 + 2 tanh(x1 + x2) Neural Networks: XOR x1 x2 0 1 -1 0 x2 x1 [not] [good] y = −2x1 −x2 + 2 tanh(x1 + x2) I I the movie was not all that good Neural Networks Taken from hnp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ Warp space Shift Nonlinear transformation Linear model: y = w · x + b y = g(w · x + b) y = g(Wx + b) Neural Networks Taken from hnp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ Linear classiﬁer Neural network …possible because we transformed the space! Deep Neural Networks Adopted from Chris Dyer } output of ﬁrst layer z = g(Vg(Wx + b) + c) z = g(Vy + c) Input Second Layer First Layer “Feedforward” computaQon (not recurrent) z = V(Wx + b) + c Check: what happens if no nonlinearity? More powerful than basic linear models? Deep Neural Networks Taken from hnp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ Feedforward Networks, BackpropagaQon LogisQc Regression with NNs P(y|x) = exp(w>f(x, y)) P y0 exp(w>f(x, y0)) ‣ Single scalar probability P(y|x) = softmax ! [w>f(x, y)]y2Y "" ‣ Compute scores for all possible labels at once (returns vector) softmax(p)i = exp(pi) P i0 exp(pi0) ‣ socmax: exps and normalizes a given vector P(y|x) = softmax(Wf(x)) ‣ Weight vector per class; W is [num classes x num feats] P(y|x) = softmax(Wg(V f(x))) ‣ Now one hidden layer Neural Networks for ClassiﬁcaQon V n features d hidden units d x n matrix num_classes x d matrix socmax W f(x) z nonlinearity (tanh, relu, …) g P(y|x) P(y|x) = softmax(Wg(V f(x))) num_classes probs Training Neural Networks ‣ Maximize log likelihood of training data ‣ i*: index of the gold label ‣ ei: 1 in the ith row, zero elsewhere. Dot by this = select ith index z = g(V f(x)) P(y|x) = softmax(Wz) L(x, i⇤) = Wz · ei⇤−log X j exp(Wz) · ej L(x, i⇤) = log P(y = i⇤|x) = log (softmax(Wz) · ei⇤) CompuQng Gradients ‣ Gradient with respect to W if i = i* zj −P(y = i|x)zj −P(y = i|x)zj @ @Wij L(x, i⇤) = zj −P(y = i|x)zj −P(y = i|x)zj otherwise ‣ Looks like logisQc regression with z as the features! i j { L(x, i⇤) = Wz · ei⇤−log X j exp(Wz) · ej W Neural Networks for ClassiﬁcaQon V socmax W f(x) z g P(y|x) P(y|x) = softmax(Wg(V f(x))) @L @W z CompuQng Gradients: BackpropagaQon z = g(V f(x)) AcQvaQons at hidden layer ‣ Gradient with respect to V: apply the chain rule err(root) = ei⇤−P(y|x) dim = m dim = d @L(x, i⇤) @z = err(z) = W >err(root) L(x, i⇤) = Wz · ei⇤−log X j exp(Wz) · ej [some math…] @L(x, i⇤) @Vij = @L(x, i⇤) @z @z @Vij BackpropagaQon: Picture V socmax W f(x) z g P(y|x) P(y|x) = softmax(Wg(V f(x))) @L @W err(root) err(z) z ‣ Can forget everything acer z, treat it as the output and keep backpropping BackpropagaQon: Takeaways ‣ Gradients of output weights W are easy to compute — looks like logisQc regression with hidden layer z as feature vector ‣ Can compute derivaQve of loss with respect to z to form an “error signal” for backpropagaQon ‣ Easy to update parameters based on “error signal” from next layer, keep pushing error signal back as backpropagaQon ‣ Need to remember the values from the forward computaQon ApplicaQons NLP with Feedforward Networks Botha et al. (2017) … Fed raises interest rates in order to … f(x) ?? emb(raises) ‣ Word embeddings for each word form input ‣ ~1000 features here — smaller feature vector than in sparse models, but every feature ﬁres on every example emb(interest) emb(rates) ‣ Weight matrix learns posiQon-dependent processing of the words previous word curr word next word other words, feats, etc. ‣ Part-of-speech tagging with FFNNs NLP with Feedforward Networks ‣ Hidden layer mixes these diﬀerent signals and learns feature conjuncQons Botha et al. (2017) NLP with Feedforward Networks ‣ MulQlingual tagging results: Botha et al. (2017) ‣ Gillick used LSTMs; this is smaller, faster, and bener SenQment Analysis ‣ Deep Averaging Networks: feedforward neural network on average of word embeddings from input Iyyer et al. (2015) SenQment Analysis { { Bag-of-words Tree RNNs / CNNS / LSTMS Wang and Manning (2012) Kim (2014) Iyyer et al. (2015) Coreference ResoluQon ‣ Feedforward networks idenQfy coreference arcs Clark and Manning (2015), Wiseman et al. (2015) President Obama signed… He later gave a speech… ? ImplementaQon Details ComputaQon Graphs ‣ CompuQng gradients is hard! ‣ AutomaQc diﬀerenQaQon: instrument code to keep track of derivaQves y = x * x (y,dy) = (x * x, 2 * x * dx) codegen ‣ ComputaQon is now something we need to reason about symbolically ‣ Use a library like Pytorch or Tensorﬂow. This class: Pytorch ComputaQon Graphs in Pytorch P(y|x) = softmax(Wg(V f(x))) class FFNN(nn.Module): def __init__(self, inp, hid, out): super(FFNN, self).__init__() self.V = nn.Linear(inp, hid) self.g = nn.Tanh() self.W = nn.Linear(hid, out) self.softmax = nn.Softmax(dim=0) def forward(self, x): return self.softmax(self.W(self.g(self.V(x)))) ‣ Deﬁne forward pass for ComputaQon Graphs in Pytorch P(y|x) = softmax(Wg(V f(x))) ffnn = FFNN() loss.backward() probs = ffnn.forward(input) loss = torch.neg(torch.log(probs)).dot(gold_label) optimizer.step() def make_update(input, gold_label): ffnn.zero_grad() # clear gradient variables ei*: one-hot vector of the label (e.g., [0, 1, 0]) Training a Model Deﬁne a computaQon graph For each epoch: Compute loss on batch For each batch of data: Decode test set Autograd to compute gradients and take step Batching ‣ Batching data gives speedups due to more eﬃcient matrix operaQons ‣ Need to make the computaQon graph process a batch at the same Qme probs = ffnn.forward(input) # [batch_size, num_classes] loss = torch.sum(torch.neg(torch.log(probs)).dot(gold_label)) ... ‣ Batch sizes from 1-100 ocen work well def make_update(input, gold_label) # input is [batch_size, num_feats] # gold_label is [batch_size, num_classes] ... Next Time ‣ More implementaQon details: pracQcal training techniques ‣ Word representaQons / word vectors ‣ word2vec, GloVe "
148,"Greg Durrett CS388: Natural Language Processing Lecture 14: Word Embeddings Recall: Feedforward NNs V n features d hidden units d x n matrix num_classes x d matrix soEmax W f(x) z nonlinearity (tanh, relu, …) g P(y|x) P(y|x) = softmax(Wg(V f(x))) num_classes probs Recall: BackpropagaNon V d hidden units soEmax W f(x) z g P(y|x) P(y|x) = softmax(Wg(V f(x))) @L @W err(root) @z @V err(z) z f(x) This Lecture ‣ Training ‣ Word representaNons ‣ word2vec ‣ EvaluaNng word embeddings Training Tips Training Basics ‣ Basic formula: compute gradients on batch, use ﬁrst-order opt. method ‣ How to iniNalize? How to regularize? What opNmizer to use? ‣ This lecture: some pracNcal tricks. Take deep learning or opNmizaNon courses to understand this further How does iniNalizaNon aﬀect learning? V n features d hidden units d x n matrix m x d matrix soEmax W f(x) z nonlinearity (tanh, relu, …) g P(y|x) P(y|x) = softmax(Wg(V f(x))) ‣ How do we iniNalize V and W? What consequences does this have? ‣ Nonconvex problem, so iniNalizaNon ma[ers! ‣ Nonlinear model…how does this aﬀect things? ‣ If cell acNvaNons are too large in absolute value, gradients are small ‣ ReLU: larger dynamic range (all posiNve numbers), but can produce big values, can break down if everything is too negaNve How does iniNalizaNon aﬀect learning? IniNalizaNon 1) Can’t use zeroes for parameters to produce hidden layers: all values in that hidden layer are always 0 and have gradients of 0, never change ‣ Can do random uniform / normal iniNalizaNon with appropriate scale U "" − r 6 fan-in + fan-out, + r 6 fan-in + fan-out # ‣ Glorot iniNalizer: ‣ Want variance of inputs and gradients for each layer to be the same ‣ Batch normalizaNon (Ioﬀe and Szegedy, 2015): periodically shiE+rescale each layer to have mean 0 and variance 1 over a batch (useful if net is deep) 2) IniNalize too large and cells are saturated Dropout ‣ ProbabilisNcally zero out parts of the network during training to prevent overﬁdng, use whole network at test Nme Srivastava et al. (2014) ‣ Similar to beneﬁts of ensembling: network needs to be robust to missing signals, so it has redundancy ‣ Form of stochasNc regularizaNon ‣ One line in Pytorch/Tensorﬂow OpNmizer ‣ Adam (Kingma and Ba, ICLR 2015) is very widely used ‣ AdapNve step size like Adagrad, incorporates momentum OpNmizer ‣ Wilson et al. NIPS 2017: adapNve methods can actually perform badly at test Nme (Adam is in pink, SGD in black) ‣ Check dev set periodically, decrease learning rate if not making progress Structured PredicNon ‣ Four elements of a machine learning method: ‣ Model: feedforward, RNNs, CNNs can be deﬁned in a uniform framework ‣ ObjecNve: many loss funcNons look similar, just changes the last layer of the neural network ‣ Inference: deﬁne the network, your library of choice takes care of it (mostly…) ‣ Training: lots of choices for opNmizaNon/hyperparameters Word RepresentaNons Word RepresentaNons ‣ ConNnuous model <-> expects conNnuous semanNcs from input ‣ “Can tell a word by the company it keeps” Firth 1957 ‣ Neural networks work very well at conNnuous data, but words are discrete Discrete Word RepresentaNons good enjoyable great 0 ﬁsh cat ‣ Brown clusters: hierarchical agglomeraNve hard clustering (each word has one cluster, not some posterior distribuNon like in mixture models) ‣ Maximize ‣ Useful features for tasks like NER, not suitable for NNs dog … is go 0 0 1 1 1 1 1 1 0 0 P(wi|wi−1) = P(ci|ci−1)P(wi|ci) Brown et al. (1992) Word Embeddings Botha et al. (2017) … Fed raises interest rates in order to … f(x) ?? emb(raises) ‣ Word embeddings for each word form input emb(interest) emb(rates) previous word curr word next word other words, feats, etc. ‣ Part-of-speech tagging with FFNNs ‣ What properNes should these vectors have? good enjoyable bad dog great is ‣ Want a vector space where similar words have similar embeddings the movie was great the movie was good ~ ~ Word Embeddings ‣ Goal: come up with a way to produce these embeddings word2vec ConNnuous Bag-of-Words ‣ Predict word from context the dog bit the man ‣ Parameters: d x |V| (one d-length vector per voc word), |V| x d output parameters (W) dog the + size d soEmax MulNply by W gold label = bit, no manual labeling required! Mikolov et al. (2013) d-dimensional word embeddings P(w|w−1, w+1) = softmax (W(c(w−1) + c(w+1))) size |V| x d Skip-Gram the dog bit the man ‣ Predict one word of context from word bit soEmax MulNply by W gold = dog ‣ Parameters: d x |V| vectors, |V| x d output parameters (W) (also usable as vectors!) ‣ Another training example: bit -> the P(w0|w) = softmax(We(w)) Mikolov et al. (2013) Hierarchical SoEmax ‣ Matmul + soEmax over |V| is very slow to compute for CBOW and SG ‣ Hierarchical soEmax: P(w|w−1, w+1) = softmax (W(c(w−1) + c(w+1))) ‣ Standard soEmax: [|V| x d] x d log(|V|) dot products of size d, … … the a ‣ Huﬀman encode vocabulary, use binary classiﬁers to decide which branch to take |V| x d parameters Mikolov et al. (2013) P(w0|w) = softmax(We(w)) ‣ log(|V|) binary decisions Skip-Gram with NegaNve Sampling ‣ d x |V| vectors, d x |V| context vectors (same # of params as before) Mikolov et al. (2013) (bit, the) => +1 (bit, cat) => -1 (bit, a) => -1 (bit, ﬁsh) => -1 ‣ Take (word, context) pairs and classify them as “real” or not. Create random negaNve examples by sampling from unigram distribuNon words in similar contexts select for similar c vectors P(y = 1|w, c) = ew·c ew·c + 1 ‣ ObjecNve = log P(y = 1|w, c) −1 k n X i=1 log P(y = 0|wi, c) sampled ConnecNons with Matrix FactorizaNon Levy et al. (2014) ‣ Skip-gram model looks at word-word co-occurrences and produces two types of vectors word pair counts |V| |V| |V| d d |V| context vecs word vecs ‣ Looks almost like a matrix factorizaNon…can we interpret it this way? Skip-Gram as Matrix FactorizaNon Levy et al. (2014) |V| |V| Mij = PMI(wi, cj) −log k PMI(wi, cj) = P(wi, cj) P(wi)P(cj) = count(wi,cj) D count(wi) D count(cj) D ‣ If we sample negaNve examples from the uniform distribuNon over words num negaNve samples ‣ …and it’s a weighted factorizaNon problem (weighted by word freq) Skip-gram objecNve exactly corresponds to factoring this matrix: GloVe Pennington et al. (2014) X i,j f(count(wi, cj)) "" w> i cj + ai + bj −log count(wi, cj)) #2 ‣ ObjecNve = ‣ Also operates on counts matrix, weighted regression on the log co-occurrence matrix (weights f) ‣ Constant in the dataset size (just need counts), quadraNc in voc size ‣ By far the most common word vectors used today (5000+ citaNons) word pair counts |V| |V| Preview: Context-dependent Embeddings Peters et al. (2018) ‣ Train a neural language model to predict the next word given previous words in the sentence, use its internal representaNons as word vectors ‣ Context-sensiCve word embeddings: depend on rest of the sentence ‣ Huge improvements across nearly all NLP tasks over GloVe they hit the balls they dance at balls ‣ How to handle diﬀerent word senses? One vector for balls EvaluaNon EvaluaNng Word Embeddings ‣ What properNes of language should word embeddings capture? good enjoyable bad dog great is cat wolf Cger was ‣ Similarity: similar words are close to each other ‣ Analogy: Paris is to France as Tokyo is to ??? good is to best as smart is to ??? Similarity Levy et al. (2015) ‣ SVD = singular value decomposiNon on PMI matrix ‣ GloVe does not appear to be the best when experiments are carefully controlled, but it depends on hyperparameters + these disNncNons don’t ma[er in pracNce Hypernymy DetecNon ‣ Hypernyms: detecNve is a person, dog is a animal ‣ word2vec (SGNS) works barely be[er than random guessing here ‣ Do word vectors encode these relaNonships? Chang et al. (2017) Analogies queen king woman man (king - man) + woman = queen ‣ Why would this be? ‣ woman - man captures the diﬀerence in the contexts that these occur in king + (woman - man) = queen ‣ Dominant change: more “he” with man and “she” with woman — similar to diﬀerence between king and queen Analogies Levy et al. (2015) ‣ These methods can perform well on analogies on two diﬀerent datasets using two diﬀerent methods cos(b, a2 −a1 + b1) Maximizing for b: Add = Mul = cos(b2, a2) cos(b2, b1) cos(b2, a1) + ✏ Using SemanNc Knowledge Faruqui et al. (2015) ‣ Structure derived from a resource like WordNet Original vector for false Adapted vector for false ‣ Doesn’t help most problems Using Word Embeddings ‣ Approach 1: learn embeddings as parameters from your data ‣ Approach 2: iniNalize using GloVe/ELMo, keep ﬁxed ‣ Approach 3: iniNalize using GloVe, ﬁne-tune ‣ Faster because no need to update these parameters ‣ Works best for some tasks, but not used for ELMo ‣ OEen works pre[y well ComposiNonal SemanNcs ‣ What if we want embedding representaNons for whole sentences? ‣ Skip-thought vectors (Kiros et al., 2015), similar to skip-gram generalized to a sentence level (more later) ‣ Is there a way we can compose vectors to make sentence representaNons? Summing? ‣ Will return to this in a few weeks as we move on to syntax and semanNcs Takeaways ‣ Lots to tune with neural networks ‣ Word vectors: learning word -> context mappings has given way to matrix factorizaNon approaches (constant in dataset size) ‣ Training: opNmizer, iniNalizer, regularizaNon (dropout), … ‣ Hyperparameters: dimensionality of word embeddings, layers, … ‣ Next Nme: RNNs and CNNs ‣ Lots of pretrained embeddings work well in pracNce, they capture some desirable properNes ‣ Even be[er: context-sensiNve word embeddings (ELMo) "
149,"CS388: Natural Language Processing Lecture 8: RNNs Greg Durrett Administrivia ‣ Project 1 due Thursday at 5pm Recall: Training Tips ‣ Parameter iniDalizaDon is criDcal to get good gradients, some useful heurisDcs (e.g., Glorot iniDalizer) ‣ Dropout is an eﬀecDve regularizer ‣ Think about your opDmizer: Adam or tuned SGD work well Recall: Word Vectors good enjoyable bad dog great is Recall: ConDnuous Bag-of-Words ‣ Predict word from context the dog bit the man dog the + sum, size d P(w|w−1, w+1) soUmax MulDply by W ‣ Matrix factorizaDon approaches useful for learning vectors from really large data Mikolov et al. (2013) Using Word Embeddings ‣ Approach 1: learn embeddings directly from data in your neural model, no pretraining ‣ Approach 2: pretrain using GloVe, keep ﬁxed ‣ Approach 3: iniDalize using GloVe, ﬁne-tune ‣ Faster because no need to update these parameters ‣ Not as commonly used anymore ‣ OUen works pre^y well ‣ Need to make sure GloVe vocabulary contains all the words you need ComposiDonal SemanDcs ‣ What if we want embedding representaDons for whole sentences? ‣ Skip-thought vectors (Kiros et al., 2015), similar to skip-gram generalized to a sentence level (more later) ‣ Is there a way we can compose vectors to make sentence representaDons? Summing? RNNs? This Lecture ‣ Vanishing gradient problem ‣ Recurrent neural networks ‣ LSTMs / GRUs ‣ ApplicaDons / visualizaDons RNN Basics RNN MoDvaDon ‣ Feedforward NNs can’t handle variable length input: each posiDon in the feature vector has ﬁxed semanDcs ‣ Instead, we need to: 1) Process each word in a uniform way the movie was great that was great ! 2) …while sDll exploiDng the context that that token occurs in ‣ These don’t look related (great is in two diﬀerent orthogonal subspaces) RNN AbstracDon ‣ Cell that takes some input x, has some hidden state h, and updates that hidden state and produces output y (all vector-valued) previous h next h (previous c) (next c) input x output y RNN Uses ‣ Transducer: make some predicDon for each element in a sequence ‣ Acceptor/encoder: encode a sequence into a ﬁxed-sized vector and use that for some purpose the movie was great predict senDment (matmul + soUmax) translate the movie was great DT NN VBD JJ paraphrase/compress output y = score for each tag, then soUmax Elman Networks input xt prev hidden state ht-1 ht output yt ‣ Computes output from hidden state ‣ Updates hidden state based on input and current hidden state ‣ Long history! (invented in the late 1980s) yt = tanh(Uht + by) Elman (1990) ht = tanh(Wxt + V ht−1 + bh) Training Elman Networks the movie was great predict senDment ‣ “BackpropagaDon through Dme”: build the network as one big computaDon graph, some parameters are shared ‣ RNN potenDally needs to learn how to “remember” informaDon for a long Dme! it was my favorite movie of 2016, though it wasn’t without problems -> + ‣ “Correct” parameter update is to do a be^er job of remembering the senDment of favorite Vanishing Gradient ‣ Gradient diminishes going through tanh; if not in [-2, 2], gradient is almost 0 <- gradient <- smaller gradient <- Dny gradient h^p://colah.github.io/posts/2015-08-Understanding-LSTMs/ LSTMs/GRUs Gated ConnecDons ‣ Designed to ﬁx “vanishing gradient” problem using gates ‣ Vector-valued “forget gate” f computed based on input and previous hidden state ‣ Sigmoid: elements of f are in [0, 1] f = σ(W xfxt + W hfht−1) ht = ht−1 ⊙f + func(xt) = ht-1 f ht ht = tanh(Wxt + V ht−1 + bh) gated Elman ‣ If f = 1, we simply sum up a funcDon of all inputs — gradient doesn’t vanish! LSTMs ‣ “Cell” c in addiDon to hidden state h ‣ Vector-valued forget gate f depends on the h hidden state ‣ Basic communicaDon ﬂow: x -> c -> h, each step of this process is gated in addiDon to gates from previous Dmesteps ct = ct−1 ⊙f + func(xt, ht−1) f = σ(W xfxt + W hfht−1) LSTMs xj f g i o hj hj-1 cj-1 cj h^p://colah.github.io/posts/2015-08-Understanding-LSTMs/ Goldberg lecture notes ‣ f, i, o are gates that control informaDon ﬂow ‣ g reﬂects the main computaDon of the cell LSTMs xj f g i o hj hj-1 cj-1 cj ‣ Can we ignore c in our current computaDon? ‣ Can we ignore a parDcular input x? ‣ Can an LSTM sum up its inputs x? ‣ Can we output something without changing c? LSTMs xj f g i o hj hj-1 cj-1 cj h^p://colah.github.io/posts/2015-08-Understanding-LSTMs/ Goldberg lecture notes ‣ Ignoring recurrent state enDrely: ‣ Lets us discard stopwords ‣ Summing inputs: ‣ Lets us get feedforward layer over token ‣ Ignoring input: ‣ Lets us compute a bag-of-words representaDon LSTMs ‣ Gradient sDll diminishes, but in a controlled way and generally by less — usually iniDalize forget gate = 1 to remember everything to start <- gradient similar gradient <- h^p://colah.github.io/posts/2015-08-Understanding-LSTMs/ Understanding LSTM Parameters ‣ IniDalize hidden layer randomly ‣ Need to learn how the gates work: what do we forget/remember? xj f g i o hj hj-1 cj-1 cj h^p://colah.github.io/posts/2015-08-Understanding-LSTMs/ ‣ g uses an arbitrary nonlinearity, this is the “layer” of the cell GRUs xj f g i o hj hj-1 cj-1 cj hj-1 sj-1 xj sj ‣ GRU: faster, a bit simpler ‣ LSTM: more complex and slower, may work a bit be^er X hj sj σ X + 1-z z σ tanh r ‣ Two gates: z (forget, mixes s and h) and r (mixes h and x) What do RNNs produce? ‣ Encoding of each word — can pass this to another layer to make a predicDon (can also pool these to get a diﬀerent sentence encoding) = ‣ Encoding of the sentence — can pass this a decoder or make a classiﬁcaDon decision about the sentence the movie was great ‣ RNN can be viewed as a transformaDon of a sequence of vectors into a sequence of context-dependent vectors MulDlayer BidirecDonal RNN ‣ Sentence classiﬁcaDon based on concatenaDon of both ﬁnal outputs ‣ Token classiﬁcaDon based on concatenaDon of both direcDons’ token representaDons the movie was great the movie was great Training RNNs the movie was great ‣ Loss = negaDve log likelihood of probability of gold label (or use SVM or other loss) P(y|x) ‣ Backpropagate through enDre network ‣ Example: senDment analysis Training RNNs the movie was great ‣ Loss = negaDve log likelihood of probability of gold predicDons, summed over the tags ‣ Loss terms ﬁlter back through network P(ti|x) ‣ Example: language modeling (predict next word given context) ApplicaDons What can LSTMs model? ‣ SenDment ‣ TranslaDon ‣ Language models ‣ Encode one sentence, predict ‣ Move leU-to-right, per-token predicDon ‣ Encode sentence + then decode, use token predicDons for a^enDon weights (later in the course) Visualizing LSTMs ‣ Train character LSTM language model (predict next character based on history) over two datasets: War and Peace and Linux kernel source code Karpathy et al. (2015) ‣ Counter: know when to generate \n ‣ Visualize acDvaDons of speciﬁc cells (components of c) to understand them Visualizing LSTMs Karpathy et al. (2015) ‣ Binary switch: tells us if we’re in a quote or not ‣ Visualize acDvaDons of speciﬁc cells to see what they track ‣ Train character LSTM language model (predict next character based on history) over two datasets: War and Peace and Linux kernel source code Visualizing LSTMs Karpathy et al. (2015) ‣ Stack: acDvaDon based on indentaDon ‣ Visualize acDvaDons of speciﬁc cells to see what they track ‣ Train character LSTM language model (predict next character based on history) over two datasets: War and Peace and Linux kernel source code Visualizing LSTMs Karpathy et al. (2015) ‣ Uninterpretable: probably doing double-duty, or only makes sense in the context of another acDvaDon ‣ Visualize acDvaDons of speciﬁc cells to see what they track ‣ Train character LSTM language model (predict next character based on history) over two datasets: War and Peace and Linux kernel source code What can LSTMs model? ‣ SenDment ‣ TranslaDon ‣ Language models ‣ Encode one sentence, predict ‣ Move leU-to-right, per-token predicDon ‣ Encode sentence + then decode, use token predicDons for a^enDon weights (next lecture) ‣ Textual entailment ‣ Encode two sentences, predict Natural Language Inference A man inspects the uniform of a ﬁgure The man is sleeping An older and younger man smiling Two men are smiling and laughing at cats playing A boy plays in the snow A boy is outside entails contradicts neutral ‣ Long history of this task: “Recognizing Textual Entailment” challenge in 2006 (Dagan, Glickman, Magnini) ‣ Early datasets: small (hundreds of pairs), very ambiDous (lots of world knowledge, temporal reasoning, etc.) Premise Hypothesis SNLI Dataset Bowman et al. (2015) ‣ Show people capDons for (unseen) images and solicit entailed / neural / contradictory statements ‣ >500,000 sentence pairs 100D LSTM: 78% accuracy 300D LSTM: 80% accuracy (Bowman et al., 2016) 300D BiLSTM: 83% accuracy (Liu et al., 2016) ‣ Encode each sentence and process ‣ Later: be^er models for this Takeaways ‣ RNNs can transduce inputs (produce one output for each input) or compress the whole input into a vector ‣ Useful for a range of tasks with sequenDal input: senDment analysis, language modeling, natural language inference, machine translaDon ‣ Next Dme: CNNs and neural CRFs "
15,"School of Computer Science Probabilistic Graphical Models Mean Fiend Approximation & Topic Models Eric Xing Lecture 15, March 5, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 Variational Principle Exact variational formulation  : the marginal polytope, difficult to characterize  : the negative entropy function, no explicit form Mean field method: non-convex inner bound and exact form of entropy Bethe approximation and loopy belief propagation: polyhedral outer bound and non-convex Bethe approximation © Eric Xing @ CMU, 2005-2014 2 Mean Field Approximation © Eric Xing @ CMU, 2005-2014 3 Mean Field Methods For a given tractable subgraph F, a subset of canonical parameters is Inner approximation Mean field solves the relaxed problem  is the exact dual function restricted to © Eric Xing @ CMU, 2005-2014 4 For an exponential family with sufficient statistics defined on graph G, the set of realizable mean parameter set Idea: restrict p to a subset of distributions associated with a tractable subgraph Tractable Subgraphs © Eric Xing @ CMU, 2005-2014 5 Example: Naïve Mean Field for Ising Model Ising model in {0,1} representation Mean parameters For fully disconnected graph F, The dual decomposes into sum, one for each node µs = E p[Xs] = P [Xs = 1] for all s V, and µst = E p[XsXt] = P [(Xs,Xt) = (1,1)] for all (s,t) E. © Eric Xing @ CMU, 2005-2014 6 Naïve Mean Field for Ising Model  Optimization Problem  Update Rule  resembles “message” sent from node to  forms the “mean field” applied to from its neighborhood  Also yields lower bound on log partition function 7 © Eric Xing @ CMU, 2005-2014 Z X f E X H P Q KL F f a a Q Q a log ) ( log ) ( ) || (       Geometry of Mean Field Mean field optimization is always non-convex for any exponential family in which the state space is finite Recall the marginal polytope is a convex hull  contains all the extreme points  If it is a strict subset, then it must be non-convex Example: two-node Ising model  It has a parabolic cross section along , hence non-convex © Eric Xing @ CMU, 2005-2014 8 )] ( [ X p G )}] ( [{ c c X q G Exact: Clusters: (intractable) Cluster-based approx. to the Gibbs free energy (Wiegerinck 2001, Xing et al 03,04) 9 © Eric Xing @ CMU, 2005-2014 Mean field approx. to Gibbs free energy Given a disjoint clustering, {C1, … , CI}, of all variables Let Mean-field free energy  Will never equal to the exact Gibbs free energy no matter what clustering is used, but it does always define a lower bound of the likelihood Optimize each qi(xc)'s.  Variational calculus …  Do inference in each qi(xc) using any tractable algorithm ), ( ) ( i i i q q C X X             i C i C i i C i C i i C i i i i C i q q E q G x x x x x x ln ) ( MF            i x i i i i x i j i j i x x j i i i j i x q x q x x q x x x q x q G ln ) ( ) ( e.g., MF   (naïve mean field) 10 © Eric Xing @ CMU, 2005-2014 ) , | ( ) ( , , , , * i j i i i i q MB H C E C H C H i p q   X x X X Theorem: The optimum GMF approximation to the cluster marginal is isomorphic to the cluster posterior of the original distribution given internal evidence and its generalized mean fields: GMF algorithm: Iterate over each qi The Generalized Mean Field theorem 11 © Eric Xing @ CMU, 2005-2014 [xing et al. UAI 2003] A generalized mean field algorithm 12 © Eric Xing @ CMU, 2005-2014 [xing et al. UAI 2003] A generalized mean field algorithm 13 © Eric Xing @ CMU, 2005-2014 Theorem: The GMF algorithm is guaranteed to converge to a local optimum, and provides a lower bound for the likelihood of evidence (or partition function) the model. Convergence theorem 14 © Eric Xing @ CMU, 2005-2014 Gibbs predictive distribution:                 i j i j i ij i i i i A x X X x X p N  0 exp ) | ( }) : { | ( i j i j x X p N   j x j x mean field equation: }) : { | ( exp ) ( i j i j i q j i ij i i i i j X X p A X X X X q j q i j N N                     0 j q j X } : { i j j X j q N    Xi Approximate p(X) by fully factorized q(X)=Piqi(Xi) For Boltzmann distribution p(X)=exp{i < j qijXiXj+qioXi}/Z : Xi xjqj resembles a “message” sent from node j to i {xjqj : j Ni} forms the “mean field” applied to Xi from its neighborhood } : { i q j j X j N    j q j X The naive mean field approximation 15 © Eric Xing @ CMU, 2005-2014 Cluster marginal of a square block Ck:                      k k MBC k k MB j k C i k C k k C j i X q j i ij C i i i j i ij C X X X X X X q , ) ( ' , , ' exp ) (    0 Virtually a reparameterized Ising model of small size. Example 1: Generalized MF approximations to Ising models 16 © Eric Xing @ CMU, 2005-2014 GMF approximation to Ising models GMF2x2 GMF4x4 BP Attractive coupling: positively weighted Repulsive coupling: negatively weighted 17 © Eric Xing @ CMU, 2005-2014 GMFr GMFb BP Example 2: Sigmoid belief network 18 © Eric Xing @ CMU, 2005-2014 Example 3: Factorial HMM 19 © Eric Xing @ CMU, 2005-2014 Automatic Variational Inference  Currently for each new model we have to  derive the variational update equations  write application-specific code to find the solution  Each can be time consuming and error prone  Can we build a general-purpose inference engine which automates these procedures? ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... ... ... ... ... A A A A x2 x3 x1 xN yk2 yk3 yk1 ykN ... ... y12 y13 y11 y1N ... S2 S3 S1 SN ... fHMM Mean field approx. Structured variational approx.  20 © Eric Xing @ CMU, 2005-2014 Probabilistic Topic Models Humans cannot afford to deal with (e.g., search, browse, or measure similarity) a huge number of text documents We need computers to help out … 21 © Eric Xing @ CMU, 2005-2014 How to get started?  Here are some important elements to consider before you start:  Task:  Embedding? Classification? Clustering? Topic extraction? …  Data representation:  Input and output (e.g., continuous, binary, counts, …)  Model:  BN? MRF? Regression? SVM?  Inference:  Exact inference? MCMC? Variational?  Learning:  MLE? MCLE? Max margin?  Evaluation:  Visualization? Human interpretability? Perperlexity? Predictive accuracy?  It is better to consider one element at a time! 22 © Eric Xing @ CMU, 2005-2013 Tasks: document embedding  Say, we want to have a mapping …, so that  Compare similarity  Classify contents  Cluster/group/categorizing  Distill semantics and perspectives  ..  23 © Eric Xing @ CMU, 2005-2014 Summarizing the data using topics © Eric Xing @ CMU, 2005-2013 24 See how data changes over time © Eric Xing @ CMU, 2005-2013 25 User interest modeling using topics © Eric Xing @ CMU, 2005-2013 26 http://cogito-demos.ml.cmu.edu/cgi-bin/recommendation.cgi Representation: Data:  Each document is a vector in the word space  Ignore the order of words in a document. Only count matters!  A high-dimensional and sparse representation – Not efficient text processing tasks, e.g., search, document classification, or similarity measure – Not effective for browsing As for the Arabian and Palestinean voices that are against the current negotiations and the so-called peace process, they are not against peace per se, but rather for their well-founded predictions that Israel would NOT give an inch of the West bank (and most probably the same for Golan Heights) back to the Arabs. An 18 months of ""negotiations"" in Madrid, and Washington proved these predictions. Now many will jump on me saying why are you blaming israelis for no-result negotiations. I would say why would the Arabs stall the negotiations, what do they have to loose ? Arabian negotiations against peace Israel Arabs blaming Bag of Words Representation 27 © Eric Xing @ CMU, 2005-2014 How to Model Semantic? Q: What is it about? A: Mainly MT, with syntax, some learning A Hierarchical Phrase-Based Model for Statistical Machine Translation We present a statistical phrase-based Translation model that uses hierarchical phrases—phrases that contain sub-phrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical Phrase based model achieves a relative Improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. Source Target SMT Alignment Score BLEU Parse Tree Noun Phrase Grammar CFG likelihood EM Hidden Parameters Estimation argMax MT Syntax Learning 0.6 0.3 0.1 Unigram over vocabulary Topics Mixing Proportion Topic Models 28 © Eric Xing @ CMU, 2005-2014 Why this is Useful? Q: What is it about? A: Mainly MT, with syntax, some learning A Hierarchical Phrase-Based Model for Statistical Machine Translation We present a statistical phrase-based Translation model that uses hierarchical phrases—phrases that contain sub-phrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical Phrase based model achieves a relative Improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system. MT Syntax Learning Mixing Proportion 0.6 0.3 0.1  Q: give me similar document?  Structured way of browsing the collection  Other tasks  Dimensionality reduction  TF-IDF vs. topic mixing proportion  Classification, clustering, and more … 29 © Eric Xing @ CMU, 2005-2014 Topic Models: The Big Picture Unstructured Collection Structured Topic Network Topic Discovery Dimensionality Reduction w1 w2 wn x x x x T1 Tk T2 x x x x Word Simplex Topic Simplex 30 © Eric Xing @ CMU, 2005-2014 words documents W  D words topic topic topic topic documents LSI Topic models words  documents words topics topics documents P(w|z) P(z) P(w) Topic-Mixing is via repeated word labeling d W x   ' = LSI versus Topic Model (probabilistic LSI) 31 © Eric Xing @ CMU, 2005-2014 Words in Contexts “It was a nice shot. ” 32 © Eric Xing @ CMU, 2005-2014 Words in Contexts (con'd) the opposition Labor Party fared even worse, with a predicted 35 seats, seven less than last election. 33 © Eric Xing @ CMU, 2005-2014 ""Words"" in Contexts (con'd) Sivic et al. ICCV 2005 34 © Eric Xing @ CMU, 2005-2014 Objects are bags of elements Mixtures are distributions over elements Objects have mixing vector   Represents each mixtures’ contributions Object is generated as follows:  Pick a mixture component from   Pick an element from that component Admixture Models money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 money1 stream2 bank1 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 bank1 money1 stream2 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 money1 stream2 bank1 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 bank1 money1 stream2 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 money1 stream2 bank1 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 bank1 money1 stream2 … 0.1 0.1 0.5 ….. 0.1 0.5 0.1 ….. 0.5 0.1 0.1 ….. money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 money1 stream2 bank1 money1 bank1 bank1 loan1 river2 stream2 bank1 money1 river2 bank1 money1 bank1 loan1 bank1 money1 stream2 35 © Eric Xing @ CMU, 2005-2014 Topic Models Generating a document Prior θ z w β Nd N K      from , | Draw - from Draw - each word For prior the from : 1 n z k n n n l multinomia z w l multinomia z n Draw      Which prior to use? 36 © Eric Xing @ CMU, 2005-2014 Choices of Priors Dirichlet (LDA) (Blei et al. 2003)  Conjugate prior means efficient inference  Can only capture variations in each topic’s intensity independently Logistic Normal (CTM=LoNTAM) (Blei & Lafferty 2005, Ahmed & Xing 2006)  Capture the intuition that some topics are highly correlated and can rise up in intensity together  Not a conjugate prior implies hard inference 37 Generative Semantic of LoNTAM Generating a document      from , | Draw - from Draw - each word For prior the from : 1 n z k n n n l multinomia z w l multinomia z n Draw      - Log Partition Function - Normalization Constant      log log exp , ~ , ~                                1 1 1 1 1 1 1 0 K i K i i i K K K i i e C e N LN            z w β Nd N K μ Σ 38 Posterior inference © Eric Xing @ CMU, 2005-2013 39 Posterior inference results z w b N a ✓ K Bayesian model inference ….. input output system ….. cortex cortical areas ….. Topics Topic proportions Topic assignments D © Eric Xing @ CMU, 2005-2013 40 Joint likelihood of all variables © Eric Xing @ CMU, 2005-2013 41 We are interested in computing the posterior, and the data likelihood! A possible query:  Close form solution?  Sum in the denominator over Tn terms, and integrate over n k-dimensional topic vectors Learning: What to learn? What is the objective function? Inference and Learning are both intractable                   } { 1 , , , ) | ( ) | ( ) | ( ) | ( ) ( m n n z N n n m n m n z m n d d d p p z p x p D p            ) ( ) | ( ) | ( ) | ( ) | ( ) ( ) , ( ) | ( } { , , , D p d d p p z p w p D p D p D p m n n z i n n m n m n z m n n n                             ? ) | ( ? ) | ( ,   D z p D p m n n  42 © Eric Xing @ CMU, 2005-2014 Approximate Inference Variational Inference  Mean field approximation (Blei et al)  Expectation propagation (Minka et al)  Variational 2nd-order Taylor approximation (Xing) Markov Chain Monte Carlo  Gibbs sampling (Griffiths et al) 43 © Eric Xing @ CMU, 2005-2014 Mean-field assumption True posterior Break the dependency using the fully factorized distribution Mean-field family usually does NOT include the true posterior. © Eric Xing @ CMU, 2005-2013 44 μ μ Update each marginals Update In LDA, We obtain © Eric Xing @ CMU, 2005-2013 45 This is also a Dirichlet---the same as its prior! Coordinate ascent algorithm for LDA © Eric Xing @ CMU, 2005-2013 46 ) | } { , ( D z P  β γ z w μ Σ Ahmed&Xing Blei&Lafferty Σ* is assumed to be diagonal Σ* is full matrix Log Partition Function 1 log 1 1          K i i e           n n n z q q z q     * *, , : 1 γ z w μ* Σ* φ β Multivariate Quadratic Approx. Tangent Approx. Closed Form Solution for μ*, Σ* Numerical Optimization to fit μ*, Diag(Σ*) Choice of q() does matter 47 © Eric Xing @ CMU, 2005-2014 Tangent Approximation 48 © Eric Xing @ CMU, 2005-2014 How to evaluate? Empirical Visualization: e.g., topic discovery on New York Times © Eric Xing @ CMU, 2005-2014 49 How to evaluate? w β  z μ Σ 50 © Eric Xing @ CMU, 2005-2014 • Test on Synthetic Text where ground truth is known: Comparison: accuracy and speed L2 error in topic vector est. and # of iterations  Varying Num. of Topics  Varying Voc. Size  Varying Num. Words Per Document 51 © Eric Xing @ CMU, 2005-2014 Comparison: perplexity 52 © Eric Xing @ CMU, 2005-2014 Classification Result on PNAS collection  PNAS abstracts from 1997-2002  2500 documents  Average of 170 words per document  Fitted 40-topics model using both approaches  Use low dimensional representation to predict the abstract category  Use SVM classifier  85% for training and 15% for testing Classification Accuracy -Notable Difference -Examine the low dimensional representations below 53 © Eric Xing @ CMU, 2005-2014 What makes topic models useful - -- The Zoo of Topic Models! It is a building block of many models. © Eric Xing @ CMU, 2005-2013 54 Williamson et al. 2010 Chang & Blei, 2009 Boyd-Graber & Blei, 2008 Wang & Blei, 2008 McCallum et al. 2007 Titov & McDonald, 2008 ! N d "" d w d , n z d , n K # k y d , d ' $ N d ' "" d ' w d ' , n z d ' , n (b) x z w N D    T A,A rd ad d ! ! T "" # k $ k % M & d ! D ' P a r s e t r e e s g r o u p e d i n t o M d o c u m e n t s Conclusion  GM-based topic models are cool  Flexible  Modular  Interactive  There are many ways of implementing topic models  unsupervised  supervised  Efficient Inference/learning algorithms  GMF, with Laplace approx. for non-conjugate dist.  MCMC  Many applications  …  Word-sense disambiguation  Image understanding  Network inference 55 © Eric Xing @ CMU, 2005-2014 Summary on VI  Variational methods in general turn inference into an optimization problem via exponential families and convex duality  The exact variational principle is intractable to solve; there are two distinct components for approximations:  Either inner or outer bound to the marginal polytope  Various approximation to the entropy function  Mean field: non-convex inner bound and exact form of entropy  BP: polyhedral outer bound and non-convex Bethe approximation  Kikuchi and variants: tighter polyhedral outer bounds and better entropy approximations (Yedidia et. al. 2002) © Eric Xing @ CMU, 2005-2014 56 "
150,"CS388: Natural Language Processing Lecture 9: CNNs, Neural CRFs Greg Durrett Administrivia ‣ Project 1 due today at 5pm ‣ Mini 2 out tonight, due in two weeks This Lecture ‣ CNNs ‣ CNNs for SenLment ‣ Neural CRFs CNNs ConvoluLonal Layer ‣ Applies a ﬁlter over patches of the input and returns that ﬁlter’s acLvaLons ‣ ConvoluLon: take dot product of ﬁlter with a patch of the input Each of these cells is a vector with mulLple values Images: RGB values (3 dim); text: word vector (50+ dim) image: n x n x k ﬁlter: m x m x k oﬀsets sum over dot products activationij = k−1 X io=0 k−1 X jo=0 image(i + io, j + jo) · ﬁlter(io, jo) ConvoluLonal Layer image: n x n x k acLvaLons: (n - m + 1) x (n - m + 1) x 1 ﬁlter: m x m x k ‣ Applies a ﬁlter over patches of the input and returns that ﬁlter’s acLvaLons ‣ ConvoluLon: take dot product of ﬁlter with a patch of the input ConvoluLons for NLP ‣ Combines evidence locally in a sentence and produces a new (but sLll variable-length) representaLon ‣ Input and ﬁlter are 2-dimensional instead of 3-dimensional the movie was good vector for each word sentence: n words x k vec dim ﬁlter: m x k acLvaLons: (n - m + 1) x 1 Compare: CNNs vs. LSTMs ‣ Both LSTMs and convoluLonal layers transform the input using context the movie was good the movie was good n x k c ﬁlters, m x k each O(n) x c n x k n x 2c BiLSTM with hidden size c ‣ LSTM: “globally” looks at the enLre sentence (but local for many problems) ‣ CNN: local depending on ﬁlter width + number of layers CNNs for SenLment CNNs for SenLment Analysis the movie was good n x k c ﬁlters, m x k each n x c max pooling over the sentence c-dimensional vector projecLon + so`max P(y|x) W ‣ Max pooling: return the max acLvaLon of a given ﬁlter over the enLre sentence; like a logical OR (sum pooling is like logical AND) the movie was good . 0.03 0.02 0.1 1.1 0.0 max = 1.1 “good” ﬁlter output } ‣ Filter “looks like” the things that will cause it to have high acLvaLon Understanding CNNs for SenLment the movie was good . 0.03 0.02 0.1 1.1 0.0 max = 1.1 0.1 0.3 0.1 “bad” “okay” “terrible” } Understanding CNNs for SenLment the movie was good . 0.03 0.02 0.1 1.1 0.0 max = 1.1 0.1 “bad” } ‣ Filters are iniLalized randomly and then learned 1.1 0.1 0.3 0.1 Features for classiﬁcaLon layer (or more NN layers) ‣ Takes variable-length input and turns it into ﬁxed-length output Understanding CNNs for SenLment the movie was great . 0.03 0.02 0.1 1.8 0.0 max = 1.8 } ‣ Word vectors for similar words are similar, so convoluLonal ﬁlters will have similar outputs Understanding CNNs for SenLment the movie was good . 0.03 0.05 0.1 } max = 1.5 “not good” not 0.2 1.5 ‣ Analogous to bigram features in bag-of-words models } } ‣ Indicator feature of text containing bigram <-> max pooling of a ﬁlter that matches that bigram Understanding CNNs for SenLment What can CNNs learn? the movie was not good the movie was not really all that good the cinematography was good, the music great, but the movie was bad I entered the theater in the bloom of youth and le> as an old man Deep ConvoluLonal Networks ‣ Low-level ﬁlters: extract low-level features from the data Zeiler and Fergus (2014) Deep ConvoluLonal Networks ‣ High-level ﬁlters: match larger and more “semanLc pajerns” Zeiler and Fergus (2014) CNNs: ImplementaLon ‣ Input is batch_size x n x k matrix, ﬁlters are c x m x k matrix (c ﬁlters) ‣ All computaLon graph libraries support eﬃcient convoluLon operaLons ‣ Typically use ﬁlters with m ranging from 1 to 5 or so (mulLple ﬁlter widths in a single convnet) CNNs for Sentence ClassiﬁcaLon ‣ QuesLon classiﬁcaLon, senLment, etc. Kim (2014) ‣ Conv+pool, then use feedforward layers to classify the movie was good W ‣ Can use mulLple types of input vectors (ﬁxed iniLalizer and learned) Sentence ClassiﬁcaLon quesLon type classiﬁcaLon subjecLvity/objecLvity detecLon movie review senLment product reviews ‣ Also eﬀecLve at document-level text classiﬁcaLon Kim (2014) Neural CRF Basics NER Revisited ‣ Features in CRFs: I[tag=B-LOC & curr_word=Hangzhou], I[tag=B-LOC & prev_word=to], I[tag=B-LOC & curr_preﬁx=Han] Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O ‣ Downsides: ‣ Lexical features mean that words need to be seen in the training data ‣ Linear model can’t capture feature conjuncLons as eﬀecLvely (can’t look at more than 2 words with a single feature) ‣ Linear model over features LSTMs for NER Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou B-PER I-PER O O O B-LOC ‣ Transducer (LM-like model) ‣ What are the strengths and weaknesses of this model compared to CRFs? LSTMs for NER Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou B-PER I-PER O O O B-LOC ‣ BidirecLonal transducer model ‣ What are the strengths and weaknesses of this model compared to CRFs? Neural CRFs Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou ‣ Neural CRFs: bidirecLonal LSTMs (or some NN) compute emission potenLals, capture structural constraints in transiLon potenLals Neural CRFs y1 y2 yn … φe φt P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) φe(yi, i, x) = w>fe(yi, i, x) ‣ Neural network computes unnormalized potenLals that are consumed and “normalized” by a structured model W is a num_tags x len(f) matrix ‣ ConvenLonal: ‣ Neural: ‣ f(i, x) could be the output of a feedforward neural network looking at the words around posiLon i, or the ith output of an LSTM, … ‣ Inference: compute f, use Viterbi φe(yi, i, x) = W > yif(i, x) CompuLng Gradients y1 y2 yn … φe φt P(y|x) = 1 Z n Y i=2 exp(φt(yi−1, yi)) n Y i=1 exp(φe(yi, i, x)) ‣ For linear model: @L @φe,i = −P(yi = s|x) + I[s is gold] φe(yi, i, x) = w>fe(yi, i, x) ‣ ConvenLonal: @φe,i wi = fe,i(yi, i, x) chain rule say to mulLply together, gives our update ‣ For neural model: compute gradient of phi w.r.t. parameters of neural net “error signal”, compute with F-B ‣ Neural: φe(yi, i, x) = W > yif(i, x) Neural CRFs Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou 1) Compute f(x) 2) Run forward-backward 3) Compute error signal 4) Backprop (no knowledge of sequenLal structure required) FFNN Neural CRF for NER Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O to Hangzhou today e(Hangzhou) previous word curr word next word e(today) e(to) φe = Wg(V f(x, i)) f(x, i) = [emb(xi−1), emb(xi), emb(xi+1)] FFNN LSTM Neural CRFs Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou ‣ BidirecLonal LSTMs compute emission (or transiLon) potenLals LSTMs for NER Barack Obama will travel to Hangzhou today for the G20 meeFng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou B-PER I-PER O O O B-LOC ‣ How does this compare to neural CRF? “NLP (Almost) From Scratch” Collobert, Weston, et al. 2008, 2011 ‣ LM2: word vectors learned from a precursor to word2vec/GloVe, trained for 2 weeks (!) on Wikipedia ‣ WLL: independent classiﬁcaLon; SLL: neural CRF CNN Neural CRFs travel to Hangzhou today for ‣ Append to each word vector an embedding of the relaFve posiFon of that word conv+pool+FFNN ‣ ConvoluLon over the sentence produces a posiLon-dependent representaLon … … 0 1 -2 travel Hangzhou today -1 to 2 for CNN NCRFs vs. FFNN NCRFs ‣ Sentence approach (CNNs) is comparable to window approach (FFNNs) except for SRL where they claim it works much bejer Collobert and Weston 2008, 2011 Neural CRFs with LSTMs ‣ Neural CRF using character LSTMs to compute word representaLons Chiu and Nichols (2015), Lample et al. (2016) Neural CRFs with LSTMs Chiu and Nichols (2015), Lample et al. (2016) ‣ Chiu+Nichols: character CNNs instead of LSTMs ‣ Lin/Passos/Luo: use external resources like Wikipedia ‣ LSTM-CRF captures the important aspects of NER: word context (LSTM), sub-word features (character LSTMs), outside knowledge (word embeddings) Takeaways ‣ CNNs are a ﬂexible way of extracLng features analogous to bag of n- grams, can also encode posiLonal informaLon ‣ All kinds of NNs can be integrated into CRFs for structured inference. Can be applied to NER, other tagging, parsing, … ‣ This concludes the ML/DL-heavy porLon of the course. StarLng Tuesday: syntax, then semanLcs "
151,"CS388: Natural Language Processing Lecture 10: Syntax I Greg Durrett Slides adapted from Dan Klein, UC Berkeley Recall: CNNs vs. LSTMs ‣ Both LSTMs and convoluLonal layers transform the input using context the movie was good the movie was good n x k c ﬁlters, m x k each O(n) x c n x k n x 2c BiLSTM with hidden size c ‣ LSTM: “globally” looks at the enLre sentence (but local for many problems) ‣ CNN: local depending on ﬁlter width + number of layers Recall: CNNs the movie was good n x k c ﬁlters, m x k each n x c max pooling over the sentence c-dimensional vector projecLon + soZmax P(y|x) W ‣ Max pooling: return the max acLvaLon of a given ﬁlter over the enLre sentence; like a logical OR (sum pooling is like logical AND) Recall: Neural CRFs Barack Obama will travel to Hangzhou today for the G20 mee=ng . PERSON LOC ORG B-PER I-PER O O O B-LOC B-ORG O O O O O Barack Obama will travel to Hangzhou 1) Compute f(x) 2) Run forward-backward 3) Compute error signal 4) Backprop (no knowledge of sequenLal structure required) This Lecture ‣ ConsLtuency formalism ‣ Context-free grammars and the CKY algorithm ‣ Reﬁning grammars ‣ DiscriminaLve parsers ConsLtuency Syntax ‣ Study of word order and how words form sentences ‣ Why do we care about syntax? ‣ Recognize verb-argument structures (who is doing what to whom?) ‣ MulLple interpretaLons of words (noun or verb? Fed raises… example) ‣ Higher level of abstracLon beyond words: some languages are SVO, some are VSO, some are SOV, parsing can canonicalize ConsLtuency Parsing ‣ Tree-structured syntacLc analyses of sentences ‣ Common things: noun phrases, verb phrases, preposiLonal phrases ‣ Bogom layer is POS tags ‣ Examples will be in English. ConsLtuency makes sense for a lot of languages but not all sentenLal complement whole embedded sentence adverbial phrase ConsLtuency Parsing The rat the cat chased squeaked I raced to Indianapolis , unimpeded by traﬃc Challenges ‣ PP agachment § If we do no annota+on, these trees diﬀer only in one rule: § VP → VP PP same parse as “the cake with some icing” Challenges ‣ NP internal structure: tags + depth of analysis ConsLtuency ‣ How do we know what the consLtuents are? ‣ ConsLtuency tests: ‣ SubsLtuLon by proform (e.g., pronoun) ‣ CleZing (It was with a spoon that…) ‣ Answer ellipsis (What did they eat? the cake) (How? with a spoon) ‣ SomeLmes consLtuency is not clear, e.g., coordinaLon: she went to and bought food at the store Context-Free Grammars, CKY CFGs and PCFGs § Write symbolic or logical rules: § Use deduc4on systems to prove parses from words § Minimal grammar on “Fed raises” sentence: 36 parses § Simple 10-rule grammar: 592 parses § Real-size grammar: many millions of parses § This scaled very badly, didn’t yield broad-coverage tools Grammar (CFG) Lexicon ROOT → S S → NP VP NP → DT NN NP → NN NNS NN → interest NNS → raises VBP → interest VBZ → raises … NP → NP PP VP → VBP NP VP → VBP NP PP PP → IN NP ‣ Context-free grammar: symbols which rewrite as one or more symbols ‣ Lexicon consists of “preterminals” (POS tags) rewriLng as terminals (words) ‣ CFG is a tuple (N, T, S, R): N = nonterminals, T = terminals, S = start symbol (generally a special ROOT symbol), R = rules ‣ PCFG: probabiliLes associated with rewrites, normalize by source symbol 0.2 0.5 0.3 0.7 0.3 1.0 1.0 1.0 1.0 1.0 1.0 1.0 EsLmaLng PCFGs ‣ Maximum likelihood PCFG: count and normalize! Same as HMMs / Naive Bayes S → NP VP NP → PRP NP → DT NN … 1.0 0.5 0.5 ‣ Tree T is a series of rule applicaLons r. P(T) = Y r2T P(r|parent(r)) BinarizaLon ‣ To parse eﬃciently, we need our PCFGs to be at most binary (not CNF) VP VBD NP PP PP sold the book to her for $3 P(VP → VBD NP PP PP) = 0.2 VP VBD VP NP PP VP PP VP VBD VP-[NP PP PP] NP PP VP-[PP PP] PP ‣ Lossless: ‣ Lossy: P(VP → VBZ PP) = 0.1 … Chomsky Normal Form VP VBD VP-[NP PP PP] VBD PP VP-[PP PP] PP P(VP → VBD VP-[NP PP PP]) = 0.2 VP VBD VP NP PP VP PP ‣ Lossless: ‣ Lossy: P(VP → VBD VP) = 0.2 P(VP → NP VP) = 0.03 P(VP-[NP PP PP] → NP VP-[PP PP]) = 1.0 P(VP-[PP PP] → PP PP) = 1.0 P(VP → PP PP) = 0.001 ‣ DeterminisLc symbols make this the same as before ‣ Makes diﬀerent independent assumpLons, not the same PCFG CKY He wrote a long report on Mars NP PP NP ‣ Find argmax P(T|x) = argmax P(T, x) ‣ Dynamic programming: chart maintains the best way of building symbol X over span (i, j) ‣ Loop over all split points k, apply rules X -> Y Z to build X in every possible way ‣ CKY = Viterbi, also an algorithm called inside-outside = forward-backward Cocke-Kasami-Younger i j k X Z Y Unary Rules SBAR S the rat the cat chased squeaked NP NNS mice ‣ Unary producLons in treebank need to be dealt with by parsers ‣ Binary trees over n words have at most n-1 nodes, but you can have unlimited numbers of nodes with unaries (S → SBAR → NP → S → …) ‣ In pracLce: enforce at most one unary over each span, modify CKY accordingly Results Klein and Manning (2003) ‣ Standard dataset for English: Penn Treebank (Marcus et al., 1993) ‣ EvaluaLon: F1 over labeled consLtuents of the sentence ‣ Vanilla PCFG: ~75 F1 ‣ Best PCFGs for English: ~90 F1 ‣ Other languages: results vary widely depending on annotaLon + complexity of the grammar ‣ SOTA: 95 F1 Reﬁning GeneraLve Grammars PCFG Independence AssumpLons 11% 9% 6% NP PP DT NN PRP 9% 9% 21% NP PP DT NN PRP 7% 4% 23% NP PP DT NN PRP All NPs NPs under S NPs under VP ‣ Language is not context-free: NPs in diﬀerent contexts rewrite diﬀerently ‣ Can we make the grammar “less context-free”? Rule AnnotaLon ‣ VerLcal (parent) annotaLon: add the parent symbol to each node, can do grandparents too § Ver$cal Markov order: rewrites depend on past k ancestor nodes. (cf. parent annota$on) Order 1 Order 2 72% 73% 74% 75% 76% 77% 78% 79% 1 2v 2 3v 3 0 5000 10000 15000 20000 25000 1 2v 2 3v 3 Symbols ‣ Like a trigram HMM tagger, incorporates more context Order 1 Order ∞ ‣ Horizontal annotaLon: remember the states of mulL-arity rules during binarizaLon Annotated Tree Klein and Manning (2003) ‣ 75 F1 with basic PCFG => 86.3 F1 with this highly customized PCFG (SOTA was 90 F1 at the Lme, but with more complex methods) Lexicalized Parsers § What’s diﬀerent between basic PCFG scores here? Wh (l i l) l ; d b d? ‣ Even with parent annotaLon, these trees have the same rules. Need to use the words Lexicalized Parsers § Add “head words” to each phrasal node § Syntac4c vs. seman4c heads § Headship not in (most) treebanks § Usually use head rules, e.g.: § NP: § Take leFmost NP § Take rightmost N* § Take rightmost JJ § Take right child § VP: § Take leFmost VB* § Take leFmost VP § Take leF child ‣ Annotate each grammar symbol with its “head word”: most important word of that consLtuent ‣ Rules for idenLfying headwords (e.g., the last word of an NP before a preposiLon is typically the head) ‣ Collins and Charniak (late 90s): ~89 F1 with these DiscriminaLve Parsers CRF Parsing He wrote a long report on Mars . PP NP He wrote a long report on Mars . PP NP VP VBD NP My report Fig. 1 report—on Mars wrote—on Mars CRF Parsing Taskar et al. (2004) Hall, Durreg, and Klein (2014) Durreg and Klein (2015) score LeZ child last word = report ∧NP PP NP w>f NP PP NP 2 5 7 = f NP PP NP 2 5 7 He wrote a long report on Mars . PP NP NP = 2 5 7 wrote a long report on Mars . wrote a long report on Mars . ‣ Can learn that we report [PP], which is common due to repor=ng on things ‣ Can “neuralize” this as well like neural CRFs for NER + Discrete ConLnuous He wrote a long report on Mars NP PP NP ‣ Chart remains discrete! ‣ Feedforward pass on nets ‣ Run CKY dynamic program ‣ Discrete feature computaLon + Discrete ConLnuous … Parsing a sentence: Durreg and Klein (ACL 2015) Joint Discrete and ConLnuous Parsing Neural CRF Parsing Stern et al. (2017), Kitaev et al. (2018) ‣ Simpler version: score cons=tuents rather than rule applicaLons score w>f NP 2 7 = He wrote a long report on Mars . PP NP NP 2 5 7 wrote a long report on Mars . ‣ Use BiLSTMs (Stern) or self-agenLon (Kitaev) to compute span embeddings ‣ 91-93 F1, 95 F1 with ELMo (SOTA). Great on other langs too! Takeaways ‣ PCFGs esLmated generaLvely can perform well if suﬃciently engineered ‣ Neural CRFs work well for consLtuency parsing ‣ Next Lme: revisit lexicalized parsing as dependency parsing Survey ‣ Write one thing you like about the class ‣ Write one thing you don’t like about the class "
152,"CS388: Natural Language Processing Lecture 11: Dependency Parsing I Greg Durrett Administrivia ‣ Project 1 graded by Tuesday ‣ Survey results: ‣ If you have comments on the code, please send them to me (either anonymously or non-anonymously) ‣ Bit rate ‣ Clearer slides/notaKon ‣ Some annoyances from projects: slow debugging/training, etc. Recall: ConsKtuency ‣ Tree-structured syntacKc analyses of sentences ‣ Nonterminals (NP, VP, etc.) as well as POS tags (boQom layer) ‣ Structured is deﬁned by a CFG Recall: CKY He wrote a long report on Mars NP PP NP ‣ Find argmax P(T|x) = argmax P(T, x) ‣ Dynamic programming: chart maintains the best way of building symbol X over span (i, j) ‣ Loop over all split points k, apply rules X -> Y Z to build X in every possible way Cocke-Kasami-Younger i j k X Z Y Outline ‣ Dependency representaKon, contrast with consKtuency ‣ ProjecKvity ‣ Graph-based dependency parsers ‣ DiscriminaKve consKtuency parsing DiscriminaKve Parsers CRF Parsing Taskar et al. (2004) Hall, DurreQ, and Klein (2014) DurreQ and Klein (2015) score Lec child last word = report ∧NP PP NP w>f NP PP NP 2 5 7 = f NP PP NP 2 5 7 He wrote a long report on Mars . PP NP NP = 2 5 7 wrote a long report on Mars . wrote a long report on Mars . ‣ Can learn that we report [PP], which is common due to repor'ng on things ‣ Can “neuralize” this as well like neural CRFs for NER + Discrete ConKnuous He wrote a long report on Mars NP PP NP ‣ Chart remains discrete! ‣ Feedforward pass on nets ‣ Run CKY dynamic program ‣ Discrete feature computaKon + Discrete ConKnuous … Parsing a sentence: DurreQ and Klein (ACL 2015) Joint Discrete and ConKnuous Parsing Neural CRF Parsing Stern et al. (2017), Kitaev et al. (2018) ‣ Simpler version: score cons'tuents rather than rule applicaKons score w>f = He wrote a long report on Mars . NP 2 7 ‣ Use BiLSTMs to compute embeddings of each word, embeddings at edge of span characterize that span ‣ 91-93 F1, 95 F1 with ELMo (SOTA). Great on other langs too! He wrote a long report on Mars . BiLSTM Dependency RepresentaKon Lexicalized Parsing S(ran) NP(dog) VP(ran) PP(to) NP(house) DT(the) NN(house) TO(to) VBD(ran) DT(the) NN(dog) the house to ran the dog Dependency Parsing DT NN TO VBD DT NN the house to ran the dog ‣ Dependency syntax: syntacKc structure is deﬁned by these arcs ‣ Head (parent, governor) connected to dependent (child, modiﬁer) ‣ Each word has exactly one parent except for the ROOT symbol, dependencies must form a directed acyclic graph ROOT ‣ POS tags same as before, usually run a tagger ﬁrst as preprocessing Dependency Parsing DT NN TO VBD DT NN the house to ran the dog ‣ SKll a noKon of hierarchy! Subtrees ocen align with consKtuents Dependency Parsing DT NN TO VBD DT NN the house to ran the dog ‣ Can label dependencies according to syntacKc funcKon det ‣ Major source of ambiguity is in the structure, so we focus on that more (labeling separately with a classiﬁer works preQy well) nsubj pobj det prep Dependency vs. ConsKtuency: PP AQachment ‣ ConsKtuency: several rule producKons need to change the children ate the cake with a spoon ‣ Dependency: one word (with) assigned a diﬀerent parent Dependency vs. ConsKtuency: PP AQachment ‣ More predicate-argument focused view of syntax ‣ “What’s the main verb of the sentence? What is its subject and object?” — easier to answer under dependency parsing ‣ ConsKtuency: ternary rule NP -> NP CC NP Dependency vs. ConsKtuency: CoordinaKon dogs in houses and cats ‣ Dependency: ﬁrst item is the head Dependency vs. ConsKtuency: CoordinaKon dogs in houses and cats ‣ CoordinaKon is decomposed across a few arcs as opposed to being a single rule producKon as in consKtuency ‣ Can also choose and to be the head ‣ In both cases, headword doesn’t really represent the phrase — consKtuency representaKon makes more sense [dogs in houses] and cats dogs in [houses and cats] Universal Dependencies ‣ Annotate dependencies with the same representaKon in many languages hQp://universaldependencies.org/ English Bulgarian Czech Swiss ProjecKvity DT NN TO VBD DT NN the house to ran the dog ‣ Any subtree is a conKguous span of the sentence <-> tree is projec've ProjecKvity ‣ ProjecKve <-> no “crossing” arcs dogs in houses and cats the dog ran to the house credit: Language Log ‣ Crossing arcs: ProjecKvity in other languages credit: Pitler et al. (2013) ‣ Swiss-German has famous non-context-free construcKons ProjecKvity Pitler et al. (2013) ‣ Many trees in other languages are nonprojecKve ‣ Number of trees produceable under diﬀerent formalisms ProjecKvity ‣ Many trees in other languages are nonprojecKve ‣ Some other formalisms (that are harder to parse in), most useful one is 1- Endpoint-Crossing ‣ Number of trees produceable under diﬀerent formalisms Pitler et al. (2013) Graph-Based Parsing Deﬁning Dependency Graphs ‣ Words in sentence x, tree T is a collecKon of directed edges (parent(i), i) for each word i ‣ Each word has exactly one parent. Edges must form a projecKve tree ‣ Log-linear CRF (discriminaKve): ‣ Example of a feature = I[head=to & modiﬁer=house] (more in a few slides) the house to ran the dog ROOT P(T|x) = exp X i w>f(i, parent(i), x) ! ‣ Parsing = idenKfy parent(i) for each word Generalizing CKY wrote a long report on Mars 4 5 4 2 5 ‣ score(2, 7, 4) = max(score(2, 7, 4), new score) ‣ new score = score(2, 5, 4) + score(5, 7, 5) + edge score(4 -> 5) ‣ Score matrix with three dimensions: start, end, and head, start <= head < end ‣ Time complexity of this? ‣ Many spurious deriva'ons: can build the same tree in many ways…need a beQer algorithm 4 = report 5 = on 4 Eisner’s Algorithm: O(n3) DT NN TO VBD DT NN the house to ran the dog ROOT ‣ Complete items: head is at “tall end”, may be missing children on tall side ‣ Incomplete items: arc from “tall” to “short” end, word on short end may also be missing children ‣ Cubic-Kme algorithm ‣ Maintain two dynamic programming charts with dimension [n, n, 2]: Eisner’s Algorithm: O(n3) DT NN TO VBD DT NN the house to ran the dog ROOT + ‣ Complete item: all children are aQached, head is at the “tall end” ‣ Incomplete item: arc from “tall end” to “short end”, may sKll expect children ‣ Take two adjacent complete items, add arc and build incomplete item = or + = ‣ Take an incomplete item, complete it (other case is symmetric) Eisner’s Algorithm: O(n3) DT NN TO VBD DT NN the house to ran the dog ROOT 1) Build incomplete span 2) Promote to complete 3) Build incomplete span + = + or = Eisner’s Algorithm: O(n3) DT NN TO VBD DT NN the house to ran the dog ROOT + = + or = 4) Promote to complete Eisner’s Algorithm: O(n3) DT NN TO VBD DT NN the house to ran the dog ROOT ‣ We’ve built lec children and right children of ran as complete items ‣ AQaching to ROOT makes an incomplete item with lec children, aQaches with right children subsequently to ﬁnish the parse Eisner’s Algorithm DT NN TO VBD DT NN the house to ran the dog ROOT ‣ Eisner’s algorithm doesn’t have split point ambiguiKes like CKY does ‣ Lec and right children are built independently, heads are edges of spans ‣ Charts are n x n x 2 because we need to track arc direcKon / lec vs right Eisner: n5 Building Systems ‣ Can implement decoding and marginal computaKon using Eisner’s algorithm to max/sum over projecKve trees ‣ Conceptually the same as inference/learning for sequenKal CRFs for NER, can also use margin-based methods Features in Graph-Based Parsing ‣ Dynamic program exposes the parent and child indices ‣ McDonald et al. (2005) — conjuncKons of parent and child words + POS, POS of words in between, POS of surrounding words DT NN TO VBD DT NN the house to ran the dog ROOT ‣ HEAD=TO & MOD=NN ‣ HEAD=TO & MOD-1=the ‣ HEAD=TO & MOD=house ‣ ARC_CROSSES=DT f(i, parent(i), x) Higher-Order Parsing Koo and Collins (2009) ‣ Track addiKonal state during parsing so we can look at “grandparents” (and siblings). O(n4) dynamic program or use approximate search DT NN TO VBD DT NN the house to ran the dog ROOT f(i, parent(i), parent(parent(i)), x) Biaﬃne Neural Parsing ‣ Neural CRFs for dependency parsing: let c = LSTM embedding of i, p = LSTM embedding of parent(i). score(i, parent(i), x) = pTUc Dozat and Manning (2017) (num words x hidden size) (num words x num words) LSTM looks at words and POS EvaluaKng Dependency Parsing ‣ UAS: unlabeled aQachment score. Accuracy of choosing each word’s parent (n decisions per sentence) ‣ Log-linear CRF parser, decoding with Eisner algorithm: 91 UAS ‣ LAS: addiKonally consider label for each edge ‣ Higher-order features from Koo parser: 93 UAS ‣ Best English results with neural CRFs: 95-96 UAS Takeaways ‣ Dependency parsing also has eﬃcient dynamic programs for inference ‣ Dependency formalism provides an alternaKve to consKtuency, parKcularly useful in how portable it is across languages ‣ CRFs + neural CRFs (again) work well "
153,"CS388: Natural Language Processing Lecture 12: Dependency II Greg Durrett Recall: Dependencies DT NN TO VBD DT NN the house to ran the dog ‣ Dependency syntax: syntacEc structure is deﬁned by dependencies ‣ Head (parent, governor) connected to dependent (child, modiﬁer) ‣ Each word has exactly one parent except for the ROOT symbol ‣ Dependencies must form a directed acyclic graph ROOT Recall: ProjecEvity ‣ ProjecEve <-> no “crossing” arcs dogs in houses and cats the dog ran to the house credit: Language Log ‣ Crossing arcs: Recall: Eisner’s Algorithm ‣ LeY and right children are built independently, heads are edges of spans ‣ Complete item: all children are aZached, head is at the “tall end” ‣ Incomplete item: arc from “tall end” to “short end”, may sEll expect children DT NN TO VBD DT NN the house to ran the dog ROOT This Lecture ‣ TransiEon-based (shiY-reduce) dependency parsing ‣ Approximate, greedy inference — fast, but a liZle bit weird! ShiY-Reduce Parsing ShiY-Reduce Parsing ‣ Similar to determinisEc parsers for compilers ‣ A tree is built from a sequence of incremental decisions moving leY to right through the sentence ‣ ShiYs consume the buﬀer, reduces build a tree on the stack ‣ Stack containing parEally-built tree, buﬀer containing rest of sentence ‣ Also called transiEon-based parsing ShiY-Reduce Parsing I ate some spaghe` bolognese ROOT ‣ ShiY 1: Stack: [ROOT I] Buﬀer: [ate some spaghe` bolognese] ‣ ShiY: top of buﬀer -> top of stack ‣ IniEal state: Stack: [ROOT] Buﬀer: [I ate some spaghe` bolognese] ‣ ShiY 2: Stack: [ROOT I ate] Buﬀer: [some spaghe` bolognese] ShiY-Reduce Parsing I ate some spaghe` bolognese ROOT ‣ State: Stack: [ROOT I ate] Buﬀer: [some spaghe` bolognese] ‣ LeY-arc (reduce): Let denote the stack, = stack ending in w-1 σ ‣ “Pop two elements, add an arc, put them back on the stack” ‣ State: Stack: [ROOT ate] Buﬀer: [some spaghe` bolognese] I σ|w−2, w−1 ! σ|w−1 w−1 w−2 is now a child of , σ|w−2, w−1 ! σ|w−1 Arc-Standard Parsing ‣ Start: stack contains [ROOT], buﬀer contains [I ate some spaghe` bolognese] ‣ ShiY: top of buﬀer -> top of stack ‣ LeY-Arc: σ|w−2, w−1 ! σ|w−1 w−1 w−2 ‣ Right-Arc σ|w−2, w−1 ! σ|w−2 is now a child of , w−1 w−2 , I ate some spaghe` bolognese ‣ End: stack contains [ROOT], buﬀer is empty [] ‣ How many transiEons do we need if we have n words in a sentence? is now a child of ROOT ‣ Arc-standard system: three operaEons Arc-Standard Parsing [I ate some spaghe` bolognese] [ROOT] [ROOT I] [ROOT I ate] [ROOT ate] I S S L ‣ Could do the leY arc later! But no reason to wait ‣ Can’t aZach ROOT <- ate yet even though this is a correct dependency! S top of buﬀer -> top of stack LA RA [ate some spaghe` bolognese] [some spaghe` bolognese] [some spaghe` bolognese] I ate some spaghe` bolognese ROOT pop two, leY arc between them pop two, right arc between them Arc-Standard Parsing [ROOT ate] I [some spaghe` bolognese] [ROOT ate some spaghe`] I [bolognese] [ROOT ate spaghe`] I some [bolognese] S L I ate some spaghe` bolognese S ROOT S S top of buﬀer -> top of stack LA RA pop two, leY arc between them pop two, right arc between them Arc-Standard Parsing [ROOT ate spaghe` bolognese] I some [ROOT ate spaghe`] I some bolognese [ROOT ate] I some bolognese spaghe` ‣ Stack consists of all words that are sEll waiEng for right children, end with a bunch of right-arc ops [ROOT] I some bolognese spaghe` ate [] I ate some spaghe` bolognese ROOT [] [] [] Final state: R R S top of buﬀer -> top of stack LA RA pop two, leY arc between them pop two, right arc between them Other Systems ‣ Arc-eager (Nivre, 2004): lets you add right arcs sooner and keeps items on stack, separate reduce acEon that clears out the stack ‣ Arc-swiY (Qi and Manning, 2017): explicitly choose a parent from what’s on the stack ‣ Many ways to decompose these, which one works best depends on the language and features (nonprojecEve variants too!) Building ShiY-Reduce Parsers [ROOT ate some spaghe`] I [bolognese] ‣ MulE-way classiﬁcaEon problem: shiY, leY-arc, or right-arc? [ROOT] [I ate some spaghe` bolognese] ‣ How do we make the right decision in this case? ‣ How do we make the right decision in this case? (all three acEons legal) ‣ Only one legal move (shiY) argmaxa2{S,LA,RA}w>f(stack, bu↵er, a) Features for ShiY-Reduce Parsing [ROOT ate some spaghe`] I [bolognese] ‣ Features to know this should leY-arc? ‣ One of the harder feature design tasks! ‣ In this case: the stack tag sequence VBD - DT - NN is preZy informaEve — looks like a verb taking a direct object which has a determiner in it ‣ Things to look at: top words/POS of buﬀer, top words/POS of stack, leYmost and rightmost children of top items on the stack Training a Greedy Model [ROOT ate some spaghe`] I [bolognese] ‣ Train a classiﬁer to predict the right decision using these as training data ‣ Can turn a tree into a decision sequence a by building an oracle ‣ Training data assumes you made correct decisions up to this point and teaches you to make the correct decision, but what if you screwed up… argmaxy2{S,LA,RA}w>f(y, stack, bu↵er) Greedy training ‣ Greedy: 2n local training examples State space Gold end state Start state ‣ Non-gold states unobserved during training: consider making bad decisions but don’t condi*on on bad decisions Speed Tradeoﬀs UnopEmized S-R{ { { { Chen and Manning (2014) OpEmized S-R Graph-based Neural S-R ‣ Many early-2000s consEtuency parsers were ~5 sentences/sec ‣ Using S-R used to mean taking a performance hit compared to graph-based, that’s no longer true Global Decoding Global Decoding [ROOT gave him] I [dinner] ‣ Correct: Right-arc, ShiY, Right-arc, Right-arc I gave him dinner ROOT [ROOT gave] I [dinner] him [ROOT gave dinner] I [] him [ROOT gave] I [] him dinner ‣ Is it a problem that we make decisions greedily? Global Decoding: A Cartoon S LA RA ‣ Both wrong! Also both probably low scoring! RA S ‣ Correct, high scoring opEon [ROOT gave him] I [dinner] I gave him dinner ROOT [ROOT gave him dinner] I [] LA [ROOT gave] I him [dinner] Global Decoding: A Cartoon [ROOT gave him] I [dinner] I gave him dinner ROOT ‣ Lookahead can help us avoid ge`ng stuck in bad spots ‣ Global model: maximize sum of scores over all decisions ‣ Similar to how Viterbi works: we maintain uncertainty over the current state so that if another one looks more opEmal going forward, we can use that one Global ShiY-Reduce Parsing [ROOT gave him] I [dinner] I gave him dinner ROOT ‣ Global: ‣ Can we do search exactly? ‣ No! Use beam search ‣ Greedy: repeatedly execute s abest(s) abest argmaxaw>f(s, a) si+1 = ai(si) ‣ How many states s are there? argmaxs,aw>f(s, a) = 2n X i=1 w>f(si, ai) Beam Search ‣ Maintain a beam of k plausible states at the current Emestep, expand each and only keep top k best new ones Fed VBD VBN NNP raises +1.2 +0.9 +0.7 NN +0.3 VBZ +1.2 VBZ -2.0 NNS-1.0 Not expanded … VBZ DT NNS +1.2 -1.0 -5.3 … … PRP -5.8 Not expanded ‣ Beam size of k, n words, s states, Eme complexity -2.0 O(nks log(ks)) ‣ Maintain priority queue to eﬃciently add things ‣ Example: POS How good is beam search? ‣ k=1: greedy search ‣ Choosing beam size: ‣ 2 is usually beZer than 1 ‣ Usually don’t use larger than 50 ‣ Depends on problem structure Global ShiY-Reduce Parsing [ROOT gave him dinner] I [] [ROOT gave] I him [dinner] LA RA S -1.2 +0.9 [ROOT gave him] I [] -3.0 dinner [ROOT gave dinner] I [] -2.0 him [ROOT gave dinner] I him +2.0 [] ‣ Beam search gave us the lookahead to make the right decision Global Training ‣ If using global inference, should train the parser in a global fashion as well: use structured perceptron / structured SVM ‣ Model treats an enEre derivaEon as something to featurize ‣ No algorithm like Viterbi for doing eﬃcient parsing, so use beam search State-of-the-art Parsers State-of-the-art Parsers ‣ 2012: Maltparser was SOTA was for transiEon-based (~90 UAS) ‣ 2010: Koo’s 3rd-order parser was SOTA for graph-based (~93 UAS) ‣ 2014: Chen and Manning got 92 UAS with transiEon-based neural model ‣ 2005: Eisner algorithm graph-based parser was SOTA (~91 UAS) ‣ 2016: Improvements to Chen and Manning State-of-the-art Parsers Chen and Manning (2014) Parsey McParseFace Andor et al. (2016) ‣ Close to state-of-the-art, released by Google publicly ‣ 94.61 UAS on the Penn Treebank using a global transiEon-based system with early updaEng (compared to 95.8 for Dozat, 93.7 for Koo in 2009) ‣ Feedforward neural nets looking at words and POS associated with ‣ Words at the top of the stack ‣ Those words’ children ‣ Words in the buﬀer ‣ Feature set pioneered by Chen and Manning (2014), Google ﬁne-tuned it ‣ AddiEonal data harvested via “tri-training”, form of self-training (a.k.a. SyntaxNet) Stack LSTMs Dyer et al. (2015) ‣ Use LSTMs over stack, buﬀer, past acEon sequence. Trained greedily ‣ Slightly less good than Parsey Recap ‣ ShiY-reduce parsing can work nearly as well as graph-based ‣ Arc-standard system for transiEon-based parsing ‣ Purely greedy or more “global” approaches ‣ Next Eme: semanEc parsing "
154,"CS388: Natural Language Processing Lecture 13: Seman8cs I Greg Durrett Slides adapted from Dan Klein, UC Berkeley Administrivia ‣ Mini 2 due *today* at 5pm Recall: Dependencies DT NN TO VBD DT NN the house to ran the dog ‣ Dependency syntax: syntac8c structure is deﬁned by dependencies ‣ Head (parent, governor) connected to dependent (child, modiﬁer) ‣ Each word has exactly one parent except for the ROOT symbol ‣ Dependencies must form a directed acyclic graph ROOT Recall: ShiX-Reduce Parsing I ate some spagheZ bolognese ROOT ‣ State: Stack: [ROOT I ate] Buﬀer: [some spagheZ bolognese] ‣ LeX-arc (reduce opera8on): Let denote the stack σ ‣ “Pop two elements, add an arc, put them back on the stack” ‣ State: Stack: [ROOT ate] Buﬀer: [some spagheZ bolognese] I σ|w−2, w−1 ! σ|w−1 w−1 w−2 is now a child of , Where are we now? ‣ Early in the class: sentences are just sequences of words ‣ Why is this useful? What does this allow us to do? ‣ Now we can understand them in terms of tree structures as well ‣ We’re going to see how parsing can be a stepping stone towards more formal representa8ons of language meaning Today ‣ First-order logic ‣ CCG parsing for database queries ‣ Composi8onal seman8cs with ﬁrst-order logic ‣ Lambda-DCS for ques8on answering First-Order Logic First-order Logic ‣ sings is a predicate (with one argument), func8on f: en8ty => true/false ‣ Powerful logic formalism including things like en88es, rela8ons, and quan8ﬁca8ons ‣ [[sings]] = denota,on, set of en88es which sing (sort of like execu8ng this predicate on the world — we’ll come back to this) ‣ Proposi8ons: let a = It is day, b = It is night ‣ a ∨ b = either a is true or b is true, a => ¬b = a implies not b ‣ More complex statements: “Lady Gaga sings” ‣ sings(Lady Gaga) = true or false, have to execute this against some database (called a world) Quan8ﬁca8on ‣ ∀x sings(x) ∨ dances(x) => performs(x) ‣ ∃y ∀x friend(x,y) ‣ Universal quan8ﬁca8on: “forall” operator ‣ Existen8al quan8ﬁca8on: “there exists” operator ‣ ∀x ∃y friend(x,y) ‣ Source of ambiguity! “Everyone is friends with someone” “Everyone who sings or dances performs” “Someone sings” ‣ ∃x sings(x) Logic in NLP ‣ Ques8on answering: ‣ Informa8on extrac8on: Lady Gaga and Eminem are both musicians ∀x musician(x) => performer(x) musician(Lady Gaga) ∧ musician(Eminem) Then: performer(Lady Gaga) ∧ performer(Eminem) Who are all the American singers named Amy? λx. na8onality(x,USA) ∧ sings(x) ∧ ﬁrstName(x,Amy) ‣ Func8on that maps from x to true/false, like filter. Execute this on the world to answer the ques8on ‣ Can now do reasoning. Maybe know: ‣ Lambda calculus: powerful system for expressing these func8ons Composi8onal Seman8cs with First- Order Logic Truth-Condi8onal Seman8cs Id Name Alias Birthdate Sings? e470 Stefani Germanooa Lady Gaga 3/28/1986 T e728 Marshall Mathers Eminem 10/17/1972 T ‣ Database containing en88es, predicates, etc. ‣ Truth-condi8onal seman8cs: sentence expresses something about the world which is either true or false NP VP NNP NNP S VBP Lady Gaga sings ‣ Denota8on: evalua8on of some expression against this database ‣[[Lady Gaga]] = e470 denota8on of this string is an en8ty ‣[[sings(e470)]] = True denota8on of this expression is T/F Parses to Logical Forms NP VP NNP NNP S VBP Lady Gaga sings e470 λy. sings(y) takes one argument (y, the en8ty) and returns a logical form sings(y) λy. sings(y) sings(e470) ‣ We can use the syntac8c parse as a bridge to the lambda-calculus representa8on, build up a logical form composi,onally func8on applica8on: apply this to e470 ID Parses to Logical Forms NP VP NNP NNP S VBP Lady Gaga sings e470 λy. sings(y) sings(e470) ∧ dances(e470) VP CC VP VBP dances λy. dances(y) and VP: λy. a(y) ∧ b(y) -> VP: λy. a(y) CC VP: λy. b(y) λy. sings(y) ∧ dances(y) ‣ General rules: S: f(x) -> NP: x VP: f Parses to Logical Forms NP NNP NNP S VBD Lady Gaga was e470 λx.λy. born(y,x) born(e470,3/28/1986) VP NP March 28, 1986 born λy. born(y, 3/28/1986) VBN VP λy. born(y, 3/28/1986) ‣ How to handle tense: should we indicate that this happened in the past? ‣ Func8on takes two arguments: ﬁrst x (date), then y (en8ty) 3/28/1986 Tricky things ‣ Adverbs/temporality: Lady Gaga sang well yesterday ∃e. type(e,sing) ∧ agent(e,e470) ∧ manner(e,well) ∧ time(e,…) ‣ “Neo-Davidsonian” view of events: things with many proper8es: ‣ Quan8ﬁca8on: Everyone is friends with someone ‣ Generic: Cats eat mice (all cats eat mice? most cats? some cats?) ∀x ∃y friend(x,y) ∃y ∀x friend(x,y) (diﬀerent friends) (one friend) ‣ Same syntac8c parse for both! So syntax doesn't resolve all ambigui8es sings(Lady Gaga, time=yesterday, manner=well) ‣ Indeﬁnite: Amy ate a waﬄe ∃w. waffle(w) ∧ ate(Amy,w) QA from Parsing Lady Gaga born NP VP WHADVP VBD SQ WRB NNP NNP VBN SBARQ When was λx. born(e470,x) ‣ Execute this func8on against a knowledge base to answer the ques8on ‣ Tricky to parse due to wh-movement…would be easier if we said Lady Gaga was born when Seman8c Parsing ‣ For ques8on answering, syntac8c parsing doesn’t tell you everything you want to know, but indicates the right structure ‣ Solu8on: seman,c parsing: many forms of this task depending on seman8c formalisms ‣ Two today: CCG (looks like what we’ve been doing) and lambda-DCS CCG Parsing Combinatory Categorial Grammar ‣ Steedman+Szabolcsi 1980s: formalism bridging syntax and seman8cs ‣ Syntac8c categories (for this lecture): S, NP, “slash” categories ‣ S\NP: “if I combine with an NP on my leX side, I form a sentence” — verb NP S\NP Eminem sings e728 λy. sings(y) S sings(e728) ‣ Parallel deriva8ons of syntac8c parse and lambda calculus expression ‣ When you apply this, there has to be a parallel instance of func8on applica8on on the seman8cs side Combinatory Categorial Grammar ‣ Steedman+Szabolcsi 1980s: formalism bridging syntax and seman8cs ‣ Syntac8c categories (for this lecture): S, NP, “slash” categories ‣ S\NP: “if I combine with an NP on my leX side, I form a sentence” — verb ‣ (S\NP)/NP: “I need an NP on my right and then on my leX” — verb with a direct object NP S\NP Eminem sings e728 λy. sings(y) S sings(e728) NP (S\NP)/NP Oklahoma borders e101 Texas e89 NP λx.λy borders(y,x) S\NP λy borders(y,e89) S borders(e101,e89) CCG Parsing Zeolemoyer and Collins (2005) ‣ “What” is a very complex type: needs a noun and needs a S\NP to form a sentence. S\NP is basically a verb phrase (border Texas) ‣ Lexicon is highly ambiguous — all the challenge of CCG parsing is in picking the right lexicon entries CCG Parsing Slide credit: Dan Klein ‣ “to” needs an NP (des8na8on) and N (parent) Building CCG Parsers Zeolemoyer and Collins (2005) ‣ Model: log-linear model over deriva8ons with features on rules: P(d|x) / exp w> X r2d f(r, x) ! ‣ Can parse with a variant of CKY Eminem sings NP S\NP e728 λy. sings(y) S sings(e728) f f f = Indicator(S\NP -> sings) = Indicator(S -> NP S\NP) Building CCG Parsers Zeolemoyer and Collins (2005) ‣ Training data looks like pairs of sentences and logical forms What states border Texas λx. state(x) ∧ borders(x, e89) ‣ Texas corresponds to NP | e89 in the logical form (easy to ﬁgure out) (S/(S\NP))/N | λf.λg.λx. f(x) ∧ g(x) ‣ What corresponds to ‣ How do we infer that without being told it? ‣ Problem: we don’t know the deriva8on Lexicon What states border Texas λx. state(x) ∧ borders(x, e89) ‣ Any substring can parse to any of these in the lexicon ‣ Chunks inferred from the logic form based on rules: ‣ GENLEX: takes sentence S and logical form L. Break up logical form into chunks C(L), assume any substring of S might map to any chunk ‣ Texas -> NP: e89 is correct ‣ border Texas -> NP: e89 ‣ What states border Texas -> NP: e89 … Zeolemoyer and Collins (2005) ‣ NP: e89 ‣ (S\NP)/NP: λx. λy. borders(x,y) GENLEX ‣ Very complex and hand-engineered way of taking lambda calculus expressions and “backsolving” for the deriva8on Zeolemoyer and Collins (2005) Learning Zeolemoyer and Collins (2005) ‣ Itera8ve procedure like the EM algorithm: es8mate “best” parses that derive each logical form, retrain the parser using these parses with supervised learning ‣ We’ll talk about a simpler form of this in a few slides Applica8ons ‣ GeoQuery: answering ques8ons about states (~80% accuracy) ‣ Jobs: answering ques8ons about job pos8ngs (~80% accuracy) ‣ ATIS: ﬂight search ‣ Can do well on all of these tasks if you handcraX systems and use plenty of training data: these domains aren’t that rich ‣ What about broader QA? Lambda-DCS Lambda-DCS Liang et al. (2011), Liang (2013) ‣ Dependency-based composi8onal seman8cs — original version was less powerful than lambda calculus, lambda-DCS is as powerful ‣ Designed in the context of building a QA system from Freebase ‣ Freebase: set of en88es and rela8ons Alice Smith Bob Cooper Seaole March 15, 1961 Washington DateOfBirth PlaceOfBirth PlaceOfBirth CapitalOf ‣ [[PlaceOfBirth]] = set of pairs of (person, loca8on) Lambda-DCS Liang et al. (2011), Liang (2013) Lambda-DCS Lambda calculus Seattle λx. x = Seattle PlaceOfBirth λx.λy. PlaceOfBirth(x,y) PlaceOfBirth.Seattle λx. PlaceOfBirth(x,Seattle) ‣ Looks like a tree fragment over Freebase Seaole PlaceOfBirth ??? Profession.Scientist ∧ PlaceOfBirth.Seattle λx. Profession(x,Scientist) ∧ PlaceOfBirth(x,Seattle) Lambda-DCS Liang et al. (2011), Liang (2013) Alice Smith Bob Cooper Seaole March 15, 1961 Washington DateOfBirth PlaceOfBirth PlaceOfBirth CapitalOf Profession Scien8st Profession.Scientist ∧ PlaceOfBirth.Seattle “list of scien8sts born in Seaole” ‣ Execute this fragment against Freebase, returns Alice Smith (and others) ??? Seaole PlaceOfBirth Profession Scien8st Parsing into Lambda-DCS Berant et al. (2013) ‣ Building the lexicon: more sophis8cated process than GENLEX, but can handle thousands of predicates ‣ Log-linear model with features on rules: P(d|x) / exp w> X r2d f(r, x) ! ‣ Deriva8on d on sentence x: ‣ Similar to CRF parsers ‣ No more explicit syntax in these deriva8ons like we had in CCG Parsing with Lambda-DCS Berant et al. (2013) ‣ Learn just from ques8on-answer pairs: maximize the likelihood of the right denota8on y with the deriva8on d marginalized out For each example: Run beam search to get a set of deriva8ons Let d* = highest-scoring deriva8on in the beam with correct denota,on Do a structured perceptron update towards d* away from d Let d = highest-scoring deriva8on in the beam sum over deriva8ons d such that the denota8on of d on knowledge base K is yi Learning Berant et al. (2013) ‣ Only a small number of ques8ons are even reachable by beam search ini8ally (but some ques8ons are very easy so even a totally untrained model can answer them) ‣ During training, more and more “good” deriva8ons surface and will result in model updates ‣ Each ver8cal slice is the beam for one example. Green = correct denota8on Takeaways ‣ Can represent meaning with ﬁrst order logic and lambda calculus ‣ Useful for querying databases, ques8on answering, etc. ‣ Can bridge syntax and seman8cs and create seman8c parsers that can interpret language into lambda-calculus expressions ‣ Next 8me: neural net methods for doing this that rely less on having explicit grammars "
155,"CS388: Natural Language Processing Lecture 14: Seman9cs II / Seq2seq I Greg Durrett Administrivia ‣ Graham Neubig (CMU) talk this Friday at 11am in 6.302. “Towards Open-domain Genera9on of Programs from Natural Language” ‣ Mini 2 graded by the end of the week ‣ Project 2 out by Thursday Recall: Parses to Logical Forms NP VP NNP NNP S VBP Lady Gaga sings e470 λy. sings(y) sings(e470) ∧ dances(e470) VP CC VP VBP dances λy. dances(y) and VP: λy. a(y) ∧ b(y) -> VP: λy. a(y) CC VP: λy. b(y) λy. sings(y) ∧ dances(y) ‣ General rules: S: f(x) -> NP: x VP: f Recall: CCG ‣ Steedman+Szabolcsi 1980s: formalism bridging syntax and seman9cs ‣ Syntac9c categories (for this lecture): S, NP, “slash” categories ‣ S\NP: “if I combine with an NP on my lec side, I form a sentence” — verb ‣ (S\NP)/NP: “I need an NP on my right and then on my lec” — verb with a direct object NP S\NP Eminem sings e728 λy. sings(y) S sings(e728) NP (S\NP)/NP Oklahoma borders e101 Texas e89 NP λx.λy borders(y,x) S\NP λy borders(y,e89) S borders(e101,e89) This Lecture ‣ Lambda-DCS: more lightweight than CCG ‣ Seq2seq models ‣ Seq2seq models for seman9c parsing Lambda-DCS Lambda-DCS Liang et al. (2011), Liang (2013) ‣ Dependency-based composi9onal seman9cs — original version was less powerful than lambda calculus, lambda-DCS is as powerful ‣ Designed in the context of building a QA system from Freebase ‣ Freebase: set of en99es and rela9ons Alice Smith Bob Cooper Seafle March 15, 1961 Washington DateOfBirth PlaceOfBirth PlaceOfBirth CapitalOf ‣ [[PlaceOfBirth]] = set of pairs of (person, loca9on) Lambda-DCS Liang et al. (2011), Liang (2013) Lambda-DCS Lambda calculus Seattle λx. x = Seattle PlaceOfBirth λx.λy. PlaceOfBirth(x,y) PlaceOfBirth.Seattle λx. PlaceOfBirth(x,Seattle) ‣ Looks like a tree fragment over Freebase, denotes the set of people born in Seafle, no explicit variables Seafle PlaceOfBirth ??? Profession.Scientist ∧ PlaceOfBirth.Seattle λx. Profession(x,Scientist) ∧ PlaceOfBirth(x,Seattle) Lambda-DCS Liang et al. (2011), Liang (2013) Alice Smith Bob Cooper Seafle March 15, 1961 Washington DateOfBirth PlaceOfBirth PlaceOfBirth CapitalOf Profession Scien9st Profession.Scientist ∧ PlaceOfBirth.Seattle “list of scien9sts born in Seafle” ‣ Execute this fragment against Freebase, returns Alice Smith (and others) ??? Seafle PlaceOfBirth Profession Scien9st Parsing into Lambda-DCS Berant et al. (2013) ‣ Building the lexicon: more sophis9cated process than GENLEX, but can handle thousands of predicates ‣ Log-linear model with features on rules: ‣ Deriva9on d on sentence x: ‣ No more explicit syntax in these deriva9ons like we had in CCG ‣ Everything is a set, sets combine in a few ways P(d|x) / exp w> X r2d f(r, x) ! Parsing into Lambda-DCS Berant et al. (2013) ‣ Learn from deriva9ons: standard supervised learning, maximize probability of correct deriva9on L(✓) = n X i=1 log P(d⇤ i |xi) ‣ Problem: supervision looks like “Where was Barack Obama born” — “Hawaii” without a deriva9on Parsing into Lambda-DCS Berant et al. (2013) ‣ Learn just from ques9on-answer pairs: maximize the likelihood of the right denota9on y* with the deriva9on d marginalized out Approx procedure: for each example: Run beam search to get a set of deriva9ons Let d* = highest-scoring deriva9on in the beam with correct denota;on Do a structured perceptron update towards d* away from d Let d = highest-scoring deriva9on in the beam sum over deriva9ons d such that the denota9on of d on knowledge base K is yi L(✓) = n X i=1 log X d:[[d]]K=y⇤ i P(d|xi) Learning Berant et al. (2013) ‣ Only a small number of ques9ons are even reachable by beam search ini9ally (but some ques9ons are very easy so even a totally untrained model can answer them) ‣ During training, more and more “good” deriva9ons surface and will result in model updates ‣ Each ver9cal slice is the beam for one example. Green = correct denota9on Encoder-Decoder Models Mo9va9on ‣ Parsers have been prefy hard to build… ‣ Cons9tuency/graph-based: complex dynamic programs ‣ Transi9on-based: complex transi9on systems ‣ CCG/seman9c parsers: complex syntax/seman9cs interface, challenging inference, challenging learning ‣ For seman9c parsing in par9cular: bridging the syntax-seman9cs divide results in structural weirdnesses in parsers ‣ Encoder-decoder models can be a lot more uniform — we’ll come back to this later in the lecture Encoder-Decoder ‣ Encode a sequence into a ﬁxed-sized vector the movie was great ‣ Now use that vector to produce a series of tokens as output from a separate LSTM decoder le ﬁlm était bon [STOP] Sutskever et al. (2014) Encoder-Decoder ‣ Is this true? Sort of…we’ll come back to this later Model ‣ Generate next word condi9oned on previous word as well as hidden state the movie was great <s> ¯ h ‣ W size is |vocab| x |hidden state|, socmax over en9re vocabulary Decoder has separate parameters from encoder, so this can learn to be a language model (produce a plausible next word given current one) P(y|x) = n Y i=1 P(yi|x, y1, . . . , yi−1) P(yi|x, y1, . . . , yi−1) = softmax(W¯ h) Inference ‣ Generate next word condi9oned on previous word as well as hidden state the movie was great ‣ During inference: need to compute the argmax over the word predic9ons and then feed that to the next RNN state le <s> ‣ Need to actually evaluate computa9on graph up to this point to form input for the next state ‣ Decoder is advanced one state at a 9me un9l [STOP] is reached ﬁlm était bon [STOP] Implemen9ng seq2seq Models the movie was great ‣ Encoder: consumes sequence of tokens, produces a vector. Analogous to encoders for classiﬁca9on/tagging tasks le <s> ‣ Decoder: separate module, single cell. Takes two inputs: hidden state (vector h or tuple (h, c)) and previous token. Outputs token + new state Encoder … ﬁlm le Decoder Decoder Training ‣ Objec9ve: maximize the movie was great <s> le ﬁlm était bon le ‣ One loss term for each target-sentence word, feed the correct word regardless of model’s predic9on [STOP] était X (x,y) n X i=1 log P(y⇤ i |x, y⇤ 1, . . . , y⇤ i−1) Training: Scheduled Sampling ‣ Star9ng with p = 1 and decaying it works best ‣ Scheduled sampling: with probability p, take the gold as input, else take the model’s predic9on the movie was great la ﬁlm étais bon [STOP] le ﬁlm était ‣ Model needs to do the right thing even with its own predic9ons Bengio et al. (2015) sample Implementa9on Details ‣ Sentence lengths vary for both encoder and decoder: ‣ Typically pad everything to the right length and use a mask or indexing to access a subset of terms ‣ Encoder: looks like what you did in Mini 2. Can be a CNN/LSTM/… ‣ Decoder: also ﬂexible in terms of architecture (more next lecture). Execute one step of computa9on at a 9me, so computa9on graph is formulated as taking one input + hidden state ‣ Test 9me: do this un9l you generate the stop token ‣ Training: do this un9l you reach the gold stopping point Implementa9on Details (cont’d) ‣ Batching is prefy tricky ‣ Decoder is across 9me steps, so you probably want your label vectors to look like [num 9mesteps x batch size x num labels], iterate upwards by 9me steps ‣ Beam search: can help with lookahead. Finds the (approximate) highest scoring sequence: argmaxy n Y i=1 P(yi|x, y1, . . . , yi−1) Beam Search ‣ Maintain decoder state, token history in beam la: 0.4 <s> la le les le: 0.3 les: 0.1 log(0.4) log(0.3) log(0.1) ﬁlm: 0.4 la … ﬁlm: 0.8 le … le ﬁlm la ﬁlm log(0.3)+log(0.8) … log(0.4)+log(0.4) ‣ Do not max over the two ﬁlm states! Hidden state vectors are diﬀerent the movie was great Seq2seq Seman9c Parsing Seman9c Parsing as Transla9on Jia and Liang (2015) ‣ Write down a linearized form of the seman9c parse, train seq2seq models to directly translate into this representa9on ‣ What might be some concerns about this approach? How do we mi9gate them? “what states border Texas” lambda x ( state ( x ) and border ( x , e89 ) ) ) ‣ What are some beneﬁts of this approach compared to grammar-based? Handling Invariances ‣ Parsing-based approaches handle these the same way ‣ Possible divergences: features, diﬀerent weights in the lexicon ‣ Key idea: don’t change the model, change the data “what states border Texas” “what states border Ohio” ‣ Can we get seq2seq seman9c parsers to handle these the same way? ‣ “Data augmenta9on”: encode invariances by automa9cally genera9ng new training examples Data Augmenta9on ‣ Abstract out en99es: now we can “remix” examples and encode invariance to en9ty ID. More complicated remixes too ‣ Lets us synthesize a “what states border ohio ?” example Jia and Liang (2015) Seman9c Parsing as Transla9on Jia and Liang (2015) ‣ Prolog ‣ Lambda calculus ‣ Other DSLs ‣ Handle all of these with uniform machinery! Seman9c Parsing as Transla9on Jia and Liang (2015) ‣ Three forms of data augmenta9on all help ‣ Results on these tasks are s9ll not as strong as hand-tuned systems from 10 years ago, but the same simple model can do well at all problems Regex Predic9on ‣ Can use for other seman9c parsing-like tasks ‣ Predict regex from text ‣ Problem: requires a lot of data: 10,000 examples needed to get ~60% accuracy on prefy simple regexes Locascio et al. (2016) SQL Genera9on ‣ Convert natural language descrip9on into a SQL query against some DB ‣ How to ensure that well- formed SQL is generated? Zhong et al. (2017) ‣ Three seq2seq models ‣ How to capture column names + constants? ‣ Pointer mechanisms Afen9on ‣ Orange pieces are probably reused across many problems ‣ LSTM has to remember the value of Texas for 13 steps! ‣ Next lecture: afen9on mechanisms that let us “look back” at the input to avoid having to remember everything “what states border Texas” lambda x ( state ( x ) and border ( x , e89 ) ) ) ‣ Not too hard to learn to generate: start with lambda, always follow with x, follow that with paren, etc. This is a common ques9on Takeaways ‣ Lambda-DCS is a more lightweight formalism than lambda calculus ‣ Rather than combining syntax and seman9cs like in CCG, we can either parse to seman9c representa9ons directly or generate them with seq2seq models ‣ Seq2seq models are a very ﬂexible framework, some weaknesses can poten9ally be patched with more data "
156,"CS388: Natural Language Processing Lecture 15: A9en:on Greg Durrett This Lecture ‣ Graham Neubig (CMU) talk this Friday at 11am in 6.302. “Towards Open-domain Genera:on of Programs from Natural Language” ‣ Mini 2 graded by this weekend ‣ Project 2 out by the end of today; due *Friday* November 2 Recall: Seq2seq Model ‣ Generate next word condi:oned on previous word as well as hidden state the movie was great <s> ¯ h ‣ W size is |vocab| x |hidden state|, soamax over en:re vocabulary Decoder has separate parameters from encoder, so this can learn to be a language model (produce a plausible next word given current one) P(y|x) = n Y i=1 P(yi|x, y1, . . . , yi−1) P(yi|x, y1, . . . , yi−1) = softmax(W¯ h) Recall: Seq2seq Training ‣ Objec:ve: maximize the movie was great <s> le ﬁlm était bon le ‣ One loss term for each target-sentence word, feed the correct word regardless of model’s predic:on [STOP] était X (x,y) n X i=1 log P(y⇤ i |x, y⇤ 1, . . . , y⇤ i−1) Recall: Seman:c Parsing as Transla:on Jia and Liang (2015) ‣ Write down a linearized form of the seman:c parse, train seq2seq models to directly translate into this representa:on ‣ Might not produce well-formed logical forms, might require lots of data “what states border Texas” lambda x ( state ( x ) and border ( x , e89 ) ) ) ‣ No need to have an explicit grammar, simpliﬁes algorithms Regex Predic:on ‣ Can use for other seman:c parsing-like tasks ‣ Predict regex from text ‣ Problem: requires a lot of data: 10,000 examples needed to get ~60% accuracy on pre9y simple regexes Locascio et al. (2016) SQL Genera:on ‣ Convert natural language descrip:on into a SQL query against some DB ‣ How to ensure that well- formed SQL is generated? Zhong et al. (2017) ‣ Three seq2seq models ‣ How to capture column names + constants? ‣ Pointer mechanisms This Lecture ‣ Transformers ‣ A9en:on ‣ Copying A9en:on Problems with Seq2seq Models ‣ Need some no:on of input coverage or what input words we’ve translated ‣ Encoder-decoder models like to repeat themselves: A boy plays in the snow boy plays boy plays Un garçon joue dans la neige ‣ Oaen a byproduct of training these models poorly Problems with Seq2seq Models ‣ Unknown words: ‣ No ma9er how much data you have, you’ll need some mechanism to copy a word like Pont-de-Buis from the source to target Problems with Seq2seq Models ‣ Bad at long sentences: 1) a ﬁxed-size representa:on doesn’t scale; 2) LSTMs s:ll have a hard :me remembering for really long periods of :me RNNsearch: introduces a9en:on mechanism to give “variable-sized” representa:on Bahdanau et al. (2014) Aligned Inputs <s> le ﬁlm était bon the movie was great the movie was great le ﬁlm était bon ‣ Much less burden on the hidden state ‣ Suppose we knew the source and target would be word-by-word translated ‣ Can look at the corresponding input word when transla:ng — this could scale! le ﬁlm était bon [STOP] ‣ How can we achieve this without hardcoding it? A9en:on ‣ At each decoder state, compute a distribu:on over source inputs based on current decoder state the movie was great <s> le the movie was great the movie was great … … ‣ Use that in output layer A9en:on the movie was great h1 h2 h3 h4 <s> ¯ h1 ‣ For each decoder state, compute weighted sum of input states eij = f(¯ hi, hj) ci = X j ↵ijhj c1 ‣ Unnormalized scalar weight ‣ Normalized scalar weight ‣ Weighted sum of input hidden states (vector) le ↵ij = exp(eij) P j0 exp(eij0) P(yi|x, y1, . . . , yi−1) = softmax(W[ci; ¯ hi]) P(yi|x, y1, . . . , yi−1) = softmax(W¯ hi) ‣ No a9n: A9en:on <s> ¯ h1 eij = f(¯ hi, hj) ci = X j ↵ijhj c1 ‣ Note that this all uses outputs of hidden layers f(¯ hi, hj) = tanh(W[¯ hi, hj]) f(¯ hi, hj) = ¯ hi · hj f(¯ hi, hj) = ¯ h> i Whj ‣ Bahdanau+ (2014): addi:ve ‣ Luong+ (2015): dot product Luong et al. (2015) ‣ Luong+ (2015): bilinear le ↵ij = exp(eij) P j0 exp(eij0) What can a9en:on do? ‣ Learning to copy — how might this work? Luong et al. (2015) 0 3 2 1 0 3 2 1 ‣ LSTM can learn to count with the right weight matrix ‣ This is eﬀec:vely posi:on-based addressing What can a9en:on do? ‣ Learning to subsample tokens Luong et al. (2015) 0 3 2 1 3 1 ‣ Need to count (for ordering) and also determine which tokens are in/ out ‣ Content-based addressing A9en:on ‣ Decoder hidden states are now mostly responsible for selec:ng what to a9end to ‣ Doesn’t take a complex hidden state to walk monotonically through a sentence and spit out word-by-word transla:ons ‣ Encoder hidden states capture contextual source word iden:ty the movie was great Batching A9en:on Luong et al. (2015) the movie was great token outputs: batch size x sentence length x dimension sentence outputs: batch size x hidden size <s> hidden state: batch size x dimension eij = f(¯ hi, hj) ↵ij = exp(eij) P j0 exp(eij0) a9en:on scores = batch size x sentence length c = batch size x hidden size ci = X j ↵ijhj ‣ Make sure tensors are the right size! Alterna:ves ‣ When do we compute a9en:on? Can compute before or aaer RNN cell Bahdanau et al. (2015) <s> ¯ h1 c1 <s> c1 Luong et al. (2015) ‣ Aaer RNN cell ‣ Before RNN cell; this one is a li9le more convoluted and less standard le le Results ‣ Machine transla:on: BLEU score of 14.0 on English-German -> 16.8 with a9en:on, 19.0 with smarter a9en:on (we’ll come back to this later) Luong et al. (2015) Chopra et al. (2016) Jia and Liang (2016) ‣ Summariza:on/headline genera:on: bigram recall from 11% -> 15% ‣ Seman:c parsing: ~30% accuracy -> 70+% accuracy on Geoquery Copying Input/Pointers Unknown Words Jean et al. (2015), Luong et al. (2015) ‣ Want to be able to copy named en::es like Pont-de-Buis 1 P(yi|x, y1, . . . , yi−1) = softmax(W[ci; ¯ hi]) from a9en:on from RNN hidden state ‣ S:ll can only generate from the vocabulary Copying { { the a zebra Pont-de-Buis ecotax … ‣ Vocabulary contains “normal” vocab as well as words in input. Normalizes over both of these: ‣ Bilinear func:on of input representa:on + output hidden state { P(yi = w|x, y1, . . . , yi−1) / exp Ww[ci; ¯ hi] h> j V ¯ hi if w in vocab if w = xj Pointer Networks Vinyals et al. (2015) ‣ Only point to the input, don’t have any no:on of vocabulary ‣ Used for tasks including summariza:on and sentence ordering Results Jia and Liang (2016) ‣ In many se}ngs, a9en:on can roughly do the same things as copying ‣ For seman:c parsing, copying tokens from the input (texas) can be very useful Transformers Self-A9en:on the movie was great ‣ LSTM abstrac:on: maps each vector in a sentence to a new, context- aware vector ‣ CNNs did something similar with ﬁlters ‣ A9en:on can give us a third way to do this Vaswani et al. (2017) Self-A9en:on Vaswani et al. (2017) the movie was great ‣ Each word forms a “query” which then computes a9en:on over each word ‣ Mul:ple “heads” analogous to diﬀerent convolu:onal ﬁlters. Use parameters Wk and Vk to get diﬀerent a9en:on values + transform vectors x4 x0 4 scalar vector = sum of scalar * vector ↵i,j = softmax(x> i xj) x0 i = n X j=1 ↵i,jxj ↵k,i,j = softmax(x> i Wkxj) x0 k,i = n X j=1 ↵k,i,jVkxj Deep Transformers ‣ Supervised: transformer can replace LSTM; will revisit this when we discuss MT ‣ Unsupervised: transformers work be9er than LSTM for unsupervised pre-training of embeddings: predict word given context words ‣ Devlin et al. October 11, 2018 “BERT: Pre-training of Deep Bidirec:onal Transformers for Language Understanding” ‣ Stronger than similar methods, SOTA on ~11 tasks (including NER — 92.8 F1) Takeaways ‣ A9en:on is very helpful for seq2seq models ‣ Used for tasks including summariza:on and sentence ordering ‣ Explicitly copying input can be beneﬁcial as well ‣ Transformers are strong models we’ll come back to later Where are we going ‣ We’ve now talked about most of the important core tools for NLP ‣ Rest of the class: more focused on applica:ons ‣ Informa:on extrac:on, then MT, then a grab bag of things "
157,"CS388: Natural Language Processing Lecture 16: Informa;on Extrac;on Greg Durrett Recall: ABen;on the movie was great h1 h2 h3 h4 <s> ¯ h1 ‣ For each decoder state, compute weighted sum of input states eij = f(¯ hi, hj) ci = X j ↵ijhj c1 ‣ Unnormalized scalar weight ‣ Weighted sum of input hidden states (vector) le ↵ij = exp(eij) P j0 exp(eij0) P(yi|x, y1, . . . , yi−1) = softmax(W[ci; ¯ hi]) the movie was great ‣ Very helpful for seq2seq models Recall: Alterna;ves ‣ When do we compute aBen;on? Can compute before or aWer RNN cell Bahdanau et al. (2015) <s> ¯ h1 c1 <s> c1 Luong et al. (2015) ‣ AWer RNN cell ‣ Before RNN cell; this one is a liBle more convoluted and less standard le le This Lecture ‣ How do we represent informa;on for informa;on extrac;on? ‣ Open Informa;on Extrac;on ‣ Rela;on extrac;on ‣ Slot ﬁlling ‣ Seman;c role labeling / abstract meaning representa;on Represen;ng Informa;on Seman;c Representa;ons Example credit: Asad Sayeed person Brutus Caesar Obama Bush … stab Brutus Caesar … ‣ “World” is a set of en;;es and predicates president Obama Bush … Brutus stabs Caesar stab(Brutus, Caesar) => true ‣ Statements are logical expressions that evaluate to true or false Caesar was stabbed ∃x stab(x, Caesar) => true Neo-Davidsonian Events Example credit: Asad Sayeed Brutus stabbed Caesar with a knife at the theater on the Ides of March ∃e stabs(e, Brutus, Caesar) ‣ Lets us describe events as having proper;es ‣ Uniﬁed representa;on of events and en;;es: ∃x driver(x) ∧ clever(x) ∧ loca;on(x, America) some clever driver in America ∧ with(e, knife) ∧ loca;on(e, theater) ∧ ;me(e, Ides of March) Real Text Barack Obama signed the Aﬀordable Care act on Tuesday. He gave a speech later that aCernoon on how the act would help the American people. Several prominent Republicans were quick to denounce the new law. which Tuesday? ‣ Need to impute missing informa;on, resolve coreference, etc. ‣ S;ll unclear how to represent some things precisely or how that informa;on could be leveraged (several prominent Republicans) who? ??? which aWernoon? ∃e sign(e, Barack Obama) ∧ pa;ent(e, ACA) ∧ ;me(e, Tuesday) Other Challenges Bob and Alice were friends unGl he moved away to aHend college ‣ How to represent temporal informa;on? ‣ Represen;ng truly open-domain informa;on is very complicated! We don’t have a formal representa;on that can capture everything ∃e1∃e2 friends(e1, Bob, Alice) ∧ moved(e2, Bob) ∧ end_of(e1, e2) Bob and Alice were friends unGl around the +me he moved away to aHend college (At least) Three Solu;ons ‣ En;ty-rela;on-en;ty triples: focus on en;;es and their rela;ons (note that prominent events can s;ll be en;;es) (Lady Gaga, singerOf, Bad Romance) ‣ CraWed annota;ons to capture some subset of phenomena: predicate- argument structures (seman;c role labeling), ;me (temporal rela;ons), … ‣ Slot ﬁlling: speciﬁc ontology, populate informa;on in a predeﬁned way (Earthquake: magnitude=8.0, epicenter=central Italy, …) Open IE ‣ En;ty-rela;on-en;ty triples aren’t necessarily grounded in an ontology Barack Obama signed the Aﬀordable Care act on Tuesday. He gave a speech later that aCernoon on how the act would help the American people. Several prominent Republicans were quick to denounce the new law. (Barack Obama, signed, the Aﬀordable Care act) (Several prominent Republicans, denounce, the new law) ‣ Extract strings and let a downstream system ﬁgure it out IE: The Big Picture ‣ How do we represent informa;on? What do we extract? ‣ Slot ﬁllers ‣ En;ty-rela;on-en;ty triples (ﬁxed ontology or open) ‣ Seman;c roles ‣ Abstract meaning representa;on Seman;c Role Labeling/ Abstract Meaning Representa;on Seman;c Role Labeling ‣ Iden;fy predicate, disambiguate it, iden;fy that predicate’s arguments ‣ Verb roles from Propbank (Palmer et al., 2005) Figure from He et al. (2017) quicken: Seman;c Role Labeling Figure from He et al. (2017) ‣ Iden;fy predicates (love) using a classiﬁer (not shown) ‣ Iden;fy ARG0, ARG1, etc. as a tagging task with a BiLSTM condi;oned on love ‣ Other systems incorporate syntax, joint predicate- argument ﬁnding SRL for QA Shen and Lapata (2007) ‣ Ques;on and several answer candidates Q: Who discovered prions? AC1: In 1997, Stanley B. Prusiner, a scienGst in the United States, discovered prions… AC2: Prions were researched by… Score by matching expected answer phrase (EAP) against answer candidate (AC) Abstract Meaning Representa;on ‣ Graph-structured annota;on The boy wants to go Banarescu et al. (2014) ‣ Superset of SRL: full sentence analyses, contains coreference and mul;- word expressions as well ‣ F1 scores in the 60s: hard! ‣ So comprehensive that it’s hard to predict, but s;ll doesn’t handle tense or some other things… Summariza;on with AMR Liu et al. (2015) ‣ Merge AMRs across mul;ple sentences ‣ Summariza;on = subgraph extrac;on ‣ No real systems actually work this way (more when we talk about summariza;on) Slot Filling Slot Filling ‣ Most conserva;ve, narrow form of IE Indian Express — A massive earthquake of magnitude 7.3 struck Iraq on Sunday, 103 kms (64 miles) southeast of the city of As-Sulaymaniyah, the US Geological Survey said, reports Reuters. US Geological Survey iniGally said the quake was of a magnitude 7.2, before revising it to 7.3. magnitude time epicenter Speaker: Alan Clark “Gender Roles in the Holy Roman Empire” Allagher Center Main Auditorium This talk will discuss… speaker title location Freitag and McCallum (2000) ‣ Old work: HMMs, later CRFs trained per role Slot Filling: MUC Haghighi and Klein (2010) ‣ Key aspect: need to combine informa;on across mul;ple men;ons of an en;ty using coreference Slot Filling: Forums ‣ Extract product occurrences in cybercrime forums, but not everything that looks like a product is a product Portnoﬀ et al. (2017), DurreB et al. (2017) Not a product in this context Rela;on Extrac;on Rela;on Extrac;on ‣ Extract en;ty-rela;on-en;ty triples from a ﬁxed inventory ACE (2003-2005) During the war in Iraq, American journalists were someGmes caught in the line of ﬁre Located_In ‣ Systems can be feature-based or neural, look at surface words, syntac;c features (dependency paths), seman;c roles Na;onality ‣ Problem: limited data for scaling to big ontologies ‣ Use NER-like system to iden;fy en;ty spans, classify rela;ons between en;ty pairs with a classiﬁer Hearst PaBerns Hearst (1992) X such as [list] ciGes such as Berlin, Paris, and London. ‣ Syntac;c paBerns especially for ﬁnding hypernym-hyponym pairs (“is a” rela;ons) Berlin is a city other ciGes including Berlin Y is a X other X including Y ‣ Totally unsupervised way of harves;ng world knowledge for tasks like parsing and coreference (Bansal and Klein, 2011-2012) Distant Supervision Mintz et al. (2009) [Steven Spielberg]’s ﬁlm [Saving Private Ryan] is loosely based on the brothers’ story ‣ If two en;;es in a rela;on appear in the same sentence, assume the sentence expresses the rela;on ‣ Lots of rela;ons in our knowledge base already (e.g., 23,000 ﬁlm- director rela;ons); use these to bootstrap more training data Allison co-produced the Academy Award-winning [Saving Private Ryan], directed by [Steven Spielberg] Director Director Distant Supervision Mintz et al. (2009) ‣ Learn decently accurate classiﬁers for ~100 Freebase rela;ons ‣ Could be used to crawl the web and expand our knowledge base Open IE Open Informa;on Extrac;on ‣ Typically no ﬁxed rela;on inventory ‣ “Open”ness — want to be able to extract all kinds of informa;on from open-domain text ‣ Acquire commonsense knowledge just from “reading” about it, but need to process lots of text (“machine reading”) TextRunner ‣ Extract posi;ve examples of (e, r, e) triples via parsing and heuris;cs Barack Obama, 44th president of the United States, was born on August 4, 1961 in Honolulu => Barack_Obama, was born in, Honolulu ‣ Train a Naive Bayes classiﬁer to ﬁlter triples from raw text: uses features on POS tags, lexical features, stopwords, etc. ‣ 80x faster than running a parser (which was slow in 2007…) Banko et al. (2007) ‣ Use mul;ple instances of extrac;ons to assign probability to a rela;on Exploi;ng Redundancy Banko et al. (2007) ‣ Concrete: deﬁnitely true Abstract: possibly true but underspeciﬁed ‣ Hard to evaluate: can assess precision of extracted facts, but how do we know recall? ‣ 9M web pages / 133M sentences ‣ 2.2 tuples extracted per sentence, ﬁlter based on probabili;es ReVerb Fader et al. (2011) ‣ More constraints: open rela;ons have to begin with verb, end with preposi;on, be con;guous (e.g., was born on) ‣ Extract more meaningful rela;ons, par;cularly with light verbs ReVerb Fader et al. (2011) ‣ For each verb, iden;fy the longest sequence of words following the verb that sa;sfy a POS regex (V .* P) and which sa;sfy heuris;c lexical constraints on speciﬁcity ‣ Find the nearest arguments on either side of the rela;on ‣ Annotators labeled rela;ons in 500 documents to assess recall QA from Open IE Choi et al. (2015) Takeaways ‣ Rela;on extrac;on: can collect data with distant supervision, use this to expand knowledge bases ‣ Slot ﬁlling: ;ed to a speciﬁc ontology, but gives ﬁne-grained informa;on ‣ Open IE: extracts lots of things, but hard to know how good or useful they are ‣ Can combine with standard ques;on answering ‣ Add new facts to knowledge bases ‣ SRL/AMR: handle a bunch of phenomena, but more or less like syntax++ in terms of what they represent ‣ Many, many applica;ons and techniques "
158,"CS388: Natural Language Processing Lecture 17: Machine Transla;on I Greg Durrett Some slides adapted from Dan Klein, UC Berkeley This Lecture ‣ MT and evalua;on ‣ Word alignment ‣ Language models ‣ Phrase-based decoders ‣ Syntax-based decoders (probably next ;me) MT Basics MT Basics Trump Pope family watch a hundred years a year in the White House balcony People’s Daily, August 30, 2017 MT Ideally ‣ I have a friend => ∃x friend(x,self) ‣ May need informa;on you didn’t think about in your representa;on ‣ Everyone has a friend => => Tous a un ami ‣ Can oYen get away without doing all disambigua;on — same ambigui;es may exist in both languages J’ai une amie ∃x∀y friend(x,y) ∀x∃y friend(x,y) ‣ Hard for seman;c representa;ons to cover everything => J’ai un ami Levels of Transfer: Vauquois Triangle Slide credit: Dan Klein ‣ Today: mostly phrase-based, some syntax Phrase-Based MT ‣ Key idea: transla;on works be^er the bigger chunks you use ‣ Remember phrases from training data, translate piece-by-piece and s;tch those pieces together to translate ‣ How to iden;fy phrases? Word alignment over source-target bitext ‣ How to s;tch together? Language model over target language ‣ Decoder takes phrases and a language model and searches over possible transla;ons ‣ NOT like standard discrimina;ve models (take a bunch of transla;on pairs, learn a ton of parameters in an end-to-end way) Phrase-Based MT Unlabeled English data cat ||| chat ||| 0.9 the cat ||| le chat ||| 0.8 dog ||| chien ||| 0.8 house ||| maison ||| 0.6 my house ||| ma maison ||| 0.9 language ||| langue ||| 0.9 … Language model P(e) Phrase table P(f|e) P(e|f) / P(f|e)P(e) Noisy channel model: combine scores from translation model + language model to translate foreign to English “Translate faithfully but make fluent English” } Evalua;ng MT ‣ Fluency: does it sound good in the target language? ‣ Fidelity/adequacy: does it capture the meaning of the original? ‣ BLEU score: geometric mean of 1-, 2-, 3-, and 4-gram precision vs. a reference, mul;plied by brevity penalty ‣ Typically n = 4, wi = 1/4 ‣ r = length of reference c = length of predic;on ‣ Does this capture ﬂuency and adequacy? BLEU Score ‣ Be^er methods with human-in-the-loop ‣ HTER: human-assisted transla;on error rate ‣ If you’re building real MT systems, you do user studies. In academia, you mostly use BLEU Word Alignment Word Alignment § Input: a bitext: pairs of translated sentences § Output: alignments: pairs of translated words § Not always one-to-one! nous acceptons votre opinion . we accept your view . ‣ Input: a bitext, pairs of translated sentences nous acceptons votre opinion . ||| we accept your view nous allons changer d’avis ||| we are going to change our minds ‣ Output: alignments between words in each sentence ‣ We will see how to turn these into phrases “accept and acceptons are aligned” 1-to-Many Alignments Word Alignment ‣ Models P(f|e): probability of “French” sentence being generated from “English” sentence according to a model ‣ Correct alignments should lead to higher-likelihood genera;ons, so by op;mizing this objec;ve we will learn correct alignments ‣ Latent variable model: P(f|e) = X a P(f, a|e) = X a P(f|a, e)P(a) IBM Model 1 Brown et al. (1993) Thank you , I shall do so gladly . e ‣ Each French word is aligned to at most one English word 0 2 6 Gracias , lo hare de muy buen grado . f 5 7 7 7 7 8 a ‣ Set P(a) uniformly (no prior over good alignments) ‣ : word transla;on probability table P(fi|eai) P(f, a|e) = n Y i=1 P(fi|eai)P(ai) HMM for Alignment Brown et al. (1993) Thank you , I shall do so gladly . e ‣ Sequen;al dependence between a’s to capture monotonicity 0 2 6 Gracias , lo hare de muy buen grado . f 5 7 7 7 7 8 a ‣ Alignment dist parameterized by jump size: ‣ : same as before P(fi|eai) § Want local monotonicity: most jumps are small § HMM model (Vogel 96) § Re-es>mate using the forward-backward algorithm -2 -1 0 1 2 3 P(f, a|e) = n Y i=1 P(fi|eai)P(ai|ai−1) HMM Model à ‣ Which direc;on is this? ‣ Some mistakes, especially when you have rare words (garbage collec0on) ‣ Alignments are generally monotonic (along diagonal) Evalua;ng Word Alignment Model AER Model 1 INT 19.5 HMM E→F 11.4 HMM F→E 10.8 HMM AND 7.1 HMM INT 4.7 GIZA M4 AND 6.9 ‣ Run Model 1 in both direc;ons and intersect “intelligently” ‣ Run HMM model in both direc;ons and intersect “intelligently” ‣ “Alignment error rate”: use labeled alignments on small corpus Phrase Extrac;on ‣ Find con;guous sets of aligned words in the two languages that don’t have alignments to other words d’assister à la reunion et ||| to a^end the mee;ng and à ‣ Lots of phrases possible, count across all sentences and score by frequency assister à la reunion ||| a^end the mee;ng la reunion and ||| the mee;ng and nous ||| we … Language Modeling Phrase-Based MT Unlabeled English data cat ||| chat ||| 0.9 the cat ||| le chat ||| 0.8 dog ||| chien ||| 0.8 house ||| maison ||| 0.6 my house ||| ma maison ||| 0.9 language ||| langue ||| 0.9 … Language model P(e) Phrase table P(f|e) P(e|f) / P(f|e)P(e) Noisy channel model: combine scores from translation model + language model to translate foreign to English “Translate faithfully but make fluent English” } N-gram Language Models ‣ Simple genera;ve model: distribu;on of next word is a mul;nomial distribu;on condi;oned on previous n-1 words Maximum likelihood es;mate of this probability from a corpus I visited San _____ put a distribu;on over the next word P(x|visited San) = count(visited San, x) count(visited San) ‣ Just relies on counts, even in 2008 could scale up to 1.3M word types, 4B n-grams (all 5-grams occurring >40 ;mes on the Web) Smoothing N-gram Language Models ‣ Smoothing is very important, par;cularly when using 4+ gram models ‣ One technique is “absolute discoun;ng:” subtract oﬀ constant k from numerator, set lambda to make this normalize (k=1 is like leave-one-out) I visited San _____ P(x|visited San) = count(visited San, x) −k count(visited San) + λcount(San, x) count(San) P(x|visited San) = (1 −λ)count(visited San, x) count(visited San) + λcount(San, x) count(San) put a distribu;on over the next word! smooth this too! ‣ Kneser-Ney smoothing: this trick, plus low-order distribu;ons modiﬁed to capture fer;li;es (how many dis;nct words appear in a context) Engineering N-gram Models Pauls and Klein (2011), Heaﬁeld (2011) ‣ For 5+-gram models, need to store between 100M and 10B context- word-count triples ‣ Make it ﬁt in memory by delta encoding scheme: store deltas instead of values and use variable-length encoding Neural Language Models Mnih and Hinton (2003) ‣ Early work: feedforward neural networks looking at context I visited New _____ FFNN P(wi|wi−n, . . . , wi−1) ‣ Variable length context with RNNs: I visited New ‣ Works like a decoder with no encoder P(wi|w1, . . . , wi−1) ‣ Slow to train over lots of data! Evalua;on ‣ Perplexity: 2−1 n Pn i=1 log2 p(xi|x1,...,xi−1) ‣ (One sentence) nega;ve log likelihood: n X i=1 log p(xi|x1, . . . , xi−1) ‣ NLL (base 2) averaged over the sentence, exponen;ated ‣ NLL = -2 -> on average, correct thing has prob 1/4 -> PPL = 4. PPL is sort of like branching factor Results ‣ Kneser-Ney 5-gram model with cache: PPL = 125.7 Merity et al. (2017), Melis et al. (2017) ‣ LSTM: PPL ~ 60-80 (depending on how much you op;mize it) ‣ Evaluate on Penn Treebank: small dataset (1M words) compared to what’s used in MT, but common benchmark ‣ Melis et al.: many neural LM improvements from 2014-2017 are subsumed by just using the right regulariza;on (right dropout sewngs). So LSTMs are pre^y good Decoding Phrase-Based Decoding ‣ Phrase table: set of phrase pairs (e, f) with probabili;es P(f|e) ‣ Inputs: ‣ What we want to ﬁnd: e produced by a series of phrase-by-phrase transla;ons from an input f, possibly with reordering: ‣ Language model that scores P(ei|e1, . . . , ei−1) ⇡P(ei|ei−n−1, . . . , ei−1) Phrase lawces are big!  7        . Slide credit: Dan Klein Phrase-Based Decoding ‣ Input ‣ Transla;ons ‣ Decoding objec;ve (for 3-gram LM) Slide credit: Dan Klein Monotonic Transla;on ‣ If we translate with beam search, what state do we need to keep in the beam? ‣ What have we translated so far? ‣ What words have we produced so far? ‣ When using a 3-gram LM, only need to remember the last 2 words! Monotonic Transla;on …did not idx = 2 Mary not Mary no 4.2 -1.2 -2.9 idx = 2 idx = 2 score = log [P(Mary) P(not | Mary) P(Mary | Maria) P(not | no)] { { LM TM In reality: score = α log P(LM) + β log P(TM) …and TM is broken down into several features Monotonic Transla;on …not slap idx = 5 …a slap …no slap 8.7 -2.4 -1.1 idx = 5 idx = 5 ‣ Several paths can get us to this state, max over them (like Viterbi) …not give idx = 3 …give a idx = 4 una bofetada ||| a slap bofetada ||| slap ‣ Variable-length transla;on pieces = semi-HMM Non-Monotonic Transla;on ‣ Non-monotonic transla;on: can visit source sentence “out of order” ‣ State needs to describe which words have been translated and which haven’t translated: Maria, dio, una, bofetada ‣ Big enough phrases already capture lots of reorderings, so this isn’t as important as you think Training Decoders ‣ MERT (Och 2003): decode to get 1000- best transla;ons for each sentence in a small training set (<1000 sentences), do line search on parameters to directly op;mize for BLEU score = α log P(LM) + β log P(TM) …and TM is broken down into several features ‣ Usually 5-20 feature weights to set, want to op;mize for BLEU score which is not diﬀeren;able Moses ‣ Pharaoh (Koehn, 2004) is the decoder from Koehn’s thesis ‣ Toolkit for machine transla;on due to Philipp Koehn + Hieu Hoang ‣ Moses implements word alignment, language models, and this decoder, plus *a ton* more stuﬀ ‣ Highly op;mized and heavily engineered, could more or less build SOTA transla;on systems with this from 2007-2013 ‣ Next ;me: results on these and comparisons to neural methods Syntax Syntac;c MT ‣ Rather than use phrases, use a synchronous context-free grammar NP → [DT1 JJ2 NN3; DT1 NN3 JJ2] DT → [the, la] NN → [car, voiture] JJ → [yellow, jaune] the yellow car ‣ Assumes parallel syntax up to reordering DT → [the, le] la voiture jaune NP NP DT1 NN3 JJ2 DT1 NN3 JJ2 ‣ Transla;on = parse the input with “half” of the grammar, read oﬀ the other half Syntac;c MT Slide credit: Dan Klein ‣ Use lexicalized rules, look like “syntac;c phrases” ‣ Leads to HUGE grammars, parsing is slow Takeaways ‣ Phrase-based systems consist of 3 pieces: aligner, language model, decoder ‣ HMMs work well for alignment ‣ N-gram language models are scalable and historically worked well ‣ Decoder requires searching through a complex state space ‣ Lots of system variants incorpora;ng syntax ‣ Next ;me: neural MT "
159,"CS388: Natural Language Processing Lecture 18: Machine Transla:on II Greg Durrett Administrivia ‣ Final project proposals due November 8. Formal assignment posted Thursday ‣ Project 2 due this Friday Recall: Phrase-Based MT Unlabeled English data cat ||| chat ||| 0.9 the cat ||| le chat ||| 0.8 dog ||| chien ||| 0.8 house ||| maison ||| 0.6 my house ||| ma maison ||| 0.9 language ||| langue ||| 0.9 … Language model P(e) Phrase table P(f|e) P(e|f) / P(f|e)P(e) Noisy channel model: combine scores from translation model + language model to translate foreign to English “Translate faithfully but make fluent English” } Recall: HMM for Alignment Brown et al. (1993) Thank you , I shall do so gladly . e ‣ Sequen:al dependence between a’s to capture monotonicity 0 2 6 Gracias , lo hare de muy buen grado . f 5 7 7 7 7 8 a ‣ Alignment dist parameterized by jump size: ‣ : word transla:on table P(fi|eai) § Want local monotonicity: most jumps are small § HMM model (Vogel 96) § Re-es>mate using the forward-backward algorithm -2 -1 0 1 2 3 P(f, a|e) = n Y i=1 P(fi|eai)P(ai|ai−1) Recall: Decoding …did not idx = 2 Mary not Mary no 4.2 -1.2 -2.9 idx = 2 idx = 2 score = log [P(Mary) P(not | Mary) P(Mary | Maria) P(not | no)] { { LM TM In reality: score = α log P(LM) + β log P(TM) …and TM is broken down into several features This Lecture ‣ Neural MT details ‣ Dilated CNNs for MT ‣ Transformers for MT ‣ Syntac:c MT Syntac:c MT Levels of Transfer: Vauquois Triangle Slide credit: Dan Klein ‣ Is syntax a “beher” abstrac:on than phrases? Syntac:c MT ‣ Rather than use phrases, use a synchronous context-free grammar: constructs “parallel” trees in two languages simultaneously NP → [DT1 JJ2 NN3; DT1 NN3 JJ2] DT → [the, la] NN → [car, voiture] JJ → [yellow, jaune] the yellow car ‣ Assumes parallel syntax up to reordering DT → [the, le] la voiture jaune NP NP DT1 NN3 JJ2 DT1 NN3 JJ2 ‣ Transla:on = parse the input with “half” the grammar, read oﬀ other half Syntac:c MT Slide credit: Dan Klein ‣ Relax this by using lexicalized rules, like “syntac:c phrases” ‣ Leads to HUGE grammars, parsing is slow Neural MT Details Encoder-Decoder MT Sutskever et al. (2014) ‣ SOTA = 37.0 — not all that compe::ve… ‣ Sutskever seq2seq paper: ﬁrst major applica:on of LSTMs to NLP ‣ Basic encoder-decoder with beam search Encoder-Decoder MT ‣ Beher model from seq2seq lectures: encoder-decoder with ahen:on and copying for rare words the movie was great h1 h2 h3 h4 <s> ¯ h1 c1 distribu:on over vocab + copying … le Results: WMT English-French Classic phrase-based system: ~33 BLEU, uses addi:onal target-language data Rerank with LSTMs: 36.5 BLEU (long line of work here; Devlin+ 2014) Sutskever+ (2014) seq2seq single: 30.6 BLEU Sutskever+ (2014) seq2seq ensemble: 34.8 BLEU ‣ But English-French is a really easy language pair and there’s tons of data for it! Does this approach work for anything harder? Luong+ (2015) seq2seq ensemble with ahen:on and rare word handling: 37.5 BLEU ‣ 12M sentence pairs Results: WMT English-German ‣ Not nearly as good in absolute BLEU, but not really comparable across languages Classic phrase-based system: 20.7 BLEU Luong+ (2014) seq2seq: 14 BLEU ‣ French, Spanish = easiest German, Czech = harder Japanese, Russian = hard (gramma:cally diﬀerent, lots of morphology…) Luong+ (2015) seq2seq ensemble with rare word handling: 23.0 BLEU ‣ 4.5M sentence pairs MT Examples Luong et al. (2015) ‣ NMT systems can hallucinate words, especially when not using ahen:on — phrase-based doesn’t do this ‣ best = with ahen:on, base = no ahen:on MT Examples Luong et al. (2015) ‣ best = with ahen:on, base = no ahen:on MT Examples Zhang et al. (2017) ‣ NMT can repeat itself if it gets confused (pH or pH) ‣ Phrase-based MT oyen gets chunks right, may have more subtle ungramma:cali:es Rare Words: Word Piece Models ‣ Use Huﬀman encoding on a corpus, keep most common k (~10,000) character sequences for source and target ‣ Captures common words and parts of rare words Input: _the _eco tax _port i co _in _Po nt - de - Bu is … Output: _le _port ique _éco taxe _de _Pont - de - Bui s ‣ Subword structure may make it easier to translate ‣ Model balances transla:ng and translitera:ng without explicit switching Wu et al. (2016) Rare Words: Byte Pair Encoding ‣ Count bigram character cooccurrences Sennrich et al. (2016) ‣ Merge the most frequent pair of adjacent characters ‣ Input: a dic:onary of words represented as characters ‣ Final size = ini:al vocab + num merges. Oyen do 10k - 30k merges ‣ Simpler procedure, based only on the dic:onary ‣ Most SOTA NMT systems use this on both source + target Google’s NMT System Wu et al. (2016) ‣ 8-layer LSTM encoder-decoder with ahen:on, word piece vocabulary of 8k-32k Google’s NMT System Wu et al. (2016) Luong+ (2015) seq2seq ensemble with rare word handling: 37.5 BLEU Google’s 32k word pieces: 38.95 BLEU Google’s phrase-based system: 37.0 BLEU English-French: Luong+ (2015) seq2seq ensemble with rare word handling: 23.0 BLEU Google’s 32k word pieces: 24.2 BLEU Google’s phrase-based system: 20.7 BLEU English-German: Human Evalua:on (En-Es) Wu et al. (2016) ‣ Similar to human-level performance on English-Spanish Google’s NMT System Wu et al. (2016) Gender is correct in GNMT but not PBMT “sled” “walker” Backtransla:on ‣ Classical MT methods used a bilingual corpus of sentences B = (S, T) and a large monolingual corpus T’ to train a language model. Can neural MT do the same? Sennrich et al. (2015) s1, t1 [null], t’1 [null], t’2 s2, t2 … … ‣ Approach 1: force the system to generate T’ as targets from null inputs ‣ Approach 2: generate synthe:c sources with a T->S machine transla:on system (backtransla:on) s1, t1 MT(t’1), t’1 s2, t2 … … MT(t’2), t’2 Backtransla:on Sennrich et al. (2015) ‣ parallelsynth: backtranslate training data; makes addi:onal noisy source sentences which could be useful ‣ Gigaword: large monolingual English corpus Dilated CNNs for MT Dilated Convolu:ons ‣ Standard convolu:on: looks at every token under the ﬁlter ‣ Dilated convolu:on with gap d: looks at every dth token w = 2, d =2: gap in the ﬁlter ‣ Can chain successive dilated convolu:ons together to get a wide recep:ve ﬁeld (see a lot of the sentence) Strubell et al. (2017) w=3, d=1 w=3, d=2 w=3, d=4 ‣ Top nodes see lots of the sentence, but with diﬀerent processing CNNs for Machine Transla:on Kalchbrenner et al. (2016) ‣ “ByteNet”: operates over characters (bytes) ‣ Encode source sequence w/dilated convolu:ons ‣ Predict nth target character by looking at the nth posi:on in the source and a dilated convolu:on over the n-1 target tokens so far ‣ To deal with divergent lengths, tn actually looks at snα where α is a heuris:cally-chosen parameter ‣ Assumes mostly monotonic transla:on Compare: CNNs vs. LSTMs Kalchbrenner et al. (2016) <s> ¯ h1 c1 ‣ LSTM: looks at previous word + hidden state, ahen:on over input ‣ CNN: source encoding at this posi:on gives us “ahen:on”, target encoding gives us decoder context Ahen:on from CNN Kalchbrenner et al. (2016) ‣ Model is character-level, this visualiza:on shows which words’s characters impact the convolu:onal encoding the most ‣ Largely monotonic but does consult other informa:on Advantages of CNNs Kalchbrenner et al. (2016) ‣ LSTM with ahen:on is quadra:c: compute ahen:on over the whole input for each decoded token ‣ CNN is linear! ‣ CNN is shallower too in principle but the conv layers are very sophis:cated (3 layers each) English-German MT Results Kalchbrenner et al. (2016) Transformers for MT Self-Ahen:on Vaswani et al. (2017) the movie was great ‣ Each word forms a “query” which then computes ahen:on over each word ‣ Mul:ple “heads” analogous to diﬀerent convolu:onal ﬁlters. Use parameters Wk and Vk to get diﬀerent ahen:on values + transform vectors x4 x0 4 scalar vector = sum of scalar * vector ↵i,j = softmax(x> i xj) x0 i = n X j=1 ↵i,jxj ↵k,i,j = softmax(x> i Wkxj) x0 k,i = n X j=1 ↵k,i,jVkxj Transformers Vaswani et al. (2017) the movie was great ‣ Posi:onal encoding: augment word embedding with posi:on embeddings, each dim is a sine wave of a diﬀerent frequency. Closer points = higher dot products Transformers Vaswani et al. (2017) ‣ Encoder and decoder are both transformers ‣ Decoder consumes the previous generated token (and ahends to input), but has no recurrent state Transformers Vaswani et al. (2017) ‣ Big = 6 layers, 1000 dim for each token, 16 heads, base = 6 layers + other params halved Visualiza:on Visualiza:on Takeaways ‣ Can build MT systems with LSTM encoder-decoders, CNNs, or transformers ‣ Word piece / byte pair models are really eﬀec:ve and easy to use ‣ State of the art systems are ge}ng prehy good, but lots of challenges remain, especially for low-resource se}ngs "
16,"School of Computer Science Probabilistic Graphical Models Approximate Inference: Monte Carlo methods Eric Xing Lecture 16, March 17, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 Approaches to inference Exact inference algorithms  The elimination algorithm  Message-passing algorithm (sum-product, belief propagation)  The junction tree algorithms Approximate inference techniques  Variational algorithms  Loopy belief propagation  Mean field approximation  Stochastic simulation / sampling methods  Markov chain Monte Carlo methods 2 © Eric Xing @ CMU, 2005-2014 How to represent a joint, or a marginal distribution? Closed-form representation  E.g., Sample-based representation: 3 © Eric Xing @ CMU, 2005-2014 Monte Carlo methods Draw random samples from the desired distribution Yield a stochastic representation of a complex distribution  marginals and other expections can be approximated using sample-based averages Asymptotically exact and easy to apply to arbitrary models Challenges:  how to draw samples from a given dist. (not all distributions can be trivially sampled)?  how to make better use of the samples (not all sample are useful, or eqally useful, see an example later)?  how to know we've sampled enough?    N t t x f N x f 1 1 ) ( )] ( [ ) ( E 4 © Eric Xing @ CMU, 2005-2014 Example: naive sampling Construct samples according to probabilities given in a BN. Alarm example: (Choose the right sampling sequence) 1) Sampling:P(B)=<0.001, 0.999> suppose it is false, B0. Same for E0. P(A|B0, E0)=<0.001, 0.999> suppose it is false... 2) Frequency counting: In the samples right, P(J|A0)=P(J,A0)/P(A0)=<1/9, 8/9>. E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E1 B0 A1 M1 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E1 B0 A1 M1 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 5 © Eric Xing @ CMU, 2005-2014 Example: naive sampling Construct samples according to probabilities given in a BN. Alarm example: (Choose the right sampling sequence) 3) what if we want to compute P(J|A1) ? we have only one sample ... P(J|A1)=P(J,A1)/P(A1)=<0, 1>. 4) what if we want to compute P(J|B1) ? No such sample available! P(J|A1)=P(J,B1)/P(B1) can not be defined. For a model with hundreds or more variables, rare events will be very hard to garner evough samples even after a long time or sampling ... E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E1 B0 A1 M1 J1 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 E0 B0 A0 M0 J0 6 © Eric Xing @ CMU, 2005-2014 Monte Carlo methods (cond.) Direct Sampling  We have seen it.  Very difficult to populate a high-dimensional state space Rejection Sampling  Create samples like direct sampling, only count samples which is consistent with given evidences. Likelihood weighting, ...  Sample variables and calculate evidence weight. Only create the samples which support the evidences. Markov chain Monte Carlo (MCMC)  Metropolis-Hasting  Gibbs 7 © Eric Xing @ CMU, 2005-2014 Rejection sampling Suppose we wish to sample from dist. (X)='(X)/Z.  (X) is difficult to sample, but '(X) is easy to evaluate  Sample from a simpler dist Q(X)  Rejection sampling  Correctness:  Pitfall … ) ( / ) ( ' w.p. accept ), ( ~ * * * * x kQ x x X Q x      ) ( ) ( ' ) ( ' ) ( ) ( / ) ( ' ) ( ) ( / ) ( ' ) ( x dx x x dx x Q x kQ x x Q x kQ x x p           8 © Eric Xing @ CMU, 2005-2014 Rejection sampling  Pitfall:  Using Q=N(,q 2/d) to sample P=N(,p 2/d)  If q exceeds p by 1%, and dimensional=1000,  The optimal acceptance rate k=(q/p)d1/20,000  Big waste of samples! Adaptive rejection sampling  Using envelope functions to define Q 9 © Eric Xing @ CMU, 2005-2014 Unnormalized importance sampling Suppose sampling from P(·) is hard. Suppose we can sample from a ""simpler"" proposal distribution Q(·) instead. If Q dominates P (i.e., Q(x) > 0 whenever P(x) > 0), we can sample from Q and reweight: What is the problem here?         m m m m m m m m w x M X Q x x Q x P x M dx x Q x Q x P x dx x P x X ) ( ) ( ~ where ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( f f f f f 1 1 10 © Eric Xing @ CMU, 2005-2014 Normalized importance sampling Suppose we can only evaluate P'(x) = P(x) (e.g. for an MRF). We can get around the nasty normalization constant as follows:  Now        dx x P dx x Q x Q x P X r Q ) ( ' ) ( ) ( ) ( ' ) ( ) ( ) ( ' ) ( Let x Q x P X r                m m m m m m m m m m m m m P r r w w x X Q x r r x dx x Q x r dx x Q x r x dx x Q x Q x P x dx x P x X re whe ) ( ) ( ~ where ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ' ) ( ) ( ) ( ) ( f f f f f f  1 11 © Eric Xing @ CMU, 2005-2014 Normalized vs unnormalized importance sampling  Unormalized importance sampling is unbiased:  Normalized importance sampling is biased, e.g., for M = 1:  However, the variance of the normalized importance sampler is usually lower in practice.  Also, it is common that we can evaluate P'(x) but not P(x), e.g. P(x|e) = P'(x, e)/P(e) for Bayes net, or P(x) = P'(x)/Z for MRF.   ) ( ) ( X w X f EQ        ) ( ) ( ) ( 1 1 1 x r x r x f EQ 12 © Eric Xing @ CMU, 2005-2014 Likelihood weighting  We now apply normalized importance sampling to a Bayes net.  The proposal Q is gotten from the mutilated BN where we clamp evidence nodes, and cut their incoming arcs. Call this PM.  The unnormalized posterior is P'(x) = P(x, e).  So for f(Xi) = (Xi = xi), we get where .      m m m i m i m i i w x x w e x X P ) ( ) | ( ˆ  ) ( / ) , ( m M m m x P e x P w   13 © Eric Xing @ CMU, 2005-2014 Likelihood weighting algorithm 14 © Eric Xing @ CMU, 2005-2014 Efficiency of likelihood weighting The efficiency of importance sampling depends on how close the proposal Q is to the target P. Suppose all the evidence is at the roots. Then Q = P(X|e), and all samples have weight 1. Suppose all the evidence is at the leaves. Then Q is the prior, so many samples might get small weight if the evidence is unlikely. We can use arc reversal to make some of the evidence nodes be roots instead of leaves, but the resulting network can be much more densely connected. 15 © Eric Xing @ CMU, 2005-2014 Weighted resampling Problem of importance sampling: depends on how well Q matches P  If P(x)f(x) is strongly varying and has a significant proportion of its mass concentrated in a small region, rm will be dominated by a few samples  Note that if the high-prob mass region of Q falls into the low-prob mass region of P, the variance of can be small even if the samples come from low-prob region of P and potentially erroneous . Solution  Use heavy tail Q.  Weighted resampling * * * * * ) ( / ) ( m m m x Q x P r      m m m l l l m m m r r x Q x P x Q x P w ) ( / ) ( ) ( / ) ( 16 © Eric Xing @ CMU, 2005-2014 Weighted resampling  Sampling importance resampling (SIR): 1. Draw N samples from Q: X1 … XN 2. Constructing weights: w1 … wN , 3. Sub-sample x from {X1 … XN} w.p. (w1 … wN)  Particular Filtering  A special weighted resampler  Yield samples from posterior p(Xt|Y1:t)  Also known as sequential Monte Carlo     m m m l l l m m m r r x Q x P x Q x P w ) ( / ) ( ) ( / ) ( A A A Yt Yt+1 Y1 Xt Xt+1 X1 ... 17 © Eric Xing @ CMU, 2005-2014 Sketch of Particle Filters The starting point  Thus p(Xt|Y1:t) is represented by A sequential weighted resampler  Time update  Measurement update t t t t t t t dX X p X X p X p     ) | ( ) | ( ) | ( : : 1 1 1 1 Y Y       t t t t t t t t t t t t t t dX X Y p X p X Y p X p Y X p X p ) | ( ) | ( ) | ( ) | ( ) , | ( ) ( : : : : 1 1 1 1 1 1 1 Y Y Y Y               M m m t t m t t X Y p X Y p m t t t m t w X p X 1 1 1 ) | ( ) | ( : ), | ( ~ Y            1 1 1 1 1 1 1 1 1 1 1 1 t t t t t t t t t t t dX X Y p X p X Y p X p X p ) | ( ) | ( ) | ( ) | ( ) ( : : : Y Y Y    m m t t m t X X p w ) | ( ) ( 1 (sample from a mixture model)                      M m m t t m t t X Y p X Y p m t t t m t w X p X 1 1 1 1 1 1 1 1 1 ) | ( ) | ( : ), | ( ~ Y (reweight) ) | ( t t X p Y 1  18 © Eric Xing @ CMU, 2005-2014 PF for switching SSM Recall that the belief state has O(2t) Gaussian modes 19 © Eric Xing @ CMU, 2005-2014 PF for switching SSM  Key idea: if you knew the discrete states, you can apply the right Kalman filter at each time step.  So for each old particle m, sample from the prior, apply the KF (using parameters for St m) to the old belief state to get an approximation to  Useful for online tracking, fault diagnosis, etc. ) | ( ~ m t t m t S S P S 1  ) , ˆ ( | | m t t m t t P x 1 1 1 1     ) , | ( : : m t t t s y X P 1 1 20 © Eric Xing @ CMU, 2005-2014 Rao-Blackwellised sampling  Sampling in high dimensional spaces causes high variance in the estimate.  RB idea: sample some variables Xp, and conditional on that, compute expected value of rest Xd analytically:  This has lower variance, because of the identity:       ) | ( ~ , ) , ( ) , ( ) | ( ) , ( ) , | ( ) | ( ) , ( ) | , ( ) ( ) , | ( ) , | ( ) | ( e x p x X x f E dx X x f E e x p dx dx x x f e x x p e x p dx dx x x f e x x p X f E p m p m d m p e x X p M x p d p e x X p p x p x d d p p d p d p d p d p e X p m p d p p d p d                  1           p d p p d p d p X X X E X X X E X X | ) , ( var | ) , ( var ) , ( var      21 © Eric Xing @ CMU, 2005-2014 22 © Eric Xing @ CMU, 2005-2014 Rao-Blackwellised sampling  Sampling in high dimensional spaces causes high variance in the estimate.  RB idea: sample some variables Xp, and conditional on that, compute expected value of rest Xd analytically:  This has lower variance, because of the identity:  Hence , so is a lower variance estimator.       ) | ( ~ , ) , ( ) , ( ) | ( ) , ( ) , | ( ) | ( ) , ( ) | , ( ) ( ) , | ( ) , | ( ) | ( e x p x X x f E dx X x f E e x p dx dx x x f e x x p e x p dx dx x x f e x x p X f E p m p m d m p e x X p M x p d p e x X p p x p x d d p p d p d p d p d p e X p m p d p p d p d                  1           p d p p d p d p X X X E X X X E X X | ) , ( var | ) , ( var ) , ( var            ) , ( var | ) , ( var d p p d p X X X X X E      p d p d p X X X f E X X | ) , ( ) , (   23 © Eric Xing @ CMU, 2005-2014 Summary: Monte Carlo Methods Direct Sampling  Very difficult to populate a high-dimensional state space Rejection Sampling  Create samples like direct sampling, only count samples which is consistent with given evidences. Likelihood weighting, ...  Sample variables and calculate evidence weight. Only create the samples which support the evidences. Markov chain Monte Carlo (MCMC)  Metropolis-Hasting  Gibbs 24 © Eric Xing @ CMU, 2005-2014 "
160,"CS388: Natural Language Processing Lecture 19: Reading Comprehension Greg Durrett Administrivia ‣ Project 2 due Friday at 5pm ‣ Project proposals due next Thursday ‣ Spec posted on course website — I’ll pitch some ideas/interesNng papers from EMNLP on Tuesday Recall: CNNs for Machine TranslaNon Kalchbrenner et al. (2016) ‣ “ByteNet”: operates over characters (bytes) ‣ Encode source sequence w/dilated convoluNons ‣ Predict nth target character by looking at the nth posiNon in the source and a dilated convoluNon over the n-1 target tokens so far ‣ To deal with divergent lengths, tn actually looks at snα where α is a heurisNcally-chosen parameter ‣ Assumes mostly monotonic translaNon Recall: Transformers Vaswani et al. (2017) ‣ Encoder and decoder are both transformers ‣ Decoder consumes the previous generated token (and abends to input), but has no recurrent state This Lecture ‣ Types of quesNon answering/reading comprehension ‣ Memory networks ‣ CNN/Daily Mail task: AbenNve Reader ‣ SQuAD task: BidirecNonal AbenNon Flow Reading Comprehension Classical QuesNon Answering Q: “where was Barack Obama born” ‣ Form semanNc representaNon from semanNc parsing, execute against structured knowledge base λx. type(x, Location) ∧ born_in(Barack_Obama, x) (other representaNons like SQL possible too…) ‣ How to deal with open-domain data/relaNons? Need data to learn how to ground every predicate or need to be able to produce predicates in a zero-shot way QA from Open IE Choi et al. (2015) ‣ Why use the KB at all? Why not answer quesNons directly from text? Like informaNon retrieval! What can’t KB QA systems do? ‣ What were the main causes of World War II? — requires summarizaNon ‣ Can you get the ﬂu from a ﬂu shot? — want IR to provide an explanaNon of the answer ‣ What temperature should I cook chicken to? — could be wriben down in a KB but probably isn’t ‣ Today: can we do QA when it requires retrieving the answer from a passage? Reading Comprehension ‣ “AI challenge problem”: answer quesNon given context Richardson (2013) ‣ MCTest (2013): 500 passages, 4 quesNons per passage ‣ Two quesNons per passage explicitly require cross-sentence reasoning ‣ Recognizing Textual Entailment (2006) Baselines ‣ N-gram matching: append quesNon + each answer, return answer which gives highest n-gram overlap with a sentence ‣ Parsing: ﬁnd direct object of “pulled” in the document where the subject is James ‣ Don’t need any complex semanNc representaNons Richardson (2013) Reading Comprehension Richardson (2013) ‣ Classic textual entailment systems don’t work as well as n-grams ‣ Scores are low parNally due to quesNons spanning mulNple sentences ‣ Unfortunately not much data to train beber methods on (2000 quesNons) ngram sliding window MCTest State of the Art Sachan and Xing (2016) ‣ Match an AMR (abstract meaning representaNon) of the quesNon against the original text ‣ 70% accuracy (roughly 10% beber than baseline) Dataset Explosion ‣ 10+ QA datasets released since 2015 ‣ QuesNon answering: quesNons are in natural language ‣ “Cloze” task: word (osen an enNty) is removed from a sentence ‣ Answers: mulNple choice or require picking from the passage ‣ Require human annotaNon ‣ Answers: mulNple choice, pick from passage, or pick from vocabulary ‣ Can be created automaNcally from things that aren’t quesNons ‣ Children’s Book Test, CNN/Daily Mail, SQuAD, TriviaQA are most well- known (others: SearchQA, MS Marco, RACE, WikiHop, …) Dataset ProperNes ‣ Osen shallow methods work well because most answers are in a single sentence (SQuAD, MCTest) ‣ Some explicitly require linking between mulNple sentences (MCTest) ‣ Axis 1: QA vs. cloze ‣ Axis 2: single-sentence vs. passage ‣ Axis 3: single-document (datasets in this lecture) vs. mulN-document (TriviaQA, WikiHop, HotPotQA, …) Children’s Book Test Hill et al. (2015) ‣ Children’s Book Test: take a secNon of a children’s story, block out an enNty and predict it (one-doc mulN-sentence cloze task) ???? LSTM Language Models Hill et al. (2015) ???? ‣ Predict next word with LSTM LM She thought that … ???? thought that ‣ Context: either just the current sentence (query) or the whole document up to this point (query+context) Children’s Book Test: Results Hill et al. (2015) ‣ Present 10 opNons drawn from the text (correct + 9 distractors), ask the model to pick among them ‣ Neural LMs aren’t beber than n-gram LMs Children’s Book Test: Results Hill et al. (2015) ‣ Present 10 opNons drawn from the text (correct + 9 distractors), ask the model to pick among them ‣ Why are these results so low? Memory Networks Memory Networks ‣ Memory networks let you reference input with abenNon ‣ Encode input items into two vectors: a key and a value Memory layer q o k1 v1 k2 v2 k3 v3 ei = q · ki ↵= softmax(e) o = X i ↵ivi ‣ Keys compute abenNon weights given a query, weighted sum of values gives the output Sukhbaatar et al. (2015) Memory Networks ‣ Three layers of memory network where the query representaNon is updated addiNvely based on the memories at each step Sukhbaatar et al. (2015) ‣ How to encode the sentences? ‣ Bag of words (average embeddings) ‣ PosiNonal encoding: mulNply each word by a vector capturing posiNon in sentence bAbI ‣ EvaluaNon on 20 tasks proposed as building blocks for building “AI- complete” systems ‣ Various levels of diﬃculty, exhibit diﬀerent linguisNc phenomena Weston et al. (2014) ‣ Small vocabulary, language isn’t truly “natural” EvaluaNon: bAbI ‣ 3-hop memory network does preby well, beber than LSTM at processing these types of examples EvaluaNon: Children’s Book Test ‣ Outperforms LSTMs substanNally with the right supervision Memory Network Takeaways ‣ Useful for cloze tasks where far-back context is necessary ‣ What can we do with more basic abenNon? ‣ Memory networks provide a way of abending to abstracNons over the input CNN/Daily Mail: AbenNve Reader CNN/Daily Mail Hermann et al. (2015), Chen et al. (2016) ‣ Single-document, (usually) single- sentence cloze task ‣ Formed based on arNcle summaries — informaNon should mostly be present, makes it easier than Children’s Book Test ‣ Need to process the quesNon, can’t just use LSTM LMs CNN/Daily Mail Hermann et al. (2015), Chen et al. (2016) X visited England ||| Mary visited England ‣ LSTM reader: encode quesNon, encode passage, predict enNty ‣ Can also use textual entailment-like models X visited England Mary visited England Mary MulNclass classiﬁcaNon problem over enNNes in the document Mary CNN/Daily Mail Hermann et al. (2015) ‣ AbenNve reader: u = encode query s = encode sentence r = abenNon(u -> s) predicNon = f(candidate, u, r) ‣ Uses ﬁxed-size representaNons for the ﬁnal predicNon, mulNclass classiﬁcaNon CNN/Daily Mail ‣ Chen et al (2016): small changes to the abenNve reader Stanford AbenNve Reader 76.2 76.5 79.5 78.7 ‣ AddiNonal analysis of the task found that many of the remaining quesNons were unanswerable or extremely diﬃcult Hermann et al. (2015), Chen et al. (2016) SQuAD: BidirecNonal AbenNon Flow SQuAD ‣ Single-document, single-sentence quesNon-answering task where the answer is always a substring of the passage Rajpurkar et al. (2016) ‣ Predict start and end indices of the answer in the passage SQuAD What was Marie Curie the ﬁrst female recipient of? Rajpurkar et al. (2016) ﬁrst female recipient of the Nobel Prize . START END ‣ Like a tagging problem over the sentence (not mulNclass classiﬁcaNon), but we need some way of abending to the query BidirecNonal AbenNon Flow ‣ Passage (context) and query are both encoded with BiLSTMs ‣ Context-to-query abenNon: compute sosmax over columns of S, take weighted sum of u based on abenNon weights for each passage word Seo et al. (2016) passage H query U Sij = hi · uj ↵ij = softmaxj(Sij) ‣ dist over query ˜ ui = X j ↵ijuj ‣ query “specialized” to the ith word BidirecNonal AbenNon Flow Seo et al. (2016) Each passage word now “knows about” the query SQuAD SOTA ‣ nlnet, QANet, r-net — dueling super complex systems (much more than BiDAF…) ‣ BERT: transformer-based approach with pretraining on 3B tokens ‣ BiDAF: 73 EM / 81 F1 But how well are these doing? ‣ Can construct adversarial examples that fool these systems: add one carefully chosen sentence and performance drops to below 50% Jia and Liang (2017) ‣ SNll “surface-level” matching, not complex understanding ‣ Other challenges: recognizing when answers aren’t present, doing mulN-step reasoning Takeaways ‣ Memory networks let you reference input in an abenNon-like way, useful for generalizing language models to long-range reasoning ‣ Complex abenNon schemes can match queries against input texts and idenNfy answers ‣ Many ﬂavors of reading comprehension tasks: cloze or actual quesNons, single or mulN-sentence "
161,"CS388: Natural Language Processing Lecture 20: Summariza:on Greg Durrett Administrivia ‣ Proposals due Thursday Recall: Memory Networks ‣ Three layers of memory network where the query representa:on is updated addi:vely based on the memories at each step Sukhbaatar et al. (2015) ‣ How to encode the sentences? ‣ Bag of words (average embeddings) ‣ Posi:onal encoding: mul:ply each word by a vector capturing posi:on in sentence Recall: Bidirec:onal ASen:on Flow ‣ Passage (context) and query are both encoded with BiLSTMs ‣ Context-to-query aSen:on: compute soWmax over columns of S, take weighted sum of u based on aSen:on weights for each passage word Seo et al. (2016) passage H query U Sij = hi · uj ↵ij = softmaxj(Sij) ‣ dist over query ˜ ui = X j ↵ijuj ‣ query “specialized” to the ith word Final Projects EMNLP 2018: New Approaches ‣ Language modeling as pretraining is really eﬀec:ve ‣ Minimizing/dis:lling BERT/ELMo will be important, but this will require a lot of compute (almost certainly infeasible as a ﬁnal project) ‣ Transformers seem to be on the rise: linguis:cally-informed self aSen:on, BERT, etc. ‣ Understand transformer heads? BeSer induc:ve biases? ‣ Unsupervised MT: lots of problems here including lexicon induc:on, how to use syntax, etc. ‣ Varia:onal autoencoders: s:ll don’t work great but there’s op:mism about them EMNLP 2018: New Datasets ‣ “Conversa:onal” machine reading (need to ask clariﬁca:on ques:ons to the user): hSps://arxiv.org/pdf/1809.01494.pdf ‣ Commonsense: “Can a suit of armor conduct electricity?”: hSps://arxiv.org/pdf/1809.02789.pdf ‣ emrQA: hSps://arxiv.org/pdf/1809.00732.pdf ‣ Ques:ons as dialogue (user asks clariﬁca:on ques:ons to the system): hSps://arxiv.org/pdf/1808.07036.pdf ‣ Lots of new QA datasets, many will not prove that useful…some:mes hard to know in advance ‣ New QA seings: This Lecture ‣ Extrac:ve systems for mul:-document summariza:on ‣ Extrac:ve + compressive systems for single-document summariza:on ‣ Single-document summariza:on with neural networks Summariza:on ‣ What makes a good summary? Summariza:on BAGHDAD/ERBIL, Iraq (Reuters) - A strong earthquake hit large parts of northern Iraq and the capital Baghdad on Sunday, and also caused damage in villages across the border in Iran where state TV said at least six people had been killed. There were no immediate reports of casual:es in Iraq aWer the quake, whose epicenter was in Penjwin, in Sulaimaniyah province which is in the semi-autonomous Kurdistan region very close to the Iranian border, according to an Iraqi meteorology oﬃcial. But eight villages were damaged in Iran and at least six people were killed and many others injured in the border town of Qasr-e Shirin in Iran, Iranian state TV said. The US Geological Survey said the quake measured a magnitude of 7.3, while an Iraqi meteorology oﬃcial put its magnitude at 6.5 according to preliminary informa:on. Many residents in the Iraqi capital Baghdad rushed out of houses and tall buildings in panic. … Summariza:on Indian Express — A massive earthquake of magnitude 7.3 struck Iraq on Sunday, 103 kms (64 miles) southeast of the city of As-Sulaymaniyah, the US Geological Survey said, reports Reuters. US Geological Survey ini:ally said the quake was of a magnitude 7.2, before revising it to 7.3. The quake has been felt in several Iranian ci:es and eight villages have been damaged. Electricity has also been disrupted at many places, suggest few TV reports. A massive earthquake of magnitude 7.3 struck Iraq on Sunday. The epicenter was close to the Iranian border. Eight villages were damaged and six people were killed in Iran. Summary What makes a good summary? A strong earthquake of magnitude 7.3 struck Iraq and Iran on Sunday. The epicenter was close to the Iranian border. Eight villages were damaged and six people were killed in Iran. Summary ‣ Content selec:on: pick the right content ‣ Right content was repeated within and across documents ‣ Domain-speciﬁc (magnitude + epicenter of earthquakes are important) ‣ Genera:on: write the summary ‣ Extrac:on: pick whole sentences from the summary ‣ Compression: compress those sentences but basically just do dele:on ‣ Abstrac:on: rewrite + reexpress content freely Extrac:ve Summariza:on Extrac:ve Summariza:on: MMR ‣ Given some ar:cles and a length budget of k words, pick some sentences of total length <= k and make a summary ‣ Pick important yet diverse content: maximum marginal relevance (MMR) While summary is < k words Calculate Add highest MMR sentence that doesn’t overﬂow length “make this sentence similar to a query” “make this sentence maximally diﬀerent from all others added so far” “max over all sentences not yet in the summary” Carbonell and Goldstein (1998) Extrac:ve Summariza:on: Centroid ‣ Represent the documents and each sentences as bag-of-words with TF- IDF weigh:ng While summary is < k words Calculate score(sentence) = cosine(sent-vec, doc-vec) Radev et al. (2004) Discard all sentences whose similarity with some sentence already in the summary is too high Add the best remaining sentence that won’t overﬂow the summary Extrac:ve Summariza:on: Bigram Recall ‣ Count number of documents each bigram occurs in to measure importance Gillick and Favre (2009) score(massive earthquake) = 3 score(Iraqi capital) = 1 score(six killed) = 2 score(magnitude 7.3) = 2 ‣ ILP formula:on: c and s are indicator variables indexed over bigrams (“concepts”) and sentences, respec:vely “set ci to 1 iﬀ some sentence that contains it is included” sum of included sentences’ lengths can’t exceed L ‣ Find summary that maximizes the score of bigrams it covers Evalua:on: ROUGE ‣ Rouge-n: n-gram precision/recall/F1 of summary w.r.t. gold standard Lin (2004) ‣ Rouge-2 correlates well with human judgments for mul:-document summariza:on tasks An earthquake was detected in Iraq on Sunday A massive earthquake of magnitude 7.3 struck Iraq on Sunday ‣ Many hyperparameters: stemming, remove stopwords, etc. reference predic:on ROUGE 2 recall = 1 correct bigram (Iraq, Sunday) / 4 reference bigrams ‣ Historically: ROUGE recall @ k {words, characters}. Now: ROUGE F1 ROUGE 2 precision = 1 correct bigram (Iraq, Sunday) / 6 predicted bigrams Results Ghalandri (2017) BeSer centroid: 38.58 9.73 1.53 Gillick and Favre / bigram recall ‣ Caveat: these techniques all work beSer for mul:-document than single- document! Mul:-Document vs. Single Document ‣ “a massive earthquake hit Iraq” “a massive earthquake struck Iraq” — lots of redundancy to help select content in mul:-document case ‣ When you have a lot of documents, there are more possible sentences to extract: But eight villages were damaged in Iran and at least six people were killed and many others injured in the border town of Qasr-e Shirin in Iran, Iranian state TV said. The quake has been felt in several Iranian ci:es and eight villages have been damaged. ‣ Mul:-document summariza:on is easier? Compressive Summariza:on Compressive Summariza:on Indian Express — A massive earthquake of magnitude 7.3 struck Iraq on Sunday, 103 kms (64 miles) southeast of the city of As-Sulaymaniyah, the US Geological Survey said, reports Reuters. US Geological Survey ini:ally said the quake was of a magnitude 7.2, before revising it to 7.3. ‣ Sentence extrac:on isn’t aggressive enough at removing irrelevant content ‣ Want to extract sentences and also delete content from them Syntac:c Cuts ‣ Delete adjuncts A massive earthquake of magnitude 7.3 struck Iraq on Sunday, 103 kms (64 miles)… NP NP-LOC VBD NP NP-TMP VP S ‣ Use syntac:c rules to make certain dele:ons Berg-Kirkpatrick et al. (2011) Syntac:c Cuts ‣ Delete second parts of coordina:on structures At least six people were killed and many others injured S S CC S ‣ Use syntac:c rules to make certain dele:ons Berg-Kirkpatrick et al. (2011) Compressive ILP ‣ Recall the Gillick+Favre ILP: ‣ Now sj variables are nodes or sets of nodes in the parse tree At least six people were killed and many others injured S S CC S s2 s1 ‣ New constraint: s2 ≤ s1 “s1 is a prerequisite for s2” Berg-Kirkpatrick et al. (2011) This hasn't been Kellogg's year. Compressive Summariza:on Its president quit suddenly. The oat-bran craze has cost it market share. And now Kellogg is canceling its new cereal plant, which would have cost $1 billion. SBAR NP NP The oat-bran craze has cost Kellogg market share. x1 x2 x3 x4 x5 max x ! w>f(x) "" s.t. summary(x) obeys length limit summary(x) is grammatical summary(x) is coherent ILP: Constraints max x ! w>f(x) "" Gramma:cality constraints: allow cuts within sentences Coreference constraints: do not allow pronouns that would refer to nothing ‣ Otherwise, force its antecedent to be included in the summary ‣ If we’re conﬁdent about coreference, rewrite the pronoun (it Kellogg) s.t. summary(x) obeys length limit summary(x) is grammatical summary(x) is coherent DurreS et al. (2016) Features max x ! w>f(x) "" And now Kellogg is canceling its new cereal plant f ( ) = (SentenceIndex=4) (FirstWord=And) (NumContentWords=4) I I I } s.t. summary(x) obeys length limit summary(x) is grammatical summary(x) is coherent Document posi:on: Lexical features: Centrality: ‣ Now uses a feature-based model, where features iden:fy good content Learning max x ! w>f(x) "" ‣ Structured SVM with ROUGE as loss func:on ‣ Train on a large corpus of New York Times documents with summaries (100,000 documents) s.t. summary(x) obeys length limit summary(x) is grammatical summary(x) is coherent ‣ Augment the ILP to keep track of which bigrams are included or not, use these for loss-augmented decode Berg-Kirkpatrick et al. (2011), DurreS et al. (2016) 30 35 40 45 50 5 6 7 8 9 10 Results: New York Times Corpus Linguis:c Quality (Human study on Mechanical Turk) Content ﬁdelity (ROUGE-1: word overlap with reference) First n words Extract sentences Extract clauses Full system Yoshida et al. (2014) Neural Summariza:on Seq2seq Summariza:on ‣ Extrac:ve paradigm isn’t all that ﬂexible, even with compression ‣ Training is hard! ILPs are hard! Maybe just use seq2seq? Its president quit suddenly The <s> oat bran craze has … … Chopra et al. (2016) ‣ Train to produce summary based on document Seq2seq Summariza:on Chopra et al. (2016) ‣ Task: generate headline from ﬁrst sentence of ar:cle (can get lots of data!) no aSen:on with aSen:on reference sentence ‣ Works preSy well, though these models can generate incorrect summaries (who has the knee injury?) ‣ What happens if we try this on a longer ar:cle? Seq2seq Summariza:on See et al. (2017) ‣ What’s wrong with this summary? Pointer-Generator Model See et al. (2017) ‣ Copying approach like in Jia+Liang Seq2seq Summariza:on See et al. (2017) ‣ Solu:ons: copy mechanism, coverage, just like in MT… ‣ Things might s:ll go wrong, no way of preven:ng this… Neural Abstrac:ve Systems See et al. (2017) ‣ How abstrac:ve is this, anyway? Neural Extrac:ve Systems Nallapa: et al. (2017) pooling Neural Systems: Results See et al. (2017) ‣ Abstrac:ve systems don’t/barely beat a “lead” baseline on ROUGE ‣ Copy mechanism and coverage help substan:ally Challenges of Summariza:on ‣ True abstrac:on? ‣ Not really necessary for ar:cles ‣ Genera:ng from structured informa:on can usually be done with templates… summary ar:cle extrac:on abstrac:on compression syntax seman:cs??? seman:cs??? syntax Takeaways ‣ Extrac:ve systems built on heuris:cs / ILPs work preSy well ‣ Compression can make things beSer, especially in the single-document seing ‣ Neural systems (like MT models) can do abstrac:ve summariza:on, but they oWen just copy inputs (or deviate from inputs in bad ways) "
162,"CS388: Natural Language Processing Lecture 21: Dialogue Greg Durrett Administrivia ‣ Proposal due today at 5pm ‣ Proposals returned ASAP, then Project 2 Recall: ExtracGve SummarizaGon ‣ Count number of documents each bigram occurs in to measure importance Gillick and Favre (2009) score(massive earthquake) = 3 score(Iraqi capital) = 1 score(six killed) = 2 score(magnitude 7.3) = 2 ‣ ILP formulaGon: c and s are indicator variables indexed over concepts (bigrams) and sentences, respecGvely “set ci to 1 iﬀ some sentence that contains it is included” sum of included sentences’ lengths can’t exceed L ‣ Find summary that maximizes the score of bigrams it covers Recall: Compression ‣ Now sj variables are nodes or sets of nodes in the parse tree At least six people were killed and many others injured S S CC S s2 s1 ‣ New constraint: s2 ≤ s1 “s1 is a prerequisite for s2” Recall: Pointer-Generator See et al. (2017) Recall: Seq2seq SummarizaGon See et al. (2017) ‣ SoluGons: copy mechanism, coverage, just like in MT… ‣ Things might sGll go wrong, no way of prevenGng this… This Lecture ‣ Chatbot dialogue systems ‣ Task-oriented dialogue ‣ Other dialogue applicaGons Chatbots Turing Test (1950) ‣ ImitaGon game: A and B are locked in rooms and answer C’s quesGons via typewriter. Both are trying to act like B A B C B B trained judge C trained judge Original InterpretaGon: Standard InterpretaGon: ‣ The test is not “does this computer seem human-like to random people with a web browser?” ELIZA ‣ Created 1964-1966 at MIT, heavily scripted ‣ DOCTOR script was most successful: repeats user’s input, asks inane quesGons Weizenbaum (1966) ELIZA (.*) you (.*) me Why do you think I $2 you? ‣ IdenGfy keyword, idenGfy context, apply transformaGon rule ‣ Very lijle need to generate new content, but can only have one type of conversaGon My (.) (.*) What else comes to mind when you think of your $1? Cleverbot ‣ Carpenter (1986), online system built in 2006 ‣ “Nearest neighbors”: human says statement A, ﬁnd a human response in human-human or human-computer chats to statement A, repeat that User: Hi there cleverbot. Cleverbot: How are you? User: Doing okay. what plans do you have today? Cleverbot: I do revision. User: What are you revising for? Cleverbot: Maths history. User: How old are you? Cleverbot: That's none of your business. User: What is maths history? Cleverbot: I don’t know, farming maybe ‣ Can ooen give sensible answers, but the bot doesn’t really impose high-level discourse structure Data-Driven Approaches ‣ Can treat as a machine translaGon problem: “translate” from current ujerance to next one ‣ Filter the data, use staGsGcal measures to prune extracted phrases to get bejer performance Rijer et al. (2011) Data-Driven Approaches Rijer et al. (2011) Seq2seq models What are you doing I <s> am going home [STOP] ‣ Just like convenGonal MT, can train seq2seq models for this task ‣ Why might this model perform poorly? What might it be bad at? ‣ Hard to evaluate: Lack of Diversity Li et al. (2016) ‣ Training to maximize likelihood gives a system that prefers common responses: Lack of Diversity Li et al. (2016) ‣ SoluGon: mutual informaGon criterion; response R should be predicGve of user ujerance U as well ‣ Mutual informaGon: ‣ Standard condiGonal likelihood: log P(R|U) log P(R, U) P(R)P(U) = log P(R|U) −log P(R) ‣ log P(R) can reﬂect probabiliGes under a language model Lack of Diversity Li et al. (2016) ‣ OpenSubGtles data Future of chatbots ‣ XiaoIce: Microsoo chatbot in Chinese, 20M users, average user interacts 60 Gmes/month ‣ People do seem to like talking to them…? ‣ How deep can a conversaGon be without more semanGc grounding? Basic facts aren’t even consistent… ‣ Can force chatbots to give consistent answers, but sGll probably not very interesGng Li et al. (2016) Persona… Task-Oriented Dialogue Task-Oriented Dialogue Google, what’s the most valuable American company? Apple Who is its CEO? Tim Cook ‣ QuesGon answering/search: Task-Oriented Dialogue Siri, ﬁnd me a good sushi restaurant in Chelsea Sushi Seki Chelsea is a sushi restaurant in Chelsea with 4.4 stars on Google ‣ Personal assistants / API front-ends: How expensive is it? Entrees are around $30 each Find me something cheaper Task-Oriented Dialogue Hey Alexa, why isn’t my Amazon order here? Let me retrieve your order. Your order was scheduled to arrive at 4pm today. ‣ Personal assistants / API front-ends: It never came Okay, I can put you through to customer service. Air Travel InformaGon Service (ATIS) ‣ Given an ujerance, predict a domain-speciﬁc semanGc interpretaGon DARPA (early 1990s), Figure from Tur et al. (2010) ‣ Can formulate as semanGc parsing, but simple slot-ﬁlling soluGons (classiﬁers) work well too Full Dialogue Task ‣ Parsing / language understanding is just one piece of a system Young et al. (2013) ‣ Dialogue state: reﬂects any informaGon about the conversaGon (e.g., search history) ‣ User ujerance -> update dialogue state -> take acGon (e.g., query the restaurant database) -> say something ‣ Much more complex than chatbots! Full Dialogue Task Find me a good sushi restaurant in Chelsea restaurant_type <- sushi location <- Chelsea Sushi Seki Chelsea is a sushi restaurant in Chelsea with 4.4 stars on Google curr_result <- execute_search() How expensive is it? get_value(cost, curr_result) Entrees are around $30 each POMDP-based Dialogue Systems Young et al. (2013) ‣ Dialogue model: can look like a parser or any kind of encoder model ‣ POMDP: user is the “environment,” an ujerance is a noisy signal of state ‣ Generator: use templates or seq2seq model ‣ Where do rewards come from? Reward for compleGng task? Find me a good sushi restaurant in Chelsea restaurant_type <- sushi location <- Chelsea Sushi Seki Chelsea is a sushi restaurant in Chelsea with 4.4 stars on Google make_reservation(curr_result) How expensive is it? +1 … Okay make me a reservaGon! curr_result <- execute_search() Very indirect signal of what should happen up here User gives reward? Find me a good sushi restaurant in Chelsea restaurant_type <- sushi location <- Chelsea Sushi Seki Chelsea is a sushi restaurant in Chelsea with 4.4 stars on Google curr_result <- execute_search() How expensive is it? get_value(cost, curr_result) Entrees are around $30 each +1 +1 How does the user know the right search happened? Wizard-of-Oz Kelley (early 1980s), Ford and Smith (1982) ‣ Learning from demonstraGons: “wizard” pulls the levers and makes the dialogue system update its state and take acGons Full Dialogue Task Find me a good sushi restaurant in Chelsea restaurant_type <- sushi location <- Chelsea curr_result <- execute_search() { wizard enters these Sushi Seki Chelsea is a sushi restaurant in Chelsea with 4.4 stars on Google { wizard types this out or invokes templates ‣ Wizard can be a trained expert and know exactly what the dialogue systems is supposed to do Learning from StaGc Traces Bordes et al. (2017) ‣ Using either wizard-of-Oz or other annotaGons, can collect staGc traces and train from these Full Dialogue Task Find me a good sushi restaurant in Chelsea restaurant_type <- sushi location <- Chelsea curr_result <- execute_search() ‣ User asked for a “good” restaurant — does that mean we should ﬁlter by star raGng? What does “good” mean? ‣ Hard to change system behavior if training from staGc traces, especially if system capabiliGes or desired behavior change stars <- 4+ Goal-oriented Dialogue ‣ Big Companies: Apple Siri (VocalIQ), Google Allo, Amazon Alexa, Microsoo Cortana, Facebook M, Samsung Bixby, Tencent WeChat ‣ Startups: ‣ Lots of cool work that’s not public yet ‣ Tons of industry interest! Other Dialogue ApplicaGons Search/QA as Dialogue ‣ “Has Chris Praj won an Oscar?” / “Has he won an Oscar” QA as Dialogue ‣ Dialogue is a very natural way to ﬁnd informaGon from a search engine or a QA system Iyyer et al. (2017) ‣ QA is hard enough on its own ‣ Users move the goalposts ‣ Challenges: QA as Dialogue ‣ UW QuAC dataset: QuesGon Answering in Context Choi et al. (2018) Search as Dialogue ‣ Google can deal with misspellings, so more misspellings happen — Google has to do more! Dialogue Mission Creep System Error analysis Bejer model ‣ Fixed distribuGon (e.g., natural language sentences), error rate -> 0 Data ‣ Error rate -> ???; “mission creep” from HCI element Harder Data Most NLP tasks System Error analysis Bejer model Data Dialogue/Search/QA ??? Dialogue Mission Creep ‣ High visibility — your product has to work really well! Takeaways ‣ Some decent chatbots, but unclear how to make these more sophisGcated than they are right now ‣ Task-oriented dialogue systems are growing in scope and complexity — really exciGng systems on the way ‣ More and more problems are being formulated as dialogue — interesGng applicaGons but challenging to get working well "
163,"CS388: Natural Language Processing Lecture 22: Grounding Greg Durrett Administrivia ‣ Project 2 graded soon ‣ Final project feedback out Recall: Seq2seq Chatbots What are you doing I <s> am going home [STOP] ‣ Just like convenPonal MT, can train seq2seq models for this task ‣ Why might this model perform poorly? What might it be bad at? ‣ Hard to evaluate: Recall: Lack of Diversity Li et al. (2016) ‣ SoluPon: mutual informaPon criterion; response R should be predicPve of user u\erance U as well ‣ Mutual informaPon: ‣ Standard condiPonal likelihood: log P(R|U) log P(R, U) P(R)P(U) = log P(R|U) −log P(R) ‣ log P(R) can reﬂect probabiliPes under a language model Recall: Task-Oriented Dialogue Bordes et al. (2017) ‣ Using either wizard-of-Oz or other annotaPons, can collect staPc traces and train from these Recall: QA as Dialogue ‣ Dialogue is a very natural way to ﬁnd informaPon from a search engine or a QA system Iyyer et al. (2017) ‣ Several recent datasets on this topic, but tough to collect a staPc dataset for an interacPve applicaPon This Lecture ‣ Image capPoning / VQA ‣ Grounding with interacPon ‣ Example grounding applicaPons Basic Grounding Examples History ‣ Miller and Johnson-Laird (1976) — Language and PercepPon ‣ Harnad (1990) — Symbol grounding problem ‣ How do we connect “symbols” to the world in the right way? In a pure symbolic model the crucial connecPon between the symbols and their referents is missing; an autonomous symbol system, though amenable to a systemaPc semanPc interpretaPon, is ungrounded. In a pure connecPonist model, names are connected to objects through invariant pa\erns in their sensory projecPons, learned through exposure and feedback, but the crucial composiPonal property is missing; a network of names, though grounded, is not yet amenable to a full systemaPc semanPc interpretaPon. In the hybrid system proposed here, there is no longer any autonomous symbolic level at all; instead, there is an intrinsically dedicated symbol system, its elementary symbols (names) connected to nonsymbolic representaPons that can pick out the objects to which they refer, via connecPonist networks that extract the invariant features of their analog sensory projecPons. ‣ Neural networks (connecPonism) help us connect symbolic reasoning to sensory inputs Grounding ‣ Tie language to something concrete in the world ‣ Percepts: red means this set of RGB values, loud means lots of decibels on our microphone, so( means these properPes on our hapPc sensor… ‣ Higher-level percepts: cat means this type of pa\ern in an image ‣ Eﬀects on others: go le( means the robot turns lep, speed up means increasing actuaPon Colors McMahan and Stone (2014) ‣ What color is this? ‣ What about this? Colors McMahan and Stone (2014) ‣ When we say “yellowish-green”, what does that mean? ‣ Color descripPons governed by percepPon as well as availability: how commonly it is used (yellowish green vs. chartreuse) Colors McMahan and Stone (2014) ‣ P(ktrue | X): distribuPon parameterized in HSV space as follows: there are certain ranges where a color can “deﬁnitely apply”, others where it can apply ‣ P(ksaid | ktrue): captures availability; prior towards common colors ‣ Model combines language / reasoning with basic percepPon — characterisPc of grounding SpaPal RelaPons Golland et al. (2010) ‣ How would you indicate O1 to someone with relaPon to the other two objects? (not calling it a vase, or describing its inherent properPes) ‣ What about O2? ‣ Requires modeling listener — “right of O2” is insuﬃcient though true SpaPal RelaPons Golland et al. (2010) ‣ Grice (1975) ‣ Maxim of quality: say something true ‣ Maxim of quanPty: be as informaPve as required but no more ‣ Maxim of relaPon: be relevant ‣ Maxim of manner: avoid ambiguity ‣ Maximize expected uPlity given listener model U = 1 if correct, else 0 ‣ Say something which has a high probability of evoking the right response in the listener SpaPal RelaPons Golland et al. (2010) ‣ Listener model: ‣ SyntacPc analysis of the parPcular expression gives structure ‣ Rules (O2 = 100% prob of O2), features on words modify distribuPons as you go up the tree SpaPal RelaPons ‣ Objects are associated with coordinates, features map lexical items to distribuPons (“right” modiﬁes the distribuPon over objects to focus on those with higher x coordinate) ‣ Language -> spaPal relaPons -> distribuPon over what object is intended Golland et al. (2010) SpaPal RelaPons ‣ Put it all together: speaker will learn to say things that evoke the right interpretaPon Golland et al. (2010) ‣ Language is grounded in what the speaker understands about it InstrucPon Following ‣ Want to be able to follow instrucPons in a virtual environment ‣ “Go along the blue hall, then turn lep away from the ﬁsh painPng and walk to the end of the hallway” MacMahon et al. (2006) InstrucPon Following ‣ Basic plans derived directly from supervision ‣ “Landmarks” plans — things that should be true aper each step (which may show up in the language) Chen and Mooney (2011) InstrucPon Following ‣ Language is grounded in acPons in the world ‣ Train semanPc parser on (u\erance, acPon) pairs Chen and Mooney (2011) InstrucPon Following Tellex et al. (2011) ‣ “SpaPal descripPon clauses” -> “grounding graphs” ConnecPons to SemanPc Parsing ‣ Each grounding framework requires mapping natural language to something concrete (distribuPon in color space, object, acPon sequence) ‣ SomePmes looks like semanPc parsing, parPcularly when language -> discrete output ‣ Using linguisPc structure to capture composiPonality is open useful Image CapPoning How do we capPon these images? ‣ Need to know what’s going on in the images — objects, acPviPes, etc. ImageNet models ‣ Last layer is just a linear transformaPon away from object detecPon — should capture high-level semanPcs of the image, especially what objects are in there ‣ Train on ImageNet to do object classiﬁcaPon ImageNet models ‣ Many architectures for this: VGG, ResNet, DenseNet, etc. — all end in fully-connected layers Images -> Text Encode [Donahue et al. CVPR’15] [Sutskever et al. NIPS’14] [Vinyals et al. CVPR’15] English Sentence RNN encoder RNN decoder French Sentence CNN Encoder RNN decoder Sentence CNN Encoder RNN decoder Sentence [Venugopolan et al. NAACL’15] RNN decoder Sentence [Venugopalan et al. ICCV’15] CNN + RNN encoder What’s the grounding here? a close up of a plate of ___ a couple of bears walking across ____ food a dirt road ‣ What are the vectors really capturing? Probably some objects, but maybe not deep relaPonships Simple Baselines ‣ Simple baselines work well! ‣ D-*: condiPon on detecPons only ‣ MRNN: take the last layer of the CNN, feed into RNN ‣ k-NN: use last layer of ImageNet model, ﬁnd most similar train images based on cosine similarity with that vector Devlin et al. (2015) Simple Baselines Devlin et al. (2015) ‣ Even from CNN+RNN methods (MRNN), relaPvely few unique capPons even though it’s not quite regurgitaPng the training Video CapPoning • Generate an NL video description by training a suite of SVM-based visual recognizers and composing their outputs into a coherent sentence using a graphical model (Krishnamoorthy et al., 2013; Thomason et al., 2014) Content Selection Video Surface Realization Sentence SVOP Slide credit: Ray Mooney Video CapPoning Language Statistics Visual Confidences Subject person Verb slice Object onion Place kitchen MAP Inference on Factor Graph estimates the most likely SVOP quadruple. A person is slicing an onion in the kitchen. Slide credit: Ray Mooney Visual QuesPon Answering ‣ Answer quesPons about images Agrawal et al. (2015) ‣ Frequently much more metaphorical, require composiPonal understanding of mulPple objects + acPviPes in the image Visual QuesPon Answering ‣ CNN processing of the image, RNN processing of the language ‣ What could go wrong here? Neural Module Networks ‣ Integrate composiPonal reasoning + image recogniPon Andreas et al. (2016), Hu et al. (2017) ‣ Have neural network components like classify[color] whose use is governed by a parse of the quesPon Neural Module Networks ‣ Can also learn these structures with reinforcement learning Andreas et al. (2016), Hu et al. (2017) Visual QuesPon Answering ‣ In many cases, language as a prior is pre\y good! ‣ Balanced VQA: remove these regulariPes by having pairs of images with diﬀerent answers ‣ “Do you see a…” = yes (87% of the Pme) ‣ “How many…” = 2 (39%) ‣ “What sport…” = tennis (41%) Goyal et al. (2017) Understanding VQA • “Attentive Explanations: Justifying Decisions and Pointing to the Evidence,” Park et al., InterpML, NIPS-2017. Explanation: “Because he is on a snowy hill wearing skis” Slide credit: Ray Mooney Grounding Language in InteracPon Grounding in InteracPon Lewis et al. (2017) ‣ Corpus of dialogues — can train a model on these to learn to negoPate Grounding in InteracPon Lewis et al. (2017) ‣ Same issues as other dialogue systems: system may prefer generic choices, like accepPng the oﬀer, instead of negoPaPng harder ‣ Instead: do self-play rollouts, train with reinforcement learning to maximize reward and not likelihood of human u\erances Grounding in InteracPon Lewis et al. (2017) ‣ Interleave self-play with supervised learning, otherwise the messages stop looking like real English ‣ When two systems talk to each other, they remap what words mean and completely change the grounding Grounding in InteracPon Lewis et al. (2017) ‣ Less direct form of grounding: we understand the language used based on the eﬀects it produces in the other agent (whether human or machine) and in the ﬁnal reward ‣ More “symbolic” than grounding percepts like color, but sPll about interacPng with the world! Takeaways ‣ Lots of problems where natural language has to be interpreted in an environment and can be understood in the context of that environment ‣ Image recogniPon: parPcularly large area of research featuring big neural networks (but they somePmes learn to cheat) ‣ More complex environments/robots/simulaPons/tasks -> more complex dialogue to be learned over Pme! "
164,"CS388: Natural Language Processing Lecture 23: Unsupervised Learning Greg Durrett Some slides adapted from Leon Gu (CMU), Taylor Berg-Kirkpatrick (CMU) What data do we learn from? training data labels unlabeled data ‣ Supervised seNngs: ‣ Tagging: POS, NER ‣ Parsing: consRtuency, dependency, semanRc parsing ‣ IE, MT, QA, … ‣ Semi-supervised models ‣ Word embeddings / word clusters (helpful for nearly all tasks) ‣ Language models for machine translaRon supervised model semi- supervised model ‣ Learn linguisRc structure from unlabeled data and use it? This Lecture ‣ ExpectaRon maximizaRon for learning HMMs ‣ Discrete linguisRc structure from generaRve models: unsupervised POS inducRon ‣ ConRnuous structure with generaRve models: variaRonal autoencoders ‣ ConRnuous structure with “discriminaRve” models: transfer learning EM for HMMs Recall: Hidden Markov Models ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output y1 y2 yn x1 x2 xn … P(y, x) = P(y1) n Y i=2 P(yi|yi−1) n Y i=1 P(xi|yi) IniRal distribuRon TransiRon probabiliRes Emission probabiliRes } } } ‣ P(x|y) is a distribuRon over all words in the vocabulary — not a distribuRon over features (but could be!) ‣ MulRnomials: tag x tag transiRons, tag x word emissions ‣ ObservaRon (x) depends only on current state (y) Unsupervised Learning a b a c c c c ‣ Can we induce linguisRc structure? Thought experiment… ‣ What’s a two-state HMM that could produce this? b a c c c a a b c c a a ‣ What if I show you this sequence? ‣ What did you do? Use current model parameters + data to reﬁne your model. This is what EM will do Part-of-Speech InducRon ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output ‣ Assume we don’t have access to labeled examples — how can we learn a POS tagger? L(x1,...,D) = D X i=1 log X y P(y, xi) ‣ Key idea: opRmize ‣ OpRmizing marginal log-likelihood with no labels y: GeneraRve model explains the data x; the right HMM makes it look likely P(x) = X y P(y, x) ‣ non-convex opRmizaRon problem Part-of-Speech InducRon ‣ Input x = (x1, ..., xn) y = (y1, ..., yn) Output L(x1,...,D) = D X i=1 log X y P(y, xi) ‣ OpRmizing marginal log-likelihood with no labels y: ‣ Can’t use a discriminaRve model; , doesn’t model x ‣ What’s the point of this? Model has inducRve bias and so should learn some useful latent structure y (clustering eﬀect) ‣ EM is just one procedure for opRmizing this kind of objecRve X y P(y|x) = 1 ExpectaRon MaximizaRon log X y P(x, y|✓) ✓ = log X y q(y)P(x, y|✓) q(y) ‣ VariaRonal approximaRon q — this is a trick we’ll return to later! ≥ X y q(y) log P(x, y|✓) q(y) ‣ Jensen’s inequality (uses concavity of log) = Eq(y) log P(x, y|✓) + Entropy[q(y)] ‣ Can opRmize this lower-bound on log likelihood instead of log-likelihood ‣ CondiRon on parameters Adapted from Leon Gu ExpectaRon MaximizaRon log X y P(x, y|✓) ≥Eq(y) log P(x, y|✓) + Entropy[q(y)] ‣ If q(y) = P(y|x, ✓), this bound ends up being Rght ‣ ExpectaRon-maximizaRon: alternaRng maximizaRon of the lower bound over q and ‣ E-step: maximize w.r.t. q; that is, qt = P(y|x, ✓t−1) ‣ M-step: maximize w.r.t. ; that is, ✓ ✓t = argmax✓Eqt log P(x, y|✓) Adapted from Leon Gu ✓ ‣ Current Rmestep = t, have parameters ✓t−1 EM for HMMs ‣ ExpectaRon-maximizaRon: alternaRng maximizaRon ‣ E-step: maximize w.r.t. q; that is, qt = P(y|x, ✓t−1) ‣ M-step: maximize w.r.t. ; that is, ✓ ✓t = argmax✓Eqt log P(x, y|✓) ‣ E-step: for an HMM: run forward-backward with the given parameters ‣ M-step: set parameters to opRmize the crazy argmax term P(yi = s|x, ✓t−1), P(yi = s1, yi+1 = s2|x, ✓t−1) tag marginals at each posiRon tag pair marginals at each posiRon ‣ Compute M-Step ‣ Recall how we maximized log P(x,y): read counts oﬀ data the DT dog NN count(DT, the) = 1 count(DT, dog) = 0 count(NN, the) = 0 count(NN, dog) = 1 P(the|DT) = 1 P(dog|DT) = 0 P(the|NN) = 0 P(dog|NN) = 1 ‣ Same procedure, but maximizing P(x,y) in expectaRon under q means that q speciﬁes frac/onal counts the dog count(DT, the) = 0.9 count(DT, dog) = 0.3 count(NN, the) = 0.1 count(NN, dog) = 0.7 P(the|DT) = 0.75 P(dog|DT) = 0.25 P(the|NN) = 0.125 P(dog|NN) = 0.875 q DT: 0.9 NN: 0.1 NN: 0.7 DT: 0.3 M-Step ‣ Same for transiRon probabiliRes the dog q DT—NN: 0.6 DT—DT: 0.1 NN—DT: 0.2 NN—NN: 0.1 P(DT|DT) = 1/7 P(NN|DT) = 6/7 P(DT|NN) = 2/3 P(NN|NN) = 1/3 How does EM learn things? ‣ IniRalize (M-step 0): P(the|DT) = 0.9 P(dog|DT) = 0.05 P(marsupial|DT) = 0.05 P(the|NN) = 0.05 P(dog|NN) = 0.9 P(marsupial|NN) = 0.05 ‣ TransiRon probabiliRes: uniform the dog DT: 0.95 NN: 0.05 NN: 0.95 DT: 0.05 the marsupial DT: 0.95 NN: 0.05 NN: 0.5 DT: 0.5 ‣ E-step 1: (all values are approximate) ‣ Emissions ‣ uniform How does EM learn things? the dog DT: 0.95 NN: 0.05 NN: 0.95 DT: 0.05 the marsupial DT: 0.95 NN: 0.05 NN: 0.5 DT: 0.5 ‣ E-step 1: ‣ M-step 1: ‣ TransiRon probabiliRes (approx): P(NN|DT) = 3/4, P(DT|DT) = 1/4 ‣ Emissions aren’t so diﬀerent How does EM learn things? the dog DT: 0.95 NN: 0.05 NN: 0.95 DT: 0.05 the marsupial DT: 0.95 NN: 0.05 NN: 0.70 DT: 0.30 ‣ E-step 2: ‣ M-step 1: ‣ Emissions aren’t so diﬀerent ‣ TransiRon probabiliRes (approx): P(NN|DT) = 3/4, P(DT|DT) = 1/4 How does EM learn things? the dog DT: 0.95 NN: 0.05 NN: 0.95 DT: 0.05 the marsupial DT: 0.95 NN: 0.05 NN: 0.70 DT: 0.30 ‣ E-step 2: ‣ M-step 2: ‣ Emission P(marsupial|NN) > P(marsupial|DT) ‣ Remember to tag marsupial as NN in the future! ‣ Context constrained what we learned! That’s how data helped us How does EM learn things? ‣ Can think of q as a kind of “fracRonal annotaRon” ‣ E-step: compute annotaRons (posterior under current model) ‣ M-step: supervised learning with those fracRonal annotaRons ‣ IniRalize with some reasonable weights, alternate E and M unRl convergence EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) ‣ E-step: compute q which gives this lower bound ‣ iniRal theta EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) ‣ M-step: ﬁnd maximum of lower bound L(x1,...,D) = D X i=1 log X y P(y, xi) EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) ‣ E-step 2: re-esRmate q EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) ‣ E-step 2: re-esRmate q EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) EM’s Lower Bound slide credit: Taylor Berg-Kirkpatrick L(x1,...,D; ✓) L(x1,...,D) = D X i=1 log X y P(y, xi) Part-of-speech InducRon ‣ Merialdo (1994): you have a whitelist of tags for each word ‣ Learn parameters on k examples to start, use those to iniRalize EM, run on 1 million words of unlabeled data ‣ Tag dicRonary + data should get us started in the right direcRon… Part-of-speech InducRon ‣ Small amounts of data > large amounts of unlabeled data ‣ Running EM *hurts* performance once you have labeled data Merialdo (1994) Two Hours of AnnotaRon Garrete and Baldridge (2013) ‣ Kinyarwanda and Malagasy (two actual low-resource languages) ‣ Label propagaRon (technique for using dicRonary labels) helps a lot, with data that was collected in two hours VariaRonal Autoencoders ConRnuous Latent Variables ‣ What if we want to use conRnuous latent variables? ‣ For discrete latent variables y, we opRmized: P(x) = X y P(y, x) P(z, x) = P(z)P(x|z) P(x) = Z P(z)P(x|z)@z ‣ Can use EM here when P(z) and P(x|z) are Gaussians ‣ What if we want P(x|z) to be something more complicated, like an LSTM with z as the iniRal state? Deep GeneraRve Models the <s> movie was good[STOP] z ‣ z is a latent variable which should control the generaRon of the sentence, maybe capture something about its topic P(z, x) = P(z)P(x|z) Deep GeneraRve Models Jensen = Eq(z|x)[−log q(z|x) + log P(x, z|✓)] = Eq(z|x)[log P(x|z, ✓)] −KL(q(z|x)kP(z)) log Z z P(x, z|✓) = log Z z q(z)P(x, z|✓ q(z) ≥ Z z q(z) log P(x, z|✓) q(z) ‣ KL divergence: distance metric over distribuRons (more dissimilar <=> higher KL) “make the data likely under q” (discriminaRve) “make q close to the prior” VariaRonal Autoencoders x Input q(z|x) x distribuRon over z Maximize P(x|z,θ) “inference network” generaRve model x GeneraRve model (test): Autoencoder (training): Miao et al. (2015) Eq(z|x)[log P(x|z, ✓)] −KL(q(z|x)kP(z)) z ⇠P(z) Training VAEs x q(z|x) x “inference network” generaRve model Autoencoder (training): ‣ Choose q to be Gaussian with parameters that are computed from x Miao et al. (2015) q = N(µ(x), diag(σ2(x))) ‣ mu and sigma are computed from an LSTM over x, call their parameters φ ✓ ‣ How to handle the expectaRon? Sampling φ Eq(z|x)[log P(x|z, ✓)] −KL(q(z|x)kP(z)) Training VAEs x q(z|x) x “inference network” generaRve model Autoencoder (training): For each example x Compute q (run forward pass to compute mu and sigma) Sample z ~ q For some number of samples Compute P(x|z) and compute loss Backpropagate to update phi, theta φ ✓ Autoencoders the movie was great the <s> movie was good[STOP] ‣ Inference network (q) is the encoder and generator is the decoder + Gaussian noise ‣ Same computaRon graph as VAE, add KL divergence term to make the objecRve the same ‣ Another interpretaRon: train an autoencoder and add Gaussian noise VisualizaRon ‣ What does gradient encourage latent space to do? direcRon of beter likelihood for x prior q Eq(z|x)[log P(x|z, ✓)] + KL(q(z|x)kP(z)) What do VAEs do? ‣ Let us encode a sentence and generate similar sentences: ‣ Style transfer: also condiRon on senRment, change senRment Bowman et al. (2016), Zhao et al. (2017) ‣ …or use the latent representaRons for semi- supervised learning Self-Supervision / Transfer Learning Goals of Unsupervised Learning ‣ We want to use unlabeled data, but EM “requires” generaRve models. Are models like this really necessary? ‣ Language modeling is a “more contextualized” form of word2vec ‣ word2vec: predict nearby word given context. This wasn’t generaRve, but the supervision is free… ELMo they dance at balls dance at balls [EOS] P(xi|x1, . . . , xi−1) = LSTM(x1, . . . , xi−1) ‣ GeneraRve model of the data! ‣ Train one model in each direcRon on 1B words, use the LSTM hidden states as context-aware token representaRons learn a linear classiﬁer on top of this vector to get a POS tagger with 97.3% accuracy (~SOTA) BERT ‣ Text “inﬁlling” task: replace 15% of tokens with something else and try to predict the original I went to the MASK and bought MASK gallon of dog . My MASK kind is 2% . I went to the store and bought a gallon of milk . My favorite kind is 2% . Transformer (12-24 layers) ‣ 80% of the Rme: MASK; 10%: random word; 10%: keep same ‣ Also generate “fake” sentence pairs and try to predict real from fake I went to the MASK and bought MASK gallon of dog . I love karaoke! Results ‣ DramaRc gains on a range of sentence pair / single sentence tasks: paraphrase idenRﬁcaRon, entailment, senRment, textual similarity, … ‣ Not a generaRve model! But learns really eﬀecRve representaRons… Unsupervised Learning ‣ These models are hard to learn in an unsupervised way and too impoverished to really be all that useful ‣ Discrete linguisRc structure with generaRve models: unsupervised POS inducRon ‣ ConRnuous structure with generaRve models: variaRonal autoencoders ‣ ConRnuous structure with “discriminaRve” models ‣ Useful, but also hard to learn in pracRce ‣ ELMo / BERT seem extremely useful Takeaways ‣ EM sort of works for POS inducRon ‣ Language modeling or text inﬁlling as pretraining seems best — arguably not “unsupervised” but the annotaRon is free ‣ VAE can learn sentence representaRons ‣ Next Rme: Jessy Li guest lecture on discourse ‣ Using unlabeled data eﬀecRvely seems like one of the most important direcRons in NLP right now "
165,"Discourse Processing Jessy Li Dept. of Linguistics, UT Austin jessy@austin.utexas.edu With slides/material from Greg Durrett, Dan Jurafsky, Kenton Lee, Diane Litman, and Ani Nenkova Computational Discourse • Text is more than the sum of its individual sentences/utterances. • Discourse processing: NLP beyond the sentence/ utterance boundary. • Monologue • Dialogue • Maybe multi-party • Maybe human-machine What do you think of this text? • “Consider, for example, the difference between passages (18.71) and (18.72). Almost certainly not. The reason is that these utterances, when juxtaposed, will not exhibit coherence. Do you have a discourse? Assume that you have collected an arbitrary set of well-formed and independently interpretable utterances, for instance, by randomly selecting one sentence from each of the previous chapters of this book.” Or this? • “Assume that you have collected an arbitrary set of well-formed and independently interpretable utterances, for instance, by randomly selecting one sentence from each of the previous chapters of this book. Do you have a discourse? Almost certainly not. The reason is that these utterances, when juxtaposed, will not exhibit coherence. Consider, for example, the difference between passages (18.71) and (18.72).” What makes a text coherent? • Discourse/Topic structure • In a coherent text the parts of the discourse exhibit a sensible ordering and/or hierarchical relationship • Entity structure (“Focus”) • A coherent text is about some entity or entities, and the entity/ entities is/are referred to in a structured way throughout the text. • Rhetorical structure (“coherence/discourse relations”) • The elements in a coherent text are related via meaningful relations What makes a text coherent? • Discourse/Topic structure • In a coherent text the parts of the discourse exhibit a sensible ordering and/or hierarchical relationship • Entity structure (“Focus”) • A coherent text is about some entity or entities, and the entity/ entities is/are referred to in a structured way throughout the text. • Rhetorical structure (“coherence/discourse relations”) • The elements in a coherent text are related via meaningful relations Outline • Reference resolution • Discourse relations Outline • Reference resolution • Discourse relations • Gracie: Oh yeah ... and then Mr. and Mrs. Jones were having matrimonial trouble, and my brother was hired to watch Mrs. Jones. • George: Well, I imagine she was a very attractive woman. • Gracie: She was, and my brother watched her day and night for six months. • George: Well, what happened? • Gracie: She ﬁnally got a divorce. • George: Mrs. Jones? • Gracie: No, my brother's wife. Reference Resolution • Process of associating Bloomberg/he/his with particular person and big budget problem/it with a concept • Referring exprs.: Guilani, Bloomberg, he, it, his • Presentational it, there: non-referential • Referents: the person named Bloomberg, the concept of a big budget problem Guiliani left Bloomberg to be mayor of a city with a big budget problem. It’s unclear how he’ll be able to handle it during his term. Reference Resolution • Co-referring referring expressions: • Bloomberg, he, his • Antecedent: Bloomberg • Anaphors: he, his Guiliani left Bloomberg to be mayor of a city with a big budget problem. It’s unclear how he’ll be able to handle it during his term. Discourse Model • Needed to model reference because referring expressions (e.g. Guiliani, Bloomberg, he, it budget problem) encode information about beliefs about the referent • When a referent is ﬁrst mentioned in a discourse, a representation is evoked in the model • Information predicated of it is stored also in the model • On subsequent mention, it is accessed from the model Types of Reference • Entities, concepts, places, propositions, events, ... • According to John, Bob bought Sue an Integra, and Sue bought Fred a Legend. • But that turned out to be a lie. (a speech act) • But that was false. (proposition) • That struck me as a funny way to describe the situation. (manner of description) • That caused Sue to become rather poor. (event) • That caused them both to become rather poor. (combination of multiple events) Reference Phenomena • Indeﬁnite NPs • A homeless man hit up Bloomberg for a dollar. • Some homeless guy hit up Bloomberg for a dollar. • Deﬁnite NPs • The poor fellow only got a lecture. • Demonstratives • This homeless man got a lecture but that one got carted off to jail. • Names • Tom is afraid of Jerry. Pronouns • A large tiger escaped from the Central Park zoo chasing a tiny sparrow. It was recaptured by a brave policeman. • Referents of pronouns usually require some degree of salience in the discourse (as opposed to deﬁnite and indeﬁnite NPs, e.g.) • How do items become salient in discourse? E: So you have the engine assembly ﬁnished. Now attach the rope. By the way, did you buy the gas can today? A: Yes. E: Did it cost much? A: No. E: OK, good. Have you got it attached yet? What does “it” refer to? Why? But things get complicated really fast… Inferables I almost bought an Acura Integra today, but a door had a dent and the engine seemed noisy. Mix the ﬂour, butter, and water. Knead the dough until smooth and shiny. Discontinuous Sets John has a St. Bernard and Mary has a Yorkie. They arouse some comment when they walk them in the park. Generics I saw two Corgis and their seven puppies today. They are the funniest dogs! Coreference Resolution 2 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Input document Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Coreference Resolution 3 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Input document Cluster #1 A ﬁre in a Bangladeshi garment factory the blaze in the four-story building Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Coreference Resolution 4 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Cluster #1 A ﬁre in a Bangladeshi garment factory the blaze in the four-story building Cluster #2 a Bangladeshi garment factory the four-story building Input document Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Coreference Resolution 5 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Cluster #1 A ﬁre in a Bangladeshi garment factory the blaze in the four-story building Cluster #2 a Bangladeshi garment factory the four-story building Cluster #3 at least 37 people the deceased Input document Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Two Subproblems 6 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Input document Mention detection A ﬁre in a Bangladeshi garment factory at least 37 people … the four-story building Mention clustering Cluster #1 A ﬁre in a Bangladeshi garment factory the blaze in the four-story building Cluster #2 a Bangladeshi garment factory the four-story building Cluster #3 at least 37 people the deceased Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Previous Approach: Rule-based pipeline 7 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Candidate mentions A ﬁre in a Bangladeshi garment factory garment factory at least 37 people dead and 100 hospitalized … Input document Hand-engineered rules Syntactic parser Mention #1 Mention #2 Coreferent? A ﬁre in a Bangladeshi garment factory garment ✓/✗ garment factory ✓/✗ factory at least 37 people dead and 100 hospitalized ✓/✗ … … ✓/✗ Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein UC Berkeley Basic Architecture Voters agree when they are given a chance to decide if they ... [Voters]1 agree when [they]1 are given a [chance]2 to decide if [they]1 ... PROPERS: NER chunks PRONOUNS: PRP, PRP$ NOMINALS: all other maximal NPs [Voters]1 agree when [they]1 are given a [chance]2 to decide if [they]1 ... [Voters]1 agree when [they]1 are given a chance to decide if [they]1 ... 1 2 New 1 New Mention-Ranking Architecture [Voters]1 agree when [they]1 are given [a chance]2 to decide if [they]1 ... 1 2 New New 3 Denis and Baldridge (2008), Durrett et al. (2013) [1STWORD=a] [LENGTH=2] ... [Voters-they] [NOM-PRONOUN] ... A1 A2 A3 A4 Pr(Ai = a|x) / exp(w>f(i, a, x)) 1 1 2 New New Learning [Voters]1 agree when [they]1 are given a [chance]2 to decide if [they]1 ... 1 2 New New 3 Gimpel and Smith (2010), Durrett et al. (2013) Maximize: Ai 2 {1, 2, . . . , i −1, New} Pr(Ai = a|x) / exp(w>f(i, a, x)) A1 A2 A3 A4 n X i=1 [log Pr(Ai 2 {correct}|x)] + kwk1 Let’s think about rules… • Number agreement • John’s parents like opera. • John hates it. • John hates them. • Person and case agreement • Nominative: I, we, you, he, she, they • Accusative: me, us, you, him, her, them • Genitive: my, our, your, his, her, their • George and Edward brought bread and cheese. • They shared them. Let’s think about rules… • Gender agreement • John has a Porsche. He/it/she is attractive. • Syntactic constraints: binding theory • John bought himself a new Volvo. (himself = John) • John bought him a new Volvo. (him = not John) • Selectional restrictions • John left his plane in the hangar. • He had ﬂown it from Memphis this morning. Centering [Barack Obama]1 met with [David Cameron]2 . [He]1 said ... SUBJECT−SUBJECT Grosz al. (1995) OBJECT−SUBJECT Slide from Durrett and Klein, Easy Victories and Uphill Battles in Coreference Resolution, EMNLP 2013 Data and Evaluation • Ontonotes (5.0) • 2.9 million words, 3.5K documents • Layered annoations • News, broadcast news, broadcast conversations, blogs, Old and New Testaments • CoNLL 2011 & 2012 shared tasks • Precision, recall and F on pairs of mentions (MUC), links (B-cubed), and entity (CEAF) Coreference Results 70 Test Avg. F1 (%) 50.0 54.0 58.0 62.0 66.0 70.0 Durrett & Klein (2013) Björkelund & Kuhn (2014) Martschat & Strube (2015) 62.5 61.6 60.3 Linear models Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Coreference Results 71 Test Avg. F1 (%) 50.0 54.0 58.0 62.0 66.0 70.0 Durrett & Klein (2013) Björkelund & Kuhn (2014) Martschat & Strube (2015) Wiseman et al. (2016) Clark & Manning (2016) 65.7 64.2 62.5 61.6 60.3 Linear models Neural models Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 Coreference Results 72 Test Avg. F1 (%) 50.0 54.0 58.0 62.0 66.0 70.0 Durrett & Klein (2013) Björkelund & Kuhn (2014) Martschat & Strube (2015) Wiseman et al. (2016) Clark & Manning (2016) 65.7 64.2 62.5 61.6 60.3 Pipelined models Slide from Lee et al., End-to-end Neural Coreference Resolution, EMNLP 2017 End-to-end Neural Coreference Resolution 1 Kenton Lee Luheng He Mike Lewis Luke Zettlemoyer UWNLP University of Washington Facebook AI Research Allen Institute for Artiﬁcial Intelligence Previous Approach: Rule-based pipeline 9 A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Candidate mentions A ﬁre in a Bangladeshi garment factory garment factory at least 37 people dead and 100 hospitalized … Input document Hand-engineered rules Syntactic parser Mention #1 Mention #2 Coreferent? A ﬁre in a Bangladeshi garment factory garment ✓/✗ garment factory ✓/✗ factory at least 37 people dead and 100 hospitalized ✓/✗ … … ✓/✗ Relies on parser for: • mention detection • syntactic features for clustering (e.g. head words) 10 Our Contribution: End-to-end Approach • Joint mention detection and clustering • No preprocessing (no parser, no POS-tagger etc.) End-to-end Approach 15 • Consider all possible spans • Learn to rank antecedent spans • Factored model to prune search space 14 Span #1 Span #2 Coreferent? A A ﬁre ✓/✗ A ﬁre A ﬁre in ✓/✗ A ﬁre in A ﬁre in a ✓/✗ … … ✓/✗ O(N4) pairwise decisions Inference challenge: Can we do better than O(N4)? A ﬁre in a Bangladeshi garment factory has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in the four-story building. Witnesses say the only exit door was on the ground ﬂoor, and that it was locked when the ﬁre broke out. Input document (N words) Naive joint model is O(N4): 59 Bidirectional LSTM Word & character embeddings Head-ﬁnding attention Span representation General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Compute all span representations Neural Span Representations Coreference Architecture 60 P(yi | D) General Electric the Postal Service the company s(the company, General Electric) s(the company, the Postal Service) s(the company, ✏) = 0 Span representation General Electric the Postal Service the company s(the company, General Electric) s(the company, the Postal Service) s(the company, ✏) = 0 Coreference Architecture 61 P(yi | D) span i Span representation Coreference Architecture 62 P(yi | D) General Electric the Postal Service the company Span representation sm(i) Coreference Architecture 63 P(yi | D) General Electric the Postal Service the company Span representation sm(i) sa(i, j) Coreference Architecture 64 P(yi | D) General Electric the Postal Service the company Span representation sm(i) s(i, j) sa(i, j) Coreference Architecture 65 Span representation P(yi | D) General Electric the Postal Service the company s(the company, General Electric) s(the company, the Postal Service) s(the company, ✏) = 0 sm(i) s(i, j) sa(i, j) Coreference Results 73 Test Avg. F1 (%) 50.0 54.0 58.0 62.0 66.0 70.0 Durrett & Klein (2013) Björkelund & Kuhn (2014) Martschat & Strube (2015) Wiseman et al. (2016) Clark & Manning (2016) Our model (single) Our model (ensemble) 68.8 67.2 65.7 64.2 62.5 61.6 60.3 Pipelined models End-to-end models What makes a text coherent? • Discourse/Topic structure • In a coherent text the parts of the discourse exhibit a sensible ordering and/or hierarchical relationship • Entity structure (“Focus”) • A coherent text is about some entity or entities, and the entity/entities is/are referred to in a structured way throughout the text. • Rhetorical structure (“coherence/discourse relations”) • The elements in a coherent text are related via meaningful relations Which one is more coherent? John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could ﬁnally buy a piano. He arrived just as the store was closing for the day. John went to his favorite music store to buy a piano. It was a store John had frequented for many years. He was excited that he could ﬁnally buy a piano. It was closing just as John arrived. Entity-centric Local Coherence • Centering Theory (Grosz et al 1995): The way entities are introduced and discussed inﬂuences coherence • Entities in an utterance are ranked according to salience. • Is an entity pronominalized or not? • Is an entity in a prominent syntactic position? • Each utterance has one center (≈topic or focus). • Coherent discourses have utterances with common centers. • Entity transitions capture degrees of coherence • (e.g., in Centering theory CONTINUE > SHIFT). • Computational model: Entity Grid (Barzilay and Lapata 2005) Coherence • John hid Bill’s car keys. He was drunk. • John hid Bill’s car keys. He likes spinach. Outline • Reference resolution • Discourse relations Discourse Relations With the national announcement last week of plans to sell some breakfast items all day long, the company expects to buy even more eggs. For example, the Egg McMufﬁn, which uses one egg per sandwich, is among the company’s most popular menu items. Cause Discourse Relations With the national announcement last week of plans to sell some breakfast items all day long, the company expects to buy even more eggs. For example, the Egg McMufﬁn, which uses one egg per sandwich, is among the company’s most popular menu items. Instantia tion Discourse Relations With the national announcement last week of plans to sell some breakfast items all day long, the company expects to buy even more eggs. For example, the Egg McMufﬁn, which uses one egg per sandwich, is among the company’s most popular menu items. Two theories Rhetorical Structure Theory (RST) (Mann and Thompson 1988) Corpus: RST Discourse Treebank (Carlson et al., 2001) 385 documents from the Penn Treebank Penn Discourse Treebank (PDTB) (Prasad et al., 2008) Corpus: Wall Street Journal portion of the Penn Treebank Two theories Rhetorical Structure Theory (RST) (Mann and Thompson 1988) Corpus: RST Discourse Treebank (Carlson et al., 2001) 385 documents from the Penn Treebank Penn Discourse Treebank (PDTB) (Prasad et al., 2008) Corpus: Wall Street Journal portion of the Penn Treebank Rhetorical Structure Theory (RST) • Discourse relations “describe the relations between text parts in functional terms, identifying both the transition point of a relation and the extent of the terms related.” (Mann and Thompson 1988) • Document as a tree structure. Rhetorical Structure Theory (RST) Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure. Rhetorical Structure Theory (RST) Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure EDU EDU EDU Elementary Discourse Unit (EDU) Evidence Cause Contrast Nucleus Nucleus Nucleus Satellite Mononuclear Multinuclear Rhetorical Structure Theory (RST) Units of discourse: RST • The syntactic constructions that encode a minimum unit of meaning and/or discourse function interpretable relative to a set of contexts. (Polanyi et al., 2004) • In practice: EDUs ~ primary clauses [Xerox Corp.’s third-quarter net income grew 6.2% on 7.3% higher revenue,] [earning mixed reviews from Wall Street analysts.] [Such trappings suggest a glorious past] [but give no hint of a troubled present.] [Deciding what constitutes “terrorism” can be a legalistic exercise.] ✘ [He said] [the thrift will try to get regulators to reverse the decision.] ✘ [Once inside, she spends fours hours measuring and diagramming each room in the 80-year-old house,...] ✘ Units of discourse: RST • Same content can be packaged into varied # of EDUs • [Xerox Corp.’s third-quarter net income grew 6.2% on 7.3% higher revenue.] [This earned mixed reviews from Wall Street analysts.] • [Xerox Corp’s third-quarter net income grew 6.2% on 7.3% higher revenue,] [which earned mixed reviews from Wall Street analysts.] • [Xerox Corp’s third-quarter net income grew 6.2% on 7.3% higher revenue,] [earning mixed reviews from Wall Street analysts.] • [The 6.2% growth of Xerox Corp.’s third-quarter net income on 7.3% higher revenue earned mixed reviews from Wall Street analysts.] Information salience • Certain spans are more important and is manifested in discourse structure Nuclearity example Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, but any liquid water formed in this way would evaporate almost instantly EDU EDU Evidence Cause Contrast Nucleus Nucleus Nucleus because of the low atmospheric pressure EDU Satellite Nuclearity example Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, because of the low atmospheric pressure EDU EDU Evidence Cause Contrast Nucleus Nucleus Satellite RST Parsing EDU segmentation Tree building • Problems? • Implicit relations • Cue phrases have non-discourse use • Ambiguous connectives Constructing discourse trees: ﬁrst attempt Segment text using punctuation and cue phrases Assign relation using cue phrases Build tree using only nuclear EDUs EDU segmentation • As a binary classiﬁcation task • Make a decision for each token (Soricut and Marcu, 2003; Fisher and Roark, 2007; Subba and Di Eugenio, 2009; Joty et al., 2015) • As a sequential labeling task • Find the most likely sequence • Conditional Random Fields (Hernault et al., 2010, Xuan Bach et al., 2012, Feng and Hirst 2014) [Some analysts are concerned , however ,] [that Banco Exterior may have waited too long] [to diversify from its traditional export-related activities .] C C C C C C B C C C C C C C B C C C C C C C Neural approach • Wang et al., EMNLP 2018 Performance Constructing discourse trees EDU segmentation Tree building Modern RST Parsing • Classiﬁcation+CKY (Hernault et al., 2010; Feng & Hirst, 2012) • Sequence labeling (Ghosh et al., 2012; Joty et al., 2013, Feng & Hirst, 2014) • Shift-reduce (Sagae, 2009; Ji & Eisenstein, 2014; Heilman & Sagae, 2015) • Representation learning (Ji & Eisenstein, 2014; Li et al., 2014) CIDER • Liu and Lapata, EMNLP 2017 • Two-stage approach: • parse each sentence in a document into a tree whose leaves correspond to EDUs, • then parse the document into a tree whose leaves correspond to already pre- processed sentences Intra-sentential parser • Linear-chain CRF • Separate CRFs for structure and relations Structure parser. C: constituent; L: latent structure nodes Intra-sentential parser • Linear-chain CRF • Separate CRFs for structure and relations Relation parser. C: constituent; R: relation nodes Representation Learning: intra-sentential • Learn EDU representations via LSTMs Document level parser • Can we do something similar? Representation Learning: doc-level Performance • How many things should we evaluate? • Tree skeleton (each subtree span) • Discourse relations • Separate nuclearity from discourse relations • Should we use gold-standard human EDU segmentation? Why/why not? Performance Catch! • What is it that this system will fail to handle? • Leaky units! ~5% in RST-DT, but much much more in other genres! Two theories Rhetorical Structure Theory (RST) (Mann and Thompson 1988) Corpus: RST Discourse Treebank (Carlson et al., 2001) 385 documents from the Penn Treebank Penn Discourse Treebank (PDTB) (Prasad et al., 2008) Corpus: Wall Street Journal portion of the Penn Treebank Penn Discourse Treebank (PDTB) • Discourse relations are “predicate-argument relations between two abstract objects such as events, states and propositions.” Mitsakaki et al. (2008) • Theory-neutral: a ﬂat, linear structure • Lexically grounded Penn Discourse Treebank (PDTB) The federal government suspended sales of U.S. savings bonds because Congress hasn’t lifted the ceiling on government debt. Explicit Arg1 Arg2 Penn Discourse Treebank (PDTB) Non- explicit Implicit I never gamble too far. Implicit=In particular I quit after one try, whether I win or lose. Explicit Penn Discourse Treebank (PDTB) Non- explicit Implicit Explicit AltLex So Seita has introduced blonde cigarettes under the Gauloises label, and intends to relaunch the unsuccessful Gitanes Blondes in new packaging. AltLex The aim is to win market share from imported cigarettes, and to persuade smokers who are switching to blonde cigarettes to keep buying French. Penn Discourse Treebank (PDTB) Non- explicit Implicit Explicit AltLex EntRel Proceeds from the offering are expected to be used for remodeling the company’s Desert Inn resort in Las Vegas, refurbishing certain aircraft of the MGM Grand Air unit, and to acquire the property for the new resort. EntRel The company said it estimates the Desert Inn remodeling will cost about $32 million, and the refurbishment of the three DC-8-62 aircraft, made by McDonnell Douglas Corp., will cost around $24.5 million. Penn Discourse Treebank (PDTB) Non- explicit Implicit Explicit AltLex EntRel NoRel Jacobs is an international engineering and construction concern. NoRel Total capital investment at the site could be as much as $400 million, according to Intel. Penn Discourse Treebank (PDTB) Shallow discourse parsing: PDTB Identify connectives Implicit relation classiﬁcation Extract arguments Classify relation Explicit relation classiﬁcation Explicit relation classiﬁcation Identify connectives Extract arguments Classify relation Explicit relation classiﬁcation • Cue phrases have non-discourse usage: • John and Mary went to the theatre and saw a nice play. • Connectives in PDTB with discourse usage: 30% of the time • They vary in frequency of non-discourse usage • or: 2.8% • although: 91.4% • Binary classiﬁcation problem • Achieved accuracy of >95% (Pitler and Nenkova 2009, Lin et al. 2014) Explicit relation classiﬁcation Identify connectives Extract arguments Classify relation Explicit relation classiﬁcation • Argument arrangement: • 60.9% in the same sentence • 30.1% arg1 in previous adjacent sentence of arg2 • 9% arg1 in previous non-adjacent sentence of arg2 • Same-sentence arrangement • arg1 arg2 • arg2 arg1 • [arg1 [arg2] ] • [arg2 [arg1] ] Explicit relation classiﬁcation Identify connectives Extract arguments Classify relation Explicit relation classiﬁcation Lin et al. 2014: Args in same sentence? Assign arg1 to previous adjacent sentence. Classify each internal node of syntactic tree: Arg1 root Arg2 root Not arg1 or arg2 N Y ~92% argument position F-score with error propagation Explicit relation classiﬁcation Identify connectives Extract arguments Classify relation Explicit relation classiﬁcation • Most connectives are not ambiguous (Pitler and Nenkova 2008) • Some frequent connectives are highly ambiguous • while, Comparison 66.1% • since, Contingency 52.2% • as, Temporal 70.3% • meanwhile, Temporal 48.7% • Effective classiﬁcation (86% F-score) using connective and neighboring token POS (Lin et al. 2014) Implicit relation classiﬁcation • Classiﬁcation on adjacent sentences within the same paragraph. • x: feature vector from two arguments • y: PDTB relations (1st level or 2nd level) + EntRel + NoRel • Neural networks with… • Sophisticated attention, multi-task learning, highway networks… • Very hard task; best system F-scores: • Comparison: ~48 • Contingency: ~59 • Expansion: ~73 • Temporal: ~39 • 11 2nd level relation average F: 45-48 Applications Automatic Summarization • Content selection in compressive summarization (Hirao et al., 2013; Kikuchi et al., 2014) • Connecting product aspects in review summarization (Gerani et al., 2014) Automatic Summarization Question Generation and Answering • Discourse structure helpful for answering non-factoid (“how” and “why”) questions (Jensen et al., 2014) • Generating “why” questions using discourse annotation (Agarwal et al., 2011; Prasad and Joshi, 2008) Automatic Summarization Question Generation and Answering Machine Translation • Sentential discourse structure for MT evaluation (Guzman et al., 2014) • Discourse factors highly impact the quality of MT outputs (Li et al., 2014) Automatic Summarization Question Generation and Answering Machine Translation Sentiment Analysis • Deep analysis of rhetorical structure helps with content weighting in document polarity classiﬁcation (Hogenboom et al., 2015) • Certain relations contain more sentiment expressions than others (Trnavac and Taboada 2013) "
166,"CS388: Natural Language Processing Lecture 24: Mul9linguality + Morphology Greg Durrett Administrivia ‣ Project 2 graded; average = 19.0 ‣ Final project presenta9ons next week ‣ See Canvas announcement for who is presen9ng when ‣ Can be “work in progress”, but there should be at least preliminary results ‣ Final reports due on December 14; no slip days Dealing with other languages ‣ Many algorithms so far have been developed for English ‣ Some structures like cons9tuency parsing don’t make sense for other languages ‣ Neural methods are typically tuned to English-scale resources, may not be the best for other languages where less data is available 1) What other phenomena / challenges do we need to solve? ‣ Ques9on: 2) How can we leverage exis9ng resources to do be]er in other languages without just annota9ng massive data? ‣ Other languages present some problems not seen in English at all! This Lecture ‣ Morphology: eﬀects and challenges ‣ Cross-lingual tagging and parsing ‣ Morphology tasks: analysis, inﬂec9on, word segmenta9on Morphology What is morphology? ‣ Study of how words form ‣ Deriva9onal morphology: create a new lexeme from a base estrange (v) => estrangement (n) become (v) => unbecoming (adj) I become / she becomes ‣ Inﬂec9onal morphology: word is inﬂected based on its context ‣ May not be totally regular: enﬂame => inﬂammable ‣ Mostly applies to verbs and nouns Morphological Inﬂec9on ‣ In English: I arrive you arrive he/she/it arrives we arrive you arrive they arrive [X] arrived ‣ In French: Morphological Inﬂec9on ‣ In Spanish: Noun Inﬂec9on ‣ Nomina9ve: I/he/she, accusa9ve: me/him/her, geni9ve: mine/his/hers ‣ Not just verbs either; gender, number, case complicate things I give the children a book <=> Ich gebe den Kindern ein Buch I taught the children <=> Ich unterrichte die Kinder ‣ Da9ve: merged with accusa9ve in English, shows recipient of something Irregular Inﬂec9on ‣ Common words are olen irregular ‣ I am / you are / she is ‣ However, less common words typically fall into some regular paradigm — these are somewhat predictable ‣ Je suis / tu es / elle est ‣ Yo soy / usted está / ella es Agglu9na9ng Langauges ‣ Finnish/Turkish/ Hungarian (Finno- Ugric): what a preposi9on would do in English is instead part of the verb ‣ Many possible forms — and in newswire data, only a few are observed illa9ve: “into” adessive: “on” Morphologically-Rich Languages ‣ Many languages spoken all over the world have much richer morphology than English (Chinese is the main excep9on) ‣ CoNLL 2006 / 2007: dependency parsing + morphological analyses for ~15 mostly Indo-European languages ‣ Word piece / byte-pair encoding models for MT are pre]y good at handling these if there’s enough data ‣ SPMRL shared tasks (2013-2014): Syntac9c Parsing of Morphologically- Rich Languages Morphologically-Rich Languages ‣ Great resources for challenging your assump9ons about language and for understanding mul9lingual models! Morphological Analysis/Inﬂec9on Morphological Analysis ‣ In English, not that many word forms, lexical features on words and word vectors are pre]y eﬀec9ve ‣ When we’re building systems, we probably want to know base form + morphological features explicitly ‣ In other languages, *lots* more unseen words! Aﬀects parsing, transla9on, … ‣ How to do this kind of morphological analysis? Morphological Analysis Ám a kormány egyetlen adó csökkentését sem javasolja . n=singular|case=nomina9ve|proper=no deg=posi9ve|n=singular|case=nomina9ve n=singular|case=nomina9ve|proper=no n=singular|case=accusa9ve|proper=no|pperson=3rd|pnumber=singular mood=indica9ve|t=present|p=3rd|n=singular|def=yes But the government does not recommend reducing taxes. ‣ Why is this useful? Morphological Analysis ‣ Given a word, need to recognize what its morphological features are ‣ Lots of work on Arabic inﬂec9on (high amounts of ambiguity) ‣ Basic approach: ‣ Lexicon: tells you what possibili9es are ‣ Analyzer: sta9s9cal model that disambiguates ‣ Models are largely CRF-like: score morphological features in context Predic9ng Inﬂec9on ‣ Other direc9on: given base form + features, inﬂect the word Durre] and DeNero (2013) ‣ Hard for unknown words — need models that generalize w i n d e n Predic9ng Inﬂec9on w i n d e n i i i i a ... i1 en e est et - ... en2 n - st t te ... n1 i1 n1 = = = = = = n1 en1 en2 to wind (de) en e st t - ... en1 ‣ Other direc9on: given base form + features, inﬂect the word ‣ Hard for unknown words — need models that generalize ‣ Take a bunch of exis9ng verbs from Wik9onary, extract these change rules using character alignments Durre] and DeNero (2013) ‣ Train a CRF with character n- gram context features to learn where to apply them Morphological Reinﬂec9on Chahuneau et al. (2013) ‣ Machine transla9on where phrase table is deﬁned in terms of lemmas ‣ “Translate-and-inﬂect”: translate into uninﬂected words and predict inﬂec9on based on source side Word Segmenta9on Morpheme Segmenta9on ‣ Can we do something unsupervised rather than these complicated analyses? Creutz and Lagus (2002) ‣ unbecoming => un+becom+ing — we should be able to recognize these common pieces and split them oﬀ ‣ How do weo do this? Morpheme Segmenta9on ‣ Simple probabilis9c model Creutz and Lagus (2002) ‣ p(mi) = count(token)/count(all tokens) ‣ Train with EM: E-step involves es9ma9ng best segmenta9on with Viterbi, M-step: collect token counts allowed expected need needed all+owe+d expe+cted n+e+ed ne+ed+ed E0 M0: ed has count 3 all+ow+ed expect+ed ne+ed ne+ed+ed ‣ Some heuris9cs: reject rare morphemes, one-le]er morphemes ‣ Doesn’t handle stem changes: becoming => becom + ing E1 Chinese Word Segmenta9on ‣ LSTMs over character embeddings / character bigram embeddings to predict word boundaries ‣ Some languages including Chinese are totally untokenized Chen et al. (2015) ‣ Having the right segmenta9on can help machine transla9on Cross-Lingual Tagging and Parsing Cross-Lingual Tagging ‣ Mul9lingual POS induc9on Snyder et al. (2008) ‣ Genera9ve model of two languages simultaneously, joint alignment + tag learning ‣ Complex genera9ve model, requires Gibbs sampling for inference Cross-Lingual Tagging ‣ We have resources for languages like English — can we use these more directly? Das and Petrov (2011) I like it a lot Je l’ aime beaucoup N V PR DT ADJ N PR V ?? ‣ Tag with English tagger, project across bitext, train French tagger? ‣ Can do something smarter Cross-Lingual Tagging Das and Petrov (2011) { I like it a lot Je l’ aime beaucoup N V PR DT ADJ { ‣ Form a graph of trigrams, use these to propagate knowledge about tags Cross-Lingual Tagging Das and Petrov (2011) I like it l’ aime beaucoup l’ adore un peu l’ adore beaucoup edge weights based on similarity of contexts these trigrams occur in I love it he loves it she loves it edge weights based on alignments (middle word must be aligned) ‣ Each node is associated with a distribu9on over tags, label propaga9on updates these using the graph Cross-Lingual Tagging Das and Petrov (2011) Cross-Lingual Tagging Das and Petrov (2011) ‣ Take these trigrams and treat them as “sol training examples” and learn an HMM tagger ‣ Label propaga9on: encourages nodes with higher-weight edges between them to have similar tags ‣ Prune to only keep tags above some probability to get the lexicon (valid tag-word pairs) Cross-Lingual Tagging Das and Petrov (2011) ‣ EM-HMM/feature HMM: unsupervised methods with a greedy mapping from learned tags to gold tags ‣ Projec9on: project tags across bitext to make pseudogold corpus, train on that Cross-Lingual Parsing McDonald et al. (2011) ‣ Now that we can POS tag other languages, can we parse them too? ‣ Direct transfer: train a parser over POS sequences in one language, then apply it to another language I like tomatoes PRON VERB NOUN Je les aime PRON PRON VERB I like them PRON VERB PRON ‣ Even though we've never seen this sequence in English and don’t know the words, we can s9ll ﬁgure it out Cross-Lingual Parsing McDonald et al. (2011) ‣ Mul9-dir: transfer a parser trained on several source treebanks to the target language ‣ Mul9-proj: more complex annota9on projec9on approach Cross-Lingual Embeddings Ammar et al. (2016) ‣ mul9Cluster: use bilingual dic9onaries to form clusters of words that are transla9ons of one another, replace corpora with cluster IDs, train “monolingual” embeddings over all these corpora ‣ mul9CCA: “project” all other languages into English ‣ CCA: learn a projec9on of aligned data points into a shared space ‣ Learn a shared mul9lingual embedding space so any neural system can transfer over Cross-Lingual Embeddings Ammar et al. (2016) ‣ Word vectors work pre]y well at “intrinsic” tasks, some improvement on things like document classiﬁca9on and dependency parsing as well Where are we now? ‣ Universal dependencies: treebanks (+ tags) for 70+ languages ‣ Many languages are s9ll small, so projec9on techniques may s9ll help ‣ More corpora in other languages, less and less reliance on structured tools like parsers, and pretraining on unlabeled data means that performance on other languages is be]er than ever ‣ BERT has pretrained mul9lingual models that seem to work pre]y well (trained on a whole bunch of languages) Takeaways ‣ Many languages have richer morphology than English and pose dis9nct challenges ‣ Problems: how to analyze rich morphology, how to generate with it ‣ Can leverage resources for English using bitexts ‣ Next 9me: wrapup + ethics of NLP "
17,"School of Computer Science 1 Probabilistic Graphical Models Approximate Inference: Markov Chain Monte Carlo Eric Xing Lecture 17, March 19, 2014 X1 X2 X3 0.25 0.7 0.5 0.5 0.75 0.3 © Eric Xing @ CMU, 2005-2014 Recap of Monte Carlo Monte Carlo methods are algorithms that:  Generate samples from a given probability distribution  Estimate expectations of functions under a distribution Why is this useful?  Can use samples of to approximate itself  Allows us to do graphical model inference when we can’t compute  Expectations reveal interesting properties about  e.g. means and variances of 2 ) (x p )] ( [ x f E ) (x p ) (x p ) (x p )] ( [ x f E ) (x p ) (x p ) (x p © Eric Xing @ CMU, 2005-2014 Limitations of Monte Carlo Direct sampling  Hard to get rare events in high-dimensional spaces  Infeasible for MRFs, unless we know the normalizer Z Rejection sampling, Importance sampling  Do not work well if the proposal Q(x) is very different from P(x)  Yet constructing a Q(x) similar to P(x) can be difficult  Making a good proposal usually requires knowledge of the analytic form of P(x) – but if we had that, we wouldn’t even need to sample! Intuition: instead of a fixed proposal Q(x), what if we could use an adaptive proposal? 3 © Eric Xing @ CMU, 2005-2014 Markov Chain Monte Carlo MCMC algorithms feature adaptive proposals  Instead of Q(x’), they use Q(x’|x) where x’ is the new state being sampled, and x is the previous sample  As x changes, Q(x’|x) can also change (as a function of x’) 4 P(x) Q(x) Importance sampling with a (bad) proposal Q(x) P(x) Q(x2|x1) MCMC with adaptive proposal Q(x’|x) x1 x2 Q(x3|x2) x3 Q(x4|x3) x1 x2 x3 © Eric Xing @ CMU, 2005-2014 Metropolis-Hastings Let’s see how MCMC works in practice  Later, we’ll look at the theoretical aspects Metropolis-Hastings algorithm  Draws a sample x’ from Q(x’|x), where x is the previous sample  The new sample x’ is accepted or rejected with some probability A(x’|x)  This acceptance probability is  A(x’|x) is like a ratio of importance sampling weights  P(x’)/Q(x’|x) is the importance weight for x’, P(x)/Q(x|x’) is the importance weight for x  We divide the importance weight for x’ by that of x  Notice that we only need to compute P(x’)/P(x) rather than P(x’) or P(x) separately  A(x’|x) ensures that, after sufficiently many draws, our samples will come from the true distribution P(x) – we shall learn why later in this lecture 5          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A © Eric Xing @ CMU, 2005-2014 6 The MH Algorithm 1. Initialize starting state x(0), set t =0 2. Burn-in: while samples have “not converged”  x=x(t)  t =t +1,  sample x* ~ Q(x*|x) // draw from proposal  sample u ~ Uniform(0,1) // draw acceptance threshold  - if  x(t) = x* // transition - else  x(t) = x // stay in current state  Take samples from P(x) = : Reset t=0, for t =1:N  x(t+1) Draw sample (x(t))           ) | * ( ) ( *) | ( *) ( , 1 min ) | * ( x x Q x P x x Q x P x x A u Function Draw sample (x(t)) © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 7 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) … x0 Q(x1|x0) © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 8 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 x0 Q(x1|x0) x1 © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 9 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 x0 Q(x2|x1) x1 x2 © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 10 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 Draw but reject; set x3=x2 x0 Q(x3|x2) x1 x2 x’ (rejected) x3 © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 11 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 Draw but reject; set x3=x2 x0 Q(x3|x2) x1 x2 x’ (rejected) x3 We reject because P(x’)/Q(x’|x2) < 1 and P(x2)/Q(x2|x’) > 1, hence A(x’|x2) is close to zero! © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 12 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 Draw but reject; set x3=x2 Draw, accept x4 x0 Q(x3|x2) x1 x2 x3 x4 © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 13 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 Draw but reject; set x3=x2 Draw, accept x4 Draw, accept x5 x0 Q(x3|x2) x1 x2 x3 x4 x5 © Eric Xing @ CMU, 2005-2014 The MH Algorithm Example:  Let Q(x’|x) be a Gaussian centered on x  We’re trying to sample from a bimodal distribution P(x) 14 P(x)          ) | ' ( ) ( ) ' | ( ) ' ( , 1 min ) | ' ( x x Q x P x x Q x P x x A Initialize x(0) Draw, accept x1 Draw, accept x2 Draw but reject; set x3=x2 Draw, accept x4 Draw, accept x5 x0 Q(x3|x2) x1 x2 x3 x4 x5 The adaptive proposal Q(x’|x) allows us to sample both modes of P(x)! © Eric Xing @ CMU, 2005-2014 Theoretical aspects of MCMC The MH algorithm has a “burn-in” period  Why do we throw away samples from burn-in? Why are the MH samples guaranteed to be from P(x)?  The proposal Q(x’|x) keeps changing with the value of x; how do we know the samples will eventually come from P(x)? What is the connection between Markov Chains and MCMC? 15 © Eric Xing @ CMU, 2005-2014 Markov Chains A Markov Chain is a sequence of random variables x(1),x(2),…,x(n) with the Markov Property  is known as the transition kernel  The next state depends only on the preceding state – recall HMMs!  Note: the r.v.s x(i) can be vectors  We define x(t) to be the t-th sample of all variables in a graphical model  X(t) represents the entire state of the graphical model at time t We study homogeneous Markov Chains, in which the transition kernel is fixed with time  To emphasize this, we will call the kernel , where x is the previous state and x’ is the next state 16 ) | ( ) , , | ( ) 1 ( ) ( ) 1 ( ) 1 ( ) (      n n n n x x x P x x x x P  ) | ( ) 1 ( ) (   t t x x x P ) | ( x x T  ) | ( ) 1 ( ) (   n n x x x P © Eric Xing @ CMU, 2005-2014 MC Concepts To understand MCs, we need to define a few concepts:  Probability distributions over states: is a distribution over the state of the system x, at time t  When dealing with MCs, we don’t think of the system as being in one state, but as having a distribution over states  For graphical models, remember that x represents all variables  Transitions: recall that states transition from x(t) to x(t+1) according to the transition kernel . We can also transition entire distributions:  At time t, state x has probability mass π(t)(x). The transition probability redistributes this mass to other states x’.  Stationary distributions: is stationary if it does not change under the transition kernel: 17 ) ( ) ( x t       x t t x x T x x ) | ( ) ( ) ( ) ( ) 1 (   ) | ( x x T  ) (x      x x x T x x ) | ( ) ( ) (   for all x’ © Eric Xing @ CMU, 2005-2014 MC Concepts Stationary distributions are of great importance in MCMC. To understand them, we need to define some notions:  Irreducible: an MC is irreducible if you can get from any state x to any other state x’ with probability > 0 in a finite number of steps  i.e. there are no unreachable parts of the state space  Aperiodic: an MC is aperiodic if you can return to any state x at any time  Periodic MCs have states that need ≥2 time steps to return to (cycles)  Ergodic (or regular): an MC is ergodic if it is irreducible and aperiodic Ergodicity is important: it implies you can reach the stationary distribution , no matter the initial distribution  All good MCMC algorithms must satisfy ergodicity, so that you can’t initialize in a way that will never converge 18 ) ( ) 0 ( x  ) (x st  © Eric Xing @ CMU, 2005-2014 MC Concepts Reversible (detailed balance): an MC is reversible if there exists a distribution such that the detailed balance condition is satisfied:  Probability of x’→x and x→x’ can be different, but the joint of x amd x’ remain the same, no matter which direction to go Reversible MCs always have a stationary distribution! Proof:  The last line is the definition of a stationary distribution! 19                     x x x x x x x T x x x x T x x x T x x x T x x x T x x x T x x x T x ) | ( ) ( ) ( ) | ( ) ( ) | ( ) ( ) | ( ) ( ) | ( ) ( ) | ( ) ( ) | ( ) (         ) (x  ) | ( ) ( ) | ( ) ( x x T x x x T x       © Eric Xing @ CMU, 2005-2014 Why does Metropolis-Hastings work? Recall that we draw a sample x’ according to Q(x’|x), and then accept/reject according to A(x’|x).  In other words, the transition kernel is We can prove that MH satisfies detailed balance  Recall that  Notice this implies the following: 20 ) | ' ( ) | ' ( ) | ( x x A x x Q x x T               ) | ( ) ( ) | ( ) ( , 1 min ) | ' ( x x Q x P x x Q x P x x A 1 ) | ' (  x x A 1 ) | ( ) ( ) | ( ) (     x x Q x P x x Q x P if then and thus 1 ) ' | (  x x A © Eric Xing @ CMU, 2005-2014 Why does Metropolis-Hastings work? Now suppose A(x’|x) < 1 and A(x|x’) = 1. We have The last line is exactly the detailed balance condition  In other words, the MH algorithm leads to a stationary distribution P(x)  Recall we defined P(x) to be the true distribution of x  Thus, the MH algorithm eventually converges to the true distribution! 21 ) | ( ) ( ) | ( ) ( ) | ( ) | ( ) ( ) | ' ( ) | ( ) ( ) | ( ) ( ) | ' ( ) | ( ) ( ) | ( ) ( ) | ( ) ( ) | ' ( x x T x P x x T x P x x A x x Q x P x x A x x Q x P x x Q x P x x A x x Q x P x x Q x P x x Q x P x x A                  © Eric Xing @ CMU, 2005-2014 1 ) | ' (  x x A 1 ) | ( ) ( ) | ( ) (     x x Q x P x x Q x P if then and thus 1 ) ' | (  x x A Caveats Although MH eventually converges to the true distribution P(x), we have no guarantees as to when this will occur  The burn-in period represents the un-converged part of the Markov Chain – that’s why we throw those samples away!  Knowing when to halt burn-in is an art. We will look at some techniques later in this lecture. 22 © Eric Xing @ CMU, 2005-2014 Gibbs Sampling Gibbs Sampling is an MCMC algorithm that samples each random variable of a graphical model, one at a time  GS is a special case of the MH algorithm GS algorithms…  Are fairly easy to derive for many graphical models (e.g. mixture models, Latent Dirichlet allocation)  Have reasonable computation and memory requirements, because they sample one r.v. at a time  Can be Rao-Blackwellized (integrate out some r.v.s) to decrease the sampling variance 23 © Eric Xing @ CMU, 2005-2014 Gibbs Sampling The GS algorithm: 1. Suppose the graphical model contains variables x1,…,xn 2. Initialize starting values for x1,…,xn 3. Do until convergence: 1. Pick an ordering of the n variables (can be fixed or random) 2. For each variable xi in order: 1. Sample x from P(xi | x1, …, xi-1, xi+1, …, xn), i.e. the conditional distribution of xi given the current values of all other variables 2. Update xi ← x When we update xi, we immediately use its new value for sampling other variables xj 24 © Eric Xing @ CMU, 2005-2014 Markov Blankets The conditional P(xi | x1, …, xi-1, xi+1, …, xn) looks intimidating, but recall Markov Blankets:  Let MB(xi) be the Markov Blanket of xi, then For a BN, the Markov Blanket of x is the set containing its parents, children, and co-parents For an MRF, the Markov Blanket of x is its immediate neighbors 25 )) ( | ( ) , , , , , | ( 1 1 1 i i n i i i x MB x P x x x x x P      © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example Consider the alarm network  Assume we sample variables in the order B,E,A,J,M  Initialize all variables at t = 0 to False 26 t B E A J M 0 F F F F F 1 2 3 4 © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Sampling P(B|A,E) at t = 1: Using Bayes Rule,  (A,E) = (F,F), so we compute the following, and sample B = F 27 t B E A J M 0 F F F F F 1 F 2 3 4 ) ( ) , | ( ) , | ( B P E B A P E A B P  9980 . 0 ) 999 . 0 )( 999 . 0 ( ) , | ( 0006 . 0 ) 01 . 0 )( 06 . 0 ( ) , | (           F E F A F B P F E F A T B P © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Sampling P(E|A,B): Using Bayes Rule,  (A,B) = (F,F), so we compute the following, and sample E = T 28 t B E A J M 0 F F F F F 1 F T 2 3 4 ) ( ) , | ( ) , | ( E P E B A P B A E P  9970 . 0 ) 998 . 0 )( 999 . 0 ( ) , | ( 0142 . 0 ) 02 . 0 )( 71 . 0 ( ) , | (           F B F A F E P F B F A T E P © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Sampling P(A|B,E,J,M): Using Bayes Rule,  (B,E,J,M) = (F,T,F,F), so we compute the following, and sample A = F 29 t B E A J M 0 F F F F F 1 F T F 2 3 4 ) , | ( ) | ( ) | ( ) , , , | ( E B A P A M P A J P M J E B A P  6678 . 0 ) 71 . 0 )( 99 . 0 )( 95 . 0 ( ) , , , | ( 0087 . 0 ) 29 . 0 )( 3 . 0 )( 1 . 0 ( ) , , , | (               F M F J T E F B F A P F M F J T E F B T A P © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Sampling P(J|A): No need to apply Bayes Rule  A = F, so we compute the following, and sample J = T 30 t B E A J M 0 F F F F F 1 F T F T 2 3 4 95 . 0 ) | ( 05 . 0 ) | (       F A F J P F A T J P © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Sampling P(M|A): No need to apply Bayes Rule  A = F, so we compute the following, and sample M = F 31 t B E A J M 0 F F F F F 1 F T F T F 2 3 4 99 . 0 ) | ( 01 . 0 ) | (       F A F M P F A T M P © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Now t = 2, and we repeat the procedure to sample new values of B,E,A,J,M … 32 t B E A J M 0 F F F F F 1 F T F T F 2 F T T T T 3 4 © Eric Xing @ CMU, 2005-2014 Gibbs Sampling: An Example  Now t = 2, and we repeat the procedure to sample new values of B,E,A,J,M …  And similarly for t = 3, 4, etc. 33 t B E A J M 0 F F F F F 1 F T F T F 2 F T T T T 3 T F T F T 4 T F T F F © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs sampling  Popular inference algorithm for topic models  Integrate out topic vectors π and topics B  Only need to sample word-topic assignments z Algorithm: For all variables z = z1, z2, …, zn Draw zi (t+1) from P(zi|z-i, w) where z-i = z1 (t+1), z2 (t+1),…, zi-1 (t+1), zi+1 (t), …, zn (t) Topic Models: Collapsed Gibbs (Tom Griffiths & Mark Steyvers) 34 © Eric Xing @ CMU, 2005-2014 B K π w z M N β α Collapsed Gibbs sampling What is P(zi|z-i, w)?  It is a product of two Dirichlet-Multinomial conditional distributions: © Eric Xing @ CMU, 2005-2014 35 “word-topic” term “doc-topic” term Collapsed Gibbs sampling What is P(zi|z-i, w)?  It is a product of two Dirichlet-Multinomial conditional distributions: © Eric Xing @ CMU, 2005-2014 36 # word positions a (excluding wi) such that: wa = wi za = j # word positions a in the current document di (excluding wi) such that: za = j # word positions a (excluding wi) such that: za = j # word positions a in the current document di (excluding wi) Collapsed Gibbs illustration i wi di zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 iteration 1 37 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 ? iteration 1 2 38 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 ? iteration 1 2 39 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 ? iteration 1 2 40 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 2 ? iteration 1 2 41 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 2 1 ? iteration 1 2 42 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 2 1 1 ? iteration 1 2 43 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 2 1 1 2 ? iteration 1 2 44 © Eric Xing @ CMU, 2005-2014 Collapsed Gibbs illustration i wi di zi zi zi 1 2 3 4 5 6 7 8 9 10 11 12 . . . 50 MATHEMATICS KNOWLEDGE RESEARCH WORK MATHEMATICS RESEARCH WORK SCIENTIFIC MATHEMATICS WORK SCIENTIFIC KNOWLEDGE . . . JOY 1 1 1 1 1 1 1 1 1 1 2 2 . . . 5 2 2 1 2 1 2 2 1 2 1 1 1 . . . 2 2 1 1 2 2 2 2 1 2 2 1 2 . . . 1 … 2 2 2 1 2 2 2 1 2 2 2 2 . . . 1 iteration 1 2 … 1000 45 © Eric Xing @ CMU, 2005-2014 Gibbs Sampling is a special case of MH The GS proposal distribution is  Where x-i denotes all variables except xi Applying MH to this proposal, we find that samples are always accepted (which is exactly what GS does):  GS is simply MH with a proposal that is always accepted! 46 ) | ( ) , | , ( i i i i i i x P x x Q       x x x  1 1 , 1 min ) | ( ) ( ) | ( ) | ( ) ( ) | ( , 1 min ) | ( ) , ( ) | ( ) , ( , 1 min ) , | , ( ) , ( ) , | , ( ) , ( , 1 min ) , | , (                                                        i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i x P P x P x P P x P x P x P x P x P x x Q x P x x Q x P x x A x x x x x x x x x x x x x x x x x x © Eric Xing @ CMU, 2005-2014 Practical Aspects of MCMC How do we know if our proposal Q(x’|x) is any good?  Monitor the acceptance rate  Plot the autocorrelation function How do we know when to stop burn-in?  Plot the sample values vs time  Plot the log-likelihood vs time 47 © Eric Xing @ CMU, 2005-2014 Acceptance Rate Choosing the proposal Q(x’|x) is a tradeoff:  “Narrow”, low-variance proposals have high acceptance, but take many iterations to explore P(x) fully because the proposed x are too close  “Wide”, high-variance proposals have the potential to explore much of P(x), but many proposals are rejected which slows down the sampler A good Q(x’|x) proposes distant samples x’ with a sufficiently high acceptance rate 48 P(x) Q(x’|x) Low-variance proposal P(x) Q(x’|x) High-variance proposal © Eric Xing @ CMU, 2005-2014 Acceptance Rate Acceptance rate is the fraction of samples that MH accepts.  General guideline: proposals should have ~0.5 acceptance rate [1] Gaussian special case:  If both P(x) and Q(x’|x) are Gaussian, the optimal acceptance rate is ~0.45 for D=1 dimension and approaches ~0.23 as D tends to infinity [2] 49 [1] Muller, P. (1993). “A Generic Approach to Posterior Integration and Gibbs Sampling” [2] Roberts, G.O., Gelman, A., and Gilks, W.R. (1994). “Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms” P(x) Q(x’|x) Low-variance proposal P(x) Q(x’|x) High-variance proposal © Eric Xing @ CMU, 2005-2014 Autocorrelation function MCMC chains always show autocorrelation (AC)  AC means that adjacent samples in time are highly correlated We quantify AC with the autocorrelation function of an r.v. x: 50 Low autocorrelation High autocorrelation            k n t t k n t k t t x x x x x x x k R 1 2 1 ) ( ) )( ( ) ( © Eric Xing @ CMU, 2005-2014 Autocorrelation function The first-order AC Rx(1) can be used to estimate the Sample Size Inflation Factor (SSIF):  If we took n samples with SSIF sx, then the effective sample size is n/sx  High autocorrelation leads to smaller effective sample size!  We want proposals Q(x’|x) with low autocorrelation 51 Low autocorrelation High autocorrelation            k n t t k n t k t t x x x x x x x k R 1 2 1 ) ( ) )( ( ) ( ) 1 ( 1 ) 1 ( 1 x x x R R s    © Eric Xing @ CMU, 2005-2014 Sample Values vs Time Monitor convergence by plotting samples (of r.v.s) from multiple MH runs (chains)  If the chains are well-mixed (left), they are probably converged  If the chains are poorly-mixed (right), we should continue burn-in 52 Well-mixed chains Poorly-mixed chains © Eric Xing @ CMU, 2005-2014 Log-likelihood vs Time Many graphical models are high-dimensional  Hard to visualize all r.v. chains at once Instead, plot the complete log-likelihood vs. time  The complete log-likelihood is an r.v. that depends on all model r.v.s  Generally, the log-likelihood will climb, then eventually plateau 53 Not converged Converged © Eric Xing @ CMU, 2005-2014 Summary Markov Chain Monte Carlo methods use adaptive proposals Q(x’|x) to sample from the true distribution P(x) Metropolis-Hastings allows you to specify any proposal Q(x’|x)  But choosing a good Q(x’|x) requires care Gibbs sampling sets the proposal Q(x’|x) to the conditional distribution P(x’|x)  Acceptance rate always 1!  But remember that high acceptance usually entails slow exploration  In fact, there are better MCMC algorithms for certain models Knowing when to halt burn-in is an art 54 © Eric Xing @ CMU, 2005-2014 "
18,"School of Computer Science © Eric Xing @ CMU, 2005-2014 1 Probabilistic Graphical Models Approximate Inference: Advanced Topics in MCMC Eric Xing Lecture 18, March 24, 2014 Recap of MCMC Markov Chain Monte Carlo methods use adaptive proposals Q(x’|x) to sample from the true distribution P(x) Metropolis-Hastings allows you to specify any proposal Q(x’|x)  But choosing a good Q(x’|x) requires care Gibbs sampling sets the proposal Q(x’|x) to the conditional distribution P(x’|x)  Acceptance rate always 1!  But remember that high acceptance usually entails slow exploration  In fact, there are better MCMC algorithms for certain models Knowing when to halt burn-in is an art © Eric Xing @ CMU, 2005-2014 2 Auxiliary Variables Advanced MCMC algorithms rely on auxiliary variables  Auxiliary variables are extra r.v.s not from the original model  They are random-valued intermediate quantities that allow us to sample model r.v.s in creative ways Suppose x is an r.v. and v is an a.v.. Generally, we use a.v.s when:  P(x|v) and P(v|x) have simple forms  P(x,v) is easy to navigate © Eric Xing @ CMU, 2005-2014 3 Slice Sampling Slice sampling is an auxiliary variable MCMC algorithm  Key idea: uniformly sample the area under P’(x) = aP(X), instead of P(x)  Never evaluate expensive P(x), only evaluate cheap P’(x) © Eric Xing @ CMU, 2005-2014 4 P’(x) h x P(x) Slice Sampling When is Slice sampling useful?  Ex: Markov Random Fields where P(x) = (1/a) * exp(bx)  Normalizer (1/a) usually intractable to evaluate!  Slice sampling only requires (easy) evaluation of P’(x) = exp(bx) © Eric Xing @ CMU, 2005-2014 5 P’(x) h x P(x) Slice Sampling Slice sampling uses an a.v. h (in addition to the r.v. x)  The pair (x,h) is the position of the sampler in the area under P’(x) We only need to know P’(x) = aP(x) for some unknown a The algorithm iterates between two steps:  Step 1: sample h from  Step 2: sample x from © Eric Xing @ CMU, 2005-2014 6 )] ( ' , 0 [ ) | ( x P Uniform x h Q   ) | ( h x Q 1 0 h x P  ) ( ' otherwise if P’(x) h x (x,hold) (x,h) P’(x) h x (x,h) (xold,h) Step 1 Step 2 (uniform dist. on all x s.t. P’(x)≥h) Slice Sampling The algorithm iterates between two steps:  Step 1: sample h from  Step 2: sample x from Step 2 requires finding the set {x s.t. P’(x)≥h}  Alternative 1: rejection sampling (reject whenever we get x s.t. P’(x)<h)  Alternative 2: “Bracketing” technique (to be presented shortly) © Eric Xing @ CMU, 2005-2014 7 )] ( ' , 0 [ ) | ( x P Uniform x h Q   ) | ( h x Q 1 0 h x P  ) ( ' otherwise if P’(x) h x (x,hold) (x,h) P’(x) h x (x,h) (xold,h) Step 1 Step 2 (uniform dist. on all x s.t. P’(x)≥h) Why does this work? At convergence, the samples (x,h) will be uniformly distributed under the area of P’(x) If we marginalize out h, we get samples from P(x) = (1/a)P’(x)  Never needed to evaluate normalizer (1/a)! © Eric Xing @ CMU, 2005-2014 8 P’(x) h x P(x) Samples from P(x) Why does this work? How to marginalize out h?  We have samples (x1,h1), (x1,h2), (x2,h2), (x2,h3), …  Marginalization is just dropping h from the samples  After dropping h, left with x1, x2, x3, … which are samples from P(x)! © Eric Xing @ CMU, 2005-2014 9 P’(x) h x P(x) Samples from P(x) Handling difficult Q(x|h)  Step 2 (sampling Q(x|h)) may not be easy  For complex distributions, cannot analytically find {x s.t. P’(x) ≥ h}  However, we can still easily evaluate P’(x) at any x…  Solution: “bracketing” strategy 1. Draw a random bracket width w, and place the bracket on (xold,h) 2. Expand the bracket until the endpoints a, b are “above P’(x)”: i.e. P’(a) < h and P’(b) < h 3. Uniformly sample from within the bracket (reject samples x s.t. P’(x) < h) © Eric Xing @ CMU, 2005-2014 10 P’(x) h x w (xold,h) Satisfies detailed balance, but not as efficient because the brackets can miss other modes How to Sample from Different Model Spaces? Detailed Balance Why we need detailed balance?  Stationary distribution !  Then how can such a handle the following case? 11 ) | ( ) ( ) | ( ) ( x x T x x x T x       © Eric Xing @ CMU, 2005-2014 2 clusters 3 clusters ) (x  ) (x  Reversible Jump MCMC An MCMC algorithm that allows for model selection  Examples: choosing # clusters K, or even switching between two completely different models P1(x) and P2(x) © Eric Xing @ CMU, 2005-2014 12 2 clusters 3 clusters RJMCMC Definitions:  x – model r.v.s (the number of x’s can change depending on the model)  u – auxiliary variables used to perform “dimension matching”  m – an indicator representing which model we are currently using  P(x|m) – probability distribution for r.v.s x assuming model m RJMCMC uses two types of proposal distribution:  j(m’|m) – model proposal; switches from model m to m’. Must be reversible!  q(x’,u’|m→m’,x,u) – data proposal; proposes (x’,u’) under the new model m’, starting from (x,u) under the previous model m RJMCMC also requires a mapping function:  hm,m’(x,u) – explains how (x,u) under model m maps to (x’,u’) under m’ © Eric Xing @ CMU, 2005-2014 13 The mapping function h() Properties of hm,m’(x,u):  Is deterministic (non-random)  Takes a vector (x,u) as input, and outputs a vector (x’,u’)  Dimension of x is usually different from x’ (and likewise for u,u’)  Must be bijective (one-to-one) so that its inverse is well-defined Simple example: switching from 2 clusters to 3 clusters  Let x1, x2 be the first 2 cluster centers  Randomly draw an a.v. u to be the 3rd cluster center  Then  i.e. h2,3() maps a 2-cluster model to a 3-cluster model by setting the 3rd cluster center x3’ to u (dimension matching) © Eric Xing @ CMU, 2005-2014 14                  u x x x x x u x x h 3 2 2 1 1 2 1 3 , 2 ) , , ( RJMCMC Algorithm 1. Initialize x,u,m 2. Repeat until convergence: 1. Propose a new model m’ using j(m’|m) 2. Propose a new model state (x’,u’) using q(x’,u’|m→m’,x,u) 3. Compute the acceptance probability: © Eric Xing @ CMU, 2005-2014 15 Ratio of model probs. Inv. ratio of model proposals Inv. ratio of data proposals Absolute value of the determinant of the Jacobian of h() Equivalent to MH algorithm’s inv. ratio of proposals Q(x|x’)/Q(x’|x)                               ) , ( ) , ( det ) , , | , ( ) , , | , ( ) | ( ) | ( ) | ( ) | ( , 1 min ) , , | , , ( , u x u x h u x m m u x q u x m m u x q m m j m m j m x P m x P u x m u x m A m m The abs-det-Jacobian term A “Jacobian” is a matrix of all 1st derivatives  Example: 2-clusters to 3-clusters; recall © Eric Xing @ CMU, 2005-2014 16 ) , ( ) , ( det , u x u x h m m                                                       1 0 0 0 1 0 0 0 1 / / / / / / / / / ) , , ( ) , , ( 3 2 3 1 3 2 2 2 1 2 1 2 1 1 1 2 1 2 1 3 , 2 u x x x x x u x x x x x u x x x x x u x x u x x h                  u x x x x x u x x h 3 2 2 1 1 2 1 3 , 2 ) , , ( The Jacobian is thus 1 ) , , ( ) , , ( det 2 1 2 1 3 , 2    u x x u x x h In general, we construct h() so that the abs-det-Jacobian term is trivial (e.g. 1) The Jacobian term Why do we need the Jacobian?  It arises from a change of variables during integration!  Consider the detailed balance equation; take integrals on both sides:  g() combines the model proposal j() and the data proposal q()  For simplicity, we omit the model indicator m, because the dimensionality of (x,u) completely identifies which model m the system is in  Now perform a change of variables from (x’,u’) to (x,u) on the RHS:  The equation above holds if, for all x,x’,u,u’, © Eric Xing @ CMU, 2005-2014 17 ) , ( ) , ( det , u x u x h m m                  u d x d u x u x A u x u x g x P dxdu u x u x A u x u x g x P ) , | , ( ) , | , ( ) ( ) , | , ( ) , | , ( ) (                 dxdu u x u x h u x u x A u x u x g x P dxdu u x u x A u x u x g x P u x u x ) , ( ) , ( det ) , | , ( ) , | , ( ) ( ) , | , ( ) , | , ( ) ( ) , ( ), , ( ) , ( ) , ( det ) , | , ( ) , | , ( ) ( ) , | , ( ) , | , ( ) ( ) , ( ), , ( u x u x h u x u x A u x u x g x P u x u x A u x u x g x P u x u x               The Jacobian term Why do we need the Jacobian?  The detailed balance condition holds if, for all x,x’,u,u’,  We can now construct an acceptance probability that satisfies detailed balance (see previous lecture, MH algorithm):  Restoring the model indicator m, we get © Eric Xing @ CMU, 2005-2014 18 ) , ( ) , ( det , u x u x h m m    ) , ( ) , ( det ) , | , ( ) , | , ( ) ( ) , | , ( ) , | , ( ) ( ) , ( ), , ( u x u x h u x u x A u x u x g x P u x u x A u x u x g x P u x u x                                     ) , ( ) , ( det ) , | , ( ) , | , ( ) ( ) ( , 1 min ) , | , ( ) , ( ), , ( u x u x h u x u x g u x u x g x P x P u x u x A u x u x                               ) , ( ) , ( det ) , , | , ( ) , , | , ( ) | ( ) | ( ) | ( ) | ( , 1 min ) , , | , , ( , u x u x h u x m m u x q u x m m u x q m m j m m j m x P m x P u x m u x m A m m Question: What is our stationary distribution in our RJMCMC? © Eric Xing @ CMU, 2005-2014 19 RJMCMC Example: Clustering Models: Let m = 1,2,3,… denote the number of clusters  P(x,c|m) - probability of (observed) data x and (unknown) cluster centers c, assuming m clusters  Can be a Gaussian mixture model or any other clustering model. For this example, we don’t need to know its exact form. Proposal distributions:  j(m’|m) – switches from m to m’ clusters, where m’ = {m-1,m,m+1}  m’ = m-1 is used to decrease the number of clusters  m’ = m+1 is used to increase the number of clusters  m’ = m is used to change cluster centers c  q(x’,c’,u’|m→m’,x,c,u) – form differs depending on m’ and m  hm,m’(c,u) – again, form differs depending on m’ and m  abs-det-Jacobian – turns out that this is always 1! © Eric Xing @ CMU, 2005-2014 20 RJMCMC Example: Clustering © Eric Xing @ CMU, 2005-2014 21 x1 x2 xn … c1 cm-1 … cm x3 x1 x2 xn … c1 cm-1 … cm x3 m’=m-1 m’=m m’=m+1 Remove cluster (e.g. cm) Change cluster center(e.g. c1) Add cluster x1 x2 xn … c1 cm-1 … cm x3 cm+1 Starting state: m cluster centers x1 x2 xn … c1 cm-1 … x3 cm RJMCMC Example: Clustering We set j() as follows: For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m (change cluster center):  u, u’ are used to change the value of some ci  First, pick a cluster center i in {1,…,m} to change assignment (at uniform)  Next, draw a new cluster center u according to some proposal qcenter(u)  Finally, set c’i = u © Eric Xing @ CMU, 2005-2014 22                  1 if 5 . 0 if 2 1 if 5 . 0 ) | ( m m p m m p m m p m m j “Explore cluster centers c 2p of the time, change the number of clusters 1-2p of the time” ) ( 1 ) , , , | , , ( u q m u c x m m u c x q center                               u c c c u c h m m m m i  2 1 , , ) , ( where c’j = cj if j ≠ i, and c’i = u, and u’ = ci and Notice that reverse moves have the same probability as forward moves RJMCMC Example: Clustering For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m (change cluster center):  What does the abs-det-Jacobian look like?  Recall that hi,m,m’=m(c,u) sets c’j = cj for all j ≠ i, and c’i = u, and u’ = ci  Let’s say we’re changing ci, where i = m © Eric Xing @ CMU, 2005-2014 23                                                                                        0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 / / / / / / / / / / / / / / / / ) , ( ) , ( 2 1 2 1 2 2 2 2 1 2 1 1 2 1 1 1 , ,           u u c u c u c u u c c c c c c c u c c c c c c c u c c c c c c c u c u c h m m m m m m m m m m m m i 1 1 ) , ( ) , ( det , ,         u c u c h m m m m i Therefore In fact, the abs-det-Jacobian is 1 for any choice of i! RJMCMC Example: Clustering For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m-1 (remove a cluster):  u is empty, and u’ matches the cluster to be removed  Pick a cluster center i in {1,…,m} to remove (at uniform) © Eric Xing @ CMU, 2005-2014 24 m u c x m m u c x q 1 ) , , , | , , (                                u c c c u c h m m m m i 1 2 1 1 , , ) , (  where c’j = cj if j < i, and c’j = cj+1 if j > i, and u’ = ci and RJMCMC Example: Clustering For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m-1 (remove a cluster):  For the Jacobian, let’s assume we’re removing cluster ci where i = m  Thus we set c’j = cj for all j < m, and u’ = cm © Eric Xing @ CMU, 2005-2014 25                                                                                                1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 / / / / / / / / / / / / / / / / ) , ( ) , ( 1 2 1 1 1 2 1 1 1 2 1 2 2 2 1 2 1 1 1 2 1 1 1 1 , ,           m m m m m m m m m m m m m m m m i c u c u c u c u c c c c c c c c c c c c c c c c c c c c c c c c u c u c h 1 1 ) , ( ) , ( det 1 , ,         u c u c h m m m m i Therefore Again, the abs-det-Jacobian is 1 for any choice of i! RJMCMC Example: Clustering For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m+1 (add a cluster):  u is the center of the cluster to be added, and u’ is empty  We draw a cluster center u according to some proposal qcenter(u) © Eric Xing @ CMU, 2005-2014 26 ) ( ) , , , | , , ( u q u c x m m u c x q center                                1 2 1 1 , , ) , ( m m m m m i c c c c u c h  where c’j = cj for all j ≤ m, and c’m+1 = u and RJMCMC Example: Clustering For q(), h() and the Jacobian, consider the 3 cases separately:  m’ = m+1 (add a cluster):  For the Jacobian, recall we set c’j = cj for all j ≤m, and c’m+1 = u © Eric Xing @ CMU, 2005-2014 27                                                                                            1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 / / / / / / / / / / / / / / / / ) , ( ) , ( 1 1 2 1 1 1 2 1 2 2 2 2 1 2 1 1 2 1 1 1 1 ,           u c c c c c c c u c c c c c c c u c c c c c c c u c c c c c c c u c u c h m m m m m m m m m m m m m m m 1 1 ) , ( ) , ( det 1 ,        u c u c h m m m Therefore RJMCMC Example: Clustering Notice the following important properties:  All model changes j(m’|m) are all reversible  We can get to any number of clusters m  We can change the location of any cluster i  This ensures we converge to the stationary distribution  abs-det-Jacobian is always 1  We designed our r.v. mappings h() to make this true! Take note:  For most mixture models, we can’t simply use P(x,c|m). We need to introduce hidden cluster assignment variables z for each data point x, and incorporate them into the proposals.  The basic principle of RJMCMC remains the same, though © Eric Xing @ CMU, 2005-2014 28 Large-scale MCMC Modern datasets can be very large  Millions of data points  Require Gigabytes of memory  E.x. Yahoo web graph has ~1.4 billion nodes and 6.6 billion edges So far, we have not explained how to take advantage of parallelism in MCMC  Without parallelism, we cannot use large datasets! In the rest of this lecture, we will cover techniques that permit multiple CPUs/cores to be used with MCMC © Eric Xing @ CMU, 2005-2014 29 Taking Multiple Chains Proper use of MCMC actually requires parallelism  To determine convergence, you need to take multiple MCMC chains  Chains are independent, so you can run one chain per CPU  Once converged, you can combine samples from all chains © Eric Xing @ CMU, 2005-2014 30 Chain on core 1 Chain on core 2 Chain on core 3 Not converged Converged Taking Multiple Chains Taking multiple chains doesn’t solve all issues, though  If burn-in is long, then all chains will take a long time to converge!  We need a way to take each sample faster… © Eric Xing @ CMU, 2005-2014 31 Chain on core 1 Chain on core 2 Chain on core 3 Not converged Converged Parallel Gibbs Sampling Recall that in MRFs, we Gibbs sample by sampling from P(x|MB(x)), the conditional distribution of x given its Markov Blanket MB(x)  For MRFs, the Markov Blanket of x is just its neighbors  In the MRF below, the red node’s Markov Blanket consists of the blue nodes © Eric Xing @ CMU, 2005-2014 32 Parallel Gibbs Sampling Observe that we can Gibbs sample the two green nodes simultaneously  Neither node is part of the other’s Markov Blanket, so their conditional distributions do not depend on each other  Sampling one of the green nodes doesn’t change the conditional distribution of the other node! © Eric Xing @ CMU, 2005-2014 33 Parallel Gibbs Sampling How do we generalize this idea to the whole graph?  Find subsets of nodes, such that all nodes in a given subset are not in each other’s Markov Blankets, and the subsets cover the whole graph  The subsets should be as large as possible  Because we can Gibbs sample all nodes in a subset at the same time  At the same time, we want as few subsets as possible  The Markov Blankets of different subsets overlap, so they cannot be sampled at the same time. We must process the subsets sequentially. © Eric Xing @ CMU, 2005-2014 34 Parallel Gibbs Sampling We can find these covering subsets with k-coloring algorithms (Gonzales et al., 2011)  A k-coloring algorithm colors a graph using k colors, such that:  Every node gets one color  No edge has two nodes of the same color Trees always admit a 2-coloring (e.g. below)  Assign one color to some node, and alternate colors as you move away © Eric Xing @ CMU, 2005-2014 35 Parallel Gibbs Sampling Bipartite graphs are always 2-colorable  Color each side of the bipartite graph with opposite colors  e.x. Latent Dirichlet Allocation model is bipartite However, not all graphs have k-colorings for all k ≥ 2  In the worst case, a graph with n nodes can require n colors  The full clique is one such graph  Determining if a graph is k-colorable for k > 2 is NP-complete  In practice, we employ heuristics to find k-colorings Instead of using k-colorings, why not just Gibbs sample all variables at the same time?  The Markov Chain may become non-ergodic, and is no longer guaranteed to converge to the stationary distribution! © Eric Xing @ CMU, 2005-2014 36 Online MCMC In “online” algorithms, we need to process new data points one-at-a-time  Moreover, we have to “forget” older data points because memory is finite For such applications to be viable, we can only afford constant time work per new data point  Otherwise we will reach a point where new data can no longer be processed in a reasonable amount of time What MCMC techniques can we use to make an online algorithm? © Eric Xing @ CMU, 2005-2014 37 Sequential Monte Carlo SMC is a generalization of Particle Filters  Recall that PFs incrementally sample P(Xt|Y1:t), where the Xs are latent r.v.s and the Ys are observations under a state-space model  SMC does not assume the GM is a state-space model, or has any particular structure at all Suppose we have n r.v.s x1,…,xn  SMC first draws samples from the marginal distribution P(x1), then P(x1:2), and so on until P(x1:n)  Key idea: Construct proposals such that we sample from P(x1:k+1) in constant time, given samples from P(x1:k)  Like other MCMC algorithms, we only require that we can evaluate P’(x1:n) = aP(x1:n) for some unknown a © Eric Xing @ CMU, 2005-2014 38 Sequential Importance Sampling SIS is the foundation of Sequential Monte Carlo  It allows new variables to be sampled in constant time, without resampling older variables SIS uses proposal distributions with the following structure:  Notice we can propose xk+1 if we’ve already drawn x1:k, without having to redraw x1:k © Eric Xing @ CMU, 2005-2014 39         n k k k k n n n n n n n x x q x q x x q x q x q 2 1 : 1 1 1 1 : 1 1 : 1 1 : 1 ) | ( ) ( ) | ( ) ( ) ( Sequential Importance Sampling In normalized importance sampling, recall how the sample weights wi are defined: In SIS, the unnormalized weights r can be rewritten as a telescoping product: © Eric Xing @ CMU, 2005-2014 40   i i i P w x f X f ) ( ) ( ) ( ) ( i i i x Q x P r     j j i i r r w where and                    n k k k n n n n n n n n n n n n n n n n n n n n x x r x x r x x q x P x P x q x P x q x P x r 2 : 1 1 1 : 1 1 : 1 1 1 : 1 1 : 1 1 : 1 1 : 1 1 1 : 1 1 : 1 : 1 : 1 ) ( ) ( ) ( ) ( ) | ( ) ( ) ( ) ( ) ( ) ( ) ( ) (   ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 : 1       n n n n n n n n n x x q x P x P x  where Sequential Importance Sampling  This means the unnormalized weights r can be computed incrementally  Compute αn and use it to update r(x1:n-1) to r(x1:n)  NB: For this update to be constant time, we also require P’n(x1:n) to be computable from P’n-1(x1:n-1) in constant time  We remember the unnormalized weights r at each iteration, and compute the normalized weights w as needed from r  Thus, we can sample x AND compute the normalized weights w using constant time per new variable xn  So SIS meets the requirements for an online inference algorithm!  Even better, the samples don’t depend on each other  Assign one CPU core per sample to make the SIS algorithm parallel! © Eric Xing @ CMU, 2005-2014 41    n k k k n x x r x r 2 : 1 1 1 : 1 ) ( ) ( ) (  ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 : 1       n n n n n n n n n x x q x P x P x  where Sequential Importance Sampling SIS algorithm:  At time n = 1  Draw samples xi 1 ~ q1(x1)  Compute unnormalized weights  Compute normalized weights wi 1 by normalizing ri 1  At time n ≥ 2  Draw samples xi n ~ qn(xn|xi 1:n-1)  Compute unnormalized weights  Compute normalized weights wi n by normalizing ri n © Eric Xing @ CMU, 2005-2014 42 ) ( / ) ( 1 1 1 1 1 i i i x q x P r   ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 1 : 1 1 i n i n n i n n i n n i n i n n i n i n x x q x P x P r x r r           Sequential Importance Sampling But we are not done yet! Unfortunately, SIS suffers from a severe drawback: the variance of the samples increases exponentially with n!  See eq (31) of Doucet’s SMC tutorial for an example Resampling at each iteration will decrease the sample variance!  Similar to weighted resampling from the first MC lecture! © Eric Xing @ CMU, 2005-2014 43 Multinomial Resampling Suppose we have m samples x1,…,xm with corresponding importance weights w1,…,wm Construct a categorical distribution from these samples:  This distribution has m categories (choices)  The probability of drawing category k is wk  Drawing category k gets us xk To resample, just draw N times from this distribution  Note that N can be greater/less than m! For more advanced strategies such as systematic and residual resampling, refer to page 13 of Doucet’s SMC tutorial © Eric Xing @ CMU, 2005-2014 44 Why Resample? Apart from decreasing variance, there are other reasons… Resampling removes samples xk with low weights wk  Low-weight samples come from low-probability regions of P(x)  We want to focus computation on high-probability regions of P(x)  Notice that each sample gets an equal amount of computation, regardless of its weight wk  Resampling ensures that more computation is spent on samples xk that come from high-probability regions of P(x) Resampling prevents a small number of samples xk from dominating the empirical distribution  Resampling resets all weights wk to 1/N  This prevents sample weights wk from growing until they reach 1 © Eric Xing @ CMU, 2005-2014 45 Sequential Monte Carlo The SMC algorithm is just SIS with resampling:  At time n = 1  Draw samples xi 1 ~ q1(x1)  Compute unnormalized weights  Compute normalized weights wi 1 by normalizing ri 1  Resample wi 1, xi 1 into N equally-weighted particles xi 1  At time n ≥ 2  Draw samples xi n ~ qn(xn|xi 1:n-1)  Compute unnormalized weights  Compute normalized weights wi n by normalizing ri n  Resample wi n,xi 1:n into N equally-weighted particles xi 1:n © Eric Xing @ CMU, 2005-2014 46 ) ( / ) ( 1 1 1 1 1 i i i x q x P r   ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 1 : 1 1 i n i n n i n n i n n i n i n n i n i n x x q x P x P r x r r           Summary Slice sampling  Samples from area under P(x) Reverse Jump MCMC  Allows us to switch between different models P(x) Parallel Gibbs sampling  Exploit graph colorings to sample same-colored nodes in parallel Sequential Monte Carlo  Uses incremental proposal distributions  Provides a framework for designing online, parallel MCMC algorithms © Eric Xing @ CMU, 2005-2014 47 "
19,"School of Computer Science Probabilistic Graphical Models Bayesian Nonparametrics: Dirichlet Processes Eric Xing Lecture 19, March 26, 2014 ©Eric Xing @ CMU, 2012-2014 1 Acknowledgement: slides first drafted by Sinead Williamson How Many Clusters? ©Eric Xing @ CMU, 2012-2014 2 How Many Segments? ©Eric Xing @ CMU, 2012-2014 3 PNAS papers Research topics 1900 2000 ? Research circles How Many Topics? CS Bio Phy ©Eric Xing @ CMU, 2012-2014 4 Parametric vs nonparametric Parametric model: Assumes all data can be represented using a fixed, finite number of parameters.  Mixture of K Gaussians, polynomial regression. Nonparametric model: Number of parameters can grow with sample size. Number of parameters may be random.  Kernel density estimation. Bayesian nonparametrics: Allow an infinite number of parameters a priori. A finite data set will only use a finite number of parameters. Other parameters are integrated out. ©Eric Xing @ CMU, 2012-2014 5 Clustered data How to model this data? Mixture of Gaussians: Parametric model: Fixed finite number of parameters. ©Eric Xing @ CMU, 2012-2014 6 Bayesian finite mixture model How to choose the mixing weights and mixture parameters? Bayesian choice: Put a prior on them and integrate out: Where possible, use conjugate priors  Gaussian/inverse Wishart for mixture parameters  What to choose for mixture weights? ©Eric Xing @ CMU, 2012-2014 7 The Dirichlet distribution The Dirichlet distribution is a distribution over the (K-1)- dimensional simplex. It is parametrized by a K-dimensional vector such that and Its distribution is given by ©Eric Xing @ CMU, 2012-2014 8 Samples from the Dirichlet distribution If then for all k, and Expectation: (0.01,0.01,0.01) (100,100,100) (5,50,100) ©Eric Xing @ CMU, 2012-2014 9 Conjugacy to the multinomial If and ©Eric Xing @ CMU, 2012-2014 10 Distributions over distributions The Dirichlet distribution is a distribution over positive vectors that sum to one. We can further associate each entry with a set of parameters  e.g. finite mixture model: each entry associated with a mean and covariance. In a Bayesian setting, we want these parameters to be random. We can combine the distribution over probability vectors with a distribution over parameters to get a distribution over distributions over parameters. ©Eric Xing @ CMU, 2012-2014 11 Example: finite mixture model Gaussian distribution: distribution over means.  Sample from a Gaussian is a real-valued number. ©Eric Xing @ CMU, 2012-2014 12 Example: finite mixture model Gaussian distribution: distribution over means.  Sample from a Gaussian is a real-valued number. Dirichlet distribution:  Sample from a Dirichlet distribution is a probability vector. 1 2 3 0 0.1 0.2 0.3 0.4 0.5 1 2 3 0 0.1 0.2 0.3 0.4 0.5 1 2 3 0 0.1 0.2 0.3 0.4 0.5 ©Eric Xing @ CMU, 2012-2014 13 Example: finite mixture model Dirichlet Mixture Prior  Each element of a Dirichlet- distributed vector is associated with a parameter value drawn from some distribution.  Sample from a Dirichlet mixture prior is a probability distribution over parameters of a finite mixture model. ©Eric Xing @ CMU, 2012-2014 14 Properties of the Dirichlet distribution The coalesce rule:  Relationship to gamma distribution: If ,  If and then  Therefore, if then ©Eric Xing @ CMU, 2012-2014 15 Properties of the Dirichlet distribution The “combination” rule: The beta distribution is a Dirichlet distribution on the 1- simplex. Let and Then More generally, if then ©Eric Xing @ CMU, 2012-2014 16 Properties of the Dirichlet distribution The “Renormalization” rule: If then ©Eric Xing @ CMU, 2012-2014 17 Choosing the number of clusters Mixture of Gaussians – but how many components? What if we see more data – may find new components? ©Eric Xing @ CMU, 2012-2014 18 Bayesian nonparametric mixture models Make sure we always have more clusters than we need. Solution – infinite clusters a priori! A finite data set will always use a finite – but random – number of clusters. How to choose the prior? We want something like a Dirichlet prior – but with an infinite number of components. How such a distribution can be defined? ©Eric Xing @ CMU, 2012-2014 19 Constructing an appropriate prior Start off with Split each component according to the splitting rule: Repeat to get As , we get a vector with infinitely many components ©Eric Xing @ CMU, 2012-2014 20 The Dirichlet process Let H be a distribution on some space Ω – e.g. a Gaussian distribution on the real line. Let For Then is an infinite distribution over Ω. We write ©Eric Xing @ CMU, 2012-2014 21 Samples from the Dirichlet process Samples from the Dirichlet process are discrete. We call the point masses in the resulting distribution, atoms. The base measure H determines the locations of the atoms. ©Eric Xing @ CMU, 2012-2014 22 Samples from the Dirichlet process The concentration parameter α determines the distribution over atom sizes. Small values of α give sparse distributions. ©Eric Xing @ CMU, 2012-2014 23 Properties of the Dirichlet process For any partition A1,…,AK of Ω, the total mass assigned to each partition is distributed according to Dir(αH(A1)),…,αH(AK)) 0 1 0 0.2 0.4 0 1 0 0.2 0.4 0 1 0 0.2 0.4 ©Eric Xing @ CMU, 2012-2014 24 Definition: Finite marginals A Dirichlet process is the unique distribution over probability distributions on some space Ω, such that for any finite partition A1,…,AK of Ω, [Ferguson, 1973] 0 1 0 0.2 0.4 0 1 0 0.2 0.4 0 1 0 0.2 0.4 ©Eric Xing @ CMU, 2012-2014 25   1 1  ,   2 2  ,   5 5  ,   6 6  ,   3 3  ,   4 4  , … centroid := Image ele. :=(x, . (event, pevent) Random Partition of Probability Space ©Eric Xing @ CMU, 2012-2014 26 Dirichlet Process  A CDF, G, on possible worlds of random partitions follows a Dirichlet Process if for any measurable finite partition (1,2, .., m): (G(1), G(2), …, G(m) ) ~ Dirichlet( G0(1), …., G0(m) ) where G0 is the base measure and is the scale parameter 1  2  5  6  3  4  Thus a Dirichlet Process G defines a distribution of distribution a distribution another distribution ©Eric Xing @ CMU, 2012-2014 27 Conjugacy of the Dirichlet process Let A1,…,AK be a partition of Ω, and let H be a measure on Ω. Let P(Ak) be the mass assigned by to partition Ak. Then If we see an observation in the Jth segment (or fraction), then This must be true for all possible partitions of Ω. This is only possible if the posterior of G, given an observation x, is given by ©Eric Xing @ CMU, 2012-2014 28 Predictive distribution The Dirichlet process clusters observations. A new data point can either join an existing cluster, or start a new cluster. Question: What is the predictive distribution for a new data point? Assume H is a continuous distribution on Ω. This means for every point θ in Ω, H(θ) = 0.  Therefore θ itself should not be treated as a data point, but parameter for modeling the observed data points First data point:  Start a new cluster.  Sample a parameter θ1 for that cluster. ©Eric Xing @ CMU, 2012-2014 29 Predictive distribution We have now split our parameter space in two: the singleton θ1, and everything else. Let π1 be the atom at θ1. The combined mass of all the other atoms is π* = 1-π1. A priori, A posteriori, ©Eric Xing @ CMU, 2012-2014 30 Predictive distribution If we integrate out π1 we get ©Eric Xing @ CMU, 2012-2014 31 Predictive distribution Lets say we choose to start a new cluster, and sample a new parameter θ2 ~ H. Let π2 be the size of the atom at θ2. A posteriori, If we integrate out , we get ©Eric Xing @ CMU, 2012-2014 32 Predictive distribution In general, if mk is the number of times we have seen Xi=k, and K is the total number of observed values, We tend to see observations that we have seen before – rich-get-richer property. We can always add new features – nonparametric. ©Eric Xing @ CMU, 2012-2014 33 A few useful metaphors for DP ©Eric Xing @ CMU, 2012-2014 34 DP – a Pólya urn Process  Self-reinforcing property  exchangeable partition of samples   5 2 p   5 3 p    5 p ) ( :  p G  0   ) G DP( G 0  ~ . ~ , , | 0 1 0 1 1 G i i n G K k k i i k                 Joint: Marginal: ©Eric Xing @ CMU, 2012-2014 35 Polya urn scheme The resulting distribution over data points can be thought of using the following urn scheme. An urn initially contains a black ball of mass α. For n=1,2,… sample a ball from the urn with probability proportional to its mass. If the ball is black, choose a previously unseen color, record that color, and return the black ball plus a unit- mass ball of the new color to the urn. If the ball is not black, record it’s color and return it, plus another unit-mass ball of the same color, to the urn [Blackwell and MacQueen,1973] ©Eric Xing @ CMU, 2012-2014 36 The Chinese Restaurant Process = ) | = ( -i i k c P c 1 0 0  + 1 1 0   + 1  + 2 1  + 2 1   + 2  + 3 1  + 3 2   + 3 1 - + 1  i m 1 - + 2  i m 1 - +  i .... 1  2  ©Eric Xing @ CMU, 2012-2014 37 Exchangeability An interesting fact: the distribution over the clustering of the first N customers does not depend on the order in which they arrived. Homework: Prove to yourself that this is true. However, the customers are not independent – they tend to sit at popular tables. We say that distributions like this are exchangeable. De Finetti’s theorem: If a sequence of observations is exchangeable, there must exist a distribution given which they are iid. The customers in the CRP are iid given the underlying Dirichlet process – by integrating out the DP, they become dependent. ©Eric Xing @ CMU, 2012-2014 38 The Stick-breaking Process G0 0 0.4 0.4 0.6 0.5 0.3 0.3 0.8 0.24 ) , Beta( ~ ) - ( ~ ) ( ∏ ∑ ∑ - ∞ ∞           1 1 1 1 1 1 0 1 k k j k k k k k k k k k G G       Location Mass ©Eric Xing @ CMU, 2012-2014 39 Stick breaking construction We can represent samples from the Dirichlet process exactly. Imagine a stick of length 1, representing total probability. For k=1,2,…  Sample a beta(1,α) random variable bk.  Break off a fraction bk of the stick. This is the kth atom size  Sample a random location for this atom.  Recurse on the remaining stick. [Sethuraman, 1994] ©Eric Xing @ CMU, 2012-2014 40 xi N G i  G0 yi xi N    G0 The Stick-breaking construction The Pólya urn construction Graphical Model Representations of DP  ©Eric Xing @ CMU, 2012-2014 41 Inference in the DP mixture model ©Eric Xing @ CMU, 2012-2014 42 Inference: Collapsed sampler We can integrate out G to get the CRP. Reminder: Observations in the CRP are exchangeable. Corollary: When sampling any data point, we can always rearrange the ordering so that it is the last data point. Let zn be the cluster allocation of the nth data point. Let K be the total number of instantiated clusters. Then If we use a conjugate prior for the likelihood, we can often integrate out the cluster parameters ©Eric Xing @ CMU, 2012-2014 43 Problems with the collapsed sampler We are only updating one data point at a time. Imagine two “true” clusters are merged into a single cluster – a single data point is unlikely to “break away”. Getting to the true distribution involves going through low probability states mixing can be slow. If the likelihood is not conjugate, integrating out parameter values for new features can be difficult. Neal [2000] offers a variety of algorithms. Alternative: Instantiate the latent measure. ©Eric Xing @ CMU, 2012-2014 44 Inference: Blocked Gibbs sampler Rather than integrate out G, we can instantiate it. Problem: G is infinite-dimensional. Solution: Approximate it with a truncated stick-breaking process: ©Eric Xing @ CMU, 2012-2014 45 Inference: Blocked Gibbs sampler Sampling the cluster indicators: Sampling the stick breaking variables:  We can think of the stick breaking process as a sequence of binary decisions.  Choose zn = 1 with probability b1.  If zn ≠ 1, choose zn = 2 with probability b2.  etc.. ©Eric Xing @ CMU, 2012-2014 46 Inference: Slice sampler Problem with batch sampler: Fixed truncation introduces error. Idea:  Introduce random truncation.  If we marginalize over the random truncation, we recover the full model. Introduce a uniform random variable un for each data point. Sample indicator zn according to Only a finite number of possible values. ©Eric Xing @ CMU, 2012-2014 47 Inference: Slice sampler The conditional distribution for un is just: Conditioned on the un and the zn, the πk can be sampled according to the block Gibbs sampler. Only need to represent a finite number K of components such that ©Eric Xing @ CMU, 2012-2014 48 Summary: Bayesian Nonparametrics Examples: Dirichlet processes, stick-breaking processes … From finite, to infinite mixture, to more complex constructions (hierarchies, spatial/temporal sequences, …) Focus on the laws and behaviors of both the generative formalisms and resulting distributions Often offer explicit expression of distributions, and expose the structure of the distributions --- motivate various approximate schemes ©Eric Xing @ CMU, 2012-2014 49 "
2,"School of Computer Science Probabilistic Graphical Models Directed GMs: Bayesian Networks Eric Xing Lecture 2, January 15, 2014 © Eric Xing @ CMU, 2005-2014 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 Reading: see class homepage 1 Questions ? Scribers ? Waiting list Reading: required vs suggested © Eric Xing @ CMU, 2005-2014 2  Representation: what is the joint probability dist. on multiple variables?  How many state configurations in total? --- 28  Are they all needed to be represented?  Do we get any scientific/medical insight?  Factored representation: the chain-rule  This factorization is true for any distribution and any variable ordering  Do we save any parameterization cost?  If Xi's are independent: (P(Xi|·)= P(Xi)) ) , , , , , , , , ( 8 7 6 5 4 3 2 1 X X X X X X X X P Representing Multivariate Distribution A C F G H E D B A C F G H E D B A C F G H E D B A C F G H E D B ) , , , , , , | ( ) , , , , , | ( ) , , , , | ( ) , , , | ( ) , , | ( ) , | ( ) | ( ) ( ) , , , , , , , ( 7 6 5 4 3 2 1 8 6 5 4 3 2 1 7 5 4 3 2 1 6 4 3 2 1 5 3 2 1 4 2 1 3 1 2 1 8 7 6 5 4 3 2 1 X X X X X X X X P X X X X X X X P X X X X X X P X X X X X P X X X X P X X X P X X P X P X X X X X X X X P     i i X P X P X P X P X P X P X P X P X P X X X X X X X X P ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) , , , , , , , ( 8 7 6 5 4 3 2 1 8 7 6 5 4 3 2 1 What do we gain? What do we lose? © Eric Xing @ CMU, 2005-2014 Directed edges give causality relationships (Bayesian Network or Directed Graphical Model): Undirected edges simply give correlations between variables (Markov Random Field or Undirected Graphical model): Two types of GMs Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) P(X1, X2, X3, X4, X5, X6, X7, X8) = 1/Z exp{E(X1)+E(X2)+E(X3, X1)+E(X4, X2)+E(X5, X2) + E(X6, X3, X4)+E(X7, X6)+E(X8, X5, X6)} © Eric Xing @ CMU, 2005-2014 Notation Variable, value and index Random variable Random vector Random matrix Parameters © Eric Xing @ CMU, 2005-2014 5 Representation of directed GM © Eric Xing @ CMU, 2005-2014 6 Example: The Dishonest Casino © Eric Xing @ CMU, 2005-2014 A casino has two dice: Fair die P(1) = P(2) = P(3) = P(5) = P(6) = 1/6 Loaded die P(1) = P(2) = P(3) = P(5) = 1/10 P(6) = 1/2 Casino player switches back-&-forth between fair and loaded die once every 20 turns Game: 1. You bet $1 2. You roll (always with a fair die) 3. Casino player rolls (maybe with fair die, maybe with loaded die) 4. Highest number wins $2 7 Puzzles regarding the dishonest casino GIVEN: A sequence of rolls by the casino player 1245526462146146136136661664661636616366163616515615115146123562344 QUESTION  How likely is this sequence, given our model of how the casino works?  This is the EVALUATION problem  What portion of the sequence was generated with the fair die, and what portion with the loaded die?  This is the DECODING question  How “loaded” is the loaded die? How “fair” is the fair die? How often does the casino player change from fair to loaded, and back?  This is the LEARNING question © Eric Xing @ CMU, 2005-2014 8 Knowledge Engineering Picking variables  Observed  Hidden Picking structure  CAUSAL  Generative  Coupling Picking Probabilities  Zero probabilities  Orders of magnitudes  Relative values © Eric Xing @ CMU, 2005-2014 9 Hidden Markov Model © Eric Xing @ CMU, 2005-2014 A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... The sequence: The underlying source: Phonemes Speech signal DNA sequence dice genome function sequence of rolls 10 Probability of a parse  Given a sequence x = x1……xT and a parse y = y1, ……, yT,  To find how likely is the parse: (given our HMM and the sequence) p(x, y) = p(x1……xT, y1, ……, yT) (Joint probability) = p(y1) p(x1 | y1) p(y2 | y1) p(x2 | y2) … p(yT | yT-1) p(xT | yT) = p(y1) P(y2 | y1) … p(yT | yT-1) × p(x1 | y1) p(x2 | y2) … p(xT | yT) = p(y1, ……, yT) p(x1……xT | y1, ……, yT)  Marginal probability:  Posterior probability: We will learn how to do this explicitly (polynomial time) © Eric Xing @ CMU, 2005-2014           y y x x 1 2 1 1 2 1 y y y T t T t t t y y y N t t y x p a p p ) | ( ) , ( ) ( ,   ) ( / ) , ( ) | ( x y x x y p p p  A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... 11 Bayesian Network:  A BN is a directed graph whose nodes represent the random variables and whose edges represent direct influence of one variable on another.  It is a data structure that provides the skeleton for representing a joint distribution compactly in a factorized way;  It offers a compact representation for a set of conditional independence assumptions about a distribution;  We can view the graph as encoding a generative sampling process executed by nature, where the value for each variable is selected by nature using a distribution that depends only on its parents. In other words, each variable is a stochastic function of its parents. © Eric Xing @ CMU, 2005-2014 12 Bayesian Network: Factorization Theorem Theorem: Given a DAG, The most general form of the probability distribution that is consistent with the graph factors according to “node given its parents”: where is the set of parents of Xi, d is the number of nodes (variables) in the graph.    d i i i X P P : ) | ( ) ( 1  X X i  X P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 © Eric Xing @ CMU, 2005-2014 13 Specification of a directed GM There are two components to any GM:  the qualitative specification  the quantitative specification A C F G H E D B A C F G H E D B A C F G H E D B 0.9 0.1 c d c 0.2 0.8 0.01 0.99 0.9 0.1 d c d d c D C P(F | C,D) 0.9 0.1 c d c 0.2 0.8 0.01 0.99 0.9 0.1 d c d d c D C P(F | C,D) © Eric Xing @ CMU, 2005-2014 14 Qualitative Specification Where does the qualitative specification come from?  Prior knowledge of causal relationships  Prior knowledge of modular relationships  Assessment from experts  Learning from data  We simply link a certain architecture (e.g. a layered graph)  … © Eric Xing @ CMU, 2005-2014 15 Local Structures & Independencies Common parent  Fixing B decouples A and C ""given the level of gene B, the levels of A and C are independent"" Cascade  Knowing B decouples A and C ""given the level of gene B, the level gene A provides no extra prediction value for the level of gene C"" V-structure  Knowing C couples A and B because A can ""explain away"" B w.r.t. C ""If A correlates to C, then chance for B to also correlate to B will decrease"" The language is compact, the concepts are rich! A C B A C B A B C © Eric Xing @ CMU, 2005-2014 A C F G H E D B A C F G H E D B A C F G H E D B 16 A simple justification A B C © Eric Xing @ CMU, 2005-2014 17 I-maps Defn : Let P be a distribution over X. We define I(P) to be the set of independence assertions of the form (X Y | Z) that hold in P (however how we set the parameter-values). Defn : Let K be any graph object associated with a set of independencies I(K). We say that K is an I-map for a set of independencies I, if I(K) I. We now say that G is an I-map for P if G is an I-map for I(P), where we use I(G) as the set of independencies associated. © Eric Xing @ CMU, 2005-2014 18 Facts about I-map For G to be an I-map of P, it is necessary that G does not mislead us regarding independencies in P: any independence that G asserts must also hold in P. Conversely, P may have additional independencies that are not reflected in G Example: P1 P2 © Eric Xing @ CMU, 2005-2014 19 What is in I(G) --- local Markov assumptions of BN A Bayesian network structure G is a directed acyclic graph whose nodes represent random variables X1, . . . ,Xn. local Markov assumptions Defn : Let PaXi denote the parents of Xi in G, and NonDescendantsXi denote the variables in the graph that are not descendants of Xi. Then G encodes the following set of local conditional independence assumptions Iℓ(G): Iℓ(G): {Xi NonDescendantsXi | PaXi : i), In other words, each node Xi is independent of its nondescendants given its parents. © Eric Xing @ CMU, 2005-2014 20 Graph separation criterion D-separation criterion for Bayesian networks (D for Directed edges): Defn: variables x and y are D-separated (conditionally independent) given z if they are separated in the moralized ancestral graph Example: © Eric Xing @ CMU, 2005-2014 21 Active trail  Causal trail X → Z → Y : active if and only if Z is not observed.  Evidential trail X ← Z ← Y : active if and only if Z is not observed.  Common cause X ← Z → Y : active if and only if Z is not observed.  Common effect X → Z ← Y : active if and only if either Z or one of Z’s descendants is observed Definition : Let X, Y , Z be three sets of nodes in G. We say that X and Y are d-separated given Z, denoted d-sepG(X;Y | Z), if there is no active trail between any node X X and Y Y given Z. © Eric Xing @ CMU, 2005-2014 22 What is in I(G) --- Global Markov properties of BN  X is d-separated (directed-separated) from Z given Y if we can't send a ball from any node in X to any node in Z using the ""Bayes- ball"" algorithm illustrated bellow (and plus some boundary conditions): • Defn: I(G)all independence properties that correspond to d- separation: • D-separation is sound and complete (more details later)   ) ; ( dsep : ) ( I Y Z X Y Z X G G   © Eric Xing @ CMU, 2005-2014 23 Example:  Complete the I(G) of this graph: x1 x2 x4 x3 © Eric Xing @ CMU, 2005-2014 24 Toward quantitative specification of probability distribution Separation properties in the graph imply independence properties about the associated variables The Equivalence Theorem For a graph G, Let D1 denote the family of all distributions that satisfy I(G), Let D2 denote the family of all distributions that factor according to G, Then D1≡D2. For the graph to be useful, any conditional independence properties we can derive from the graph should hold for the probability distribution that the graph represents    d i i i X P P : ) | ( ) ( 1  X X © Eric Xing @ CMU, 2005-2014 25 a0 0.75 a1 0.25 b0 0.33 b1 0.67 a0b0 a0b1 a1b0 a1b1 c0 0.45 1 0.9 0.7 c1 0.55 0 0.1 0.3 A B C P(a,b,c.d) = P(a)P(b)P(c|a,b)P(d|c) D c0 c1 d0 0.3 0.5 d1 07 0.5 Conditional probability tables (CPTs) © Eric Xing @ CMU, 2005-2014 26 A B C P(a,b,c.d) = P(a)P(b)P(c|a,b)P(d|c) D A~N(μa, Σa) B~N(μb, Σb) C~N(A+B, Σc) D~N(μd+C, Σd) D C P(D| C) Conditional probability density func. (CPDs) © Eric Xing @ CMU, 2005-2014 27 Summary of BN semantics Defn : A Bayesian network is a pair (G, P) where P factorizes over G, and where P is specified as set of CPDs associated with G’s nodes.  Conditional independencies imply factorization  Factorization according to G implies the associated conditional independencies.  Are there other independences that hold for every distribution P that factorizes over G? © Eric Xing @ CMU, 2005-2014 28 Soundness and completeness D-separation is sound and ""complete"" w.r.t. BN factorization law Soundness: Theorem: If a distribution P factorizes according to G, then I(G) I(P). ""Completeness"": ""Claim"": For any distribution P that factorizes over G, if (X Y | Z) I(P) then d-sepG(X; Y | Z). Contrapositive of the completeness statement  ""If X and Y are not d-separated given Z in G, then X and Y are dependent in all distributions P that factorize over G.""  Is this true? © Eric Xing @ CMU, 2005-2014 29 Distributional equivalence and I- equivalence  All independence in Id(G) will be captured in If(G), is the reverse true?  Are ""not-independence"" from G all honored in Pf ? © Eric Xing @ CMU, 2005-2014 30 Soundness and completeness  Contrapositive of the completeness statement  ""If X and Y are not d-separated given Z in G, then X and Y are dependent in all distributions P that factorize over G.""  Is this true?  No. Even if a distribution factorizes over G, it can still contain additional independencies that are not reflected in the structure  Example: graph A->B, for actually independent A and B (the independence can be captured by some subtle way of parameterization)  Thm: Let G be a BN graph. If X and Y are not d-separated given Z in G, then X and Y are dependent in some distribution P that factorizes over G. © Eric Xing @ CMU, 2005-2014 31 Theorem : For almost all distributions P that factorize over G, i.e., for all distributions except for a set of ""measure zero"" in the space of CPD parameterizations, we have that I(P) = I(G) © Eric Xing @ CMU, 2005-2014 32 Uniqueness of BN Very different BN graphs can actually be equivalent, in that they encode precisely the same set of conditional independence assertions. (X Y | Z). © Eric Xing @ CMU, 2005-2014 33 I-equivalence  Defn : Two BN graphs G1 and G2 over X are I-equivalent if I(G1) = I(G2).  The set of all graphs over X is partitioned into a set of mutually exclusive and exhaustive I-equivalence classes, which are the set of equivalence classes induced by the I-equivalence relation.  Any distribution P that can be factorized over one of these graphs can be factorized over the other.  Furthermore, there is no intrinsic property of P that would allow us associate it with one graph rather than an equivalent one.  This observation has important implications with respect to our ability to determine the directionality of influence. © Eric Xing @ CMU, 2005-2014 34 Detecting I-equivalence  Defn : The skeleton of a Bayesian network graph G over V is an undirected graph over V that contains an edge {X, Y} for every edge (X, Y) in G.  Thm : Let G1 and G2 be two graphs over V. If G1 and G2 have the same skeleton and the same set of v-structures then they are I- equivalent.  graph equivalence  Same trail  But not necessarily active © Eric Xing @ CMU, 2005-2014 35 Minimum I-MAP Complete graph is a (trivial) I-map for any distribution, yet it does not reveal any of the independence structure in the distribution.  Meaning that the graph dependence is arbitrary, thus by careful parameterization an dependencies can be captured  We want a graph that has the maximum possible I(G), yet still I(P) Defn : A graph object G is a minimal I-map for a set of independencies I if it is an I-map for I, and if the removal of even a single edge from G renders it not an I-map. © Eric Xing @ CMU, 2005-2014 36 Minimum I-MAP is not unique © Eric Xing @ CMU, 2005-2014 37 Simple BNs: Conditionally Independent Observations y1  Data Model parameters y2 yn-1 yn © Eric Xing @ CMU, 2005-2014 38 The “Plate” Micro yi i=1:n  Data = {y1,…yn} Model parameters Plate = rectangle in graphical model variables within a plate are replicated in a conditionally independent manner © Eric Xing @ CMU, 2005-2014 39 Hidden Markov Model: from static to dynamic mixture models Dynamic mixture A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... Static mixture A X1 Y1 N © Eric Xing @ CMU, 2005-2014 40 Definition (of HMM)  Observation space Alphabetic set: Euclidean space:  Index set of hidden states  Transition probabilities between any two states or  Start probabilities  Emission probabilities associated with each state or in general: A A A A x2 x3 x1 xT y2 y3 y1 yT ... ...   K c c c , , ,  2 1  C d R   M , , ,  2 1  I , ) | ( ,j i i t j t a y y p     1 1 1   . , , , , l Multinomia ~ ) | ( , , , I     i a a a y y p M i i i i t t  1 1 1 1  . , , , l Multinomia ~ ) ( M y p     2 1 1   . , , , , l Multinomia ~ ) | ( , , , I    i b b b y x p K i i i i t t  1 1 1   . , | f ~ ) | ( I     i y x p i i t t  1 © Eric Xing @ CMU, 2005-2014 41 Probability of a parse  Given a sequence x = x1……xT and a parse y = y1, ……, yT,  To find how likely is the parse: (given our HMM and the sequence) p(x, y) = p(x1……xT, y1, ……, yT) (Joint probability) = p(y1) p(x1 | y1) p(y2 | y1) p(x2 | y2) … p(yT | yT-1) p(xT | yT) = p(y1) P(y2 | y1) … p(yT | yT-1) × p(x1 | y1) p(x2 | y2) … p(xT | yT) = p(y1, ……, yT) p(x1……xT | y1, ……, yT) A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... © Eric Xing @ CMU, 2005-2014 42 Summary Defn (3.2.5): A Bayesian network is a pair (G, P) where P factorizes over G, and where P is specified as set of local conditional probability dist. CPDs associated with G’s nodes. A BN capture “causality”, “generative schemes”, “asymmetric influences”, etc., between entities Local and global independence properties identifiable via d- separation criteria (Bayes ball) Computing joint likelihood amounts multiplying CPDs  But computing marginal can be difficult  Thus inference is in general hard Important special cases:  Hidden Markov models  Tree models © Eric Xing @ CMU, 2005-2014 43 "
20,"School of Computer Science Probabilistic Graphical Models Dirichlet Process and Hiarchical DP -- case studies in genetics and text analysis Eric Xing Lecture 20, March 31, 2014 Reading: © Eric Xing @ CMU, 2005-2014 1 Dirichlet Process  A CDF, G, on possible worlds of random partitions follows a Dirichlet Process if for any measurable finite partition (1,2, .., m):  A discrete distribution over a continue space (G(1), G(2), …, G(m) ) ~ Dirichlet( G0(1), …., G0(m) ) where G0 is the base measure and is the scale parameter 1  2  5  6  3  4  Thus a Dirichlet Process G defines a distribution of distribution a distribution another distribution © Eric Xing @ CMU, 2005-2014 2 Stick-breaking Process G0 0 0.4 0.4 0.6 0.5 0.3 0.3 0.8 0.24 ) , Beta( ~ ) - ( ~ ) ( ∏ ∑ ∑ - ∞ ∞           1 1 1 1 1 1 0 1 k k j k k k k k k k k k G G       Location Mass © Eric Xing @ CMU, 2005-2014 3 Chinese Restaurant Process CRP defines an exchangeable distribution on partitions over an (infinite) sequence of samples, such a distribution is formally known as the Dirichlet Process (DP) = ) | = ( -i i k c P c 1 0 0  + 1 1 0   + 1  + 2 1  + 2 1   + 2  + 3 1  + 3 2   + 3 1 - + 1  i m 1 - + 2  i m 1 - +  i .... 1  2  © Eric Xing @ CMU, 2005-2014 4 xi N G i  G0 yi xi N    G0 The Stick-breaking construction The CRP construction Graphical Model Representations of DP mixture  © Eric Xing @ CMU, 2005-2014 5 Case I: Ancestral Inference  Better recovery of the ancestors leads to better haplotyping results (because of more accurate grouping of common haplotypes)  True haplotypes are obtainable with high cost, but they can validate model more subjectively (as opposed to examining saliency of clustering)  Many other biological/scientific utilities Gn Hn1 Hn2 Ak k  N Essentially a clustering problem, but … © Eric Xing @ CMU, 2005-2014 6 Example: DP-haplotyper [Xing et al, 2004] Clustering human populations Inference: Markov Chain Monte Carlo (MCMC)  Gibbs sampling  Metropolis Hasting Gn Hn1 Hn2 A  N K G  G0 DP infinite mixture components (for population haplotypes) Likelihood model (for individual haplotypes and genotypes) © Eric Xing @ CMU, 2005-2014 7 {A} {A} {A} {A} {A} {A} … 3 1 2 4 5 6 7 8 9 The DP Mixture of Ancestral Haplotypes  The customers around a table in CRP form a cluster  associate a mixture component (i.e., a population haplotype) with a table  sample {a, } at each table from a base measure G0 to obtain the population haplotype and nucleotide substitution frequency for that component  With p(h|{}) and p(g|h1,h2), the CRP yields a posterior distribution on the number of population haplotypes (and on the haplotype configurations and the nucleotide substitution frequencies) © Eric Xing @ CMU, 2005-2014 8  Single-locus mutation model  Noisy observation model Inheritance and Observation Models … 1 i H     . for | | for ) , | ( prob with a h a h B a h a h P t t t t t t t t H             1 1 i G Ancestral pool Haplotypes Genotype  . : ) , | ( , , prob with h h g h h g P t t t G 2 1 2 1   2 i H 1 A 3 A 2 A e e i i C H A  1 i C 2 i C i i i G H H  2 1 , © Eric Xing @ CMU, 2005-2014 9  Gibbs sampling for exploring the posterior distribution under the proposed model  Integrate out the parameters such as or , and sample and  Gibbs sampling algorithm: draw samples of each random variable to be sampled given values of all the remaining variables MCMC for Haplotype Inference   k i a c e , e i h ) , | ( ) | ( ) , , | ( ] [ , ] [ ] [ c h c a h c e e e e e e i k i i i i i a h p k c p k c p       Posterior Prior x Likelihood CRP  © Eric Xing @ CMU, 2005-2014 10 MCMC for Haplotype Inference 1. Sample cie (j), from 2. Sample ak from 3. Sample hie (j) from  For DP scale parameter : a vague inverse Gamma prior © Eric Xing @ CMU, 2005-2014 11 Convergence of Ancestral Inference © Eric Xing @ CMU, 2005-2014 12 DP vs. Finite Mixture via EM 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 1 2 3 4 5 data sets individual error Series1 Series2 DP EM © Eric Xing @ CMU, 2005-2014 13 Multi-population Genetic Demography  Pool everything together and solve 1 hap problem?  --- ignore population structures  Solve 4 hap problems separately?  --- data fragmentation  Co-clustering … solve 4 coupled hap problems jointly © Eric Xing @ CMU, 2005-2014 14   ) G DP( G 0  ~ 1 3 2 4 5 6   ) G DP( G 0  ~   ) G DP( G 0  ~   ) G DP( G 0  ~   ) G DP( G 0  ~ ? ? Nature articles PNAS articles Science articles … … … Solving Multiple Clustering Problems © Eric Xing @ CMU, 2005-2014 15 How? And what is the challenge? How to share mixture components?  Because the base measure is continuous, we have zero probability of picking the same component twice.  If we want to pick the same topic twice, we need to use a discrete base measure.  For example, if we chose the base measure to be  We want there to be an infinite number of topics, so we want an infinite, discrete base measure.  We want the location of the topics to be random, so we want an infinite, discrete, random base measure. 16 © Eric Xing @ CMU, 2005-2014 Hierarchical Dirichlet Process (Teh et al, 2006) Solution: Sample the base measure from a Dirichlet process! 17 © Eric Xing @ CMU, 2005-2014  Two level CRP scheme: The Chinese restaurant franchise  At the i-th step in j-th ""group"", 0 0 0           k jk k jk jk k m prob with DP level upper the to Go m m prob with Choose . .         k k k k n prob with sample new a Draw n n prob with Choose . . Oracle [Teh et al., 2005, Xing et al. 2005] Hierarchical Dirichlet Process © Eric Xing @ CMU, 2005-2014 18 Chinese restaurant franchise Imagine a franchise of restaurants, serving an infinitely large, global menu. Each table in each restaurant orders a single dish. Let nrt be the number of customers in restaurant r sitting at table t. Let mrd be the number of tables in restaurant r serving dish d. Let m.d be the number of tables, across all restaurants, serving dish d. 19 © Eric Xing @ CMU, 2005-2014 xi N G i  G0 yi xi N    G0 The Stick-breaking construction The Pólya urn construction Recall: Graphical Model Representations of DP  © Eric Xing @ CMU, 2005-2014 20 Hierarchical DP Mixture J xi N Gj i  G0  H yi xi N j   H   J ∑ ∑ ∞ ∞ ) ( ), , Stick( ) ( ), Stick( ~ 1 1 0       k k k j j k k k k G G H                   . ' ' , , Beta ~ ' : ) , Stick( k         1 1 1 1 1 k l jl jk jk k l l jk           © Eric Xing @ CMU, 2005-2014 21 Results - Simulated Data 5 populations with 20 individuals each (two kinds of mutation rates)  5 populations share parts of their ancestral haplotypes  the sequence length = 10 Haplotype error © Eric Xing @ CMU, 2005-2014 22 Results - International HapMap DB Different sample sizes, and different # of sub-populations © Eric Xing @ CMU, 2005-2014 23 Constructing a topic model with infinitely many topics LDA: Each distribution is associated with a distribution over K topics. Problem: How to choose the number of topics? Solution:  Infinitely many topics!  Replace the Dirichlet distribution over topics with a Dirichlet process! Problem: We want to make sure the topics are shared between documents 24 © Eric Xing @ CMU, 2005-2014 © Eric Xing @ CMU, 2005-2014 Infinite Topic Models A single image with k topic zi wi N j   H k Dirichlet An LDA  zi wi N j   H Stick breaking A single image with inf-topic A DP zi wi N j   H   J  J images with inf-topic An HDP An infinite topic model Restaurants = documents; dishes = topics. Let H be a V-dimensional Dirichlet distribution, so a sample from H is a distribution over a vocabulary of V words. Sample a global distribution over topics, For each document m=1,…,M  Sample a distribution over topics, Gm~DP(γ,G0).  For each word n=1,…,Nm  Sample a topic ϕmn~Discrete(G0).  Sample a word wmk~Discrete(ϕmn). 26 © Eric Xing @ CMU, 2005-2014 The “right” number of topics 27 © Eric Xing @ CMU, 2005-2014 © Eric Xing @ CMU, 2005-2014 Dynamic Dirichlet Process  Two Main Ideas:  Infinite HMM: a hidden Markov DP (see appendix)  Dependent DP/HDP: directly evolving a DP/HDP © Eric Xing @ CMU, 2005-2014 Evolutionary Clustering Adapts the number of mixture components over time  Mixture components can die out  New mixture components are born at any time  Retained mixture components parameters evolve according to a Markovian dynamics 1900 2000 CS Bio Phy Research Papers Topics © Eric Xing @ CMU, 2005-2014 The Chinese Restaurant Process Customers correspond to data points Tables correspond to clusters/mixture components Dishes correspond to parameter of the mixtures 1  2  1  1  2  2  © Eric Xing @ CMU, 2005-2014 The Recurrent Chinese Restaurant Process  The restaurant operates in epochs  The restaurant is closed at the end of each epoch  The state of the restaurant at time epoch t depends on that at time epoch t-1  Can be extended to higher-order dependencies. Temporal DPM [Ahmed and Xing 2008] © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 Dish eaten at table 3 at time epoch 1 OR the parameters of cluster 3 at time epoch 1 ‐Customers at time T=1 are seated as before: ‐ Choose table j Nj,1 and Sample xi ~ f(j,1) ‐ Choose a new table K+1  ‐ Sample K+1,1 ~ G0 and Sample xi ~ f(K+1,1) Generative Process © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,1 3,1 N1,1=2 N2,1=3 N3,1=1 © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,1 3,1 N1,1=2 N2,1=3 N3,1=1 © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,1 3,1 N1,1=2 N2,1=3 N3,1=1   6 2   6 3   6 1    6 © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,1 3,1 N1,1=2 N2,1=3 N3,1=1   6 2 © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,2 3,1 N1,1=2 N2,1=3 N3,1=1 Sample 1,2 ~ P(.| 1,1) © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,1 1,2 3,1 N1,1=2 N2,1=3 N3,1=1     1 6 2 1   1 6 3   1 6 1    1 6 And so on …… © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,2 1,2 3,1 N1,1=2 N2,1=3 N3,1=1 At the end of epoch 2 4,2 Newly born cluster Died out cluster © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,2 1,2 3,1 N1,1=2 N2,1=3 N3,1=1 4,2 T=3 2,2 1,2 4,2 N4,2=1 N2,2=2 N1,2=2 © Eric Xing @ CMU, 2005-2014 Temporal DPM Can be extended to model higher-order dependencies Can decay dependencies over time  Pseudo-counts for table k at time t is History size Decay factory Number of customers sitting at table K at time epoch t‐w          W w w t k w N e 1 ,  © Eric Xing @ CMU, 2005-2014 2,1 1,1 3,1 T=1 T=2 2,2 1,2 3,1 N1,1=2 N2,1=3 N3,1=1 4,2 T=3 2,2 1,2 4,2 N2,3 N2,3 = © Eric Xing @ CMU, 2005-2014 TDPM Generative Power W=T  DPM W=4  TDPM W= 0 any) Independent DPMs Power‐law curve © Eric Xing @ CMU, 2005-2014 Results: NIPS 12 Building a simple dynamic topic model Chain dynamics is as before Emission model for document xk,t is:  Project k,t over the simplex  Sample xk,t|ct,i ~ Multinomial(.|Logisitic(k,t))  Unlike LDA here a document belongs to one topic Use this model to analyze NIPS12 corpus  Proceeding of NIPS conference 1987-1999 © Eric Xing @ CMU, 2005-2014 © Eric Xing @ CMU, 2005-2014 The Big Picture ci xiN    Time Model Dimension K‐means ci xi N     G DPM Dynamic clustering TDPM  Fixed-dimensions Dynamic clustering © Eric Xing @ CMU, 2005-2014 Summary A non-parametric Bayesian model for Pattern Uncovery  Finite mixture model of latent patterns (e.g., image segments, objects) infinite mixture of propotypes: alternative to model selection hierarchical infinite mixture temporal infinite mixture model Applications in general data-mining … Appendix: What if we have an HMM with unknown # of states?  E.g., “recombination” over unknown number of chromosomes? © Eric Xing @ CMU, 2005-2014 48 Ancestral chromosomes (K=5) Each individual haplotype is a mosaic of ancestral haplotypes Individual chromosomes x x x x x x x x x x x A common inheritance model to begin with … © Eric Xing @ CMU, 2005-2014 49 How many recombining ancestors? Ct Ct+1 K … 2 1 K : . 2 1 ) ' , ( ) 1 ( ) | ' ( ' , , 1 , k k e e k c k c p dr k k dr t i t i           Transition process: recombination ) , , ( , , 1 | | 1 ) , | ( ) ( , , t k a t i h I t k t i B a h p k a h I k k t k t i                 Emission process: mutation … 1 i H i G Ancestral pool Haplotypes Genotype 2 i H 1 A 3 A 2 A 1 i C 2 i C … 1 i H i G Ancestral pool Haplotypes Genotype 2 i H 1 A 3 A 2 A 1 i C 2 i C The Hidden Markov Model © Eric Xing @ CMU, 2005-2014 50  Hidden Markov Dirichlet process mixtures  Extension of HMM model to infinite ancestral space  Infinite dimensional transition matrix  Each row of the transition matrix is modeled with a DP Ct Ct+1 1 2 3 ... 1 2 3 : . … … Hidden Markov Dirichlet Process (Xing and Sohn, Bayesian Analysis, 2007) © Eric Xing @ CMU, 2005-2014 51 A H       H H y1 y2 y3 yN … ....       H H y1 y1 y2 y2 y3 y3 yN yN … ....   C1 C2 C3 CN HMDP as a Graphical Model Ancestor allele reconstruction Inferring population structure Inferring recombination hotspot © Eric Xing @ CMU, 2005-2014 52 Recombination Analysis CEU YRI HCB+JPT HapMap4 © Eric Xing @ CMU, 2005-2014 53 "
21,"School of Computer Science Probabilistic Graphical Models Infinite Feature Models: The Indian Buffet Process Eric Xing Lecture 21, April 2, 2014 ©Eric Xing @ CMU, 2012-2014 1 Acknowledgement: slides first drafted by Sinead Williamson Limitations of a simple mixture model The Dirichlet distribution and the Dirichlet process are great if we want to cluster data into non-overlapping clusters. However, DP/Dirichlet mixture models cannot share features between clusters. In many applications, data points exhibit properties of multiple latent features  Images contain multiple objects.  Actors in social networks belong to multiple social groups.  Movies contain aspects of multiple genres. ©Eric Xing @ CMU, 2012-2014 2 Latent variable models Latent variable models allow each data point to exhibit multiple features, to varying degrees. Example: Factor analysis X = WAT + ε  Rows of A = latent features  Rows of W = datapoint-specific weights for these features  ε = Gaussian noise. Example: Text Documents  Each document represented by a mixture of features. ©Eric Xing @ CMU, 2012-2014 3 Infinite latent feature models Problem: How to choose the number of features? Example: Factor analysis X = WAT + ε Each column of W (and row of A) corresponds to a feature. Question: Can we make the number of features unbounded a posteriori, as we did with the DP? Solution: allow infinitely many features a priori – ie let W (or A) have infinitely many columns (rows). Problem: We can’t represent infinitely many features! Solution: make our infinitely large matrix sparse, and keep only the selected features Griffiths and Ghaharamani, 2006 ©Eric Xing @ CMU, 2012-2014 4 The CRP: A distribution over indicator matrices  Recall that the CRP gives us a distribution over partitions of our data.  Which means that the CRP allows every data point to use one feature (table)  We can use a similar scheme to represent a distribution over binary matrices recording “feature usage” across data, where each row corresponds to a data point, and each column to a feature  And we want to encourage every data point to use a small subset of features – sparsity ©Eric Xing @ CMU, 2012-2014 5 The Indian Buffet Process (IBP)  Another culinary experience: we describe a new unbounded multi- feature model in terms of the following restaurant analogy.  The first customer enters a restaurant with an infinitely large buffet  He helps himself to Poisson(α) dishes. ©Eric Xing @ CMU, 2012-2014 6 The Indian Buffet Process (IBP)  Another culinary experience: we describe a new unbounded multi- feature model in terms of the following restaurant analogy.  The first customer enters a restaurant with an infinitely large buffet  He helps himself to Poisson(α) dishes.  The nth customer enters the restaurant  He helps himself to each dish with probability mk/n, where mk is the number of times dish k was chosen  He then tries Poisson(α/n) new dishes ©Eric Xing @ CMU, 2012-2014 7 The Indian Buffet Process (IBP)  Another culinary experience: we describe a new unbounded multi- feature model in terms of the following restaurant analogy.  The first customer enters a restaurant with an infinitely large buffet  He helps himself to Poisson(α) dishes.  The nth customer enters the restaurant  He helps himself to each dish with probability mk/n, where mk is the number of times dish k was chosen  He then tries Poisson(α/n) new dishes ©Eric Xing @ CMU, 2012-2014 8 Example ©Eric Xing @ CMU, 2012-2014 9 Data likelihood E.g.: X = WAT + ε  Rows of A = latent features (Gaussian)  Rows of W = datapoint-specific weights for these features (Gaussian)  ε = Gaussian noise. Write  Z ~ IBP(α)  V ~ N (0,σv 2)  A ~ N (0,σA 2) ©Eric Xing @ CMU, 2012-2014 10 This is equivalent to … The infinite limit of a sparse, finite latent variable model: for some sparse matrix Z. Place a beta-Bernoulli prior on Z: ©Eric Xing @ CMU, 2012-2014 11 Properties of the IBP “Rich get richer” property – “popular” dishes become more popular. The number of nonzero entries for each row is distributed according to Poisson(α) – due to exchangeability. Recall that if x1~Poisson(α1) and x2~Poisson(α2), then (x1+x2)~Poisson(α1+α2)  The number of nonzero entries for the whole matrix is distributed according to Poisson(Nα).  The number of non-empty columns is distributed according to Poisson(αHN), where ©Eric Xing @ CMU, 2012-2014 12 HN = PN n=1 1 n A two-parameter extension In the IBP, the parameter α governs both the number of nonempty columns and the number of features per data point. We might want to decouple these properties of our model. Reminder: We constructed the IBP as the limit of a finite beta- Bernoulli model where We can modify this to incorporate an extra parameter: Sollich, 2005 ©Eric Xing @ CMU, 2012-2014 13 A two-parameter extension Our restaurant scheme is now as follows:  A customer enters a restaurant with an infinitely large buffet  He helps himself to Poisson(α) dishes.  The nth customer enters the restaurant  He helps himself to each dish with probability mk/(β+n-1)  He then tries Poisson(αβ/(β+n-1)) new dishes Note  The number of features per data point is still marginally Poisson(α).  The number of non-empty columns is now  We recover the IBP when β = 1. ©Eric Xing @ CMU, 2012-2014 14 Two parameter IBP: examples Image from Griffiths and Ghahramani, 2011 ©Eric Xing @ CMU, 2012-2014 15 Beta processes and the IBP Recall the relationship between the Dirichlet process and the Chinese restaurant process:  The Dirichlet process is a prior on probability measures (distributions)  We can use this probability measure as cluster weights in a clustering model – cluster allocations are i.i.d. given this distribution.  If we integrate out the weights, we get an exchangeable distribution over partitions of the data – the Chinese restaurant process. De Finetti’s theorem tells us that, if a distribution X1, X2,… is exchangeable, there must exist a measure conditioned on which X1, X2,… are i.i.d. ©Eric Xing @ CMU, 2012-2014 16 Beta processes and the IBP Recall the finite beta-Bernoulli model: The znk are i.i.d. given the πk, but are exchangeable if we integrate out the πk. The corresponding distribution for the IBP is the infinite limit of the beta random variables, as K tends to infinity. This distribution over discrete measures is called the beta process. Samples from the beta process have infinitely many atoms with masses between 0 and 1. Thibaux and Jordan, 2007 ©Eric Xing @ CMU, 2012-2014 17 Posterior distribution of the beta process Question: Can we obtain the posterior distribution of the column probabilities in closed form? Answer: Yes!  Recall that each atom of the beta process is the infinitesimal limit of a Beta(α/K,1) random variable.  Our observation mk for that atom are a Binomial(πk,N) random variable.  We know the beta distribution is conjugate to the Binomial, so the posterior is the infinitesimal limit of a Beta(α/K+mk,N+1-mk) random variable. ©Eric Xing @ CMU, 2012-2014 18 A stick-breaking construction for the beta process We can construct the beta process using the following stick- breaking construction: Begin with a stick of unit length. For k=1,2,…  Sample a beta(α,1) random variable μk.  Break off a fraction μk of the stick. This is the kth atom size.  Throw away what’s left of the stick.  Recurse on the part of the stick that you broke off Note that, unlike the DP stick breaking construction, the atoms will not sum to one. Teh et al, 2007 ©Eric Xing @ CMU, 2012-2014 19 Building latent feature models using the IBP We can use the IBP to build latent feature models with an unbounded number of features. Let each column of the IBP correspond to one of an infinite number of features. Each row of the IBP selects a finite subset of these features. The rich-get-richer property of the IBP ensures features are shared between data points. We must pick a likelihood model that determines what the features look like and how they are combined. ©Eric Xing @ CMU, 2012-2014 20 Infinite factor analysis Problem with linear Gaussian model: Features are “all or nothing” Factor analysis: X = WAT + ε  Rows of A = latent features (Gaussian)  Rows of W = datapoint-specific weights for these features (Gaussian)  ε = Gaussian noise. Write  Z ~ IBP(α)  V ~ N (0,σv 2)  A ~ N (0,σA 2) Knowles and Ghahramani, 2007 ©Eric Xing @ CMU, 2012-2014 21 A binary model for latent networks Motivation: Discovering latent causes for observed binary data Example:  Data points = patients  Observed features = presence/absence of symptoms  Goal: Identify biologically plausible “latent causes” – eg illnesses. Idea:  Each latent feature is associated with a set of symptoms  The more features a patient has that are associated with a given symptom, the more likely that patient is to exhibit the symptom. Wood et al, 2006 ©Eric Xing @ CMU, 2012-2014 22 A binary model for latent networks We can represent this in terms of a Noisy-OR model: Intuition:  Each patient has a set of latent causes.  For each sympton, we toss a coin with probability λ for each latent cause that is “on” for that patient and associated with that feature, plus an extra coin with probability ε.  If any of the coins land heads, we exhibit that feature. ©Eric Xing @ CMU, 2012-2014 23 Inference in the IBP Recall inference methods for the DP:  Gibbs sampler based on the exchangeable model.  Gibbs sampler based on the underlying Dirichlet distribution  Variational inference  Particle filter. We can construct analogous samplers for the IBP ©Eric Xing @ CMU, 2012-2014 24 Inference in the restaurant scheme Recall the exchangeability of the IBP means we can treat any data point as if it’s our last. Let K+ be the total number of used features, excluding the current data point. Let Θ be the set of parameters associated with the likelihood – eg the Gaussian matrix A in the linear Gaussian model The prior probability of choosing one of these features is mk/N The posterior probability is proportional to In some cases we can integrate out Θ, otherwise we must sample this. Griffiths and Gharamani, 2006 ©Eric Xing @ CMU, 2012-2014 25 Inference in the restaurant scheme In addition, we must propose adding new features. Metropolis Hastings method:  Let K*old be the number of features appearing only in the current data point.  Propose K*new ~ Poisson(α/N), and let Z* be the matrix with K*new features appearing only in the current data point.  With probability accept the proposed matrix. ©Eric Xing @ CMU, 2012-2014 26 Inference in the stick-breaking construction We can also perform inference using the stick-breaking representation  Sample Z|π,Θ  Sample π|Z The posterior for atoms for which mk>0 is beta distributed. The atoms for which mk=0 can be sampled using the stick- breaking proceedure. We can use a slice sampler to avoid representing all of the atoms, or using a fixed truncation level. Teh et al, 2007 ©Eric Xing @ CMU, 2012-2014 27 Other distributions over infinite, exchangeable matrices Recall the beta-Bernoulli process construction of the IBP. We start with a beta process – an infinite sequence of values between 0 and 1 that are distributed as the infinitesimal limit of the beta distribution. We combine this with a Bernoulli process, to get a binary matrix. If we integrate out the beta process, we get an exchangeable distribution over binary matrices. Integration is straightforward due to the beta-Bernoulli conjugacy. Question: Can we construct other infinite matrices in this way? ©Eric Xing @ CMU, 2012-2014 28 The infinite gamma-Poisson process The gamma process can be thought of as the infinitesimal limit of a sequence of gamma random variables. Alternatively, The gamma distribution is conjugate to the Poisson distribution. ©Eric Xing @ CMU, 2012-2014 29 The infinite gamma-Poisson process We can associate each atom νk of the gamma process with a column of a matrix (just like we did with the atoms of a beta process) We can generate entries for the matrix as znk~Poisson(νk) IBP infinite gamma-Poisson 5 4 6 3 5 4 4 4 2 5 3 4 2 3 3 1 4 2 2 2 4 0 1 2 1 0 0 3 1 2 0 2 0 1 2 0 0 1 2 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Titsias, 2008 ©Eric Xing @ CMU, 2012-2014 30 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1)) 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 ©Eric Xing @ CMU, 2012-2014 31 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 5 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1)) ©Eric Xing @ CMU, 2012-2014 32 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 5 0 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1)) ©Eric Xing @ CMU, 2012-2014 33 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 5 0 4 5 2 0 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1)) ©Eric Xing @ CMU, 2012-2014 34 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 5 0 4 5 2 0 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1))  Sample K*n~NegBinom(α, n/(n+1)) 4 ©Eric Xing @ CMU, 2012-2014 35 4 4 0 0 5 3 7 2 0 2 1 2 1 3 7 9 6 6 4 2 3 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 5 0 4 5 2 0 3 1 0 4 The infinite gamma-Poisson process Predictive distribution for the nth row:  For each existing feature, sample a count znk~NegBinom(mk, n/(n+1)).  Sample K*n~NegBinom(α, n/(n+1)).  Partition K*n according to the CRP, and assign the resulting counts to new columns. ©Eric Xing @ CMU, 2012-2014 36 Summary Infinite latent feature selection models  IBP: generating random binary matrix  Equivalence to beta-Bernoulli process  Inference via MCMC Infinite latent feature weighting models  The gamma-Poisson process ©Eric Xing @ CMU, 2012-2014 37 Supplementary Proof of equivalence of IBP to the infinite limit of the beta- Bernoulli process ©Eric Xing @ CMU, 2012-2014 38 A sparse, finite latent variable model If we integrate out the πk, the marginal probability of a matrix Z is: where This is exchangeable (doesn’t depend on the order of the rows or columns ©Eric Xing @ CMU, 2012-2014 39 An equivalence class of matrices We can naively take the infinite limit by taking K to infinity Because all the columns are equal in expectation, as K grows we are going to have more and more empty columns. We do not want to have to represent infinitely many empty columns! Define an equivalence class [Z] of matrices where the non- zero columns are all to the left of the empty columns. Let lof(.) be a function that maps binary matrices to left- ordered binary matrices – matrices ordered by the binary number made by their rows. ©Eric Xing @ CMU, 2012-2014 40 How big is the equivalence set? All matrices in the equivalence set [Z] are equiprobable (by exchangeability of the columns), so if we know the size of the equivalence set, we know its probability. Call the vector (z1k,z2,k,…,z(n-1)k) the history of feature k at data point n (a number represented in binary form). Let Kh be the number of features possessing history h, and let K+ be the total number of features with non-zero history. The total number of lof-equivalent matrices in [Z] is ©Eric Xing @ CMU, 2012-2014 41 Probability of an equivalence class of finite binary matrices. If we know the size of the equivalence class [Z], we can evaluate its probability: ©Eric Xing @ CMU, 2012-2014 42 Taking the infinite limit We are now ready to take the limit of this finite model as K tends to infinity: ©Eric Xing @ CMU, 2012-2014 43 Proof that the IBP is lof-equivalent to the infinite beta-Bernoulli model What is the probability of a matrix Z? Let K1 (n) be the number of new features in the nth row. If we include the cardinality of [Z], this is the same as before ©Eric Xing @ CMU, 2012-2014 44 "
22,"School of Computer Science Probabilistic Graphical Models Introduction to Hilbert Space Embeddings Eric Xing Lecture 22, April 7, 2014 Acknowledgement: slides first drafted by Ankur Parikh ©Eric Xing @ CMU, 2012-2014 1 The Optimization View of Graphical Models  The connection between optimization and graphical models has led to many amazing discoveries  EM  Variational Inference  Max Margin/Max Entropy Learning  Bridge to Statistical Physics, Numerical Methods Communities  Optimization has many advantages:  It is easy to formulate  Can derive principled approximations via convex relaxations  Can use existing optimization methods.  But it has many challenges too:  Non-Gaussian continuous variables  Nonconvexity (local minima) ©Eric Xing @ CMU, 2012-2014 2 The Linear Algebra View of Graphical Models  We are going to discuss a different (still not fully understood) point of view of graphical models that revolves around linear algebra.  Compared to the optimization perspective, the linear algebra view often less intuitive to formulate.  However, it lets us solve problems that are intractable from the optimization perspective  Graphical Models with Non-Gaussian Continuous Variables.  Local Minima Free Learning in Latent Variable Models  Moreover it offers a different theoretical perspective and bridges the graphical models, kernels and tensor algebra communities. ©Eric Xing @ CMU, 2012-2014 3 Non-Gaussian Continuous Variables ©Eric Xing @ CMU, 2012-2014 4 0.4 0.8 1.2 1.6 1.9 0 0.05 0.1 Depth (meters in log10 scale) Density Histogram farther away farther away Predict [Saxena, Chung and Ng 2005] Depth Reconstruction Non-Gaussian Continuous Variables Demographics: Model relationships among different demographic variables ©Eric Xing @ CMU, 2012-2014 5 Graphical Models - What we have learned so far… ©Eric Xing @ CMU, 2012-2014 6  If variables are observed, just count from dataset  In case of hidden variables, can use Expectation Maximization….. Parameter Learning - What we have learned so far… Samples ©Eric Xing @ CMU, 2012-2014 7  Can do exact inference with Variable Elimination, Belief Propagation.  Can do approximate inference with Loopy BP, Mean Field, MCMC Inference - What we have learned so far… ©Eric Xing @ CMU, 2012-2014 8 Non-Parametric Continuous Case is Much Harder… How do we make a conditional probability table out of this? How to learn parameters? (What are the parameters?) How to perform inference? ©Eric Xing @ CMU, 2012-2014 9 Could Discretize the Distribution…. 0 1 2 3 Loses information that 0 and 1 are closer than 0 and 3 ©Eric Xing @ CMU, 2012-2014 10 Hilbert Space Embeddings of Distributions General formulation for probabilistic modeling with continuous variables. Le Song Alex Smola Bernhard Schölkopf Arthur Gretton Kenji Fukumizu ©Eric Xing @ CMU, 2012-2014 11 Why do Gaussians Work? (1) Because we have parameters (sufficient statistics) !!!! (2) It is easy to marginalize/condition etc. Bijection between (mean,variance) pair and distribution ©Eric Xing @ CMU, 2012-2014 12 I want to represent this distribution with a small vector ࢄ. Key Idea – Create Sufficient Statistic for Arbitrary Distribution ©Eric Xing @ CMU, 2012-2014 13 Idea 1: Take some Moments Problem: Lots of Distributions have the same mean! Better, but lots of distributions still have the same mean and variance! Even better, but lots of distributions still have the same first three moments! ©Eric Xing @ CMU, 2012-2014 14 But the vector is infinite……..how do we compute things with it????? Better Idea: Create Infinite Dimensional Statistic (not exactly, but right idea…) ©Eric Xing @ CMU, 2012-2014 15 Remember the Kernel Trick!!! Primal Formulation: Infinite, cannot be directly computed Dual Formulation: But the dot product is easy to compute  ©Eric Xing @ CMU, 2012-2014 16 Overview of Hilbert Space Embedding Create an infinite dimensional statistic for a distribution. Two Requirements:  Map from distributions to statistics is one-to-one  Although statistic is infinite, it is cleverly constructed such that the kernel trick can be applied. Perform Belief Propagation as if these statistics are the conditional probability tables. We will now make this construction more formal by introducing the concept of Hilbert Spaces ©Eric Xing @ CMU, 2012-2014 17 Vector Space A set of objects closed under linear combinations: Normally, you think of these “objects” as finite dimensional vectors. However, in general the objects can be functions. Nonrigorous Intuition: A function is like an infinite dimensional vector. ©Eric Xing @ CMU, 2012-2014 18 A Hilbert Space is a complete vector space equipped with an inner product. The inner product has the following properties:  Symmetry  Linearity  Nonnegativity  Zero Basically a “nice” infinite dimensional vector space, where lots of things behave like the finite case (e.g. using inner product we can define “norm” or “orthogonality”) Hilbert Space ©Eric Xing @ CMU, 2012-2014 19 Example of an inner product (just an example, inner product not required to be an integral) Non-rigorous Intuition: Like the traditional finite vector space inner product Hilbert Space Inner Product Inner product of two functions is a number scalar ©Eric Xing @ CMU, 2012-2014 20 An operator maps a function f in one Hilbert Space to another function g in the same or another Hilbert Space. Linear Operator: Non-rigorous Intuition: Operators are sort of like matrices. Linear Operators ©Eric Xing @ CMU, 2012-2014 21 Adjoints (Transposes) The adjoint of an operator is defined such that Like transpose / conjugate transpose for real / complex matrices: ©Eric Xing @ CMU, 2012-2014 22 Non-rigorous Intuition: Like Vector Space Outer Product Hilbert Space Outer Product Outer Product of two functions is an operator is implicitly defined such that ©Eric Xing @ CMU, 2012-2014 23 Reproducing Kernel Hilbert Space  Basically, a “really nice” infinite dimensional vector space where even more things behave like the finite case  We are going to “construct” our Reproducing Kernel Hilbert Space with a Mercer Kernel. A Mercer Kernel ࡷ࢞, ࢟is a function of two variables, such that:  The is a generalization of a positive definite matrix: ©Eric Xing @ CMU, 2012-2014 24 Gaussian Kernel The most common kernel that we will use is the Gaussian RBF Kernel: ©Eric Xing @ CMU, 2012-2014 25 The Feature Function Consider holding one element of the kernel fixed. We get a function of one variable which we call the feature function. The collection of feature functions is called the feature map. For a Gaussian Kernel the feature functions are unnormalized Gaussians: ©Eric Xing @ CMU, 2012-2014 26 Defining the Inner Product Define the Inner Product as: Note that: scalar ©Eric Xing @ CMU, 2012-2014 27 Reproducing Kernel Hilbert Space  Consider the set of functions that can be formed with linear combinations of these feature functions:  We define the Reproducing Kernel Hilbert Space to the completion of (like with the “holes” filled in)  Intuitively, the feature functions are like an over-complete basis for the RKHS ©Eric Xing @ CMU, 2012-2014 28 Reproducing Property It can now be derived that the inner product of a function f with ᆞ௑, evaluates a function at point x: scalar Linearity of inner product Definition of kernel Remember that ©Eric Xing @ CMU, 2012-2014 29 SVM Kernel Intuition Maps data points to RKHS Feature Functions! ©Eric Xing @ CMU, 2012-2014 30 How To Embed Distributions (Mean Map) [Smola et al. 2007] density The Hilbert Space Embedding of X ©Eric Xing @ CMU, 2012-2014 31 Mean Map If the kernel is universal, then the map from distributions to embeddings is one-to-one. Examples of universal kernels:  Gaussian RBF Kernel.  Laplace Kernel “Empirical Estimate” (not actually computable from data if feature map is infinite….but we will solve this problem in the next lecture) Mean Map cont. Data point ©Eric Xing @ CMU, 2012-2014 32 Example (Discrete)  Consider a random variable X that takes the values ૚, ૛, ૜, ૝. We want to embed it into an RKHS. Which RKHS?  The RKHS of 4 dimensional vectors in ࡾ૝. The feature functions in this RKHS are: Embedding equal to marginal probability vector in the discrete case ©Eric Xing @ CMU, 2012-2014 33 Why? Mean Map cont. If f is in the RKHS ©Eric Xing @ CMU, 2012-2014 34 Embedding Joint Distribution of 2 Variables [Smola et al. 2007] Define the uncentered cross-covariance operator ࢅࢄ implicitly such that Note now ࢌis in one Hilbert Space, while ࢍis in another. ࡯ࢅࢄ will be our embedding of the joint distribution of X and Y. Note now ࡯ࢅࢄ is an operator, just like ࡼࢄ, ࢅis a matrix. ©Eric Xing @ CMU, 2012-2014 35 Cross Covariance Operator cont. Let and (the feature functions of these two RKHSs) Then explicit form of cross-covariance operator is: Looks like the Uncentered Covariance of two variables X and Y: ©Eric Xing @ CMU, 2012-2014 36 Embedding Joint Distribution of Two Variables [Smola et al. 2007] Embed in the Tensor Product of two RKHS’s ©Eric Xing @ CMU, 2012-2014 37 Consider two finite sets: If “outer product” is defined as: Then tensor product is: (Don’t take the example too literally since this is not a vector space) “Tensor Product” Intuition ©Eric Xing @ CMU, 2012-2014 38 Tensor Product of Two Vector Spaces ©Eric Xing @ CMU, 2012-2014 39 Cross Covariance Operator cont. Proof: Move expectation outside Definition of outer product Rearrange Reproducing Property ©Eric Xing @ CMU, 2012-2014 40 Auto Covariance Operator The uncentered auto-covariance operator is: Looks like the uncentered variance of X Intuition: Analogous to ©Eric Xing @ CMU, 2012-2014 41 Conditional Embedding Operator Conditional Embedding Operator: Intuition: ©Eric Xing @ CMU, 2012-2014 42 Conditional Embedding Cont. Conditional Embedding Operator: Has Following Property: Analogous to “Slicing” a Conditional Probability Table in the Discrete Case: ©Eric Xing @ CMU, 2012-2014 43 Why We Care So we have some statistics for marginal, joint, and conditional distributions…. How does this help us define Belief Propagation? There are many parametric distributions where it is hard to define message passing Think Back: What makes Gaussians different?  Easy to marginalize, perform Chain Rule with Gaussians! ©Eric Xing @ CMU, 2012-2014 44 Why we Like Hilbert Space Embeddings We will prove these in the next lecture We can marginalize and use chain rule in Hilbert Space too!!! Sum Rule: Chain Rule: Sum Rule in RKHS: Chain Rule in RKHS: ©Eric Xing @ CMU, 2012-2014 45 Summary Hilbert Space Embedding provides a way to create a “sufficient statistic” for an arbitrary distribution. Can embed marginal, joint, and conditional distributions into the RKHS Next time:  Prove sum rule and chain rule for RKHS embedding  Performing Belief Propagation with the Embedding Operators  Why the messages are easily computed from data (and not infinite) ©Eric Xing @ CMU, 2012-2014 46 References  Smola, A. J., Gretton, A., Song, L., and Schölkopf, B., A Hilbert Space Embedding for Distributions, Algorithmic Learning Theory, E. Takimoto (Eds.), Lecture Notes on Computer Science, Springer, 2007.  L. Song. Learning via Hilbert space embedding of distributions. PhD Thesis 2008.  Song, L., Huang, J., Smola, A., and Fukumizu, K., Hilbert space embeddings of conditional distributions, International Conference on Machine Learning, 2009. ©Eric Xing @ CMU, 2012-2014 47 "
23,"School of Computer Science Probabilistic Graphical Models Kernel Graphical Models Eric Xing Lecture 23, April 9, 2014 Acknowledgement: slides first drafted by Ankur Parikh ©Eric Xing @ CMU, 2012-2014 1 Nonparametric Graphical Models How do we make a conditional probability table out of this? How to learn parameters? How to perform inference? Hilbert Space Embeddings!!!!! ©Eric Xing @ CMU, 2012-2014 2 Important Notation for this Lecture We will use the calligraphic P to denote that the probability is being treated as a matrix/vector/tensor Probabilities Probability Vectors/Matrices/Tensors ©Eric Xing @ CMU, 2012-2014 3 Review: Embedding Distribution of One Variable[Smola et al. 2007] density The Hilbert Space Embedding of X ©Eric Xing @ CMU, 2012-2014 4 Review: Cross Covariance Operator [Smola et al. 2007] Embed Joint Distribution of X and Y in the Tensor Product of two RKHS’s Embedding of ©Eric Xing @ CMU, 2012-2014 5 Review: Auto Covariance Operator [Smola et al. 2007] Only take expectation over these Embedding of ©Eric Xing @ CMU, 2012-2014 6 Review: Conditional Embedding Operator [Song et al. 2009] Conditional Embedding Operator: Has Following Property: Analogous to “Slicing” a Conditional Probability Table in the Discrete Case: ©Eric Xing @ CMU, 2012-2014 7 Slicing the Conditional Probability Matrix ©Eric Xing @ CMU, 2012-2014 8 “Slicing” the Conditional Embedding Operator ©Eric Xing @ CMU, 2012-2014 9 Why we Like Hilbert Space Embeddings We will prove these now We can marginalize and use chain rule in Hilbert Space too!!! Sum Rule: Chain Rule: Sum Rule in RKHS: Chain Rule in RKHS: ©Eric Xing @ CMU, 2012-2014 10 Sum Rules The sum rule can be expressed in two ways: First way: Second way: What is special about the second way? Intuitively, it can be expressed elegantly as matrix multiplication  Does not work in RKHS, since there is no “sum” operation for an operator Works in RKHS!!! ©Eric Xing @ CMU, 2012-2014 11 Sum Rule Equivalent view using Matrix Algebra Sum Rule (Matrix Form) ©Eric Xing @ CMU, 2012-2014 12 Chain Rule Equivalent view using Matrix Algebra Note how diagonal is used to keep Y from being marginalized out. Chain Rule (Matrix Form) Means on diagonal ©Eric Xing @ CMU, 2012-2014 13 Example What about? Only if B and C are conditionally independent given A!!! ©Eric Xing @ CMU, 2012-2014 14 Let’s now derive the matrix sum rule differently. Let ࢾ࢏ denote an indicator vector, that is 1 in the ݅௧௛position. Different Proof of Matrix Sum Rule with Expectations ©Eric Xing @ CMU, 2012-2014 15 Random Variables? Remember this is a probability vector. It is not a random variable. Similarly, this is a function in an RKHS. It is not a random variable. This is a random vector This is a random function ©Eric Xing @ CMU, 2012-2014 16 Expectation Proof of Matrix Sum Rule Cont. This is a conditional probability matrix, so it is not a random variable (despite the misleading notation), and thus the Expectation can be pulled out This is a random variable ©Eric Xing @ CMU, 2012-2014 17 Proof of RKHS Sum Rule Now apply the same technique to the RKHS Case. Move expectation outside Property of conditional embedding Property of Expectation Definition of Mean Map ©Eric Xing @ CMU, 2012-2014 18 The idea is to replace the CPTs with RKHS operators/functions. Let’s do this for a simple example first. We would like to compute Kernel Graphical Models [Song et al. 2010, Song et al. 2011] ©Eric Xing @ CMU, 2012-2014 19 Consider the Discrete Case ©Eric Xing @ CMU, 2012-2014 20 Inference as Matrix Multiplication Oops....we accidentally integrated out A ©Eric Xing @ CMU, 2012-2014 21 Put A on Diagonal Instead ©Eric Xing @ CMU, 2012-2014 22 Now it works ©Eric Xing @ CMU, 2012-2014 23 Introducing evidence Introduce evidence with delta vectors ©Eric Xing @ CMU, 2012-2014 24 Now with Kernels ©Eric Xing @ CMU, 2012-2014 25 Sum-Product with Kernels ©Eric Xing @ CMU, 2012-2014 26 Sum-Product with Kernels ©Eric Xing @ CMU, 2012-2014 27 Consider just evaluating one random variable X at a particular evidence value using the Gaussian RBF Kernel: What does this looks like? What does it mean to evaluate the mean map at a point? ©Eric Xing @ CMU, 2012-2014 28 Consider Kernel Density Estimate at point : And its empirical estimate: So evaluating the mean map at a point is like an unnormalized kernel density estimate. To find the “MAP” assignment, we can evaluate on a grid of points, and then pick the one with the highest value. Kernel Density Estimation! ©Eric Xing @ CMU, 2012-2014 29 Kernel Density Estimation with Gaussian RBF Kernel in Multiple Variables is: Like evaluating a “Huge” Covariance Operator using Gaussian RBF Kernel (without normalization): Multiple Variables ©Eric Xing @ CMU, 2012-2014 30 What is the problem with this? The empirical estimate is very inaccurate because of curse of dimensionality Empirically computing the “huge” covariance operator will have the same problem. But then what is the point of Hilbert Space Embeddings? ©Eric Xing @ CMU, 2012-2014 31 We can factorize the “Huge” Covariance Operator Hilbert Space Embeddings allow us to factorize the huge covariance operator using the graphical model structure that kernel density estimation does not do. Factorizes into smaller covariance/conditional embedding operators using the graphical model that are more efficient to estimate. ©Eric Xing @ CMU, 2012-2014 32 Kernel Graphical Models: The Overall Picture Naïve way to represent joint distribution of discrete variables is to store and manipulate a “huge” probability table. Naïve way to represent joint distribution for many continuous variables is to use multivariate kernel density estimation. Discrete Graphical Models allow us to factorize the “huge” joint distribution table into smaller factors. Kernel Graphical Models allow us to factorize joint distributions of continuous variables into smaller factors. ©Eric Xing @ CMU, 2012-2014 33 Consider an Even Simpler Graphical Model We are going to show how to estimate these operators from data. ©Eric Xing @ CMU, 2012-2014 34 The Kernel Matrix … . . . . . . . . . … ©Eric Xing @ CMU, 2012-2014 35 Empirical Estimate Auto Covariance Defined on next slide ©Eric Xing @ CMU, 2012-2014 36 Conceptually, ©Eric Xing @ CMU, 2012-2014 37 Conceptually, ©Eric Xing @ CMU, 2012-2014 38 Conceptually, ©Eric Xing @ CMU, 2012-2014 39 Rigorously, is an operator that maps vectors in to functions in such that: Its adjoint (transpose) can then be derived to be: ©Eric Xing @ CMU, 2012-2014 40 Empirical Estimate Cross Covariance ©Eric Xing @ CMU, 2012-2014 41 Getting the Kernel Matrix It can then be shown that, This is finite and easy to compute!!  However, note that the estimates of the covariance operators are not finite since: ©Eric Xing @ CMU, 2012-2014 42 Intuition 1: Why the Kernel Trick works This operator is infinite dimensional but it has at most rank N The kernel matrix is N by N, and thus the kernel trick is exploiting the low rank structure ©Eric Xing @ CMU, 2012-2014 43 Empirical Estimate of Conditional Embedding Operator 44 Sort of…… We need to regularize so that this is invertible ? regularizer ©Eric Xing @ CMU, 2012-2014 Return of Matrix Inversion Lemma Matrix Inversion Identity Using it we get, ©Eric Xing @ CMU, 2012-2014 45 But Our estimates are still Infinite…. Lets do inference and see what happens. ©Eric Xing @ CMU, 2012-2014 46 Running Inference ©Eric Xing @ CMU, 2012-2014 47 Incorporating the Evidence ©Eric Xing @ CMU, 2012-2014 48 Reparameterize the Model A B C Evidence: Finite!!!!! ©Eric Xing @ CMU, 2012-2014 49 Intuition 2: Why the Kernel Trick Works … . . . . . . . . . … ©Eric Xing @ CMU, 2012-2014 50 Intuition 2: Why the Kernel Trick Works … . . . . . . … . . . Evaluating a feature function at the N data points!!! ©Eric Xing @ CMU, 2012-2014 51 Intuition 2: Why the Kernel Trick Works Generally people interpret the kernel matrix to be a similarity matrix. However, we can also view each row of the kernel matrix as evaluating a function at the N data points. Although the function may be continuous and not easily represented analytically, we only really care about what its value is on the N data points. Thus, when we only have a finite amount of data, the computation should be inherently finite. ©Eric Xing @ CMU, 2012-2014 52 Protein Sidechains http://t3.gstatic.com/images?q=tbn:ANd9GcS_nfJy1o9yrDt3 7YlpK7i5s0f7QFqhPrG7-1CLm2AfWNt5wCE50pIKNZd0 Goal is to predict the 3D configuration of each sidechain ©Eric Xing @ CMU, 2012-2014 53 3D configuration of the sidechain is determined by two angles (spherical coordinates). Protein Sidechains http://www.math24.net/images/triple-int23.jpg ©Eric Xing @ CMU, 2012-2014 54 The Graphical Model Construct a Markov Random Field. Each side-chain angle pair is a node. There is an edge between side-chains that are nearby in the protein. Edge potentials are already determined by physics equations. ©Eric Xing @ CMU, 2012-2014 55 Goal is to find the MAP assignment of all the sidechain angle pairs. Note that this is not Gaussian. But it is easy to define a kernel between angle pairs: Can then run Kernel Belief Propagation  The Graphical Model ©Eric Xing @ CMU, 2012-2014 56 References  Smola, A. J., Gretton, A., Song, L., and Schölkopf, B., A Hilbert Space Embedding for Distributions, Algorithmic Learning Theory, E. Takimoto (Eds.), Lecture Notes on Computer Science, Springer, 2007.  L. Song. Learning via Hilbert space embedding of distributions. PhD Thesis 2008.  Song, L., Huang, J., Smola, A., and Fukumizu, K., Hilbert space embeddings of conditional distributions, International Conference on Machine Learning, 2009.  Song, L., Gretton, A., and Guestrin, C., Nonparametric Tree Graphical Models via Kernel Embeddings, Artificial Intelligence and Statistics (AISTATS), 2010.  Song, L., Gretton, A., Bickson, D., Low, Y., and Guestrin, C., Kernel Belief Propagation, International Conference on Artifical Intelligence and Statistics (AISTATS), 2011. ©Eric Xing @ CMU, 2012-2014 57 Supplemental: Kernel Belief Propagation on Trees ©Eric Xing @ CMU, 2012-2014 58 Kernel Tree Graphical Models [Song et al. 2010] The goal is to somehow replace the CPTs with RKHS operators/functions. But we need to do this in a certain way so that we can still do inference. ? ? ©Eric Xing @ CMU, 2012-2014 59 We need to “matricize” message passing to apply the RKHS trick (but matrices are not enough, we need tensors ) Message Passing/Belief Propagation © Ankur Parikh, Eric Xing @ CMU, 2012-2013 ©Eric Xing @ CMU, 2012-2014 60 Show how to represent discrete graphical models using higher order tensors Derive Tensor Message Passing Show how Tensor Message Passing can also be derived using Expectations Derive Kernel Message Passing [Song et al. 2010] using the intuition from Tensor Message Passing / Expectations (For simplicity, we will assume a binary tree – all internal nodes have 2 children). Outline © Ankur Parikh, Eric Xing @ CMU, 2012-2013 ©Eric Xing @ CMU, 2012-2014 61 Multidimensional arrays A Tensor of order N has N modes (N indices): Each mode is associated with a dimension. In the example,  Dimension of mode 1 is 4  Dimension of mode 2 is 3  Dimension of mode 3 is 4 Tensors 4 3 4 ©Eric Xing @ CMU, 2012-2014 62 Diagonal Tensors ©Eric Xing @ CMU, 2012-2014 63 Partially Diagonal Tensors ©Eric Xing @ CMU, 2012-2014 64 Multiplying a 3rd order tensor by a vector produces a matrix Tensor Vector Multiplication ©Eric Xing @ CMU, 2012-2014 65 Tensor Vector Multiplication Cont. Multiplying a 3rd order tensor by two vectors produces a vector ©Eric Xing @ CMU, 2012-2014 66 Conditional Probability Table At Leaf is a Matrix ©Eric Xing @ CMU, 2012-2014 67 CPT At Internal Node (Non-Root) is 3rd Order Tensor Note that we have ©Eric Xing @ CMU, 2012-2014 68 CPT At Root CPT at root is a matrix. ©Eric Xing @ CMU, 2012-2014 69 The Outgoing Message as a Vector (at Leaf) “bar” denotes evidence ©Eric Xing @ CMU, 2012-2014 70 The Outgoing Message At Internal Node ©Eric Xing @ CMU, 2012-2014 71 At the Root ©Eric Xing @ CMU, 2012-2014 72 Kernel Graphical Models [Song et al. 2010, Song et al. 2011] The Tensor CPTs at each node are replaced with RKHS functions/operators Leaf: Internal (non-root): Root: ©Eric Xing @ CMU, 2012-2014 73 Conditional Embedding Operator for Internal Nodes Embedding of What is ? ©Eric Xing @ CMU, 2012-2014 74 Embedding of Cross Covariance Operator in Different RKHS Embedding of ©Eric Xing @ CMU, 2012-2014 75 "
24,"School of Computer Science Probabilistic Graphical Models Spectral Learning for Graphical Models Eric Xing Lecture 24, April 14, 2014 Acknowledgement: slides drafted by Ankur Parikh ©Eric Xing @ CMU, 2012-2014 1 Latent Variable Models Ho. et al. 2012 Sequence models Parsing Mixed membership models ©Eric Xing @ CMU, 2012-2014 2 Latent Variable PCFG [Matsuzaki et al., 2005, Petrov et al. 2006] PCFG Latent Variable PCFG ©Eric Xing @ CMU, 2012-2014 3 Learning Parameters (EM) Since latent variables are not observed in the data, we have to use Expectation Maximization (EM) to learn parameters • Slow • Local Minima latent variables (unobserved in training data) Observed variable ©Eric Xing @ CMU, 2012-2014 4 Spectral Learning  Different paradigm of learning in latent variable models based on linear algebra  Theoretically,  Provably consistent  Can offer deeper insight into the identifiability  Practically,  Local minima free  As if now, performs comparably to EM with 10-100x speed-up  Can also model non-Gaussian continuous data using kernels (usually performs much better than EM in this case) ©Eric Xing @ CMU, 2012-2014 5 Related References Relevant works  Hsu et al. 2009 – Spectral HMMs (also Bailly 2009)  Siddiqi et al. 2009 – Features in Spectral Learning  Parikh et al. 2011/2012 –Tensors to Generalize to Trees/Low Treewidth Graphs  Cohen et al. 2012 / 2013 – Spectral Learning of latent PCFGs Will present it from “matrix factorization” view:  Balle et al. 2012 – Connection between Spectral Learning / Hankel Matrix Factorization  Song et al. 2013 – Spectral Learning as Hierarchical Tensor Decomposition ©Eric Xing @ CMU, 2012-2014 6 Focusing on Prediction  In many applications that use latent variable models, the end task is not to recover the latent states, but rather to use the model for prediction among observed variables.  Dynamical Systems – Predict future given past future past ©Eric Xing @ CMU, 2012-2014 7  We will only be concerned with quantities related to the observed variables:  We do not care about the latent variables explicitly. Do we still need EM to learn the parameters? Focusing on Prediction ©Eric Xing @ CMU, 2012-2014 8 But if we don’t care about the latent variables.... Why don’t we just integrate them out? Because integrating them out results in a clique  ©Eric Xing @ CMU, 2012-2014 9 Marginal Does Not Factorize Does not factorize due to the outer sum (Can somewhat distribute the sum, but doesn’t solve problem) ©Eric Xing @ CMU, 2012-2014 10 But isn’t an HMM different from a clique? It depends on the number of latent states. Consider the following model. ©Eric Xing @ CMU, 2012-2014 11 If H has only one state..... Then the observed variables are independent! ©Eric Xing @ CMU, 2012-2014 12 What if H has many states? Let us say the observed variables each have m states. Then if H has m3 states then the latent model can be exactly equivalent to a clique (depending on how parameters are set). But what about all the other cases? ©Eric Xing @ CMU, 2012-2014 13 The Question Under existing methods, latent models all require EM to learn regardless of the number of hidden states. However, is there a formulation of latent variable models where the difficulty of learning is a function of the number of latent states? This is the question that the spectral view will answer. ©Eric Xing @ CMU, 2012-2014 14 Sum Rule Equivalent view using Matrix Algebra Sum Rule (Matrix Form) ©Eric Xing @ CMU, 2012-2014 15 Important Notation Calligraphic P to denotes that the probability is being treated as a matrix/vector/tensor Probabilities Probability Vectors/Matrices/Tensors ©Eric Xing @ CMU, 2012-2014 16 Chain Rule Equivalent view using Matrix Algebra Note how diagonal is used to keep Y from being marginalized out. Chain Rule (Matrix Form) Means on diagonal ©Eric Xing @ CMU, 2012-2014 17 Graphical Models: The Linear Algebra View  In general, nothing we can say about the nature of this matrix. A and B have m states each. ©Eric Xing @ CMU, 2012-2014 18  What if we know A and B are independent?  Joint probability matrix is rank one, since all rows are multiples of one another!! Independence: The Linear Algebra View ©Eric Xing @ CMU, 2012-2014 19 Independence and Rank What about rank in between 1 and m? has rank m (at most) has rank 1 ©Eric Xing @ CMU, 2012-2014 20 Low Rank Structure A and B are not marginally independent (They are only conditionally independent given X). Assume X has k states (while A and B have m states). Then, Why? ©Eric Xing @ CMU, 2012-2014 21 Low Rank Structure ࢘ࢇ࢔࢑൑࢑ ࢘ࢇ࢔࢑൑࢑ ࢘ࢇ࢔࢑൑࢑ ࢘ࢇ࢔࢑൑࢑ ©Eric Xing @ CMU, 2012-2014 22 The Spectral View Latent variable models encode low rank dependencies among variables (both marginal and conditional) Use tools from linear algebra to exploit this structure.  Rank  Eigenvalues  SVD  Tensors ©Eric Xing @ CMU, 2012-2014 23 A More Interesting Example ࢄ૚ ࢄ૛ ࢄ૜ ࢄ૝ k states m states has rank k ©Eric Xing @ CMU, 2012-2014 24 Low Rank Matrices “Factorize” m by n We already know one factorization!!! m by k k by n If M has rank k Factor of 4 variables Factor of 3 variables Factor of 1 variable Factor of 3 variables ©Eric Xing @ CMU, 2012-2014 25 Alternate Factorizations The key insight is that this factorization is not unique. Consider Matrix Factorization. Can add any invertible transformation: The magic of spectral learning is that there exists an alternative factorization that only depends on observed variables! ©Eric Xing @ CMU, 2012-2014 26 An Alternate Factorization Let us say we only want to factorize this matrix of 4 variables such that it is product of matrices that contain at most three observed variables e.g. ©Eric Xing @ CMU, 2012-2014 27 An Alternate Factorization Note that Product of green terms (in some order) is Product of red terms (in some order) is ©Eric Xing @ CMU, 2012-2014 28 An Alternate Factorization factor of 4 variables factor of 3 variables factor of 3 variables Caveat: some factors are no longer probability tables (do not have to be non-negative) Advantage: Factors are only functions of observed variables! Can be directly computed from data without EM!!!! We will call this factorization the observable factorization. ©Eric Xing @ CMU, 2012-2014 29 Graphical Relationship ࢄ૚ ࢄ૛ ࢄ૜ ࢄ૝ ©Eric Xing @ CMU, 2012-2014 30 Another Factorization Seems we would do better empirically if you could “combine” both factorizations. Will come back to this later. ©Eric Xing @ CMU, 2012-2014 31 Relationship to Original Factorization What is the relationship between the original factorization and the new factorization? Can I choose S to get the observable factorization? ©Eric Xing @ CMU, 2012-2014 32 Relationship to Original Factorization Let ©Eric Xing @ CMU, 2012-2014 33 It may not seem very amazing at the moment (we have only reduced the size of the factor by 1) What is cool is that every latent tree of V variables has such a factorization where:  All factors are of size 3  All factors are only functions of observed variables Our Alternate Factorization factor of 4 variables factor of 3 variables factor of 3 variables ©Eric Xing @ CMU, 2012-2014 34 Training / Testing with Spectral Learning We have that In training, we compute estimates: In test time, we can compute probability estimates (let lowercase letters denote fixed evidence values): ©Eric Xing @ CMU, 2012-2014 35 Generalizing To More Variables Consider HMM with 5 observations. Using similar arguments as before we will get that: ©Eric Xing @ CMU, 2012-2014 36 reshape and decompose recursively Consistency A trivial consistent estimator is to simply attempt to estimate the “big” probability table from the data without making any conditional independence assumptions While this is consistent, it is not very statistically efficient as number of samples increases ©Eric Xing @ CMU, 2012-2014 37 Consistency A better estimate is to get compute likelihood estimates of the factorization: But this requires running EM, which will get stuck in local optima and is not guaranteed to obtain the MLE of the factorized model ©Eric Xing @ CMU, 2012-2014 38 Consistency In spectral learning, we estimate the alternate factorization from the data This is consistent and computationally tractable (at some loss of statistical efficiency due to the dependence on the inverse) ©Eric Xing @ CMU, 2012-2014 39 Where’s the Catch? Before we said that if the number of latent states was very large then the model was equivalent to a clique. Where does that scenario enter in our factorization? When does this inverse exist? ©Eric Xing @ CMU, 2012-2014 40 When Does the Inverse Exist All the matrices on the right hand side must have full rank. (This is in general a requirement of spectral learning, although it can be somewhat relaxed) ©Eric Xing @ CMU, 2012-2014 41 When m > k  The inverse cannot exist, but this situation is easily fixable (project onto lower dimensional space)  Where U, V are the top left/right k singular vectors of ©Eric Xing @ CMU, 2012-2014 42 When k > m The inverse does exist. But it no longer satisfies the following property, which we used to derive the factorization This is much more difficult to fix, and intuitively corresponds to how the problem becomes intractable if k >> m. ©Eric Xing @ CMU, 2012-2014 43 What does k>m mean? Intuitively, large k, small m means long range dependencies Consider following generative process: (1) With probability 0.5, let S= X, and with probability 0.5 let S=Y. (2) Print A n times. (3) Print S (4) Go back to step (2) With n=1 we either generate: AXAXAXA…… or AYAYAYA….. With n=2 we either generate: AAXAAXAA….. or AAYAAYAA……. ©Eric Xing @ CMU, 2012-2014 44 How many hidden states does HMM need? HMM needs 2n states. Needs to remember count as well as whether we picked S=X or S=Y However, number of observed states m does not change, so our previous spectral algorithm will break for n > 2. How to deal with this in spectral framework? ©Eric Xing @ CMU, 2012-2014 45 Making Spectral Learning Work In Practice We are only using marginals of pairs/triples of variables to construct the full marginal among the observed variables. Only works when k < m. However, in real problems we need to capture longer range dependencies. ©Eric Xing @ CMU, 2012-2014 46 Recall our factorization ࢄ૚ ࢄ૛ ࢄ૜ ࢄ૝ ©Eric Xing @ CMU, 2012-2014 47 Key Idea: Use Long-Range Features Construct feature vector of left side Construct feature vector of right side ©Eric Xing @ CMU, 2012-2014 48 Spectral Learning With Features Use more complex feature instead: ©Eric Xing @ CMU, 2012-2014 49 Experimentally, Has been shown by many authors that (with some work) spectral methods achieve comparable results to EM but are 10-50x faster  Parikh et al. 2011 / 2012  Balle et al. 2012  Cohen et al. 2012 / 2013 The following are some synthetic and real data results demonstrating the comparison between EM and spectral methods. ©Eric Xing @ CMU, 2012-2014 50 Synthetic Data [Parikh et al. 2012] Synthetic 3rd order HMM Example (Spectral/EM/Online EM): Training Samples Runtime vs. Sample Size Runtime(s) Online EM EM Spectral Training Samples Error vs. Sample Size Error Spectral Online EM EM ©Eric Xing @ CMU, 2012-2014 51 Empirical Results for Latent PCFGs [Cohen et al. 2013] ©Eric Xing @ CMU, 2012-2014 52 Timing Results on Latent PCFGs[Cohen et al. 2013] ©Eric Xing @ CMU, 2012-2014 53 It is difficult to run EM if the conditional/marginal distributions are continuous and do not easily fit into a parametric family. However, we will see that Hilbert Space Embeddings can easily be combined with spectral methods for learning nonparametric latent models. Dealing with Nonparametric, Continuous Variables ©Eric Xing @ CMU, 2012-2014 54 Connection to Hilbert Space Embeddings Recall that we could substitute features for variables Use more complex feature instead: ©Eric Xing @ CMU, 2012-2014 55 Can Also Use Infinite Dimensional Features Replace with (and similarly for other quantities) covariance operator ©Eric Xing @ CMU, 2012-2014 56 Connection to Hilbert Space Embeddings Discrete case: Continuous case: ©Eric Xing @ CMU, 2012-2014 57 Summary - EM & Spectral (Part I) EM Spectral • Aims to Find MLE so more “statistically” efficient • Can get stuck in local-optima • Lack of theoretical guarantees • Slow • Easy to derive for new models • Does not aim to find MLE so less statistically efficient. • Local-optima-free • Provably consistent • Very fast • Challenging to derive for new models (Unknown whether it can generalize to arbitrary loopy models) ©Eric Xing @ CMU, 2012-2014 58 Summary - EM & Spectral (Part II) EM Spectral • No issues with negative numbers • Allows for easy modelling with conditional distributions • Difficult to incorporate long-range features (since it increases treewidth). • Generalizes poorly to non- Gaussian continuous variables. • Problems with negative numbers. Requires explicit normalization to compute likelihood. • Allows for easy modelling with marginal distributions • Easy to incorporate long-range features. • Easy to generalize to non- Gaussian continuous variables via Hilbert Space Embeddings ©Eric Xing @ CMU, 2012-2014 59 "
25,"School of Computer Science Probabilistic Graphical Models Graph-induced structured input/output models - Case Study: Disease Association Analysis Eric Xing Lecture 25, April 16, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 Healthy Sick ACTCGTACGTAGACCTAGCATTACGCAATAATGCGA ACTCGAACCTAGACCTAGCATTACGCAATAATGCGA TCTCGTACGTAGACGTAGCATTACGCAATTATCCGA ACTCGAACCTAGACCTAGCATTACGCAATTATCCGA ACTCGTACGTAGACGTAGCATAACGCAATAATGCGA TCTCGTACCTAGACGTAGCATAACGCAATAATCCGA ACTCGAACCTAGACCTAGCATAACGCAATTATCCGA Single nucleotide polymorphism (SNP) Causal (or ""associated"") SNP Genetic Basis of Diseases 2 © Eric Xing @ CMU, 2005-2014 A . . . . T . . G . . . . . C . . . . . . T . . . . . A . . G . A . . . . A . . C . . . . . C . . . . . . T . . . . . A . . G . T . . . . T . . G . . . . . G . . . . . . T . . . . . T . . C . A . . . . A . . C . . . . . C . . . . . . T . . . . . T . . C . A . . . . T . . G . . . . . G . . . . . . A . . . . . A . . G . T . . . . T . . C . . . . . G . . . . . . A . . . . . A . . C . A . . . . A . . C . . . . . C . . . . . . A . . . . . T . . C . Data Genotype Phenotype • Cancer: Dunning et al. 2009. • Diabetes: Dupuis et al. 2010. • Atopic dermatitis: Esparza-Gordillo et al. 2009. • Arthritis: Suzuki et al. 2008 Standard Approach causal SNP a univariate phenotype: e.g., disease/control Genetic Association Mapping 3 © Eric Xing @ CMU, 2005-2014 Healthy Cancer ACTCGTACGTAGACCTAGCATTACGCAATAATGCGA ACTCGAACCTAGACCTAGCATTACGCAATAATGCGA TCTCGTACGTAGACGTAGCATTACGCAATTATCCGA ACTCGAACCTAGACCTAGCATTACGCAATTATCCGA ACTCGTACGTAGACGTAGCATAACGCAATAATGCGA TCTCGTACCTAGACGTAGCATAACGCAATAATCCGA ACTCGAACCTAGACCTAGCATAACGCAATTATCCGA Causal SNPs Genetic Basis of Complex Diseases 4 © Eric Xing @ CMU, 2005-2014 Genetic Basis of Complex Diseases Healthy Cancer ACTCGTACGTAGACCTAGCATTACGCAATAATGCGA ACTCGAACCTAGACCTAGCATTACGCAATAATGCGA TCTCGTACGTAGACGTAGCATTACGCAATTATCCGA ACTCGAACCTAGACCTAGCATTACGCAATTATCCGA ACTCGTACGTAGACGTAGCATAACGCAATAATGCGA TCTCGTACCTAGACGTAGCATAACGCAATAATCCGA ACTCGAACCTAGACCTAGCATAACGCAATTATCCGA Causal SNPs Biological mechanism 5 © Eric Xing @ CMU, 2005-2014 Intermediate Phenotype Genetic Basis of Complex Diseases Healthy Cancer ACTCGTACGTAGACCTAGCATTACGCAATAATGCGA ACTCGAACCTAGACCTAGCATTACGCAATAATGCGA TCTCGTACGTAGACGTAGCATTACGCAATTATCCGA ACTCGAACCTAGACCTAGCATTACGCAATTATCCGA ACTCGTACGTAGACGTAGCATAACGCAATAATGCGA TCTCGTACCTAGACGTAGCATAACGCAATAATCCGA ACTCGAACCTAGACCTAGCATAACGCAATTATCCGA Causal SNPs Clinical records Gene expression Association to intermediate phenotypes 6 © Eric Xing @ CMU, 2005-2014 Association with Phenome ACGTTTTACTGTACAATT Multivariate complex syndrome (e.g., asthma) age at onset, history of eczema genome‐wide expression profile ACGTTTTACTGTACAATT Traditional Approach causal SNP a univariate phenotype: gene expression level Structured Association 7 © Eric Xing @ CMU, 2005-2014 Consider one phenotype & one genotype at a time Consider multiple correlated phenotypes & genotypes jointly vs. Standard Approach New Approach Phenotypes Phenome Goal: Inferring Structured Association 8 © Eric Xing @ CMU, 2005-2014 Sparse Associations Pleotropic effects Epistatic effects 9 © Eric Xing @ CMU, 2005-2014 Sparse Learning  Linear Model:  Lasso (Sparse Linear Regression)  Why sparse solution? penalizing constraining [R.Tibshirani 96] 10 © Eric Xing @ CMU, 2005-2014 Multi-Task Extension  Multi-Task Linear Model: Coefficients for k-th task: Coefficient Matrix: Input: Output: Coefficients for a variable (2nd) Coefficients for a task (2nd) 11 © Eric Xing @ CMU, 2005-2014 Background: Sparse multivariate regression for disease association studies Structured association – a new paradigm Association to a graph-structured phenome Graph-guided fused lasso (Kim & Xing, PLoS Genetics, 2009) Association to a tree-structured phenome Tree-guided group lasso (Kim & Xing, ICML 2010) Outline 12 © Eric Xing @ CMU, 2005-2014 Subnetworks for lung physiology Subnetwork for quality of life Genetic Association for Asthma Clinical Traits TCGACGTTTTACTGTACAATT Statistical challenge How to find associations to a multivariate entity in a graph? 13 © Eric Xing @ CMU, 2005-2014 T G A A C C A T G A A G T A Genotype Trait Multivariate Regression for Single- Trait Association Analysis X y 2.1 x = x = β Association Strength ? 14 © Eric Xing @ CMU, 2005-2014 T G A A C C A T G A A G T A Genotype Trait Multivariate Regression for Single- Trait Association Analysis Many non-zero associations: Which SNPs are truly significant? 2.1 x = Association Strength 15 © Eric Xing @ CMU, 2005-2014 Genotype x = 2.1 Trait Lasso for Reducing False Positives (Tibshirani, 1996) Many zero associations (sparse results), but what if there are multiple related traits? +   J j 1  |βj | T G A A C C A T G A A G T A Lasso Penalty for sparsity Association Strength 16 © Eric Xing @ CMU, 2005-2014 Genotype (3.4, 1.5, 2.1, 0.9, 1.8) Multivariate Regression for Multiple- Trait Association Analysis T G A A C C A T G A A G T A How to combine information across multiple traits to increase the power? Association Strength x = Association strength between SNP j and Trait i: βj,i Allergy Lung physiology LD Synthetic lethal +  j i,  |βj,i| 17 © Eric Xing @ CMU, 2005-2014 Genotype (3.4, 1.5, 2.1, 0.9, 1.8) Trait Multivariate Regression for Multiple- Trait Association Analysis T G A A C C A T G A A G T A Allergy Lung physiology Association Strength x = + We introduce graph-guided fusion penalty Association strength between SNP j and Trait i: βj,i +  j i,  |βj,i| 18 © Eric Xing @ CMU, 2005-2014 Multiple-trait Association: Graph-Constrained Fused Lasso Step 1: Thresholded correlation graph of phenotypes ACGTTTTACTGTACAATT Step 2: Graph‐constrained fused lasso Lasso Penalty Graph-constrained fusion penalty Fusion 19 © Eric Xing @ CMU, 2005-2014 Fusion Penalty Fusion Penalty: | βjk - βjm | For two correlated traits (connected in the network), the association strengths may have similar values. ACGTTTTACTGTACAATT SNP j Trait m Trait k Association strength between SNP j and Trait k: βjk Association strength between SNP j and Trait m: βjm 20 © Eric Xing @ CMU, 2005-2014 ACGTTTTACTGTACAATT Overall effect Graph-Constrained Fused Lasso Fusion effect propagates to the entire network Association between SNPs and subnetworks of traits 21 © Eric Xing @ CMU, 2005-2014 ACGTTTTACTGTACAATT Multiple-trait Association: Graph-Weighted Fused Lasso Subnetwork structure is embedded as a densely connected nodes with large edge weights Edges with small weights are effectively ignored Overall effect 22 © Eric Xing @ CMU, 2005-2014 Estimating Parameters Quadratic programming formulation Graph-constrained fused lasso Graph-weighted fused lasso Many publicly available software packages for solving convex optimization problems can be used 23 © Eric Xing @ CMU, 2005-2014 Improving Scalability Iterative optimization • Update βk • Update djk’s, djml’s Original problem Equivalently Using a variational formulation 24 © Eric Xing @ CMU, 2005-2014 Previous Works vs. Our Approach Previous approach Our approach PCA-based approach (Weller et al., 1996, Mangin et al., 1998) Implicit representation of trait correlations Hard to interpret the derived traits Explicit representation of trait correlations Extension of module network for eQTL study (Lee et al., 2009) Average traits within each trait cluster Loss of information Original data for traits are used Network-based approach (Chen et al., 2008, Emilsson et al., 2008) Separate association analysis for each trait (no information sharing) Single-trait association are combined in light of trait network modules Joint association analysis of multiple traits 25 © Eric Xing @ CMU, 2005-2014 50 SNPs taken from HapMap chromosome 7, CEU population 10 traits Trait Correlation Matrix True Regression Coefficients Single SNP-Single Trait Test Significant at α = 0.01 Lasso Graph-guided Fused Lasso Thresholded Trait Correlation Network Simulation Results Phenotypes SNPs No association High association 26 © Eric Xing @ CMU, 2005-2014 Simulation Results 27 © Eric Xing @ CMU, 2005-2014 Asthma Trait Network Subnetwork for quality of life Subnetwork for lung physiology Phenotype Correlation Network Subnetwork for Asthma symptoms 28 © Eric Xing @ CMU, 2005-2014 Results from Single-SNP/Trait Test © Eric Xing @ CMU, 2005-2014 29 Single‐Marker Single‐Trait Test SNPs Permutation test α = 0.05 Permutation test α = 0.01 No association High association Trait Network Phenotypes Phenotypes Lung physiology‐related traits I • Baseline FEV1 predicted value: MPVLung • Pre FEF 25-75 predicted value • Average nitric oxide value: online • Body Mass Index • Postbronchodilation FEV1, liters: Spirometry • Baseline FEV1 % predicted: Spirometry • Baseline predrug FEV1, % predicted • Baseline predrug FEV1, % predicted Q551R SNP • Codes for amino‐acid changes in the intracellular signaling portion of the receptor • Exon 11 Trait Network Lasso Graph‐guided Fused Lasso Comparison of Gflasso with Others © Eric Xing @ CMU, 2005-2014 30 Single‐Marker Single‐Trait Test SNPs Phenotypes Phenotypes ? Lung physiology‐related traits I • Baseline FEV1 predicted value: MPVLung • Pre FEF 25-75 predicted value • Average nitric oxide value: online • Body Mass Index • Postbronchodilation FEV1, liters: Spirometry • Baseline FEV1 % predicted: Spirometry • Baseline predrug FEV1, % predicted • Baseline predrug FEV1, % predicted Q551R SNP • Codes for amino‐acid changes in the intracellular signaling portion of the receptor • Exon 11 No association High association Linkage Disequilibrium Structure in IL-4R gene SNP Q551R SNP rs3024660 SNP rs3024622 r2 =0.64 r2 =0.07 31 © Eric Xing @ CMU, 2005-2014 IL4R Gene Chr16: 27,325,251-27,376,098 IL4R SNPs in intron SNPs in exon SNPs in promotor region exons introns Q551R rs3024660 rs3024622 32 © Eric Xing @ CMU, 2005-2014 Gene Expression Trait Analysis TCGACGTTTTACTGTACAATT Genes Samples Statistical challenge How to find associations to a multivariate entity in a tree? 33 © Eric Xing @ CMU, 2005-2014 TCGACGTTTTACTGTACAATT Tree-guided Group Lasso Why tree? Tree represents a clustering structure Scalability to a very large number of phenotypes Graph : O(|V|2) edges Tree : O(|V|) edges Expression quantitative trait mapping (eQTL) Agglomerative hierarchical clustering is a popular tool 34 © Eric Xing @ CMU, 2005-2014 Tree-Guided Group Lasso In a simple case of two genes • Low height • Tight correlation • Joint selection • Large height • Weak correlation • Separate selection h h Genotypes Genotypes 35 © Eric Xing @ CMU, 2005-2014 L1 penalty • Lasso penalty • Separate selection L2 penalty • Group lasso • Joint selection h Elastic net Select the child nodes jointly or separately? Tree-Guided Group Lasso In a simple case of two genes Tree-guided group lasso 36 © Eric Xing @ CMU, 2005-2014 h2 h1 Select the child nodes jointly or separately? Joint selection Separate selection Tree-Guided Group Lasso For a general tree Tree-guided group lasso 37 © Eric Xing @ CMU, 2005-2014 h2 h1 Select the child nodes jointly or separately? Tree-Guided Group Lasso For a general tree Tree-guided group lasso Joint selection Separate selection 38 © Eric Xing @ CMU, 2005-2014 Previously, in Jenatton, Audibert & Bach, 2009 Balanced Shrinkage 39 © Eric Xing @ CMU, 2005-2014 Estimating Parameters Second-order cone program Many publicly available software packages for solving convex optimization problems can be used Also, variational formulation 40 © Eric Xing @ CMU, 2005-2014 Illustration with Simulated Data True association strengths Lasso Tree‐guided group lasso SNPs Phenotypes No association High association 41 © Eric Xing @ CMU, 2005-2014 Structured Association Phenome Structure Graph-guided fused lasso (Kim & Xing, PLoS Genetics, 2009) Graph Tree-guided fused lasso (Kim & Xing, Submitted) Tree Temporally smoothed lasso (Kim, Howrylak, Xing, Submitted) Dynamic Trait Genome Structure Stochastic block regression (Kim & Xing, UAI, 2008) Linkage Disequilibrium Multi-population group lasso (Puniyani, Kim, Xing, Submitted) Population Structure Epistasis ACGTTTTACTGTACAATT Group lasso with networks (Lee, Kim, Xing, Submitted) 42 © Eric Xing @ CMU, 2005-2014 Computation Time 43 © Eric Xing @ CMU, 2005-2014 Proximal Gradient Descent Original Problem: Approximation Problem: Gradient of the Approximation: 44 © Eric Xing @ CMU, 2005-2014 Geometric Interpretation Smooth approximation Uppermost Line Nonsmooth Uppermost Line Smooth 45 © Eric Xing @ CMU, 2005-2014 Convergence Rate Theorem: If we require and set , the number of iterations is upper bounded by: Remarks: state of the art IPM method for for SOCP converges at a rate 46 © Eric Xing @ CMU, 2005-2014 Multi-Task Time Complexity Pre-compute: Per-iteration Complexity (computing gradient) Tree: Graph: IPM for SOCP Proximal-Gradient IPM for SOCP Proximal-Gradient Proximal-Gradient: Independent of Sample Size Linear in #.of Tasks 47 © Eric Xing @ CMU, 2005-2014 Experiments Multi-task Graph Structured Sparse Learning (GFlasso) 48 © Eric Xing @ CMU, 2005-2014 Experiments Multi-task Tree-Structured Sparse Learning (TreeLasso) 49 © Eric Xing @ CMU, 2005-2014 Conclusions Novel statistical methods for joint association analysis to correlated phenotypes  Graph-structured phenome : graph-guided fused lasso Tree-structured phenome : tree-guided group lasso Advantages Greater power to detect weak association signals Fewer false positives Joint association to multiple correlated phenotypes Other structures In phenotypes: dynamic trait In genotypes: linkage disequilibrium, population structure, epistasis 50 © Eric Xing @ CMU, 2005-2014 Reference  Tibshirani R (1996) Regression shrinkage and selection via the lasso. Journal of Royal Statistical Society, Series B 58:267–288.  Weller J, Wiggans G, Vanraden P, Ron M (1996) Application of a canonical transformation to detection of quantitative trait loci with the aid of genetic markers in a multi-trait experiment. Theoretical and Applied Genetics 92:998–1002.  Mangin B, Thoquet B, Grimsley N (1998) Pleiotropic QTL analysis. Biometrics 54:89–99.  Chen Y, Zhu J, Lum P, Yang X, Pinto S, et al. (2008) Variations in DNA elucidate molecular networks that cause disease. Nature 452:429–35.  Lee SI, Dudley A, Drubin D, Silver P, Krogan N, et al. (2009) Learning a prior on regulatory potential from eQTL data. PLoS Genetics 5:e1000358.  Emilsson V, Thorleifsson G, Zhang B, Leonardson A, Zink F, et al. (2008) Genetics of gene expression and its effect on disease. Nature 452:423–28.  Kim S, Xing EP (2009) Statistical estimation of correlated genome associations to a quantitative trait network. PLoS Genetics 5(8): e1000587.  Kim S, Xing EP (2008) Sparse feature learning in high-dimensional space via block regularized regression. In Proceedings of the 24th International Conference on Uncertainty in Artificial Intelligence (UAI), pages 325-332. AUAI Press.  Kim S, Xing EP (2010) Exploiting a hierarchical clustering tree of gene-expression traits in eQTL analysis. Submitted.  Kim S, Howrylak J, Xing EP (2010) Dynamic-trait association analysis via temporally-smoothed lasso. Submitted.  Puniyani K, Kim S, Xing EP (2010) Multi-population GWA mapping via multi-task regularized regression. Submitted.  Lee S, Kim S, Xing EP (2010) Leveraging genetic interaction networks and regulatory pathways for joint mapping of epistatic and marginal eQTLs. Submitted.  Dunning AM et al. (2009) Association of ESR1 gene tagging SNPs with breast cancer risk. Hum Mol Genet. 18(6):1131-9.  Esparza-Gordillo J et al. (2009) A common variant on chromosome 11q13 is associated with atopic dermatitis. Nature Genetics 41:596-601.  Suzuki A et al. (2008) Functional SNPs in CD244 increase the risk of rheumatoid arthritis in a Japanese population. Nature Genetics 40:1224-1229.  Dupuis J et al. (2010) New genetic loci implicated in fasting glucose homeostasis and their impact on type 2 diabetes risk. Nature Genetics 42:105-116. 51 © Eric Xing @ CMU, 2005-2014 "
257,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 1: Introduction CS447: Natural Language Processing (J. Hockenmaier) Course Staff Professor: Julia Hockenmaier juliahmr@illinois.edu Teaching assistants: Dhruv Agarwal dhruva2@illinois.edu Sai Krishna Bollam sbollam2@illinois.edu Zubin Pahuja zpahuja2@illinois.edu ""2 CS447: Natural Language Processing (J. Hockenmaier) Today’s lecture Course Overview: What is NLP? What will you learn in this course? Course Admin: How will we teach this course? How will you be assessed in this course? ""3 CS447: Natural Language Processing (J. Hockenmaier) What is Natural Language Processing? ""4 CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center CS447: Natural Language Processing (J. Hockenmaier) What is Natural Language Processing really ? ""6 CS447: Natural Language Processing (J. Hockenmaier) NLP in the news… ""7 CS447: Natural Language Processing (J. Hockenmaier) IBM’s Watson wins at Jeopardy! ""8 CS447: Natural Language Processing (J. Hockenmaier) Machine Translation ""9 Google Translate CS447: Natural Language Processing (J. Hockenmaier) Dialog systems, chatbots, digital assistants ""10 CS447: Natural Language Processing (J. Hockenmaier) Siri [Jurafsky & Martin, 3rd ed] ""11 3 (a) (b) Figure 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses when posed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 m/s. So what’ll its velocity be, a second after that? Student: Um. I’m thinking. 29.4 m/s Tutor: Right. Now back to the man and his keys. We’ve shown that they both have an 3 (a) (b) Figure 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses when posed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 m/s. So what’ll its velocity be, a second after that? Student: Um. I’m thinking. 29.4 m/s Tutor: Right. Now back to the man and his keys. We’ve shown that they both have an 3 (a) (b) Figure 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses when posed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 m/s. So what’ll its velocity be, a second after that? Student: Um. I’m thinking. 29.4 m/s Tutor: Right. Now back to the man and his keys. We’ve shown that they both have an 3 (a) (b) Figure 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses when posed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 m/s. So what’ll its velocity be, a second after that? Student: Um. I’m thinking. 29.4 m/s Tutor: Right. Now back to the man and his keys. We’ve shown that they both have an CS447: Natural Language Processing (J. Hockenmaier) Sentiment/opinion analysis ""12 Source: www.amazon.com CS447: Natural Language Processing (J. Hockenmaier) Have chatbots developed their own language? https://code.facebook.com/posts/1686672014972296/deal-or- no-deal-training-ai-bots-to-negotiate/ Bob: “I can can I I everything else.” Alice: “Balls have zero to me to me to me to me to me to me to me to me to.” No, not really… http://www.snopes.com/facebook-ai-developed-own-language/ ""13 CS447: Natural Language Processing (J. Hockenmaier) How well does that Barbie do? Barbie: “Do you have any sisters?’’ Child: “Yeah, I only have one.’’ Barbie: “What’s something nice that your sister does for you?’’ Child: “She does nothing nice to me’’ Barbie: “Well, what is the last nice thing your sister did?’’ Child: “She helped me with my project — and then she destroyed it.’’ Barbie: “Oh, yeah, tell me more!’’ Child: “That’s it, Barbie,’’ Barbie: “Have you told your sister lately how cool she is?’’ Child: “No. She is not cool,’’ Barbie: “You never know, she might appreciate hearing it’’ (Thanks to Barbara Grosz for pointing out the example from the NYT) https://www.nytimes.com/2015/09/20/magazine/barbie-wants-to-get-to-know-your-child.html ""14 CS447: Natural Language Processing (J. Hockenmaier) What is the current state of NLP? Lots of commercial applications and interest. Some applications are working pretty well already, others not so much. A lot of hype around “deep learning” and “AI” —Neural nets are powerful classiﬁers and sequence models —Public libraries (Tensorﬂow, Torch, Caffe, etc.) and datasets make it easy for anybody to get a model up and running —“End-to-end” models put into question whether we still need the traditional NLP pipeline that this class is built around —We’re still in the middle of this paradigm shift —But many of the fundamental problems haven’t gone away ""15 CS447: Natural Language Processing (J. Hockenmaier) What will you learn in this class? ""16 CS447: Natural Language Processing (J. Hockenmaier) The topics of this class ""17 We want to identify the structure and meaning of words, sentences, texts and conversations N.B.: we do not deal with speech (no signal processing) We mainly deal with language analysis/understanding, and less with language generation/production We focus on fundamental concepts, methods, models, and algorithms, not so much on current research: -Data (natural language): linguistic concepts and phenomena -Representations: grammars, automata, etc. -Statistical models over these representations -Learning & inference algorithms for these models CS447: Natural Language Processing (J. Hockenmaier) What you should learn You should be able to answer the following questions: -What makes natural language difﬁcult for computers? -What are the core NLP tasks? -What are the main modeling techniques used in NLP? We won’t be able to cover the latest research… (this requires more time, and a much stronger background in machine learning than I am able to assume for this class) … but I would still like you to get an understanding of: -How well does current NLP technology work (or not)? -What NLP software is available? -How to read NLP research papers [4 credits section] ""18 CS447: Natural Language Processing (J. Hockenmaier) Building a computer that ‘understands’ text: The NLP pipeline ""19 CS447: Natural Language Processing (J. Hockenmaier) ""20 CS447: Natural Language Processing (J. Hockenmaier) Task: Tokenization/segmentation We need to split text into words and sentences. - Languages like Chinese don’t have spaces between words. - Even in English, this cannot be done deterministically: There was an earthquake near D.C. You could even feel it in Philadelphia, New York, etc. NLP task: What is the most likely segmentation/tokenization? ""21 CS447: Natural Language Processing (J. Hockenmaier) Task: Part-of-speech-tagging ""22 Open the pod door, Hal. Verb Det Noun Noun , Name . Open the pod door , Hal . open: verb, adjective, or noun? Verb: open the door Adjective: the open door Noun: in the open CS447: Natural Language Processing (J. Hockenmaier) We want to know the most likely tags T for the sentence S We need to deﬁne a statistical model of P(T | S), e.g.: We need to estimate the parameters of P(T |S), e.g.: P( ti =V | ti-1 =N ) = 0.3 How do we decide? ""23 argmax T P(T|S) argmax T P(T|S) = argmax T P(T)P(S|T) P(T) =def ’ i P(ti|ti−1) P(S|T) =def ’ i P(wi|i) argmax T P(T|S) = argmax T P(T)P(S|T) P(T) =def ’ i P(ti|ti−1) P(S|T) =def ’ i P(wi|i) argmax T P(T|S) = argmax T P(T)P(S|T) P(T) =def ’ i P(ti|ti−1) P(S|T) =def ’ i P(wi|i) P(wi | ti) CS447: Natural Language Processing (J. Hockenmaier) Disambiguation requires statistical models Ambiguity is a core problem for any NLP task Statistical models* are one of the main tools to deal with ambiguity. *more generally: a lot of the models (classiﬁers, structured prediction models) you learn about in CS446 (Machine Learning) can be used for this purpose. You can learn more about the connection to machine learning in CS546 (Machine learning in Natural Language). These models need to be trained (estimated, learned) before they can be used (tested). We will see lots of examples in this class (CS446 is NOT a prerequisite for CS447) ""24 CS447: Natural Language Processing (J. Hockenmaier) “I made her duck” What does this sentence mean? “duck”: noun or verb? “make”: “cook X” or “cause X to do Y” ? “her”: “for her” or “belonging to her” ? Language has different kinds of ambiguity, e.g.: Structural ambiguity “I eat sushi with tuna” vs. “I eat sushi with chopsticks” “I saw the man with the telescope on the hill” Lexical (word sense) ambiguity “I went to the bank”: ﬁnancial institution or river bank? Referential ambiguity “John saw Jim. He was drinking coffee.” ""25 CS447: Natural Language Processing (J. Hockenmaier) “I made her duck cassoulet” (Cassoulet = a French bean casserole) The second major problem in NLP is coverage: We will always encounter unfamiliar words and constructions. Our models need to be able to deal with this. This means that our models need to be able to generalize from what they have been trained on to what they will be used on. ""26 CS447: Natural Language Processing (J. Hockenmaier) Task: Syntactic parsing ""27 Verb Det Noun Noun , Name . Open the pod door , Hal . NOUN NP VP NP S CS447: Natural Language Processing (J. Hockenmaier) Observation: Structure corresponds to meaning Correct analysis Incorrect analysis eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks eat sushi with chopsticks NP NP NP VP PP V P eat with tuna sushi NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks ""28 CS447: Natural Language Processing (J. Hockenmaier) Overgeneration English ..... John Mary saw. with tuna sushi ate I. Did you went there? .... ""29 Undergeneration I ate the cake that John had made for me yesterday I want you to go there. John and Mary eat sushi for dinner. Did you go there? I ate sushi with tuna. John saw Mary. CS447: Natural Language Processing (J. Hockenmaier) NLP and automata theory What kind of grammar/automaton is required to analyze natural language? What class of languages does natural language fall into? Chomsky (1956)’s hierarchy of formal languages was originally developed to answer (some of) these questions. ""30 CS447: Natural Language Processing (J. Hockenmaier) Task: Semantic analysis ""31 Verb Det Noun Noun , Name . Open the pod door , Hal . NOUN NP VP NP S ∃x∃y(pod_door(x) & Hal(y) & request(open(x, y))) CS447: Natural Language Processing (J. Hockenmaier) Representing meaning We need a meaning representation language. “Shallow” semantic analysis: Template-ﬁlling (Information Extraction) Named-Entity Extraction: Organizations, Locations, Dates,... Event Extraction “Deep” semantic analysis: (Variants of) formal logic ∃x∃y(pod_door(x)& Hal(y) & request(open(x,y))) We also distinguish between Lexical semantics (the meaning of words) and Compositional semantics (the meaning of sentences) ""32 CS447: Natural Language Processing (J. Hockenmaier) ∃x∃y(pod_door(x) & Hal(y) & request(open(x, y))) ∃x∃y(pod_door(x) & Hal(y) & request(open(x, y))) Multimodal NLP: mapping from language to the world ""33 System request(open(door2, SYS)) CS447: Natural Language Processing (J. Hockenmaier) More than a decade ago, Carl Lewis stood on the threshold of what was to become the greatest athletics career in history. He had just broken two of the legendary Jesse Owens' college records, but never believed he would become a corporate icon, the focus of hundreds of millions of dollars in advertising. His sport was still nominally amateur. Eighteen Olympic and World Championship gold medals and 21 world records later, Lewis has become the richest man in the history of track and field -- a multi- millionaire. Who is Carl Lewis? Did Carl Lewis break any world records? (and how do you know that?) Understanding texts ""34 CS447: Natural Language Processing (J. Hockenmaier) Summary: The NLP Pipeline An NLP system may use some or all of the following steps: Tokenizer/Segmenter to identify words and sentences Morphological analyzer/POS-tagger to identify the part of speech and structure of words Word sense disambiguation to identify the meaning of words Syntactic/semantic Parser to obtain the structure and meaning of sentences Coreference resolution/discourse model to keep track of the various entities and events mentioned ""35 CS447: Natural Language Processing (J. Hockenmaier) Course Admin ""36 CS447: Natural Language Processing (J. Hockenmaier) This class consists of... … Lectures: Wednesdays and Fridays, 12:30pm–1:45 pm, DCL1310 … Ofﬁce: Julia: Wednesdays and Fridays, 2pm–3pm, Siebel 3324 Dhruv: TBD, Siebel 0207 Sai Krishna: TBD, Siebel 0207 Zubin: TBD, online … Websites: Syllabus, slides, policies, etc: http://courses.engr.illinois.edu/cs447 Discussions: piazza.com/illinois/fall2018/cs447 Grades, submitting assignments: http://compass2g.illinois.edu … Readings: Textbook + additional readings (http://courses.engr.illinois.edu/cs447) … Assessment: 4+1 assignments, 2 exams (4th credit hour: project or survey) ""37 CS447: Natural Language Processing (J. Hockenmaier) ""38 Lectures and ofﬁce hours Attend! Ask questions! Participate! CS447: Natural Language Processing (J. Hockenmaier) Reading Course website: (slides, reading) https://courses.engr.illinois.edu/cs447/fa2018/syllabus.html The textbook: https://web.stanford.edu/~jurafsky/slp3/ Jurafsky and Martin, Speech and Language Processing (3rd edition PDFs in prep.; 2nd edition, 2008 in print) For some assignments: The NLTK book (http://www.nltk.org/book) ""39 CS447: Natural Language Processing (J. Hockenmaier) Assessment If you take this class for 3 hours credit: 1/3 homework assignments 1/3 midterm exam 1/3 ﬁnal exam If you take this class for 4 hours credit: 1/4 homework assignments 1/4 midterm exam 1/4 ﬁnal exam 1/4 literature review or project We reserve the right to improve your grade by up to 5% depending on your class participation. If you’re in between grades, but attended class and participated frequently and actively in in-class discussions etc., we will give you the higher grade. ""40 CS447: Natural Language Processing (J. Hockenmaier) Homework assignments What? 4 assignments (mostly programming), plus homework 0 We use Python and the Natural Language Toolkit (NLTK) Why? To make sure you can put what you’ve learned to practice. How? You will have one weeks to complete HW0. You will have three weeks to complete HW1, HW2, HW3, HW4. Grades will be based on your write-up and your code. Submit your assignments on Compass. Late policy? No late assignments will be accepted (sorry). ""41 CS447: Natural Language Processing (J. Hockenmaier) Homework assignments Schedule: Week 1: Friday, 08/31 HW0 out Week 2: Friday, 09/07 HW0 due, HW1 out Week 5: Friday, 09/28 HW1 due, HW2 out Week 8: Friday, 10/19 HW2 due, HW3 out Week 11: Friday, 11/09 HW3 due, HW4 out Week 14: Friday, 12/07 HW4 due Points per assignment: HW0 = 2 points (Did you submit [on time]? Was it in the right format?) HW1,HW2,HW3,HW4 = 10 points per assignment ""42 CS447: Natural Language Processing (J. Hockenmaier) Exams What? Midterm exam: Friday, Oct 12, in class Final exam: Wednesday, Dec 12, in class (based on material after ﬁrst midterm) Why? To make sure you understand what you learned well enough to explain and apply it. How? Essay questions and problem questions Closed-book (no cheatsheets, no electronics, etc.) Will be based on lectures and readings ""43 CS447: Natural Language Processing (J. Hockenmaier) 4th credit hour: Research Projects What? You need to read and describe a few (2–3) NLP papers on a particular task, implement an NLP system for this task and describe it in a written report. Why? To make sure you get a deeper knowledge of NLP by reading original papers and by building an actual system. When? Fri, Oct 5: Proposal due (What topic? What papers will you read?) Fri, Nov 9: Progress report due (Are your experiments on track?) Thu, Dec 13: Final report due (Summary of papers, your system) ""44 CS447: Natural Language Processing (J. Hockenmaier) 4th credit hour: Literature Survey What? You need to read and describe several (5-7) NLP papers on a particular task or topic, and produce a written report that compares and critiques these approaches. Why? To make sure you get a deeper knowledge of NLP by reading original papers, even if you don’t build an actual system. When? Fri, Oct 5: Proposal due (What topic? What papers will you read?) Fri, Nov 9: Progress report due (Is your paper on track?) Thu, Dec 13: Final report due (Summary of papers) ""45 CS447: Natural Language Processing (J. Hockenmaier) Lectures 2–5: Morphology, language models) Lectures 7–10: Sequence labeling (POS tagging etc.) Lectures 11–12: Syntax and Parsing Lecture 13: Review for midterm —————— Midterm exam ————————— Lectures 15–18: Semantics Lectures 19–22: Machine Translation Lectures 23–24: Discourse, Dialog Lectures 25–27: Neural NLP Lecture 28: Review for Final Exam —————— Final exam ————————— Course Outline (tentative) ""46 CS447: Natural Language Processing (J. Hockenmaier) Today’s readings Today’s lecture: Jurafsky and Martin Chapter 1 (2nd edition) http://www.cs.colorado.edu/~martin/SLP/Updates/1.pdf ""47 "
258,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 2: Finite-state methods for morphology CS447: Natural Language Processing (J. Hockenmaier) A bit more admin… !2 CS447: Natural Language Processing (J. Hockenmaier) HW 0 HW0 will come out later today (check the syllabus.html page on the website) We will assume Python 3.5.2 for our assignments (you shouldn’t have to load any additional modules or libraries besides the ones we provide) You get 2 points for HW0 (HW1—HW4 have 10 points each) 1 point for uploading something to Compass 1 point for uploading a tar.gz ﬁle with the correct name and ﬁle structure !3 CS447: Natural Language Processing (J. Hockenmaier) Compass and enrollment… We won’t be able to grade more than 100 assignments (and HW0 is only worth 2 points) -Lecture slides and the PDFs for the assignments will always be posted on the class website. -You don’t need to be on Compass to get access. -Piazza is also available to everybody. If you are planning to drop this class, please do so ASAP, so that others can take your spot. If you just got into the class, it is likely to take 24 hours to get access to Compass. !4 CS447: Natural Language Processing (J. Hockenmaier) DRES accommodations If you need any disability related accommodations, talk to DRES (http://disability.illinois.edu, disability@illinois.edu, phone 333-4603) If you are concerned you have a disability-related condition that is impacting your academic progress, there are academic screening appointments available on campus that can help diagnosis a previously undiagnosed disability by visiting the DRES website and selecting “Sign-Up for an Academic Screening” at the bottom of the page.” Come and talk to me as well, especially once you have a letter of accommodation from DRES. Do this early enough so that we can take your requirements into account for exams and assignments. !5 CS447: Natural Language Processing (J. Hockenmaier) Last lecture The NLP pipeline: Tokenization — POS tagging — Syntactic parsing — Semantic analysis — Coreference resolution Why is NLP difﬁcult? Ambiguity Coverage !6 CS447: Natural Language Processing (J. Hockenmaier) Today’s lecture What is the structure of words? (in English, Chinese, Arabic,…) Morphology: the area of linguistics that deals with this. How can we identify the structure of words? We need to build a morphological analyzer (parser). We will use ﬁnite-state transducers for this task. Finite-State Automata and Regular Languages (Review) NB: No probabilities or machine learning yet. We’re thinking about (symbolic) representations today. !7 CS447: Natural Language Processing (J. Hockenmaier) Morphology: What is a word? !8 CS447: Natural Language Processing (J. Hockenmaier) uygarlaştıramadıklarımızdanmışsınızcasına uygar_laş_tır_ama_dık_lar_ımız_dan_mış_sınız_casına “as if you are among those whom we were not able to civilize (=cause to become civilized )” uygar: civilized _laş: become _tır: cause somebody to do something _ama: not able _dık: past participle _lar: plural _ımız: 1st person plural possessive (our) _dan: among (ablative case) _mış: past _sınız: 2nd person plural (you) _casına: as if (forms an adverb from a verb) !9 A Turkish word K. Oﬂazer pc to J&M CS447: Natural Language Processing (J. Hockenmaier) Basic word classes (parts of speech) Content words (open-class): Nouns: student, university, knowledge,... Verbs: write, learn, teach,... Adjectives: difficult, boring, hard, .... Adverbs: easily, repeatedly,... Function words (closed-class): Prepositions: in, with, under,... Conjunctions: and, or,... Determiners: a, the, every,... !10 CS447: Natural Language Processing (J. Hockenmaier) Words aren’t just deﬁned by blanks Problem 1: Compounding “ice cream”, “website”, “web site”, “New York-based” Problem 2: Other writing systems have no blanks Chinese: 我开始写⼩小说 = 我 开始 写 ⼩小说 I start(ed) writing novel(s) Problem 3: Clitics English: “doesn’t” , “I’m” , Italian: “dirglielo” = dir + gli(e) + lo tell + him + it !11 CS447: Natural Language Processing (J. Hockenmaier) Of course he wants to take the advanced course too. He already took two beginners’ courses. This is a bad question. Did I mean: How many word tokens are there? (16 to 19, depending on how we count punctuation) How many word types are there? (i.e. How many different words are there? Again, this depends on how you count, but it’s usually much less than the number of tokens) How many words are there? !12 CS447: Natural Language Processing (J. Hockenmaier) Of course he wants to take the advanced course too. He already took two beginners’ courses. The same (underlying) word can take different forms: course/courses, take/took We distinguish concrete word forms (take, taking) from abstract lemmas or dictionary forms (take) Different words may be spelled/pronounced the same: of course vs. advanced course two vs. too How many words are there? !13 CS447: Natural Language Processing (J. Hockenmaier) Inﬂection creates different forms of the same word: Verbs: to be, being, I am, you are, he is, I was, Nouns: one book, two books Derivation creates different words from the same lemma: grace ⇒ disgrace ⇒ disgraceful ⇒ disgracefully Compounding combines two words into a new word: cream ⇒ ice cream ⇒ ice cream cone ⇒ ice cream cone bakery Word formation is productive: New words are subject to all of these processes: Google ⇒ Googler, to google, to ungoogle, to misgoogle, googlification, ungooglification, googlified, Google Maps, Google Maps service,... !14 How many different words are there? CS447: Natural Language Processing (J. Hockenmaier) Inﬂectional morphology in English Verbs: Inﬁnitive/present tense: walk, go 3rd person singular present tense (s-form): walks, goes Simple past: walked, went Past participle (ed-form): walked, gone Present participle (ing-form): walking, going Nouns: Common nouns inﬂect for number: singular (book) vs. plural (books) Personal pronouns inﬂect for person, number, gender, case: I saw him; he saw me; you saw her; we saw them; they saw us. !15 CS447: Natural Language Processing (J. Hockenmaier) Derivational morphology Nominalization: V + -ation: computerization V+ -er: killer Adj + -ness: fuzziness Negation: un-: undo, unseen, ... mis-: mistake,... Adjectivization: V+ -able: doable N + -al: national !16 CS447: Natural Language Processing (J. Hockenmaier) Morphemes: stems, afﬁxes dis-grace-ful-ly preﬁx-stem-sufﬁx-sufﬁx Many word forms consist of a stem plus a number of afﬁxes (preﬁxes or sufﬁxes) Inﬁxes are inserted inside the stem. Circumﬁxes (German gesehen) surround the stem Morphemes: the smallest (meaningful/grammatical) parts of words. Stems (grace) are often free morphemes. Free morphemes can occur by themselves as words. Afﬁxes (dis-, -ful, -ly) are usually bound morphemes. Bound morphemes have to combine with others to form words. !17 CS447: Natural Language Processing (J. Hockenmaier) Morphemes and morphs There are many irregular word forms: Plural nouns add -s to singular: book-books, but: box-boxes, fly-flies, child-children Past tense verbs add -ed to inﬁnitive: walk-walked, but: like-liked, leap-leapt One morpheme (e.g. for plural nouns) can be realized as different surface forms (morphs): -s/-es/-ren Allomorphs: two different realizations (-s/-es/-ren) of the same underlying morpheme (plural) !18 CS447: Natural Language Processing (J. Hockenmaier) Morphological parsing and generation !19 CS447: Natural Language Processing (J. Hockenmaier) Morphological parsing disgracefully dis grace ful ly preﬁx stem sufﬁx sufﬁx NEG grace+N +ADJ +ADV !20 CS447: Natural Language Processing (J. Hockenmaier) Morphological generation We cannot enumerate all possible English words, but we would like to capture the rules that deﬁne whether a string could be an English word or not. That is, we want a procedure that can generate (or accept) possible English words… grace, graceful, gracefully disgrace, disgraceful, disgracefully, ungraceful, ungracefully, undisgraceful, undisgracefully,… without generating/accepting impossible English words *gracelyful, *gracefuly, *disungracefully,… NB: * is linguists’ shorthand for “this is ungrammatical” !21 CS447: Natural Language Processing (J. Hockenmaier) Overgeneration English Undergeneration grace disgrace foobar disgraceful google, misgoogle, ungoogle, googler, … … ..... gracelyful disungracefully grclf .... !22 CS447: Natural Language Processing (J. Hockenmaier) Review: Finite-State Automata and Regular Languages !23 CS447: Natural Language Processing (J. Hockenmaier) Formal languages An alphabet ∑ is a set of symbols: e.g. ∑= {a, b, c} A string ω is a sequence of symbols, e.g ω=abcb. The empty string ε consists of zero symbols. The Kleene closure ∑* (‘sigma star’) is the (inﬁnite) set of all strings that can be formed from ∑: ∑*= {ε, a, b, c, aa, ab, ba, aaa, ...} A language L ⊆ ∑* over ∑ is also a set of strings. Typically we only care about proper subsets of ∑* (L ⊂ Σ). !24 CS447: Natural Language Processing (J. Hockenmaier) An automaton is an abstract model of a computer. It reads an input string symbol by symbol. It changes its internal state depending on the current input symbol and its current internal state. Automata and languages !25 a b a c d e Automaton Input string 1. read input q Current state 2. change state Automaton q’ New state a Current input symbol CS447: Natural Language Processing (J. Hockenmaier) Automata and languages The automaton either accepts or rejects the input string. Every automaton deﬁnes a language (the set of strings it accepts). !26 a b a c d e Automaton Input string read accept! reject! Input string is in the language Input string is NOT in the language CS447: Natural Language Processing (J. Hockenmaier) Automata and languages Different types of automata deﬁne different language classes: - Finite-state automata deﬁne regular languages - Pushdown automata deﬁne context-free languages - Turing machines deﬁne recursively enumerable languages !27 CS447: Natural Language Processing (J. Hockenmaier) Recursively enumerable The Chomsky Hierarchy The structure of English words can be described by a regular (= ﬁnite-state) grammar. Context-sensitive Mildly context-sensitive Context-free Regular !28 CS447: Natural Language Processing (J. Hockenmaier) Finite-state automata A (deterministic) ﬁnite-state automaton (FSA) consists of: -a ﬁnite set of states Q = {q0….qN}, including a start state q0 and one (or more) ﬁnal (=accepting) states (say, qN) -a (deterministic) transition function δ(q,w) = q’ for q, q’ ∈ Q, w ∈ Σ !29 final state (note the double line) q0 q3 q2 q1 q4 q4 a b c x y move from state q2 to state q4 if you read ‘y’ start state CS447: Natural Language Processing (J. Hockenmaier) q0 a q3 q2 q1 b a q0 a q3 q2 q1 b a b a a a b a a a b a a a b a a a q0 a q3 q2 q1 b a q0 a q3 q2 q1 b a b a a a !30 q0 a q3 q2 q1 b a Start in q0 Accept! We’ve reached the end of the string, and are in an accepting state. CS447: Natural Language Processing (J. Hockenmaier) q0 a q3 q2 q1 b a b q0 a q3 q2 q1 b a b !31 Start in q0 Reject! (q1 is not a ﬁnal state) Rejection: Automaton does not end up in accepting state CS447: Natural Language Processing (J. Hockenmaier) !32 Reject! (There is no transition labeled ‘c’) Rejection: Transition not deﬁned q0 a q3 q2 q1 b a q0 a q3 q2 q1 b a b a c b a c b a c q0 a q3 q2 q1 b a b a c q0 a q3 q2 q1 b a Start in q0 CS447: Natural Language Processing (J. Hockenmaier) Finite State Automata (FSAs) A ﬁnite-state automaton M =〈Q, Σ, q0, F, δ〉 consists of: - A ﬁnite set of states Q = {q0, q1,.., qn} - A ﬁnite alphabet Σ of input symbols (e.g. Σ = {a, b, c,...}) - A designated start state q0 ∈ Q - A set of ﬁnal states F ⊆Q - A transition function δ: - The transition function for a deterministic (D)FSA: Q × Σ → Q δ(q,w) = q’ for q, q’ ∈ Q, w ∈ Σ If the current state is q and the current input is w, go to q’ - The transition function for a nondeterministic (N)FSA: Q × Σ → 2Q δ(q,w) = Q’ for q ∈ Q, Q’ ⊆ Q, w ∈ Σ If the current state is q and the current input is w, go to any q’ ∈ Q’ !33 CS447: Natural Language Processing (J. Hockenmaier) Every NFA can be transformed into an equivalent DFA: Recognition of a string w with a DFA is linear in the length of w Finite-state automata deﬁne the class of regular languages L1 = { anbm } = {ab, aab, abb, aaab, abb,… } is a regular language, L2 = { anbn } = {ab, aabb, aaabbb,…} is not (it’s context-free). You cannot construct an FSA that accepts all the strings in L2 and nothing else. Finite State Automata (FSAs) q3 q3 b q0 a q3 q2 b a q1 q3 q0 q3 b a !34 CS447: Natural Language Processing (J. Hockenmaier) Regular Expressions Regular expressions can also be used to deﬁne a regular language. Simple patterns: -Standard characters match themselves: ‘a’, ‘1’ -Character classes: ‘[abc]’, ‘[0-9]’, negation: ‘[^aeiou]’ (Predeﬁned: \s (whitespace), \w (alphanumeric), etc.) -Any character (except newline) is matched by ‘.’ Complex patterns: (e.g. ^[A-Z]([a-z])+\s ) -Group: ‘(…)’ -Repetition: 0 or more times: ‘*’, 1 or more times: ‘+’ -Disjunction: ‘...|…’ -Beginning of line ‘^’ and end of line ‘$’ !35 CS447: Natural Language Processing (J. Hockenmaier) Finite-state methods for morphology !36 CS447: Natural Language Processing (J. Hockenmaier) q0 stem preﬁx q1 q3 q2 dis-grace: sufﬁx q0 q1 stem q3 q2 grace-ful: stem q0 q1 q2 preﬁx sufﬁx q3 q3 dis-grace-ful: Finite state automata for morphology grace: !37 q0 stem q3 q1 CS447: Natural Language Processing (J. Hockenmaier) Union: merging automata grace, dis-grace, grace-ful, dis-grace-ful q0 q1 ε stem sufﬁx q3 q3 preﬁx q3 q2 !38 CS447: Natural Language Processing (J. Hockenmaier) Some irregular words require stem changes: Past tense verbs: teach-taught, go-went, write-wrote Plural nouns: mouse-mice, foot-feet, wife-wives Stem changes !39 CS447: Natural Language Processing (J. Hockenmaier) q3 q1 noun1 FSAs for derivational morphology q0 q3 q5 -ation q3 q6 -er -iz q2 -e q3 q3 adj1 -able q4 q3 q7 noun2 -al noun2 = {nation, form,…} noun3 q10 -al q3 q11 -e noun3 = {natur, structur,…} noun1 = {fossil,mineral,...} adj1 = {equal, neutral} adj2 = {minim, maxim} q3 q9 adj2 q8 -al -iz CS447: Natural Language Processing (J. Hockenmaier) FSAs can recognize (accept) a string, but they don’t tell us its internal structure. We need is a machine that maps (transduces) the input string into an output string that encodes its structure: Recognition vs. Analysis !41 c a t s Input (Surface form) c a t +N +pl Output (Lexical form) CS447: Natural Language Processing (J. Hockenmaier) Finite-state transducers A ﬁnite-state transducer T = 〈Q, Σ, Δ, q0, F, δ, σ〉 consists of: - A ﬁnite set of states Q = {q0, q1,.., qn} - A ﬁnite alphabet Σ of input symbols (e.g. Σ = {a, b, c,...}) - A ﬁnite alphabet Δ of output symbols (e.g. Δ = {+N, +pl,...}) - A designated start state q0 ∈ Q - A set of ﬁnal states F ⊆ Q - A transition function δ: Q × Σ → 2Q δ(q,w) = Q’ for q ∈ Q, Q’ ⊆ Q, w ∈ Σ - An output function σ: Q × Σ → Δ* σ(q,w) = ω for q ∈ Q, w ∈ Σ, ω ∈ Δ* If the current state is q and the current input is w, write ω. (NB: Jurafsky&Martin deﬁne σ: Q × Σ* → Δ*. Why is this equivalent?) !42 CS447: Natural Language Processing (J. Hockenmaier) An FST T = Lin ⨉ Lout deﬁnes a relation between two regular languages Lin and Lout: Lin = {cat, cats, fox, foxes, ...} Lout = {cat+N+sg, cat+N+pl, fox+N+sg, fox+N+pl ...} T = { ⟨cat, cat+N+sg⟩, ⟨cats, cat+N+pl⟩, ⟨fox, fox+N+sg⟩, ⟨foxes, fox+N+pl⟩ } Finite-state transducers !43 CS447: Natural Language Processing (J. Hockenmaier) Some FST operations Inversion T-1: The inversion (T-1) of a transducer switches input and output labels. This can be used to switch from parsing words to generating words. Composition (T◦T’): (Cascade) Two transducers T = L1 ⨉ L2 and T’ = L2 ⨉ L3 can be composed into a third transducer T’’ = L1 ⨉ L3. Sometimes intermediate representations are useful !44 CS447: Natural Language Processing (J. Hockenmaier) English spelling rules Peculiarities of English spelling (orthography) The same underlying morpheme (e.g. plural-s) can have different orthographic “surface realizations” (-s, -es) This leads to spelling changes at morpheme boundaries: E-insertion: fox +s = foxes E-deletion: make +ing = making !45 CS447: Natural Language Processing (J. Hockenmaier) Side note: “Surface realization”? This terminology comes from Chomskyan Transformational Grammar. Dominant early approach in theoretical linguistics, superseded by other approaches (“minimalism”). Not computational, but has some historical inﬂuence on computational linguistics (e.g. Penn Treebank) “Surface” = standard English (Chinese, Hindi, etc.). “Surface string” = a written sequence of characters or words vs. “Deep”/“Underlying” structure/representation: A more abstract representation. Might be the same for different sentences with the same meaning. !46 CS447: Natural Language Processing (J. Hockenmaier) Intermediate representations English plural -s: cat ⇒ cats dog ⇒ dogs but: fox ⇒ foxes, bus ⇒ buses buzz ⇒ buzzes We deﬁne an intermediate representation to capture morpheme boundaries (^) and word boundaries (#): Lexicon: cat+N+PL fox+N+PL ⇒ Intermediate representation: cat^s# fox^s# ⇒ Surface string: cats foxes Intermediate-to-Surface Spelling Rule: If plural ‘s’ follows a morpheme ending in ‘x’,‘z’ or ‘s’, insert ‘e’. !47 CS447: Natural Language Processing (J. Hockenmaier) FST composition/cascade: !48 CS447: Natural Language Processing (J. Hockenmaier) Tlex: Lexical to intermediate level !49 CS447: Natural Language Processing (J. Hockenmaier) Te-insert: intermediate to surface level !50 q0 a:a,…,r:r, t:t,…,w:w, y:y s:s, x:x, z:z q2 a:a,…,r:r,t:t, …,w:w,y:y #:ε q3 q1 ^:ε q6 #:ε q3 q3 ^:e q5 s:s q3 q8 #:ε ^ = morpheme boundary # = word boundary ε = empty string s:s, x:x, z:z q4 #:ε a:a,…,r:r, t:t,…,w:w,y:y ^:e q7 s:s Intermediate-to- Surface Spelling Rule: If plural ‘s’ follows a morpheme ending in ‘x’,‘z’ or ‘s’, insert ‘e’. CS447: Natural Language Processing (J. Hockenmaier) Dealing with ambiguity book: book +N +sg or book +V? Generating words is generally unambiguous, but analyzing words often requires disambiguation. We need a nondeterministic FST. - Efﬁciency problem: Not every nondeterministic FST can be translated into a deterministic one! We also need a scoring function to identify which analysis is more likely. - We may need to know the context in which the word appears: (I read a book vs. I book flights) !51 CS447: Natural Language Processing (J. Hockenmaier) Other applications of FSTs !52 CS447: Natural Language Processing (J. Hockenmaier) A computer therapist? Computer: TELL ME MORE ABOUT YOUR FAMILY Human: My mother takes care of me. Computer: WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU Human: My father. Computer: YOUR FATHER Human: You are like my father in some ways. Computer: WHAT RESEMBLANCE DO YOU SEE Human: You are not very aggressive but I think you don't want me to notice that. Computer: WHAT MAKES YOU THINK I AM NOT VERY AGGRESSIVE Human: You don't argue with me. Computer: WHY DO YOU THINK I DON'T ARGUE WITH YOU Human: You are afraid of me. !53 Weizenbaum (1966), ELIZA. CS447: Natural Language Processing (J. Hockenmaier) ELIZA as a FST cascade Human: You don't argue with me. Computer: WHY DO YOU THINK I DON'T ARGUE WITH YOU 1. Replace you with I and me with you: I don't argue with you. 2. Replace <...> with Why do you think <...>: Why do you think I don't argue with you. What about other NLP tasks? Could we write an FST for machine translation? !54 CS447: Natural Language Processing (J. Hockenmaier) What about compounds? Semantically, compounds have hierarchical structure: (((ice cream) cone) bakery) not (ice ((cream cone) bakery)) ((computer science) (graduate student)) not (computer ((science graduate) student)) We will need context-free grammars to capture this underlying structure. !55 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts Morphology (word structure): stems, afﬁxes Derivational vs. inﬂectional morphology Compounding Stem changes Morphological analysis and generation Finite-state automata Finite-state transducers Composing ﬁnite-state transducers !56 CS447: Natural Language Processing (J. Hockenmaier) Today’s reading This lecture follows closely Chapter 3.1-7 in J&M 2008 Optional readings (see website) Karttunen and Beesley '05, Mohri (1997), the Porter stemmer, Sproat et al. (1996) !57 "
259,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 3: Language models CS447: Natural Language Processing (J. Hockenmaier) Last lecture’s key concepts Morphology (word structure): stems, afﬁxes Derivational vs. inﬂectional morphology Compounding Stem changes Morphological analysis and generation Finite-state automata Finite-state transducers Composing ﬁnite-state transducers !2 CS447: Natural Language Processing (J. Hockenmaier) Finite-state transducers -FSTs deﬁne a relation between two regular languages. -Each state transition maps (transduces) a character from the input language to a character (or a sequence of characters) in the output language -By using the empty character (ε), characters can be deleted (x:ε) or inserted(ε:y) -FSTs can be composed (cascaded), allowing us to deﬁne intermediate representations. !3 x:y x:ε ε:y CS447: Natural Language Processing (J. Hockenmaier) Today’s lecture How can we distinguish word salad, spelling errors and grammatical sentences? Language models deﬁne probability distributions over the strings in a language. N-gram models are the simplest and most common kind of language model. We’ll look at how they’re deﬁned, how to estimate (learn) them, and what their shortcomings are. We’ll also review some very basic probability theory. !4 CS447: Natural Language Processing (J. Hockenmaier) Why do we need language models? Many NLP tasks return output in natural language: -Machine translation -Speech recognition -Natural language generation -Spell-checking Language models deﬁne probability distributions over (natural language) strings or sentences. We can use them to score/rank possible sentences: If PLM(A) > PLM(B), choose sentence A over B !5 CS447: Natural Language Processing (J. Hockenmaier) Reminder: Basic Probability Theory !6 CS447: Natural Language Processing (J. Hockenmaier) !7 P( ) = 2/15 P(blue) = 5/15 P(blue | ) = 2/5 P( ) = 1/15 P(red) = 5/15 P( ) = 5/15 P( or ) = 2/15 P( |red) = 3/5 Pick a random shape, then put it back in the bag. Sampling with replacement CS447: Natural Language Processing (J. Hockenmaier) !8 Pick a random shape, then put it back in the bag. What sequence of shapes will you draw? P( ) P( ) = 1/15 × 1/15 × 1/15 × 2/15 = 2/50625 = 3/15 × 2/15 × 2/15 × 3/15 = 36/50625 P( ) = 2/15 P(blue) = 5/15 P(blue | ) = 2/5 P( ) = 1/15 P(red) = 5/15 P( ) = 5/15 P( or ) = 2/15 P( |red) = 3/5 Sampling with replacement CS447: Natural Language Processing (J. Hockenmaier) Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' !9 P(of) = 3/66 P(Alice) = 2/66 P(was) = 2/66 P(to) = 2/66 P(her) = 2/66 P(sister) = 2/66 P(,) = 4/66 P(') = 4/66 Sampling with replacement CS447: Natural Language Processing (J. Hockenmaier) P(of) = 3/66 P(Alice) = 2/66 P(was) = 2/66 P(to) = 2/66 P(her) = 2/66 P(sister) = 2/66 P(,) = 4/66 P(') = 4/66 beginning by, very Alice but was and? reading no tired of to into sitting sister the, bank, and thought of without her nothing: having conversations Alice once do or on she it get the book her had peeped was conversation it pictures or sister in, 'what is the use had twice of a book''pictures or' to !10 In this model, P(English sentence) = P(word salad) Sampling with replacement CS447: Natural Language Processing (J. Hockenmaier) Probability theory: terminology Trial: Picking a shape, predicting a word Sample space Ω: The set of all possible outcomes (all shapes; all words in Alice in Wonderland) Event ω ⊆ Ω: An actual outcome (a subset of Ω) (predicting ‘the’, picking a triangle) !11 CS447: Natural Language Processing (J. Hockenmaier) Kolmogorov axioms: 1) Each event has a probability between 0 and 1. 2) The null event has probability 0. The probability that any event happens is 1. 3) The probability of all disjoint events sums to 1. The probability of events !12 0 ⇥P(ω ⊆Ω) ⇥1 P(⇤) = 0 and P(Ω) = 1 ! ωi⊆Ω P(ωi) = 1 if ⇥j ̸= i : ωi ⌅ωj = ⇤ and ! i ωi = Ω CS447: Natural Language Processing (J. Hockenmaier) Bernoulli distribution (two possible outcomes) The probability of success (=head,yes) The probability of head is p. The probability of tail is 1−p. Categorical distribution (N possible outcomes) The probability of category/outcome ci is pi (0≤ pi ≤1 ∑i pi = 1) Discrete probability distributions: single trials !13 CS447: Natural Language Processing (J. Hockenmaier) The conditional probability of X given Y, P(X | Y), is deﬁned in terms of the probability of Y, P( Y ), and the joint probability of X and Y, P(X,Y): Joint and Conditional Probability P(X|Y ) = P(X, Y ) P(Y ) P(blue | ) = 2/5 !14 CS447: Natural Language Processing (J. Hockenmaier) !15 Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' P(wi+1 = of | wi = tired) = 1 P(wi+1 = of | wi = use) = 1 P(wi+1 = sister | wi = her) = 1 P(wi+1 = beginning | wi = was) = 1/2 P(wi+1 = reading | wi = was) = 1/2 P(wi+1 = bank | wi = the) = 1/3 P(wi+1 = book | wi = the) = 1/3 P(wi+1 = use | wi = the) = 1/3 Conditioning on the previous word CS447: Natural Language Processing (J. Hockenmaier) !16 English Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?' Word Salad beginning by, very Alice but was and? reading no tired of to into sitting sister the, bank, and thought of without her nothing: having conversations Alice once do or on she it get the book her had peeped was conversation it pictures or sister in, 'what is the use had twice of a book''pictures or' to Now, P(English) ⪢ P(word salad) P(wi+1 = of | wi = tired) = 1 P(wi+1 = of | wi = use) = 1 P(wi+1 = sister | wi = her) = 1 P(wi+1 = beginning | wi = was) = 1/2 P(wi+1 = reading | wi = was) = 1/2 P(wi+1 = bank | wi = the) = 1/3 P(wi+1 = book | wi = the) = 1/3 P(wi+1 = use | wi = the) = 1/3 Conditioning on the previous word CS447: Natural Language Processing (J. Hockenmaier) The chain rule The joint probability P(X,Y) can also be expressed in terms of the conditional probability P(X | Y) This leads to the so-called chain rule: !17 P(X, Y ) = P(X|Y )P(Y ) P(X1, X2, . . . , Xn) = P(X1)P(X2|X1)P(X3|X2, X1)....P(Xn|X1, ...Xn−1) = P(X1) n ! i=2 P(Xi|X1 . . . Xi−1) CS447: Natural Language Processing (J. Hockenmaier) Two random variables X and Y are independent if If X and Y are independent, then P(X | Y) = P(X): Independence P(X, Y ) = P(X)P(Y ) P(X|Y ) = P(X, Y ) P(Y ) = P(X)P(Y ) P(Y ) (X , Y independent) = P(X) !18 CS447: Natural Language Processing (J. Hockenmaier) Probability models Building a probability model consists of two steps: 1. Deﬁning the model 2. Estimating the model’s parameters (= training/learning ) Models (almost) always make independence assumptions. That is, even though X and Y are not actually independent, our model may treat them as independent. This reduces the number of model parameters that we need to estimate (e.g. from n2 to 2n) !19 CS447: Natural Language Processing (J. Hockenmaier) Language modeling with n-grams !20 CS447: Natural Language Processing (J. Hockenmaier) A language model over a vocabulary V assigns probabilities to strings drawn from V*. Recall the chain rule: An n-gram language model assumes each word depends only on the last n−1 words: Language modeling with N-grams P(w1...wi) = P(w1)P(w2|w1)P(w3|w1w2)...P(wi|w1...wi−1) Pngram(w1...wi) := P(w1)P(w2|w1)...P( wi ⇤⇥#⌅ nth word | wi−n−1...wi−1 ⇤ ⇥# ⌅ prev. n−1 words ) !21 CS447: Natural Language Processing (J. Hockenmaier) N-gram models Unigram model P(w1)P(w2)...P(wi) Bigram model P(w1)P(w2|w1)...P(wi|wi−1) Trigram model P(w1)P(w2|w1)...P(wi|wi−2wi−1) N-gram model P(w1)P(w2|w1)...P(wi|wi−n−1...wi−1) N-gram models assume each word (event) depends only on the previous n−1 words (events). Such independence assumptions are called Markov assumptions (of order n−1). !22 P(wi|w1...wi−1) :⇡P(wi|wi−n−1...wi−1) CS447: Natural Language Processing (J. Hockenmaier) 1. Bracket each sentence by special start and end symbols: <s> Alice was beginning to get very tired… </s> (We only assign probabilities to strings <s>...</s>) 2. Count the frequency of each n-gram…. C(<s> Alice) = 1, C(Alice was) = 1,…. 3. .... and normalize these frequencies to get the probability: This is called a relative frequency estimate of P(wn | wn−1) Estimating N-gram models !23 P(wn|wn−1) = C(wn−1wn) C(wn−1) CS447: Natural Language Processing (J. Hockenmaier) Start and End symbols <s>… <\s> Why do we need a start-of-sentence symbol? This is just a mathematical convenience, since it allows us to write e.g. P(w1 | <s>) for the probability of the ﬁrst word in analogy to P(wi+1 | wi ) for any other word. Why do we need an end-of-sentence symbol? This is necessary if we want to compare the probability of strings of different lengths (and actually deﬁne a probability distribution over V*). We include <\s> in the vocabulary V, require that each string ends in <\s> and that <\s> can only appear at the end of sentences, and estimate P(wi+1 = <\s> | wi ). !24 CS447: Natural Language Processing (J. Hockenmaier) Parameter estimation (training) Parameters: the actual probabilities P(wi = ‘the’ | wi-1 = ‘on’) = ??? We need (a large amount of) text as training data to estimate the parameters of a language model. The most basic estimation technique: relative frequency estimation (= counts) P(wi = ‘the’ | wi-1 = ‘on’) = C(‘on the’) / C(‘on’) Also called Maximum Likelihood Estimation (MLE) MLE assigns all probability mass to events that occur in the training corpus. !25 CS447: Natural Language Processing (J. Hockenmaier) How do we use language models? Independently of any application, we can use a language model as a random sentence generator (i.e we sample sentences according to their language model probability) Systems for applications such as machine translation, speech recognition, spell-checking, generation, often produce multiple candidate sentences as output. -We prefer output sentences SOut that have a higher probability -We can use a language model P(SOut) to score and rank these different candidate output sentences, e.g. as follows: argmaxSOut P(SOut | Input) = argmaxSOut P(Input | SOut)P(SOut) !26 CS447: Natural Language Processing (J. Hockenmaier) Using n-gram models to generate language !27 CS447: Natural Language Processing (J. Hockenmaier) Generating from a distribution !28 How do you generate text from an n-gram model? That is, how do you sample from a distribution P(X |Y=y)? -Assume X has N possible outcomes (values): {x1, …, xN} and P(X=xi | Y=y) = pi -Divide the interval [0,1] into N smaller intervals according to the probabilities of the outcomes -Generate a random number r between 0 and 1. -Return the x1 whose interval the number is in. x1 x2 x3 x4 x5 0 p1 p1+p2 p1+p2+p3 p1+p2+p3+p4 1 r CS447: Natural Language Processing (J. Hockenmaier) Generating the Wall Street Journal !29 CS447: Natural Language Processing (J. Hockenmaier) Generating Shakespeare !30 CS447: Natural Language Processing (J. Hockenmaier) Intrinsic vs Extrinsic Evaluation How do we know whether one language model is better than another? There are two ways to evaluate models: -intrinsic evaluation captures how well the model captures what it is supposed to capture (e.g. probabilities) -extrinsic (task-based) evaluation captures how useful the model is in a particular task. Both cases require an evaluation metric that allows us to measure and compare the performance of different models. !31 CS447: Natural Language Processing (J. Hockenmaier) How do we evaluate models? Deﬁne an evaluation metric (scoring function). We will want to measure how similar the predictions of the model are to real text. Train the model on a ‘seen’ training set Perhaps: tune some parameters based on held-out data (disjoint from the training data, meant to emulate unseen data) Test the model on an unseen test set (usually from the same source (e.g. WSJ) as the training data) Test data must be disjoint from training and held-out data Compare models by their scores (more on this next week). !32 CS447: Natural Language Processing (J. Hockenmaier) Intrinsic Evaluation of Language Models: Perplexity !33 CS447: Natural Language Processing (J. Hockenmaier) Perplexity Perplexity is the inverse of the probability of the test set (as assigned by the language model), normalized by the number of word tokens in the test set. Minimizing perplexity = maximizing probability! Language model LM1 is better than LM2 if LM1 assigns lower perplexity (= higher probability) to the test corpus w1…wN NB: the perplexity of LM1 and LM2 can only be directly compared if both models use the same vocabulary. !34 CS447: Natural Language Processing (J. Hockenmaier) The inverse of the probability of the test set, normalized by the number of tokens in the test set. Assume the test corpus has N tokens, w1…wN If the LM assigns probability P(w1, …, wi−n) to the test corpus, its perplexity, PP(w1…wN), is deﬁned as: A LM with lower perplexity is better because it assigns a higher probability to the unseen test corpus. Perplexity !35 PP(w1...wN) = P(w1...wN)−1 N = N ⇥ 1 P(w1...wN) = N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|w1...wi−1) =def N ⇧ ⌅ ⌅ ⇤ N % 1 ( | ) PP(w1...wN) = P(w1...wN)−1 N = N ⇥ 1 P(w1...wN) = N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|w1...wi−1) =def N ⇧ ⌅ ⌅ ⇤ N % 1 P(w |w w ) CS447: Natural Language Processing (J. Hockenmaier) Given a test corpus with N tokens, w1…wN, and an n-gram model P(wi | wi−1, …, wi−n+1) we compute its perplexity PP(w1…wN) as follows: Perplexity PP(w1…wn) (Chain rule) (N-gram model) !36 PP(w1...wN) = P(w1...wN)−1 N = N ⇥ 1 P(w1...wN) = N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|w1...wi−1) =def N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|wi−n...wi−1) PP(w1...wN) = P(w1...wN)−1 N = N ⇥ 1 P(w1...wN) = N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|w1...wi−1) =def N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|wi−n...wi−1) PP(w1...wN) = P(w1...wN)−1 N = N ⇥ 1 P(w1...wN) = N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|w1...wi−1) =def N ⇧ ⌅ ⌅ ⇤ N % i=1 1 P(wi|wi−n...wi−1) = def N s N ’ i=1 1 P(wi|wi−1,...,wi−n+1) CS447: Natural Language Processing (J. Hockenmaier) Practical issues Since language model probabilities are very small, multiplying them together often yields to underﬂow. It is often better to use logarithms instead, so replace with !37 PP(w1...wN) =def N s N ’ i=1 1 P(wi|wi−1,...,wi−n+1) PP(w1...wN) =def exp ✓ −1 N N Â i=1 logP(wi|wi−1,...,wi−n+1 ◆ CS447: Natural Language Processing (J. Hockenmaier) Perplexity and LM order Bigram LMs have lower perplexity than unigram LMs Trigram LMs have lower perplexity than bigram LMs … Example from the textbook (WSJ corpus, trained on 38M tokens, tested on 1.5 M tokens, vocabulary: 20K word types) !38 Unigram Bigram Trigram Perplexity 962 170 109 CS447: Natural Language Processing (J. Hockenmaier) Extrinsic (Task-Based) Evaluation of LMs: Word Error Rate !39 CS447: Natural Language Processing (J. Hockenmaier) Intrinsic vs. Extrinsic Evaluation Perplexity tells us which LM assigns a higher probability to unseen text This doesn’t necessarily tell us which LM is better for our task (i.e. is better at scoring candidate sentences) Task-based evaluation: -Train model A, plug it into your system for performing task T -Evaluate performance of system A on task T. -Train model B, plug it in, evaluate system B on same task T. -Compare scores of system A and system B on task T. !40 CS447: Natural Language Processing (J. Hockenmaier) Originally developed for speech recognition. How much does the predicted sequence of words differ from the actual sequence of words in the correct transcript? Insertions: “eat lunch” → “eat a lunch” Deletions: “see a movie” → “see movie” Substitutions: “drink ice tea”→ “drink nice tea” Word Error Rate (WER) WER = Insertions + Deletions + Substitutions Actual words in transcript !41 CS447: Natural Language Processing (J. Hockenmaier) But…. … unseen test data will contain unseen words !42 CS447: Natural Language Processing (J. Hockenmaier) Getting back to Shakespeare… !43 CS447: Natural Language Processing (J. Hockenmaier) Generating Shakespeare !44 CS447: Natural Language Processing (J. Hockenmaier) Shakespeare as corpus The Shakespeare corpus consists of N=884,647 word tokens and a vocabulary of V=29,066 word types Shakespeare produced 300,000 bigram types out of V2= 844 million possible bigram types. 99.96% of possible bigrams don’t occur in the corpus. Our relative frequency estimate assigns non-zero probability to only 0.04% of the possible bigrams That percentage is even lower for trigrams, 4-grams, etc. 4-grams look like Shakespeare because they are Shakespeare! !45 CS447: Natural Language Processing (J. Hockenmaier) We estimated a model on 440K word tokens, but: Only 30,000 word types occur in the training data Any word that does not occur in the training data has zero probability! Only 0.04% of all possible bigrams (over 30K word types) occur in the training data Any bigram that does not occur in the training data has zero probability (even if we have seen both words in the bigram) MLE doesn’t capture unseen events !46 CS447: Natural Language Processing (J. Hockenmaier) Zipf’s law: the long tail 1 10 100 1000 10000 100000 1 10 100 1000 10000 100000 Frequency (log) Number of words (log) How many words occur N times? Word frequency (log-scale) In natural language: -A small number of events (e.g. words) occur with high frequency -A large number of events occur with very low frequency !47 A few words are very frequent English words, sorted by frequency (log-scale) w1 = the, w2 = to, …., w5346 = computer, ... Most words are very rare How many words occur once, twice, 100 times, 1000 times? the r-th most common word wr has P(wr) ∝ 1/r CS447: Natural Language Processing (J. Hockenmaier) So…. … we can’t actually evaluate our MLE models on unseen test data (or system output)… … because both are likely to contain words/n-grams that these models assign zero probability to. We need language models that assign some probability mass to unseen words and n-grams. We will get back to this on Friday. !48 CS447: Natural Language Processing (J. Hockenmaier) To recap…. !49 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts N-gram language models Independence assumptions Relative frequency (maximum likelihood) estimation Evaluating language models: Perplexity, WER Zipf’s law Today’s reading: Jurafsky and Martin, Chapter 4, sections 1-4 (2008 edition) Chapter 3 (3rd Edition) Friday’s lecture: Handling unseen events! !50 "
26,"School of Computer Science Probabilistic Graphical Models Structured Sparse Additive Models Eric Xing Lecture 26, April 21, 2014 Reading: See class website 1 © Eric Xing @ CMU, 2005-2014 Acknowledgement: based on slides drafted by Junming Yin Outline Nonparametric regression and kernel smoothing Additive models Sparse additive models (SpAM) Structured sparse additive models (GroupSpAM) 2 © Eric Xing @ CMU, 2005-2014 Nonparametric Regression and Kernel Smoothing 3 © Eric Xing @ CMU, 2005-2014 Non-linear functions: 4 © Eric Xing @ CMU, 2005-2014 LR with non-linear basis functions LR does not mean we can only deal with linear relationships We are free to design (non-linear) features under LR where the j(x) are fixed basis functions (and we define 0(x) = 1). Example: polynomial regression: We will be concerned with estimating (distributions over) the weights θ and choosing the model order M. ) ( ) ( x x y T m j j         1 0   3 2 1 x x x x , , , : ) (   5 © Eric Xing @ CMU, 2005-2014 Basis functions There are many basis functions, e.g.:  Polynomial  Radial basis functions  Sigmoidal  Splines, Fourier, Wavelets, etc 1   j j x x) (               2 2 2s x x j j   exp ) (           s x x j j    ) ( 6 © Eric Xing @ CMU, 2005-2014 1D and 2D RBFs 1D RBF After fit: 7 © Eric Xing @ CMU, 2005-2014 Good and Bad RBFs A good 2D RBF Two bad 2D RBFs 8 © Eric Xing @ CMU, 2005-2014 Overfitting and underfitting x y 1 0    2 2 1 0 x x y         5 0 j j jx y  9 © Eric Xing @ CMU, 2005-2014 Bias and variance We define the bias of a model to be the expected generalization error even if we were to fit it to a very (say, infinitely) large training set. By fitting ""spurious"" patterns in the training set, we might again obtain a model with large generalization error. In this case, we say the model has large variance. 10 © Eric Xing @ CMU, 2005-2014 Locally weighted linear regression The algorithm: Instead of minimizing now we fit θ to minimize Where do wi's come from?  where x is the query point for which we'd like to know its corresponding y Essentially we put higher weights on (errors on) training examples that are close to the query point (than those that are further away from the query)     n i i T i y J 1 2 2 1 ) ( ) (   x     n i i T i i y w J 1 2 2 1 ) ( ) (   x            2 2 2 ) ( exp x xi i w 11 © Eric Xing @ CMU, 2005-2014 Parametric vs. non-parametric Locally weighted linear regression is another example we are running into of a non-parametric algorithm. (what are the others?) The (unweighted) linear regression algorithm that we saw earlier is known as a parametric learning algorithm  because it has a fixed, finite number of parameters (the θ), which are fit to the data;  Once we've fit the θ and stored them away, we no longer need to keep the training data around to make future predictions.  In contrast, to make predictions using locally weighted linear regression, we need to keep the entire training set around. The term ""non-parametric"" (roughly) refers to the fact that the amount of stuff we need to keep in order to represent the hypothesis grows linearly with the size of the training set. 12 © Eric Xing @ CMU, 2005-2014 Parametric model:  Assumes all data can be represented using a fixed, finite number of parameters.  Examples: polynomial regression Nonparametric model:  Number of parameters can grow with sample size.  Examples: nonparametric regression Parametric vs. non-parametric 13 © Eric Xing @ CMU, 2005-2014 Regression — probabilistic interpretation What regular regression does: Assume yk was originally generated using the following recipe: Computational task is to find the Maximum Likelihood estimation of θ ) , ( 2 0   N   k T k y x 14 © Eric Xing @ CMU, 2005-2014 Nonparametric regression is concerned with estimating the regression function from a training set The “parameter” to be estimated is the whole function m(x) No parametric assumption such as linearity is made about the regression function m(x)  More flexible than parametric model  However, usually require keeping the entire training set (memory-based) Nonparametric Regression: Formal Definition 15 © Eric Xing @ CMU, 2005-2014 Kernel Smoother The simplest nonparametric regression estimator Local weighted (smooth) average of The weight depends on the distance to Nadaraya-Watson kernel estimator K is the smoothing kernel function K(x)>=0 and h is the bandwidth 16 © Eric Xing @ CMU, 2005-2014 It satisfies Different types Kernel Function 17 © Eric Xing @ CMU, 2005-2014 Bandwidth The choice of bandwidth h is much more important than the type of kernel K  Small h -> rough estimates  Large h -> smoother estimates  In practice: cross-validation or plug-in methods 18 © Eric Xing @ CMU, 2005-2014 Linear Smoothers Kernel smoothers are examples of linear smoothers  For each x, the estimator is a linear combination of  Other examples: smoothing splines, locally weighted polynomial, etc 19 © Eric Xing @ CMU, 2005-2014   y X X X T T 1 *    Linear Smoothers (con’t) Define be the fitted values of the training examples, then  The n x n matrix S is called the smoother matrix with  The fitted values are the smoother version of original values Recall the regression function can be viewed as  P is the conditional expectation operator that projects a random variable (it is Y here) onto the linear space of X  It plays the role of smoother in the population setting 20 © Eric Xing @ CMU, 2005-2014 Additive Models 21 © Eric Xing @ CMU, 2005-2014 Additive Models Due to curse of dimensionality, smoothers break down in high dimensional setting (where the definition of “neighborhood” is tricky) Hastie & Tibshirani (1990) proposed the additive model  Each is a smooth one-dimensional component function However, the model is not identifiable  Can add a constant to one component function and subtract the same constant from another component  Can be easily fixed by assuming 22 © Eric Xing @ CMU, 2005-2014 Backfitting The optimization problem in the population setting is It can be shown that the optimum is achieved at  is the conditional expectation operator onto jth input space  is the partial residual 23 © Eric Xing @ CMU, 2005-2014 Backfitting (con’t) Replace conditional operator by smoother matrix results in the backfitting algorithm  Initialize:  Cycle: for  Centering:  This is the current fitted values of the jth component on the n training examples  This is a coordinate descent algorithm 24 © Eric Xing @ CMU, 2005-2014 Example 48 rock samples from a petroleum reservoir The response: permeability The covariates: the area of pores, perimeter in pixels and shape (perimeter/sqrt(area)) 25 © Eric Xing @ CMU, 2005-2014 Sparse Additive Models (SpAM) 26 © Eric Xing @ CMU, 2005-2014 SpAM A sparse version of additive models (Ravikumar et. al 2009) Can perform component/variable selection for additive models even when n << p The optimization problem in the population setting is  behaves like an l1 ball across different components to encourage functional sparsity  If each component function is constrained to have the linear form, the formulation reduces to standard lasso (Tibshirani 1996) 27 © Eric Xing @ CMU, 2005-2014 SpAM Backfitting The optimum is achieved by soft-thresholding step  is the partial residual; is the positive part  (thresholding condition) As in standard additive models, replace by  is the empirical estimate of 28 © Eric Xing @ CMU, 2005-2014 Example n =150, p = 200 (only 4 component functions are non-zeros) 29 © Eric Xing @ CMU, 2005-2014 Structured Sparse Additive Models (GroupSpAM) 30 © Eric Xing @ CMU, 2005-2014 GroupSpAM Exploit structured sparsity in the nonparametric setting The simplest structure is a non-overlapping group (or a partition of the original p variables) The optimization problem in the population setting is Challenges:  New difficulty to characterize the thresholding condition at group level  No closed-form solution to the stationary condition, in the form of soft- thresholding step 31 © Eric Xing @ CMU, 2005-2014 Thresholding Conditions Theorem: the whole group g of functions if and only if  is the partial residual after removing all functions from group g  Necessity: straightforward to prove  Sufficiency: more involved (see Yin et. al, 2012) 32 © Eric Xing @ CMU, 2005-2014 GroupSpAM Backfitting 33 © Eric Xing @ CMU, 2005-2014 Experiments Sample size n=150 and dimension p = 200, 1000  34 © Eric Xing @ CMU, 2005-2014 Experiments (p = 200) Performance based on 100 independent simulations (t = 0) Performance based on 100 independent simulations (t = 2) 35 © Eric Xing @ CMU, 2005-2014 Experiments (p = 1000) Performance based on 100 independent simulations (t = 0) Performance based on 100 independent simulations (t = 2) 36 © Eric Xing @ CMU, 2005-2014 Estimated Component Functions 37 © Eric Xing @ CMU, 2005-2014 GroupSpAM with Overlap Allow overlap between the different groups (Jacob et al., 2009) Idea: decompose each original component function to be a sum of a set of latent functions and then apply the functional group penalty to the decomposed  The resulting support is a union of pre-defined groups  Can be reduced to the GroupSpAM with disjoint groups and solved by the same backfitting algorithm 38 © Eric Xing @ CMU, 2005-2014 Breast Cancer Data Sample size n = 295 tumors (metastatic vs non-metastatic) and dimension p = 3,510 genes. Goal: identify few genes that can predict the types of tumors. Group structure: each group consists of the set of genes in a pathway and groups are overlapping. 39 © Eric Xing @ CMU, 2005-2014 Summary Novel statistical method for structured functional sparsity in nonparametric additive models  Functional sparsity at the group level in additive models.  Can easily incorporate prior knowledge of the structures among the covariates.  Highly flexible: no assumptions are made on the design matrices or on the correlation of component functions in each group.  Benefit of group sparsity: better performance in terms of support recovery and prediction accuracy in additive models. 40 © Eric Xing @ CMU, 2005-2014 References Hastie, T. and Tibshirani, R. Generalized Additive Models. Chapman & Hall/CRC, 1990. Buja, A., Hastie, T., and Tibshirani, R. Linear Smoothers and Additive Models. Ann. Statist. Volume 17, Number 2 (1989), 453-510. Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L. Sparse additive models. JRSSB, 71(5):1009–1030, 2009. Yin, J., Chen, X., and Xing, E. Group Sparse Additive Models, ICML, 2012 41 © Eric Xing @ CMU, 2005-2014 "
260,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 4: Smoothing CS447: Natural Language Processing (J. Hockenmaier) Last lecture’s key concepts Basic probability review: joint probability, conditional probability Probability models Independence assumptions Parameter estimation: relative frequency estimation (aka maximum likelihood estimation) Language models N-gram language models: unigram, bigram, trigram… ""2 CS447: Natural Language Processing (J. Hockenmaier) N-gram language models A language model is a distribution P(W) over the (inﬁnite) set of strings in a language L To deﬁne a distribution over this inﬁnite set, we have to make independence assumptions. N-gram language models assume that each word wi depends only on the n−1 preceding words: Pn-gram(w1 … wT) := ∏i=1..T P(wi | wi−1, …, wi−(n−1)) Punigram(w1 … wT) := ∏i=1..T P(wi) Pbigram(w1 … wT) := ∏i=1..T P(wi | wi−1) Ptrigram(w1 … wT) := ∏i=1..T P(wi | wi−1, wi−2) ""3 CS447: Natural Language Processing (J. Hockenmaier) Quick note re. notation Consider the sentence W = “John loves Mary” For a trigram model we could write: P(w3 = Mary | w1 w2 = “John loves” ) This notation implies that we treat the preceding bigram w1w2 as one single conditioning variable P( X | Y ) Instead, we typically write: P(w3 = Mary | w2 = loves, w1 = John) Although this is less readable (John loves → loves, John), this notation gives us more ﬂexibility, since it implies that we treat the preceding bigram w1w2 as two conditioning variables P( X | Y, Z ) ""4 CS447: Natural Language Processing (J. Hockenmaier) Parameter estimation (training) Parameters: the actual probabilities (numbers) P(wi = ‘the’ | wi-1 = ‘on’) = 0.0123 We need (a large amount of) text as training data to estimate the parameters of a language model. The most basic estimation technique: relative frequency estimation (= counts) P(wi = ‘the’ | wi-1 = ‘on’) = C(‘on the’) / C(‘on’) This assigns all probability mass to events in the training corpus. Also called Maximum Likelihood Estimation (MLE) ""5 CS447: Natural Language Processing (J. Hockenmaier) Recall the Shakespeare example: Only 30,000 word types occurred. Any word that does not occur in the training data has zero probability! Only 0.04% of all possible bigrams occurred. Any bigram that does not occur in the training data has zero probability! Testing: unseen events will occur ""6 CS447: Natural Language Processing (J. Hockenmaier) Zipf’s law: the long tail 1 10 100 1000 10000 100000 1 10 100 1000 10000 100000 Frequency (log) Number of words (log) How many words occur N times? Word frequency (log-scale) In natural language: -A small number of events (e.g. words) occur with high frequency -A large number of events occur with very low frequency ""7 A few words are very frequent English words, sorted by frequency (log-scale) w1 = the, w2 = to, …., w5346 = computer, ... Most words are very rare How many words occur once, twice, 100 times, 1000 times? the r-th most common word wr has P(wr) ∝ 1/r CS447: Natural Language Processing (J. Hockenmaier) So…. … we can’t actually evaluate our MLE models on unseen test data (or system output)… … because both are likely to contain words/n-grams that these models assign zero probability to. We need language models that assign some probability mass to unseen words and n-grams. ""8 CS447: Natural Language Processing (J. Hockenmaier) How can we design language models* that can deal with previously unseen events? *actually, probabilistic models in general Today’s lecture ""9 P(seen) = 1.0 ??? P(seen) < 1.0 P(unseen) > 0.0 MLE model Smoothed model CS447: Natural Language Processing (J. Hockenmaier) Dealing with unseen events Relative frequency estimation assigns all probability mass to events in the training corpus But we need to reserve some probability mass to events that don’t occur in the training data Unseen events = new words, new bigrams Important questions: What possible events are there? How much probability mass should they get? ""10 CS447: Natural Language Processing (J. Hockenmaier) What unseen events may occur? Simple distributions: P(X = x) (e.g. unigram models) Possibility: The outcome x has not occurred during training (i.e. is unknown): -We need to reserve mass in P( X ) for x Questions: -What outcomes x are possible? -How much mass should they get? ""11 CS447: Natural Language Processing (J. Hockenmaier) What unseen events may occur? Simple conditional distributions: P( X = x | Y = y) (e.g. bigram models) Case 1: The outcome x has been seen, but not in the context of Y = y: -We need to reserve mass in P( X | Y=y ) for X = x Case 2: The conditioning variable y has not been seen: -We have no P( X | Y = y ) distribution. -We need to drop the conditioning variable Y = y and use P( X ) instead. ""12 CS447: Natural Language Processing (J. Hockenmaier) What unseen events may occur? Complex conditional distributions (with multiple conditioning variables) P( X = x | Y = y, Z = z) (e.g. trigram models) Case 1: The outcome X = x was seen, but not in the context of (Y=y, Z=z): -We need to reserve mass in P( X | Y = y, Z = z) Case 2: The joint conditioning event (Y=y, Z=z) hasn’t been seen: - We have no P( X | Y=y, Z=z) distribution. - But we can drop z and use P( X | Y=y) instead. ""13 CS447: Natural Language Processing (J. Hockenmaier) Examples Training data: The wolf is an endangered species Test data: The wallaby is endangered -Case 1: P(wallaby), P(wallaby | the), P( wallaby | the, <s>): What is the probability of an unknown word (in any context)? -Case 2: P(endangered | is) What is the probability of a known word in a known context, if that word hasn’t been seen in that context? -Case 3: P(is | wallaby) P(is | wallaby, the) P(endangered | is, wallaby): What is the probability of a known word in an unseen context? ""14 Unigram Bigram Trigram P(the) P(the | <s>) P(the | <s>) × P(wallaby) × P( wallaby | the) × P( wallaby | the, <s>) × P(is) × P(is | wallaby) × P(is | wallaby, the) × P(endangered) × P(endangered | is) × P(endangered | is, wallaby) CS447: Natural Language Processing (J. Hockenmaier) Smoothing: Reserving mass in P( X ) for unseen events ""15 CS447: Natural Language Processing (J. Hockenmaier) Dealing with unknown words: The simple solution Training: -Assume a ﬁxed vocabulary (e.g. all words that occur at least twice (or n times) in the corpus) -Replace all other words by a token <UNK> -Estimate the model on this corpus. Testing: -Replace all unknown words by <UNK> -Run the model. This requires a large training corpus to work well. ""16 CS447: Natural Language Processing (J. Hockenmaier) Use a different estimation technique: -Add-1(Laplace) Smoothing -Good-Turing Discounting Idea: Replace MLE estimate Combine a complex model with a simpler model: -Linear Interpolation -Modiﬁed Kneser-Ney smoothing Idea: use bigram probabilities of wi to calculate trigram probabilities of wi Dealing with unknown events P(w) = C(w) N P(wi|wi−n...wi−1) P(wi|wi−1) ""17 CS447: Natural Language Processing (J. Hockenmaier) MLE P(wi) = C(wi) ∑jC(wj) = C(wi) N Add One P(wi) = C(wi)+1 ∑j(C(wj)+1) = C(wi)+1 N+V Assume every (seen or unseen) event occurred once more than it did in the training data. Example: unigram probabilities Estimated from a corpus with N tokens and a vocabulary (number of word types) of size V. Add-1 (Laplace) smoothing ""18 MLE P(wi) = C(wi) ∑jC(wj) = C(wi) N Add One P(wi) = C(wi)+1 ∑j(C(wj)+1) = C(wi)+1 N+V CS447: Natural Language Processing (J. Hockenmaier) Bigram counts Original: Smoothed: ""19 CS447: Natural Language Processing (J. Hockenmaier) Bigram probabilities Smoothed: Original: Problem: Add-one moves too much probability mass from seen to unseen events! ""20 CS447: Natural Language Processing (J. Hockenmaier) We can “reconstitute” pseudo-counts c* for our training set of size N from our estimate: Unigrams: Bigrams: Reconstituting the counts c⇥ i = P(wi)·N = C(wi)+1 N +V ·N = (C(wi)+1)· N N +V ""21 P(wi): probability that the next word is wi. N: number of word tokens we generate Plug in the model definition of P(wi) V: size of vocabulary Rearrange (to see dependence on N and V) P(wi–1wi): probability of bigram “wi–1wi”. C(wi–1): frequency of wi–1 (in training data) Plug in the model definition of P(wi | wi–1) c⇤(wi|wi−1) = P(wi|wi−1)·C(wi−1) = C(wi−1wi)+1 C(wi−1)+V ·C(wi−1) CS447: Natural Language Processing (J. Hockenmaier) Reconstituted Bigram counts Original: Reconstituted: ""22 CS447: Natural Language Processing (J. Hockenmaier) Summary: Add-One smoothing P(wi|wi−1 = the) = C(the wi)+1 25, 545+30, 000 Advantage: Very simple to implement Disadvantage: Takes away too much probability mass from seen events. Assigns too much total probability mass to unseen events. The Shakespeare example (V = 30,000 word types; ‘the’ occurs 25,545 times) Bigram probabilities for ‘the …’: ""23 CS447: Natural Language Processing (J. Hockenmaier) Add-K smoothing Variant of Add-One smoothing: For any k > 0 (typically, k < 1) This is still too simplistic to work well. ""24 Add K P(wi) = C(wi)+k N +kV CS447: Natural Language Processing (J. Hockenmaier) f = 1 f > 1 Good-Turing smoothing Basic idea: Use total frequency of events that occur only once to estimate how much mass to shift to unseen events - “occur only once” (in training data): frequency f = 1 - “unseen” (in training data): frequency f = 0 (didn’t occur) ""25 f = 0 f = 1 f > 1 Relative Frequency Estimate Good Turing Estimate CS447: Natural Language Processing (J. Hockenmaier) MLE f = 1 f > 1 P(seen) + P(unseen) = 1 MLE N N + 0 = 1 Good Turing 2·N2 +...+m·Nm ∑m i=1 i·Ni + 1·N1 ∑m i=1 i·Ni = ∑m i=1 i·Ni ∑m i=1 i·Ni P(seen) + P(unseen) = 1 MLE N N + 0 = 1 Good Turing 2·N2 +...+m·Nm ∑m i=1 i·Ni + 1·N1 ∑m i=1 i·Ni = ∑m i=1 i·Ni ∑m i=1 i·Ni Good-Turing smoothing Nc: number of event types that occur c times (can be counted) N1: number of event types that occur once N = 1N1+…+ mNm: total number of observed event tokens ""26 GT f=0 f = 1 f > 1 P(seen) + P(unseen) = 1 MLE N N + 0 = 1 Good Turing 2·N2 +...+m·Nm ∑m i=1 i·Ni + 1·N1 ∑m i=1 i·Ni = ∑m i=1 i·Ni ∑m i=1 i·Ni CS447: Natural Language Processing (J. Hockenmaier) Good-Turing Smoothing General principle: Reassign the probability mass of all events that occur k times in the training data to all events that occur k–1 times. Nk events occur k times, with a total frequency of k⋅Nk The probability mass of all words that appear k–1 times becomes: ""27 There are Nk-1 words w that occur k–1 times in the training data. Good-Turing replaces the original count ck–1 of w with a new count c*k–1: c⇤ k−1 = k ·Nk Nk−1 Â w:C(w)=k−1 P GT(w) = Â w0:C(w0)=k P MLE(w0) = Â w0:C(w0)=k k N = k ·Nk N Â w:C(w)=k−1 P GT(w) = Â w0:C(w0)=k P MLE(w0) = Â w0:C(w0)=k k N = k ·Nk N Â w:C(w)=k−1 P GT(w) = Â w0:C(w0)=k P MLE(w0) = Â w0:C(w0)=k k N = k ·Nk N Â w:C(w)=k−1 P GT(w) = Â w0:C(w0)=k P MLE(w0) = Â w0:C(w0)=k k N = k ·Nk N CS447: Natural Language Processing (J. Hockenmaier) Good-Turing smoothing The Maximum Likelihood estimate of the probability of a word w that occurs k–1 times PMLE(w) = C(w)/N ""28 The Good-Turing estimate of the probability of a word w that occurs k–1 times: PGT(w) = c*k–1 / N: P GT(w) = c⇤ k−1 N = ✓ k·Nk Nk−1 ◆ N = k ·Nk N ·Nk−1 P MLE(w) = ck−1 N = k −1 N CS447: Natural Language Processing (J. Hockenmaier) Problem 1: What happens to the most frequent event? Problem 2: We don’t observe events for every k. Variant: Simple Good-Turing Replace Nn with a ﬁtted function f(n): Requires parameter tuning (on held-out data): Set a,b so that f(n) ≅Nn for known values. Use cn* only for small n Problems with Good-Turing f(n) = a + b log(n) ""29 CS447: Natural Language Processing (J. Hockenmaier) Smoothing: Reserving mass in P(X |Y) for unseen events ""30 CS447: Natural Language Processing (J. Hockenmaier) We don’t see “Bob was reading”, but we see “__ was reading”. We estimate P(reading |’Bob was’) = 0 but P(reading | ‘was’) > 0 Use (n –1)-gram probabilities to smooth n-gram probabilities: Linear Interpolation (1) ""31 P( wi |wi−2wi−1 =’Bob was’) P( wi |wi−1 =’was’) P( wi |wi−2wi−1 = ’Bob was’) 1−λ ˜ P LI(wi|wi−nwi−n+1... wi−2wi−1) | {z } smoothed n-gram = l ˆ P(wi|wi−nwi−n+1... wi−2wi−1) | {z } unsmoothed n-gram +(1−l) ˜ P LI(wi|wi−n+1... wi−2wi−1) | {z } smoothed (n-1)-gram λ CS447: Natural Language Processing (J. Hockenmaier) What happens to P(w | …)? The smoothed probability Psmoothed-trigram(wi | wi−2 wi−1) is a linear combination of Punsmoothed-trigram(wi | wi−2 wi−1) and Pbigram(wi | wi−1): ""32 λ 0 1 0 1 0 1 punsmoothed-trigram pbigram psmoothed-trigram λ 0 1 0 1 0 1 punsmoothed-trigram pbig psmoothed-trigram CS447: Natural Language Processing (J. Hockenmaier) We’ve never seen “Bob was reading”, but we might have seen “__ was reading”, and we’ve certainly seen “__ reading” (or <UNK>) Psmoothed(wi = reading | wi−1 = was, wi−2 = Bob) = λ3 Punsmoothed-trigram(wi = reading | wi−1 = was, wi−2 = Bob) + λ2 Punsmoothed-bigram(wi = reading | wi−1 = was) + λ1 Punsmoothed-unigram(wi = reading) Linear Interpolation (2) ""33 ˜ P(wi|wi−1,wi−2) =l3 · ˆ P(wi|wi−1,wi−2) +l2 · ˆ P(wi|wi−1) +l1 · ˆ P(wi) for l1 +l2 +l3 = 1 CS447: Natural Language Processing (J. Hockenmaier) Interpolation: Setting the λs Method A: Held-out estimation Divide data into training and held-out data. Estimate models on training data. Use held-out data (and some optimization technique) to ﬁnd the λ that gives best model performance. Often: λ is a learned function of the frequencies of wi–n…wi–1 Method B: λ is some (deterministic) function of the frequencies of wi–n...wi–1 ""34 CS447: Natural Language Processing (J. Hockenmaier) Subtract a constant factor D <1 from each nonzero n-gram count, and interpolate with PAD(wi | wi–1): If S seen word types occur after wi-2 wi-1 in the training data, this reserves the probability mass P(U) = (S ×D)/C(wi-2wi-1) to be computed according to P(wi | wi–1). Set: N.B.: with N1, N2 the number of n-grams that occur once or twice, D = N1/(N1+2N2) works well in practice Absolute discounting ""35 (1−l) = P(U) = S·D C(wi−2wi−1) P AD(wi|wi−1,wi−2) = max(C(wi−2wi−1wi)−D,0) C(wi−2wi−1) +(1−l)P AD(wi|wi−1) non-zero if trigram wi-2wi-1wi is seen CS447: Natural Language Processing (J. Hockenmaier) Kneser-Ney smoothing Observation: “San Francisco” is frequent, but “Francisco” only occurs after “San”. Solution: the unigram probability P(w) should not depend on the frequency of w, but on the number of contexts in which w appears N+1(●w): number of contexts in which w appears = number of word types w’ which precede w N+1(●●) = ∑ w’ N+1(●w’) Kneser-Ney smoothing: Use absolute discounting, but use P(w) = N+1(●w)/N+1(●●) Modiﬁed Kneser-Ney smoothing: Use different D for bigrams and trigrams (Chen & Goodman ’98) ""36 CS447: Natural Language Processing (J. Hockenmaier) To recap…. ""37 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts Dealing with unknown words Dealing with unseen events Good-Turing smoothing Linear Interpolation Absolute Discounting Kneser-Ney smoothing Today’s reading: Jurafsky and Martin, Chapter 4, sections 1-4 ""38 "
261,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 5: Part-of-Speech Tagging CS447: Natural Language Processing (J. Hockenmaier) POS tagging Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Raw text Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB the_DT board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. Tagged text Tagset: NNP: proper noun CD: numeral, JJ: adjective, ... POS tagger !2 CS447: Natural Language Processing (J. Hockenmaier) Why POS tagging? POS tagging is a prerequisite for further analysis: –Speech synthesis: How to pronounce “lead”? INsult or inSULT, OBject or obJECT, OVERﬂow or overFLOW, DIScount or disCOUNT, CONtent or conTENT –Parsing: What words are in the sentence? –Information extraction: Finding names, relations, etc. –Machine Translation: The noun “content” may have a different translation from the adjective. !3 CS447: Natural Language Processing (J. Hockenmaier) POS Tagging Words often have more than one POS: -The back door (adjective) -On my back (noun) -Win the voters back (particle) -Promised to back the bill (verb) The POS tagging task is to determine the POS tag for a particular instance of a word. Since there is ambiguity, we cannot simply look up the correct POS in a dictionary. These examples from Dekang Lin !4 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning a tagset !5 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning a tag set We have to deﬁne an inventory of labels for the word classes (i.e. the tag set) -Most taggers rely on models that have to be trained on annotated (tagged) corpora. Evaluation also requires annotated corpora. -Since human annotation is expensive/time-consuming, the tag sets used in a few existing labeled corpora become the de facto standard. -Tag sets need to capture semantically or syntactically important distinctions that can easily be made by trained human annotators. !6 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning an annotation scheme A lot of NLP tasks require systems to map natural language text to another representation: POS tagging: Text ⟶ POS tagged text Syntactic Parsing: Text ⟶ parse trees Semantic Parsing: Text ⟶ meaning representations …: Text ⟶ … !7 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning an annotation scheme Training and evaluating models for these NLP tasks requires large corpora annotated with the desired representations. Annotation at scale is expensive, so a few existing corpora and their annotations and annotation schemes (tag sets, etc.) often become the de facto standard for the ﬁeld. It is difﬁcult to know what the ‘right’ annotation scheme should be for any particular task How difﬁcult is it to achieve high accuracy for that annotation? How useful is this annotation scheme for downstream tasks in the pipeline? ➩ We often can’t know the answer until we’ve annotated a lot of data… !8 CS447: Natural Language Processing (J. Hockenmaier) Word classes Open classes: Nouns, Verbs, Adjectives, Adverbs Closed classes: Auxiliaries and modal verbs Prepositions, Conjunctions Pronouns, Determiners Particles, Numerals (see Appendix for details) !9 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning a tag set Tag sets have different granularities: Brown corpus (Francis and Kucera 1982): 87 tags Penn Treebank (Marcus et al. 1993): 45 tags Simpliﬁed version of Brown tag set (de facto standard for English now) NN: common noun (singular or mass): water, book NNS: common noun (plural): books Prague Dependency Treebank (Czech): 4452 tags Complete morphological analysis: AAFP3----3N----: nejnezajímavějším Adjective Regular Feminine Plural Dative….Superlative [Hajic 2006, VMC tutorial] !10 CS447: Natural Language Processing (J. Hockenmaier) How much ambiguity is there? Most word types are unambiguous: Number of tags per word type: But a large fraction of word tokens are ambiguous Original Brown corpus: 40% of tokens are ambiguous !11 NB: These numbers are based on word/tag combinations in the corpus. Many combinations that don’t occur in the corpus are equally correct. CS447: Natural Language Processing (J. Hockenmaier) Evaluating POS taggers !12 CS447: Natural Language Processing (J. Hockenmaier) Evaluation setup: Split data into separate training, (development) and test sets. Better setup: n-fold cross validation: Split data into n sets of equal size Run n experiments, using set i to test and remainder to train This gives average, maximal and minimal accuracies When comparing two taggers: Use the same test and training data with the same tag set Evaluating POS taggers !13 D E V TRAINING T E S T D E V TRAINING T E S T or CS447: Natural Language Processing (J. Hockenmaier) Evaluation metric: test accuracy How many words in the unseen test data can you tag correctly? State of the art on Penn Treebank: around 97%. ➩ How many sentences can you tag correctly? Compare your model against a baseline Standard: assign to each word its most likely tag (use training corpus to estimate P(t|w) ) Baseline performance on Penn Treebank: around 93.7% … and a (human) ceiling How often do human annotators agree on the same tag? Penn Treebank: around 97% !14 CS447: Natural Language Processing (J. Hockenmaier) Is POS-tagging a solved task? Penn Treebank POS-tagging accuracy ≈ human ceiling Yes, but: Other languages with more complex morphology need much larger tag sets for tagging to be useful, and will contain many more distinct word forms in corpora of the same size They often have much lower accuracies !15 CS447: Natural Language Processing (J. Hockenmaier) Generate a confusion matrix (for development data): How often was a word with tag i mistagged as tag j: See what errors are causing problems: -Noun (NN) vs ProperNoun (NNP) vs Adj (JJ) -Preterite (VBD) vs Participle (VBN) vs Adjective (JJ) Qualitative evaluation !16 Correct Tags Predicted Tags % of errors caused by mistagging VBN as JJ CS447: Natural Language Processing (J. Hockenmaier) Building a POS tagger !17 CS447: Natural Language Processing (J. Hockenmaier) She promised to back the bill w = w(1) w(2) w(3) w(4) w(5) w(6) t = t(1) t(2) t(3) t(4) t(5) t(6) PRP VBD TO VB DT NN What is the most likely sequence of tags t= t(1)…t(N) for the given sequence of words w= w(1)…w(N) ? t* = argmaxt P(t | w) Statistical POS tagging !18 CS447: Natural Language Processing (J. Hockenmaier) POS tagging with generative models P(t,w): the joint distribution of the labels we want to predict (t) and the observed data (w). We decompose P(t,w) into P(t) and P(w | t) since these distributions are easier to estimate. Models based on joint distributions of labels and observed data are called generative models: think of P(t)P(w | t) as a stochastic process that ﬁrst generates the labels, and then generates the data we see, based on these labels. !19 gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) CS447: Natural Language Processing (J. Hockenmaier) Hidden Markov Models (HMMs) HMMs are the most commonly used generative models for POS tagging (and other tasks, e.g. in speech recognition) HMMs make speciﬁc independence assumptions when deﬁning P(t) and P(w | t): P(t) is an n-gram model over tags: Bigram HMM: P(t) = P(t(1))P(t(2) | t(1))P(t(3) | t(2))… P(t(N) | t(N-1)) Trigram HMM: P(t) = P(t(1))P(t(2)| t(1))P(t(3)| t(2),t(1))…P(t(n) | t(N-1),t(N-2)) P(ti | tj) or P(ti | tj,tk) are called transition probabilities In P(w | t) each word is generated by its tag: P(w | t) = P(w(1) | t(1))P(w(2) | t(2))… P(w(N) | t(N)) P(w | t) are called emission probabilities !20 CS447: Natural Language Processing (J. Hockenmaier) HMMs as probabilistic automata DT JJ NN 0.7 0.3 0.4 0.6 0.55 VBZ 0.45 0.5 the 0.2 a 0.1 every 0.1 some 0.1 no 0.01 able ... ... 0.003 zealous ... ... 0.002 zone 0.00024 abandonment 0.001 yields ... ... 0.02 acts An HMM deﬁnes Transition probabilities: P( ti | tj) Emission probabilities: P( wi | ti ) !21 CS447: Natural Language Processing (J. Hockenmaier) How would the automaton for a trigram HMM with transition probabilities P(ti | tjtk) look like? What about unigrams or n-grams? ??? ??? !22 CS447: Natural Language Processing (J. Hockenmaier) DT JJ NN VBZ q0 Encoding a trigram model as FSA JJ_DT NN_DT JJ NN VBZ DT <S> DT_<S> <S> JJ_JJ NN_JJ VBZ_NN NN_NN Bigram model: States = Tag Unigrams Trigram model: States = Tag Bigrams !23 CS447: Natural Language Processing (J. Hockenmaier) HMM deﬁnition !24 A HMM λ = (A,B,⇥) consists of • a set of N states Q = {q1,....qN with Q0 ⇤Q a set of initial states and QF ⇤Q a set of ﬁnal (accepting) states • an output vocabulary of M items V = {v1,...vm} • an N ×N state transition probability matrix A with aij the probability of moving from qi to qj. (∑N j=1aij = 1 ⇧i; 0 ⌅aij ⌅1 ⇧i, j) • an N ×M symbol emission probability matrix B with bij the probability of emitting symbol vj in state qi (∑N j=1bij = 1 ⇧i; 0 ⌅bij ⌅1 ⇧i, j) • an initial state distribution vector ⇥= ⟨⇥1,...,⇥N with ⇥i the probability of being in state qi at time t = 1. (∑N i=1⇥i = 1 0 ⌅⇥i ⌅1 ⇧i) A HMM λ = (A,B,⇥) consists of • a set of N states Q = {q1,....qN with Q0 ⇤Q a set of initial states and QF ⇤Q a set of ﬁnal (accepting) states • an output vocabulary of M items V = {v1,...vm} • an N ×N state transition probability matrix A with aij the probability of moving from qi to qj. (∑N j=1aij = 1 ⇧i; 0 ⌅aij ⌅1 ⇧i, j) • an N ×M symbol emission probability matrix B with bij the probability of emitting symbol vj in state qi (∑N j=1bij = 1 ⇧i; 0 ⌅bij ⌅1 ⇧i, j) • an initial state distribution vector ⇥= ⟨⇥1,...,⇥N with ⇥i the probability of being in state qi at time t = 1. (∑N i=1⇥i = 1 0 ⌅⇥i ⌅1 ⇧i) A HMM λ = (A,B,⇥) consists of • a set of N states Q = {q1,....qN with Q0 ⇤Q a set of initial states and QF ⇤Q a set of ﬁnal (accepting) states • an output vocabulary of M items V = {v1,...vm} • an N ×N state transition probability matrix A with aij the probability of moving from qi to qj. (∑N j=1aij = 1 ⇧i; 0 ⌅aij ⌅1 ⇧i, j) • an N ×M symbol emission probability matrix B with bij the probability of emitting symbol vj in state qi (∑N j=1bij = 1 ⇧i; 0 ⌅bij ⌅1 ⇧i, j) • an initial state distribution vector ⇥= ⟨⇥1,...,⇥N with ⇥i the probability of being in state qi at time t = 1. (∑N i=1⇥i = 1 0 ⌅⇥i ⌅1 ⇧i) A HMM λ = (A,B,⇥) consists of • a set of N states Q = {q1,....qN with Q0 ⇤Q a set of initial states and QF ⇤Q a set of ﬁnal (accepting) states • an output vocabulary of M items V = {v1,...vm} • an N ×N state transition probability matrix A with aij the probability of moving from qi to qj. (∑N j=1aij = 1 ⇧i; 0 ⌅aij ⌅1 ⇧i, j) • an N ×M symbol emission probability matrix B with bij the probability of emitting symbol vj in state qi (∑N j=1bij = 1 ⇧i; 0 ⌅bij ⌅1 ⇧i, j) • an initial state distribution vector ⇥= ⟨⇥1,...,⇥N with ⇥i the probability of being in state qi at time t = 1. (∑N i=1⇥i = 1 0 ⌅⇥i ⌅1 ⇧i) A HMM λ = (A,B,⇥) consists of • a set of N states Q = {q1,....qN with Q0 ⇤Q a set of initial states and QF ⇤Q a set of ﬁnal (accepting) states • an output vocabulary of M items V = {v1,...vm} • an N ×N state transition probability matrix A with aij the probability of moving from qi to qj. (∑N j=1aij = 1 ⇧i; 0 ⌅aij ⌅1 ⇧i, j) • an N ×M symbol emission probability matrix B with bij the probability of emitting symbol vj in state qi (∑N j=1bij = 1 ⇧i; 0 ⌅bij ⌅1 ⇧i, j) • an initial state distribution vector ⇥= ⟨⇥1,...,⇥N with ⇥i the probability of being in state qi at time t = 1. (∑N i=1⇥i = 1 0 ⌅⇥i ⌅1 ⇧i) } CS498JH: Introduction to NLP An example HMM !25 D N V A . D 0.8 0.2 N 0.7 0.3 V 0.6 0.4 A 0.8 0.2 . Transition Matrix A the man ball throws sees red blue . D 1 N 0.7 0.3 V 0.6 0.4 A 0.8 0.2 . 1 Emission Matrix B D N V A . π 1 Initial state vector π D N V A . CS447: Natural Language Processing (J. Hockenmaier) Building an HMM tagger To build an HMM tagger, we have to: -Train the model, i.e. estimate its parameters (the transition and emission probabilities) Easy case: we have a corpus labeled with POS tags (supervised learning) -Deﬁne and implement a tagging algorithm that ﬁnds the best tag sequence t* for each input sentence w: t* = argmaxt P(t)P(w | t) !26 CS498JH: Introduction to NLP We count how often we see titj and wj_ti etc. in the data (use relative frequency estimates): Learning the transition probabilities: Learning the emission probabilities: Learning an HMM from labeled data !27 P(tj|ti) = C(titj) C(ti) Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB the_DT board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. P(wj|ti) = C(wj ti) C(ti) CS447: Natural Language Processing (J. Hockenmaier) Learning an HMM from unlabeled data We can’t count anymore. We have to guess how often we’d expect to see titj etc. in our data set. Call this expected count〈C(...)〉 -Our estimate for the transition probabilities: -Our estimate for the emission probabilities: -We will talk about how to obtain these counts on Friday !28 Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Tagset: NNP: proper noun CD: numeral, JJ: adjective,... ˆ P(tj|ti) = ⟨C(titj)⇥ ⟨C(ti)⇥ ˆ P(wj|ti) = ⟨C(wj ti)⇥ ⟨C(ti)⇥ CS447: Natural Language Processing (J. Hockenmaier) Finding the best tag sequence The number of possible tag sequences is exponential in the length of the input sentence: Each word can have up to T tags. There are N words. There are up to NT possible tag sequences. We cannot enumerate all NT possible tag sequences. But we can exploit the independence assumptions in the HMM to deﬁne an efﬁcient algorithm that returns the tag sequence with the highest probability !29 CS447: Natural Language Processing (J. Hockenmaier) States Bookkeeping: the trellis We use a N×T table (“trellis”) to keep track of the HMM. The HMM can assign one of the T tags to each of the N words. w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT Words (“time steps”) !30 word w(i) has tag tj CS447: Natural Language Processing (J. Hockenmaier) Computing P(t,w) for one tag sequence w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT P(w(1)|q1) P(w(2) | qj) P(w(i) | qi) P(t(1)=q1) P(qj | q1) P(qi | q...) P(q..| qi) P(w(i+1) | qi+1) P(w(N) | qj) P(qj | q..) !31 -One path through the trellis = one tag sequence -We just multiply the probabilities as before CS447: Natural Language Processing (J. Hockenmaier) Using the trellis to ﬁnd t* Let trellis[i][j] (word w(j) and tag tj) store the probability of the best tag sequence for w(1)…w(i) that ends in tj trellis[i][j] = max P(w(1)…w(i), t(1)…, t(i) = tj ) We can recursively compute trellis[i][j] from the entries in the previous column trellis[i-1][j] trellis[i][j] = P(w(i)|tj) ⋅Max (trellis[i-1][k]P(tj |tk)) At the end of the sentence, we pick the highest scoring entry in the last column of the trellis !32 CS498JH: Introduction to NLP !33 w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT Retrieving t* = argmaxt P(t,w) By keeping one backpointer from each cell to the cell in the previous column that yields the highest probability, we can retrieve the most likely tag sequence when we’re done. CS447: Natural Language Processing (J. Hockenmaier) More about this on Friday… !34 CS447: Natural Language Processing (J. Hockenmaier) Appendix: English parts of speech !35 CS447: Natural Language Processing (J. Hockenmaier) Nouns Nouns describe entities and concepts: Common nouns: dog, bandwidth, dog, ﬁre, snow, information - Count nouns have a plural (dogs) and need an article in the singular (the dog barks) - Mass nouns don’t have a plural (*snows) and don’t need an article in the singular (snow is cold, metal is expensive). But some mass nouns can also be used as count nouns: Gold and silver are metals. Proper nouns (Names): Mary, Smith, Illinois, USA, France, IBM Penn Treebank tags: NN: singular or mass NNS: plural NNP: singular proper noun NNPS: plural proper noun !36 CS447: Natural Language Processing (J. Hockenmaier) (Full) verbs Verbs describe activities, processes, events: eat, write, sleep, …. Verbs have different morphological forms: inﬁnitive (to eat), present tense (I eat), 3rd pers sg. present tense (he eats), past tense (ate), present participle (eating), past participle (eaten) Penn Treebank tags: VB: inﬁnitive (base) form VBD: past tense VBG: present participle VBD: past tense VBN: past participle VBP: non-3rd person present tense VBZ: 3rd person singular present tense !37 CS447: Natural Language Processing (J. Hockenmaier) Adjectives Adjectives describe properties of entities: blue, hot, old, smelly,… Adjectives have an... … attributive use (modifying a noun): the blue book … and a predicative use (e.g. as argument of be): The book is blue. Many gradable adjectives also have a… ...comparative form: greater, hotter, better, worse ...superlative form: greatest, hottest, best, worst Penn Treebank tags: JJ: adjective JJR: comparative JJS: superlative !38 CS447: Natural Language Processing (J. Hockenmaier) Adverbs Adverbs describe properties of events/states. - Manner adverbs: slowly (slower, slowest) fast, hesitantly,… - Degree adverbs: extremely, very, highly…. - Directional and locative adverbs: here, downstairs, left - Temporal adverbs: yesterday, Monday,… Adverbs modify verbs, sentences, adjectives or other adverbs: Apparently, the very ill man walks extremely slowly NB: certain temporal and locative adverbs (yesterday, here) can also be classiﬁed as nouns Penn Treebank tags: RB: adverb RBR: comparative adverb RBS: superlative adverb !39 CS447: Natural Language Processing (J. Hockenmaier) Auxiliary and modal verbs Copula: be with a predicate She is a student. I am hungry. She was ﬁve years old. Modal verbs: can, may, must, might, shall,… She can swim. You must come Auxiliary verbs: -Be, have, will when used to form complex tenses: He was being followed. She has seen him. We will have been gone. -Do in questions, negation: Don’t go. Did you see him? Penn Treebank tags: MD: modal verbs !40 CS447: Natural Language Processing (J. Hockenmaier) Prepositions Prepositions occur before noun phrases to form a prepositional phrase (PP): on/in/under/near/towards the wall, with(out) milk, by the author, despite your protest PPs can modify nouns, verbs or sentences: I drink [coffee [with milk]] I [drink coffee [with my friends]] Penn Treebank tags: IN: preposition TO: ‘to’ (inﬁnitival ‘to eat’ and preposition ‘to you’) !41 CS447: Natural Language Processing (J. Hockenmaier) Conjunctions Coordinating conjunctions conjoin two elements: X and/or/but X [ [John]NP and [Mary]NP] NP, [ [Snow is cold]S but [ﬁre is hot]S ]S. Subordinating conjunctions introduce a subordinate (embedded) clause: [ He thinks that [snow is cold]S ]S [ She wonders whether [it is cold outside]S ]S Penn Treebank tags: CC: coordinating IN: subordinating (same as preposition) !42 CS447: Natural Language Processing (J. Hockenmaier) Particles Particles resemble prepositions (but are not followed by a noun phrase) and appear with verbs: come on he brushed himself off turning the paper over turning the paper down Phrasal verb: a verb + particle combination that has a different meaning from the verb itself Penn Treebank tags: RP: particle !43 CS447: Natural Language Processing (J. Hockenmaier) Pronouns Many pronouns function like noun phrases, and refer to some other entity: -Personal pronouns: I, you, he, she, it, we, they -Possessive pronouns: mine, yours, hers, ours -Demonstrative pronouns: this, that, -Reﬂexive pronouns: myself, himself, ourselves -Wh-pronouns (question words): what, who, whom, how, why, whoever, which Relative pronouns introduce relative clauses the book that [he wrote] Penn Treebank tags: PRP: personal pronoun PRP$ possessive WP: wh-pronoun !44 CS447: Natural Language Processing (J. Hockenmaier) Determiners Determiners precede noun phrases: the/that/a/every book -Articles: the, an, a -Demonstratives: this, these, that -Quantiﬁers: some, every, few,… Penn Treebank tags: DT: determiner !45 "
262,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 6: HMM algorithms CS447: Natural Language Processing (J. Hockenmaier) Recap: Statistical POS tagging she1 promised2 to3 back4 the5 bill6 w = w1 w2 w3 w4 w5 w6 t = t1 t2 t3 t4 t5 t6 PRP1 VBD2 TO3 VB4 DT5 NN6 What is the most likely sequence of tags t for the given sequence of words w ? ""2 CS447: Natural Language Processing (J. Hockenmaier) Statistical POS tagging with HMMs What is the most likely sequence of tags t for the given sequence of words w ? Hidden Markov Models deﬁne P(t) and P(w|t) as: Transition probabilities: P(t) = ∏i P(ti | ti−1) [bigram HMM] or P(t) = ∏i P(ti | ti−1, ti−2) [trigram HMM] Emission probabilities: P(w | t) = ∏i P(wi | ti) ""3 gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) maxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) CS447: Natural Language Processing (J. Hockenmaier) HMMs as probabilistic automata DT JJ NN 0.7 0.3 0.4 0.6 0.55 VBZ 0.45 0.5 the 0.2 a 0.1 every 0.1 some 0.1 no 0.01 able ... ... 0.003 zealous ... ... 0.002 zone 0.00024 abandonment 0.001 yields ... ... 0.02 acts A (bigram) HMM deﬁnes Transition probabilities: P( ti | ti-1) Emission probabilities: P( wi | ti ) ""4 CS447: Natural Language Processing (J. Hockenmaier) HMMs as probabilistic automata Transition probabilities P(ti | ti−1): Probability of going from one state (ti−1) of the automaton to the next (ti) “Markov model”: We’re making a Markov [independence] assumption for how to move between states of the automaton Emission probabilities P(wi | ti): Probability of emitting a symbol (wi) in a given state of the automaton (ti) “Hidden Markov model”: The data that we see (at test time) consists only of the words w, and we ﬁnd tags for w by searching for the most likely sequence of (hidden) states of the automaton (the tags t) that generated the data w ""5 CS498JH: Introduction to NLP An example HMM ""6 D N V A . D 0.8 0.2 N 0.7 0.3 V 0.6 0.4 A 0.8 0.2 . Transition Matrix A the man ball throws sees red blue . D 1 N 0.7 0.3 V 0.6 0.4 A 0.8 0.2 . 1 Emission Matrix B D N V A . π 1 Initial state vector π D N V A . CS447: Natural Language Processing (J. Hockenmaier) ! Using HMMs for tagging -The input to an HMM tagger is a sequence of words, w. The output is the most likely sequence of tags, t, for w. -For the underlying HMM model, w is a sequence of output symbols, and t is the most likely sequence of states (in the Markov chain) that generated w. ""7 argmax t P( t ⇤⇥#⌅ Outputtagger | w ⇤⇥#⌅ Inputtagger ) = argmax t P(w,t) P(w) = argmax t P(w,t) = argmax t P( w ⇤⇥#⌅ OutputHMM | t ⇤⇥#⌅ StatesHMM )P( t ⇤⇥#⌅ StatesHMM ) CS447: Natural Language Processing (J. Hockenmaier) How would the automaton for a trigram HMM with transition probabilities P(ti | ti-2ti-1) look like? What about unigrams or n-grams? ??? ??? ""8 CS447: Natural Language Processing (J. Hockenmaier) DT JJ NN VBZ q0 Encoding a trigram model as FSA JJ_DT NN_DT JJ NN VBZ DT <S> DT_<S> <S> JJ_JJ NN_JJ VBZ_NN NN_NN Bigram model: States = Tag Unigrams Trigram model: States = Tag Bigrams ""9 CS447: Natural Language Processing (J. Hockenmaier) Trigram HMMs In a trigram HMM tagger, each state qi corresponds to a POS tag bigram (the tags of the current and preceding word): qi=tjtk Emission probabilities depend only on the current POS tag: States tjtk and titk use the same emission probabilities P(wi | tk) ""10 CS447: Natural Language Processing (J. Hockenmaier) Building an HMM tagger To build an HMM tagger, we have to: -Train the model, i.e. estimate its parameters (the transition and emission probabilities) Easy case: we have a corpus labeled with POS tags (supervised learning) -Deﬁne and implement a tagging algorithm that ﬁnds the best tag sequence t* for each input sentence w: t* = argmaxt P(t)P(w | t) ""11 CS498JH: Introduction to NLP Learning an HMM Where do we get the transition probabilities P(tj | ti) (matrix A) and the emission probabilities P(wj | ti) (matrix B) from? Case 1: We have a POS-tagged corpus. - This is learning from labeled data, aka “supervised learning” Case 2: We have a raw (untagged) corpus and a tagset. - This is learning from unlabeled data, aka “unsupervised learning” ""12 Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB the_DT board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Tagset: NNP: proper noun CD: numeral, JJ: adjective,... CS498JH: Introduction to NLP We count how often we see titj and wj_ti etc. in the data (use relative frequency estimates): Learning the transition probabilities: Learning the emission probabilities: We might use some smoothing, but this is pretty trivial… Learning an HMM from labeled data ""13 P(tj|ti) = C(titj) C(ti) Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB the_DT board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. P(wj|ti) = C(wj ti) C(ti) CS447: Natural Language Processing (J. Hockenmaier) Learning an HMM from unlabeled data We can’t count anymore. We have to guess how often we’d expect to see titj etc. in our data set. Call this expected count〈C(...)〉 -Our estimate for the transition probabilities: -Our estimate for the emission probabilities: -We will talk about how to obtain these counts on Friday ""14 Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Tagset: NNP: proper noun CD: numeral, JJ: adjective,... ˆ P(tj|ti) = ⟨C(titj)⇥ ⟨C(ti)⇥ ˆ P(wj|ti) = ⟨C(wj ti)⇥ ⟨C(ti)⇥ CS447: Natural Language Processing (J. Hockenmaier) Finding the best tag sequence The number of possible tag sequences is exponential in the length of the input sentence: Each word can have up to T tags. There are N words. There are up to TN possible tag sequences. We cannot enumerate all TN possible tag sequences. But we can exploit the independence assumptions in the HMM to deﬁne an efﬁcient algorithm that returns the tag sequence with the highest probability ""15 CS447: Natural Language Processing (J. Hockenmaier) Dynamic Programming for HMMs ""16 CS498JH: Introduction to NLP The three basic problems for HMMs We observe an output sequence w=w1...wN: w=“she promised to back the bill” Problem I (Likelihood): ﬁnd P(w | λ ) Given an HMM λ = (A, B, π), compute the likelihood of the observed output, P(w | λ ) Problem II (Decoding): ﬁnd Q=q1..qT Given an HMM λ = (A, B, π), what is the most likely sequence of states Q=q1..qN ≈ t1...tN to generate w? Problem III (Estimation): ﬁnd argmax λ P(w | λ ) Find the parameters A, B, π which maximize P(w | λ) ""17 CS498JH: Introduction to NLP How can we solve these problems? I. Likelihood of the input w: Compute P(w | λ ) for the input w and HMM λ II. Decoding (= tagging) the input w: Find the best tags t*=argmaxt P(t | w,λ) for the input w and HMM λ III. Estimation (= learning the model): Find the best model parameters λ*=argmax λ P(t, w | λ) for the training data w These look like hard problems: With T tags, every input string w1...n has Tn possible tag sequences Can we ﬁnd efﬁcient (polynomial-time) algorithms? ""18 CS447: Natural Language Processing (J. Hockenmaier) Dynamic programming Dynamic programming is a general technique to solve certain complex search problems by memoization 1.) Recursively decompose the large search problem into smaller subproblems that can be solved efﬁciently –There is only a polynomial number of subproblems. 2.) Store (memoize) the solution of each subproblem in a common data structure –Processing this data structure takes polynomial time ""19 CS498JH: Introduction to NLP Dynamic programming algorithms for HMMs I. Likelihood of the input: Compute P(w| λ ) for an input sentence w and HMM λ ⇒ Forward algorithm II. Decoding (=tagging) the input: Find best tags t*=argmaxt P(t | w,λ) for an input sentence w and HMM λ ⇒ Viterbi algorithm III. Estimation (=learning the model): Find best model parameters λ*=argmax λ P(t, w | λ) for training data w ⇒ Forward-Backward algorithm ""20 CS447: Natural Language Processing (J. Hockenmaier) Tags Bookkeeping: the trellis We use a N×T table (“trellis”) to keep track of the HMM. The HMM can assign one of the T tags to each of the N words. w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) t1 ... tj ... tT Words (“time steps”) ""21 word w(i) has tag tj CS447: Natural Language Processing (J. Hockenmaier) One tag sequence = one path through trellis w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT ""22 One path through the trellis = one tag sequence CS447: Natural Language Processing (J. Hockenmaier) Computing P(t,w) for one tag sequence w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT P(w(1)|q1) P(w(2) | qj) P(w(i) | qi) P(t(1)=q1) P(qj | q1) P(qi | q...) P(q..| qi) P(w(i+1) | qi+1) P(w(N) | qj) P(qj | q..) ""23 One path through the trellis = one tag sequence To get its probability, we just multiply the initial state and all emission and transition probabilities CS447: Natural Language Processing (J. Hockenmaier) The Viterbi algorithm ""24 CS447: Natural Language Processing (J. Hockenmaier) Finding the best tag sequence The number of possible tag sequences is exponential in the length of the input sentence: Each word can have up to T tags. There are N words. There are up to TN possible tag sequences. We cannot enumerate all TN possible tag sequences. But we can exploit the independence assumptions in the HMM to deﬁne an efﬁcient algorithm that returns the tag sequence with the highest probability in linear (O(N)) time. ""25 CS447: Natural Language Processing (J. Hockenmaier) Notation: ti/wi vs t(i)/w(i) To make the distinction between the i-th word/tag in the vocabulary/tag set and the i-th word/tag in the sentence clear: use superscript notation w(i) for the i-th token in the sequence and subscript notation wi for the i-th type in the inventory (tagset/vocabulary): ""26 CS447: Natural Language Processing (J. Hockenmaier) HMM decoding We observe a sentence w = w(1)…w(N) w= “she promised to back the bill” We want to use an HMM tagger to ﬁnd its POS tags t t* = argmaxt P(w, t) = argmaxt P(t(1))·P(w(1)| t(1))·P(t(2)| t(1))·…·P(w(N)| t(N)) To do this efﬁciently, we will use dynamic programming to exploit the independence assumptions in the HMM. ""27 CS447: Natural Language Processing (J. Hockenmaier) The Viterbi algorithm A dynamic programming algorithm which ﬁnds the best (=most probable) tag sequence t* for an input sentence w: t* = argmaxt P(w | t)P(t) Complexity: linear in the sentence length. With a bigram HMM, Viterbi runs in O(T2N) steps for an input sentence with N words and a tag set of T tags. The independence assumptions of the HMM tell us how to break up the big search problem (ﬁnd t* = argmaxt P(w | t)P(t)) into smaller subproblems. The data structure used to store the solution of these subproblems is the trellis. ""28 CS447: Natural Language Processing (J. Hockenmaier) HMM independences 1. Emissions depend only on the current tag: … P(w(i) = man | t(i) = NN )… We only have to multiply the emission probability P(w(i) | tj ) with the probability of the best tag sequence that gets us to t(i) = tj ""29 CS447: Natural Language Processing (J. Hockenmaier) HMM independences 2. Transition probabilities to the current tag t(i) depend only on the previous tag t(i−1): … P( t(i) = NN | t(i−1) = DT ) -Assume the probability of the best tag sequence for the preﬁx w(1)…w(i−1) that ends in the tag t(i−1) = tj is known, and stored in a variable max[i−1][j]. -To compute the probability of the best tag sequence for w(1)…w(i-1)w(i) that ends in the tags t(i-1)t(i) = tjtk, multiply max[i−1][j] with P(tk | tj) and P(w(i) | tk) -To compute the probability of the best tag sequence for w(1)…w(i-1)w(i) that ends in t(i) = tk , consider all possible tags t(i-1) = tj for the preceding word: max[i][k] = maxj ( max[i−1][j] P(tk | tj) )P(w(i) | tk) ""30 CS447: Natural Language Processing (J. Hockenmaier) HMM independences 3. The current tag also determines the transition probability of the next tag: … P( t(i+1) = VBZ | t(i) = NN )… We cannot ﬁx the current tag t(i) based on the probability of getting to t(i) (and producing w(i)) We have to wait until we have reached the last word in the sequence. Then, we can trace back to get the best tag sequence for the entire sentence. ""31 CS447: Natural Language Processing (J. Hockenmaier) Using the trellis to ﬁnd t* Let trellis[i][j] (word w(j) and tag tj) store the probability of the best tag sequence for w(1)…w(i) that ends in tj trellis[i][j] = max P(w(1)…w(i), t(1)…, t(i) = tj ) We can recursively compute trellis[i][j] from the entries in the previous column trellis[i-1][j] trellis[i][j] = P(w(i)| tj) ⋅Maxk( trellis[i-1][k]P(tj | tk) ) At the end of the sentence, we pick the highest scoring entry in the last column of the trellis ""32 CS447: Natural Language Processing (J. Hockenmaier) At any given cell -For each cell in the preceding column: multiply its entry with the transition probability to the current cell. -Keep a single backpointer to the best (highest scoring) cell in the preceding column -Multiply this score with the emission probability of the current word ""33 w(n-1) w(n) t1 P(w(1..n-1), t(n-1)=t1) ... ... ti P(w(1..n-1), t(n-1)=ti) ... ... tN P(w(1..n-1), tn-1=ti) P(ti |t1) P(ti |ti) P(ti |tN) trellis[n][i] = P(w(n)|ti) ⋅Max(trellis[n-1][j]P(ti |ti)) CS447: Natural Language Processing (J. Hockenmaier) At the end of the sentence In the last column (i.e. at the end of the sentence) pick the cell with the highest entry, and trace back the backpointers to the ﬁrst word in the sentence. ""34 CS498JH: Introduction to NLP ""35 w(1) w(2) ... w(i-1) w(i) w(i+1) ... w(N-1) w(N) q1 ... qj ... qT Retrieving t* = argmaxt P(t,w) By keeping one backpointer from each cell to the cell in the previous column that yields the highest probability, we can retrieve the most likely tag sequence when we’re done. CS447: Natural Language Processing (J. Hockenmaier) The Viterbi algorithm Viterbi( w1…n){ for t (1...T) // INITIALIZATION: first column trellis[1][t].viterbi = p_init[t] × p_emit[t][w1] for i (2...n){ // RECURSION: every other column for t (1....T){ trellis[i][t] = 0 for t’ (1...T){ tmp = trellis[i-1][t’].viterbi × p_trans[t’][t] if (tmp > trellis[i][t].viterbi){ trellis[i][t].viterbi = tmp trellis[i][t].backpointer = t’}} trellis[i][t].viterbi ×= p_emit[t][wi]}} t_max = NULL, vit_max = 0; // FINISH: find the best cell in the last column for t (1...T) if (trellis[n][t].vit > vit_max){t_max = t; vit_max = trellis[n][t].value } return unpack(n, t_max); } ""36 CS447: Natural Language Processing (J. Hockenmaier) Unpacking the trellis unpack(n, t){ i = n; tags = new array[n+1]; while (i > 0){ tags[i] = t; t = trellis[i][t].backpointer; i--; } return tags; } ""37 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts HMM taggers Learning HMMs from labeled text Viterbi for HMMs Dynamic programming Independence assumptions in HMMs The trellis ""38 CS447: Natural Language Processing (J. Hockenmaier) Supplementary material: Viterbi for Trigram HMMs ""39 CS447: Natural Language Processing (J. Hockenmaier) Trigram HMMs In a Trigram HMM, transition probabilities are of the form: P(t(i) = ti | t(i−1) = tj, t(i−2) = tk ) The i-th tag in the sequence inﬂuences the probabilities of the (i+1)-th tag and the (i+2)-th tag: … P(t(i+1) | t(i), t(i−1)) … P(t(i+2) | t(i+1), t(i)) Hence, each row in the trellis for a trigram HMM has to correspond to a pair of tags — the current and the preceding tag: (abusing notation) trellis[i]⟨j,k⟩: word w(i) has tag tj, word w(i−1) has tag tk The trellis now has T2 rows. But we still need to consider only T transitions into each cell, since the current word’s tag is the next word’s preceding tag: Transitions are only possible from trellis[i]⟨j,k⟩ to trellis[i+1]⟨l,j⟩ ""40 "
263,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 7: Sequence Labeling CS447: Natural Language Processing (J. Hockenmaier) Recap: Statistical POS tagging with HMMs !2 CS447: Natural Language Processing (J. Hockenmaier) She promised to back the bill w = w(1) w(2) w(3) w(4) w(5) w(6) t = t(1) t(2) t(3) t(4) t(5) t(6) PRP VBD TO VB DT NN What is the most likely sequence of tags t= t(1)…t(N) for the given sequence of words w= w(1)…w(N) ? t* = argmaxt P(t | w) Recap: Statistical POS tagging !3 CS447: Natural Language Processing (J. Hockenmaier) POS tagging with generative models P(t,w): the joint distribution of the labels we want to predict (t) and the observed data (w). We decompose P(t,w) into P(t) and P(w | t) since these distributions are easier to estimate. Models based on joint distributions of labels and observed data are called generative models: think of P(t)P(w | t) as a stochastic process that ﬁrst generates the labels, and then generates the data we see, based on these labels. !4 gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) s’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) CS447: Natural Language Processing (J. Hockenmaier) Hidden Markov Models (HMMs) HMMs are generative models for POS tagging (and other tasks, e.g. in speech recognition) Independence assumptions of HMMs P(t) is an n-gram model over tags: Bigram HMM: P(t) = P(t(1))P(t(2) | t(1))P(t(3) | t(2))… P(t(N) | t(N-1)) Trigram HMM: P(t) = P(t(1))P(t(2)| t(1))P(t(3)| t(2),t(1))…P(t(n) | t(N-1),t(N-2)) P(ti | tj) or P(ti | tj,tk) are called transition probabilities In P(w | t) each word is generated by its own tag: P(w | t) = P(w(1) | t(1))P(w(2) | t(2))… P(w(N) | t(N)) P(w | t) are called emission probabilities !5 CS447: Natural Language Processing (J. Hockenmaier) Viterbi algorithm Task: Given an HMM, return most likely tag sequence t(1)…t(N) for a given word sequence (sentence) w(1)…w(N) Data structure (Trellis): N×T table for sentence w(1)…w(N) and tag set {t1,…tT}. Cell trellis[i][j] stores score of best tag sequence for w(1) …w(j) that ends in tag tj and a backpointer to the cell corresponding to the tag of the preceding word trellis[i−1][k] Basic procedure: Fill trellis from left to right Initalize trellis[1][k] := P(tk) × P(w(1) | tk) For trellis[i][j]: - Find best preceding tag k* = argmaxk(trellis[i−1][k] × P(tj | tk)), - Add backpointer from trellis[i][j] to trellis[i−1][k*]; - Set trellis[i][j] := trellis[i−1][k*] × P(tj | tk*) × P(w(i) | tj) Return tag sequence that ends in the highest scoring cell argmaxk(trellis[N][k]) in the last column !6 CS447: Natural Language Processing (J. Hockenmaier) Viterbi: At any given cell -For each cell in the preceding column: multiply its entry with the transition probability to the current cell. -Keep a single backpointer to the best (highest scoring) cell in the preceding column -Multiply this score with the emission probability of the current word !7 w(n-1) w(n) t1 P(w(1..n-1), t(n-1)=t1) ... ... ti P(w(1..n-1), t(n-1)=ti) ... ... tN P(w(1..n-1), tn-1=ti) P(ti |t1) P(ti |ti) P(ti |tN) trellis[n][i] = P(w(n)|ti) ⋅Max(trellis[n-1][j]P(ti |ti)) CS447: Natural Language Processing (J. Hockenmaier) Other HMM algorithms The Forward algorithm: Computes P(w) by replacing Viterbi’s max() with sum() Learning HMMs from raw text with the EM algorithm: -We have to replace the observed counts (from labeled data) with expected counts (according to the current model) -Renormalizing these expected counts will give a new model -This will be “better” than the previous model, but we will have to repeat this multiple times to get to decent model The Forward-Backward algorithm: A dynamic programming algorithm for computing the expected counts of tag bigrams and word-tag occurrences in a sentence under a given HMM !8 CS447: Natural Language Processing (J. Hockenmaier) Sequence labeling !9 CS447: Natural Language Processing POS tagging !10 Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 . Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB IBM_NNP ‘s_POS board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. Task: assign POS tags to words CS447: Natural Language Processing Noun phrase (NP) chunking !11 Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 . [NP Pierre Vinken] , [NP 61 years] old , will join [NP IBM] ‘s [NP board] as [NP a nonexecutive director] [NP Nov. 2] . Task: identify all non-recursive NP chunks CS447: Natural Language Processing The BIO encoding We deﬁne three new tags: – B-NP: beginning of a noun phrase chunk – I-NP: inside of a noun phrase chunk – O: outside of a noun phrase chunk !12 [NP Pierre Vinken] , [NP 61 years] old , will join [NP IBM] ‘s [NP board] as [NP a nonexecutive director] [NP Nov. 2] . Pierre_B-NP Vinken_I-NP ,_O 61_B-NP years_I-NP old_O ,_O will_O join_O IBM_B-NP ‘s_O board_B-NP as_O a_B-NP nonexecutive_I-NP director_I-NP Nov._B-NP 29_I-NP ._O CS447: Natural Language Processing Shallow parsing !13 Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 . [NP Pierre Vinken] , [NP 61 years] old , [VP will join] [NP IBM] ‘s [NP board] [PP as] [NP a nonexecutive director] [NP Nov. 2] . Task: identify all non-recursive NP, verb (“VP”) and preposition (“PP”) chunks CS447: Natural Language Processing The BIO encoding for shallow parsing We deﬁne several new tags: – B-NP B-VP B-PP: beginning of an NP, “VP”, “PP” chunk – I-NP I-VP I-PP: inside of an NP, “VP”, “PP” chunk – O: outside of any chunk !14 Pierre_B-NP Vinken_I-NP ,_O 61_B-NP years_I-NP old_O ,_O will_B-VP join_I-VP IBM_B-NP ‘s_O board_B-NP as_B-PP a_B-NP nonexecutive_I-NP director_I-NP Nov._B- NP 29_I-NP ._O [NP Pierre Vinken] , [NP 61 years] old , [VP will join] [NP IBM] ‘s [NP board] [PP as] [NP a nonexecutive director] [NP Nov. 2] . CS447: Natural Language Processing Named Entity Recognition !15 Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 . [PERS Pierre Vinken] , 61 years old , will join [ORG IBM] ‘s board as a nonexecutive director [DATE Nov. 2] . Task: identify all mentions of named entities (people, organizations, locations, dates) CS447: Natural Language Processing The BIO encoding for NER We deﬁne many new tags: – B-PERS, B-DATE, …: beginning of a mention of a person/date... – I-PERS, I-DATE, …: inside of a mention of a person/date... – O: outside of any mention of a named entity !16 Pierre_B-PERS Vinken_I-PERS ,_O 61_O years_O old_O ,_O will_O join_O IBM_B-ORG ‘s_O board_O as_O a_O nonexecutive_O director_O Nov._B-DATE 29_I-DATE ._O [PERS Pierre Vinken] , 61 years old , will join [ORG IBM] ‘s board as a nonexecutive director [DATE Nov. 2] . CS447: Natural Language Processing Many NLP tasks are sequence labeling tasks Input: a sequence of tokens/words: Pierre Vinken , 61 years old , will join IBM ‘s board as a nonexecutive director Nov. 29 . Output: a sequence of labeled tokens/words: POS-tagging: Pierre_NNP Vinken_NNP ,_, 61_CD years_NNS old_JJ ,_, will_MD join_VB IBM_NNP ‘s_POS board_NN as_IN a_DT nonexecutive_JJ director_NN Nov._NNP 29_CD ._. Named Entity Recognition: Pierre_B-PERS Vinken_I-PERS ,_O 61_O years_O old_O ,_O will_O join_O IBM_B-ORG ‘s_O board_O as_O a_O nonexecutive_O director_O Nov._B-DATE 29_I-DATE ._O !17 CS447 Natural Language Processing Graphical models for sequence labeling !18 CS447: Natural Language Processing Directed graphical models Graphical models are a notation for probability models. In a directed graphical model, each node represents a distribution over a random variable: – P(X) = Arrows represent dependencies (they deﬁne what other random variables the current node is conditioned on) – P(Y) P(X | Y ) = – P(Y) P(Z) P(X | Y, Z) = Shaded nodes represent observed variables. White nodes represent hidden variables – P(Y) P(X | Y) with Y hidden and X observed = !19 X X Y X Y Z X Y CS447: Natural Language Processing HMMs as graphical models HMMs are generative models of the observed input string w They ‘generate’ w with P(w,t) = ∏iP(t(i)| t(i−1))P(w(i)| t(i) ) When we use an HMM to tag, we observe w, and need to ﬁnd t t(1) t(2) t(3) t(4) w(1) w(2) w(3) w(4) CS447: Natural Language Processing Models for sequence labeling Sequence labeling: Given an input sequence w = w(1)…w(n), predict the best (most likely) label sequence t = t(1)…t(n) Generative models use Bayes Rule: Discriminative (conditional) models model P(t |w) directly !21 gmaxt P(t|w) directly (in a conditional model) es’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) es’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) es’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) gmaxt P(t|w) directly (in a conditional model) es’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) Estimate argmaxt P(t|w) directly (in a conditional model) or use Bayes’ Rule (and a generative model): argmax t P(t|w) = argmax t P(t,w) P(w) = argmax t P(t,w) = argmax t P(t)P(w|t) CS447: Natural Language Processing Advantages of discriminative models We’re usually not really interested in P(w | t). – w is given. We don’t need to predict it! Why not model what we’re actually interested in: P(t | w) Modeling P(w | t) well is quite difﬁcult: – Preﬁxes (capital letters) or sufﬁxes are good predictors for certain classes of t (proper nouns, adverbs,…) – Se we don’t want to model words as atomic symbols, but in terms of features – These features may also help us deal with unknown words – But features may not be independent Modeling P(t | w) with features should be easier: – Now we can incorporate arbitrary features of the word, because we don’t need to predict w anymore !22 CS447: Natural Language Processing Discriminative probability models A discriminative or conditional model of the labels t given the observed input string w models P(t | w) = ∏iP(t(i) |w(i), t(i−1)) directly. t(1) t(2) t(3) t(4) w(1) w(2) w(3) w(4) CS447: Natural Language Processing Discriminative models There are two main types of discriminative probability models: –Maximum Entropy Markov Models (MEMMs) –Conditional Random Fields (CRFs) MEMMs and CRFs: –are both based on logistic regression –have the same graphical model –require the Viterbi algorithm for tagging –differ in that MEMMs consist of independently learned distributions, while CRFs are trained to maximize the probability of the entire sequence CS447: Natural Language Processing Probabilistic classiﬁcation Classiﬁcation: Predict a class (label) c for an input x There are only a (small) ﬁnite number of possible class labels Probabilistic classiﬁcation: – Model the probability P( c | x) P(c|x) is a probability if 0 ≤ P (ci | x) ≤ 1, and ∑iP( ci | x) = 1 –Return the class c* = argmaxi P (ci | x) that has the highest probability One standard way to model P( c | x) is logistic regression (used by MEMMs and CRFs) !25 CS447: Natural Language Processing Using features Think of feature functions as useful questions you can ask about the input x: – Binary feature functions: ffirst-letter-capitalized(Urbana) = 1 ffirst-letter-capitalized(computer) = 0 – Integer (or real-valued) features: fnumber-of-vowels(Urbana) = 3 Which speciﬁc feature functions are useful will depend on your task (and your training data). !26 CS447: Natural Language Processing From features to probabilities We associate a real-valued weight wic with each feature function fi(x) and output class c Note that the feature function fi(x) does not have to depend on c as long as the weight does (note the double index wic) This gives us a real-valued score for predicting class c for input x: score(x,c) = ∑iwic fi(x) This score could be negative, so we exponentiate it: score(x,c) = exp( ∑iwic fi(x)) To get a probability distribution over all classes c, we renormalize these scores: P(c | x) = score(x,c)∕∑j score(x,cj) = exp( ∑iwic fi(x))∕∑j exp( ∑iwij fi(x)) !27 CS447: Natural Language Processing Learning = ﬁnding weights w We use conditional maximum likelihood estimation (and standard convex optimization algorithms) to ﬁnd/learn w (for more details, attend CS446 and CS546) The conditional MLE training objective: Find the w that assigns highest probability to all observed outputs ci given the inputs xi Learning: ﬁnding w ˆ w = argmax w ∏ i P(ci|xi,w) = argmax w ⇥ i log(P(ci|xi,w)) = argmax⇥log ! e⇥j w j f j(xi,c) ( ′) ⇥ !28 CS447: Natural Language Processing (J. Hockenmaier) Terminology Models that are of the form P(c | x) = score(x,c)∕∑j score(x,cj) = exp( ∑iwic fi(x))∕∑j exp( ∑iwij fi(x)) are also called loglinear models, Maximum Entropy (MaxEnt) models, or multinomial logistic regression models. CS446 and CS546 should give you more details about these. The normalizing term ∑j exp( ∑iwij fi(x)) is also called the partition function and is often abbreviated as Z !29 CS447: Natural Language Processing MEMMs use a MaxEnt classiﬁer for each P(t(i) |w(i), t(i−1)): Since we use w to refer to words, let’s use λjk as the weight for the feature function fj(t(i−1), w(i)) when predicting tag tk: Maximum Entropy Markov Models t(i−1) t(i) w(i) P(t(i) = tk | t(i−1),w(i)) = exp(Âj ljk f j(t(i−1),w(i)) Âl exp(Âj ljl f j(t(i−1),w(i)) CS447: Natural Language Processing (J. Hockenmaier) Viterbi for MEMMs trellis[n][i] stores the probability of the most likely (Viterbi) tag sequence t(1)…(n) that ends in tag ti for the preﬁx w(1)…w(n) Remember that we do not generate w in MEMMs. So: trellis[n][i] = maxt(1)..(n−1)[ P(t(1)…(n−1), t(n)=ti | w(1)…(n)) ] = maxj [ trellis[n−1][j] × P(ti | tj, w(n)) ] = maxj [maxt(1)..(n−2)[P(t(1)..(n−2), t(n−1)=tj | w(1)..(n−1))] ×P(ti | tj,w(n))] !31 w(n−1) w(n) t1 maxt(1)..(n−2) P(t(1)..(n−2), t(n−1)=t1 | w(1)..(n−1)) ... ... ti maxt(1)..(n−2) P(t(1)..(n−2), t(n−1)=ti | w(1)..(n−1)) ... ... tT maxt(1)..(n−2) P(t(1)..(n−2), t(n−1)=tT | w(1)..(n−1)) P(ti |t1,w(n)) P(ti |ti,w(n)) P(ti |tT,w(n)) trellis[n][i] = maxj[trellis[n−1][j] ×P(ti |tj, w(n))] CS447: Natural Language Processing Today’s key concepts Sequence labeling tasks: POS tagging NP chunking Shallow Parsing Named Entity Recognition Discriminative models: Maximum Entropy classiﬁers MEMMs !32 CS447: Natural Language Processing (J. Hockenmaier) Supplementary material: Other HMM algorithms (very brieﬂy…) !33 CS447: Natural Language Processing (J. Hockenmaier) The Forward algorithm trellis[n][i] stores the probability mass of all tag sequences t(1)…(n) that end in tag ti for the preﬁx w(1)…w(n) trellis[n][i] = ∑t(1)..(n−1)[ P(w(1)…(n), t(1)…(n−1), t(n)=ti) ] = ∑j [ trellis[n−1][j] × P(ti | tj) ] × P( w(n) | ti) = ∑j [∑t(1)..(n−2)[P(w(1)..(n−1),t(1)..(n−2),t(n−1)=tj)] ×P(ti | tj)] × P(w(n) | ti) Last step: computing P(w): P(w(1)…(N)) = ∑j trellis[N][j] !34 w(n−1) w(n) t1 ∑t(1)..(n−2) P(w(1)..(n−1), t(1)..(n−2), t(n−1)=t1) ... ... ti ∑t(1)..(n−2) P(w(1)..(n−1), t(1)..(n−2), t(n−1)=ti) ... ... tT ∑t(1)..(n−2) P(w(1)..(n−1), t(1)..(n−2), t(n−1)=tT) P(ti |t1) P(ti |ti) P(ti |tT) trellis[n][i] = P(w(n)|ti)× ∑j[trellis[n−1][j]×P(ti |tj)] CS447: Natural Language Processing (J. Hockenmaier) Learning an HMM from unlabeled text We can’t count anymore. We have to guess how often we’d expect to see titj etc. in our data set. Call this expected count 〈C(…)〉 -Our estimate for the transition probabilities: -Our estimate for the emission probabilities: -Our estimate for the initial state probabilities: !35 Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 . Tagset: NNP: proper noun CD: numeral, JJ: adjective,... ˆ P(tj|ti) = ⟨C(titj)⇥ ⟨C(ti)⇥ ˆ P(wj|ti) = ⟨C(wj ti)⇥ ⟨C(ti)⇥ p(ti) = hC(Tag of ﬁrst word = ti)i Number of sentences CS447: Natural Language Processing (J. Hockenmaier) Expected counts Emission probabilities with observed counts C(w, t) P(w | t) = C(w, t)∕C(t) = C(w, t)∕∑w’ C(w’, t) Emission probabilities with expected counts〈C(w, t)〉 P(w | t) = 〈C(w, t)〉∕〈C(t)〉 = 〈C(w, t)〉∕∑w’ 〈C(w’, t)〉 〈C(w, t)〉: How often do we expect to see word w with tag t in our training data (under a given HMM)? We know how often the word w appears in the data, but we don’t know how often it appears with tag t We need to sum up 〈C(w(i)=w, t)〉 for any occurrence of w We can show that 〈C(w(i)=w, t)〉 = P(t(i)=t | w) (NB: Transition counts 〈C(t(i)=t, t(i+1)=t’)〉 work in a similar fashion) !36 CS447: Natural Language Processing (J. Hockenmaier) Forward-Backward: P(t(i)=t | w(1)..(N)) P( t(i)=t | w(1)…(N)) = P( t(i)=t, w(1)…(N)) ∕ P(w(1)…(N)) w(1)…(N) = w(1)…(i)w(i+1)…(N) Due to HMM’s independence assumptions: P( t(i)=t, w(1)…(N)) = P(t(i)=t, w(1)…(i)) × P(w(i+1)...(N) | t(i) =t) The forward algorithm gives P(w(1)…(N)) = ∑t forward[N][t] Forward trellis: forward[i][t] = P(t(i)=t, w(1)…(i)) Gives the total probability mass of the preﬁx w(1)…(i), summed over all tag sequences t(1)…(i) that end in tag t(i)=t Backward trellis: backward[i][t] = P(w(i+1)...(N) | t(i)=t) Gives the total probability mass of the sufﬁx w(i+1)…(N), summed over all tag sequences t(i+1)…(N), if we assign tag t(i)=t to w(i) !37 CS447 Natural Language Processing The backward trellis is ﬁlled from right to left. backward[i][t] provides P(w(i+1)...(N) | t(i) =t) NB: ∑tbackward[1][t] = P(w(i+1)…(N)) = ∑tforward[N][t] Initialization (last column): backward[N][t] = 1 Recursion (any other column): backward[i][t] = ∑t’ P(t’ |t)×P(w(i+1) |t’)×backward[i+1][t’] The Backward algorithm !38 w(1) ... w(i-1) w (i) w (i+1) ... w (N) t1 ... t ... tT CS447 Natural Language Processing How do we compute 〈C(ti) |wj〉 !39 w(1) ... w(i-1) w (i) w (i+1) ... w (N) t1 ... t ... tT 〈C(t,w(i)) | w 〉 = P(t(i) =t, w)/P(w) with P(t(i) =t, w) = forward[i][t] backward[i][t] P(w) = ∑t forward[N][t] CS447: Natural Language Processing (J. Hockenmaier) The importance of tag dictionaries Forward-Backward assumes that each tag can be assigned to any word. No guarantee that the learned HMM bears any resemblance to the tags we want to get out of a POS tagger. A tag dictionary lists the possible POS tags for words. Even a partial dictionary that lists only the tags for the most common words and contains at least a few words for each tag provides enough constraints to get signiﬁcantly closer to a model that produces linguistically correct (and hence useful) POS tags. !40 a DT back JJ, NN, VB, VBP, RP an DT bank NN, VB, VBP and CC … … America NNP zebra NN "
264,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 8: Formal Grammars of English CS447: Natural Language Processing (J. Hockenmaier) Recap: Wednesday’s lecture !2 CS447 Natural Language Processing Graphical models for sequence labeling !3 CS447: Natural Language Processing Directed graphical models Graphical models are a notation for probability models. In a directed graphical model, each node represents a distribution over a random variable: – P(X) = Arrows represent dependencies (they deﬁne what other random variables the current node is conditioned on) – P(Y) P(X | Y ) = – P(Y) P(Z) P(X | Y, Z) = Shaded nodes represent observed variables. White nodes represent hidden variables – P(Y) P(X | Y) with Y hidden and X observed = !4 X X Y X Y Z X Y CS447: Natural Language Processing HMMs as graphical models HMMs are generative models of the observed input string w They ‘generate’ w with P(w,t) = ∏iP(t(i)| t(i−1))P(w(i)| t(i) ) When we use an HMM to tag, we observe w, and need to ﬁnd t t(1) t(2) t(3) t(4) w(1) w(2) w(3) w(4) CS447: Natural Language Processing Discriminative probability models A discriminative or conditional model of the labels t given the observed input string w models P(t | w) = ∏iP(t(i) |w(i), t(i−1)) directly. t(1) t(2) t(3) t(4) w(1) w(2) w(3) w(4) CS447: Natural Language Processing Discriminative models There are two main types of discriminative probability models: –Maximum Entropy Markov Models (MEMMs) –Conditional Random Fields (CRFs) MEMMs and CRFs: –are both based on logistic regression –have the same graphical model –require the Viterbi algorithm for tagging –differ in that MEMMs consist of independently learned distributions, while CRFs are trained to maximize the probability of the entire sequence CS447: Natural Language Processing Probabilistic classiﬁcation Classiﬁcation: Predict a class (label) c for an input x There are only a (small) ﬁnite number of possible class labels Probabilistic classiﬁcation: – Model the probability P( c | x) P(c|x) is a probability if 0 ≤ P (ci | x) ≤ 1, and ∑iP( ci | x) = 1 –Return the class c* = argmaxi P (ci | x) that has the highest probability One standard way to model P( c | x) is logistic regression (used by MEMMs and CRFs) !8 CS447: Natural Language Processing MEMMs use a MaxEnt classiﬁer for each P(t(i) |w(i), t(i−1)): Since we use w to refer to words, let’s use λjk as the weight for the feature function fj(t(i−1), w(i)) when predicting tag tk: Maximum Entropy Markov Models t(i−1) t(i) w(i) P(t(i) = tk | t(i−1),w(i)) = exp(Âj ljk f j(t(i−1),w(i)) Âl exp(Âj ljl f j(t(i−1),w(i)) CS447: Natural Language Processing Using features Think of feature functions as useful questions you can ask about the input x: – Binary feature functions: ffirst-letter-capitalized(Urbana) = 1 ffirst-letter-capitalized(computer) = 0 – Integer (or real-valued) features: fnumber-of-vowels(Urbana) = 3 Which speciﬁc feature functions are useful will depend on your task (and your training data). !10 CS447: Natural Language Processing From features to probabilities We associate a real-valued weight wic with each feature function fi(x) and output class c Note that the feature function fi(x) does not have to depend on c as long as the weight does (note the double index wic) This gives us a real-valued score for predicting class c for input x: score(x,c) = ∑iwic fi(x) This score could be negative, so we exponentiate it: score(x,c) = exp( ∑iwic fi(x)) To get a probability distribution over all classes c, we renormalize these scores: P(c | x) = score(x,c)∕∑j score(x,cj) = exp( ∑iwic fi(x))∕∑j exp( ∑iwij fi(x)) !11 CS447: Natural Language Processing Learning = ﬁnding weights w We use conditional maximum likelihood estimation (and standard convex optimization algorithms) to ﬁnd/learn w (for more details, attend CS446 and CS546) The conditional MLE training objective: Find the w that assigns highest probability to all observed outputs ci given the inputs xi Learning: ﬁnding w ˆ w = argmax w ∏ i P(ci|xi,w) = argmax w ⇥ i log(P(ci|xi,w)) = argmax⇥log ! e⇥j w j f j(xi,c) ( ′) ⇥ !12 CS447: Natural Language Processing (J. Hockenmaier) Terminology Models that are of the form P(c | x) = score(x,c)∕∑j score(x,cj) = exp( ∑iwic fi(x))∕∑j exp( ∑iwij fi(x)) are also called loglinear models, Maximum Entropy (MaxEnt) models, or multinomial logistic regression models. CS446 and CS546 should give you more details about these. The normalizing term ∑j exp( ∑iwij fi(x)) is also called the partition function and is often abbreviated as Z !13 CS447: Natural Language Processing (J. Hockenmaier) Features for Sequence Labeling What features are useful to model P(t(i) |w(i), t(i−1)) ? The identity of the previous label Properties of the current word: -w(i) starts with/contains a capital letter/number,… -w(i) contains the character “A” (“B”, …”Z”, …1, 2, …0,….) -w(i) ends in “ing”, “ed”, …. -… Feature engineering is essential for any practical We typically deﬁne feature templates (e.g. let any of the ﬁrst, or last, n (=1,2,3,…) characters be used as a separate feature. This results in a very large number of actual features (and weights to be learned) Methods for feature selection become essential !14 CS447: Natural Language Processing (J. Hockenmaier) Feature Engineering Feature engineering (ﬁnding useful features) is essential to get good performance out of any classiﬁer This requires domain expertise We typically deﬁne feature templates (e.g. let any of the ﬁrst, or last, n (=1,2,3,…) characters be used as a separate feature. This results in a very large number of actual features (and weights to be learned) Methods for feature selection become essential. !15 CS447: Natural Language Processing (J. Hockenmaier) On to new material.. !16 CS447: Natural Language Processing (J. Hockenmaier) Previous key concepts NLP tasks dealing with words... -POS-tagging, morphological analysis … require ﬁnite-state representations, -Finite-State Automata and Finite-State Transducers … the corresponding probabilistic models, -Probabilistic FSAs and Hidden Markov Models -Estimation: relative frequency estimation, EM algorithm … and appropriate search algorithms -Dynamic programming: Forward, Viterbi, Forward-Backward !17 CS447: Natural Language Processing (J. Hockenmaier) The next key concepts NLP tasks dealing with sentences... -Syntactic parsing and semantic analysis … require (at least) context-free representations, -Context-free grammars, uniﬁcation grammars … the corresponding probabilistic models, -Probabilistic Context-Free Grammars, Loglinear models -Estimation: Relative Frequency estimation, EM algorithm, etc. … and appropriate search algorithms -Dynamic programming: chart parsing, inside-outside algorithm !18 CS447: Natural Language Processing (J. Hockenmaier) Search Algorithm (e.g Viterbi) Dealing with ambiguity Structural Representation (e.g FSA) Scoring Function (Probability model, e.g HMM) !19 CS447: Natural Language Processing (J. Hockenmaier) Today’s lecture Introduction to natural language syntax (‘grammar’): Constituency and dependencies Context-free Grammars Dependency Grammars A simple CFG for English !20 CS447: Natural Language Processing (J. Hockenmaier) What is grammar? !21 No, not really, not in this class CS447: Natural Language Processing (J. Hockenmaier) What is grammar? Grammar formalisms (= linguists’ programming languages) A precise way to deﬁne and describe the structure of sentences. (N.B.: There are many different formalisms out there, which each deﬁne their own data structures and operations) Speciﬁc grammars (= linguists’ programs) Implementations (in a particular formalism) for a particular language (English, Chinese,....) !22 CS447: Natural Language Processing (J. Hockenmaier) Can we deﬁne a program that generates all English sentences? The number of sentences is inﬁnite. But we need our program to be ﬁnite. !23 CS447: Natural Language Processing (J. Hockenmaier) Overgeneration Undergeneration John saw Mary. I ate sushi with tuna. I ate the cake that John had made for me yesterday I want you to go there. John made some cake. English Did you go there? ..... John Mary saw. with tuna sushi ate I. Did you went there? .... !24 CS447: Natural Language Processing (J. Hockenmaier) Noun (Subject) Verb (Head) Noun (Object) I eat sushi. Basic sentence structure !25 CS447: Natural Language Processing (J. Hockenmaier) This is a dependency graph: I eat sushi. sbj obj eat sushi I sbj obj !26 CS447: Natural Language Processing (J. Hockenmaier) A ﬁnite-state-automaton (FSA) Noun (Subject) Noun (Object) Verb (Head) !27 CS447: Natural Language Processing (J. Hockenmaier) A Hidden Markov Model (HMM) Noun (Subject) Noun (Object) Verb (Head) I, you, .... eat, drink sushi, ... !28 CS447: Natural Language Processing (J. Hockenmaier) Words take arguments I eat sushi. ✔ I eat sushi you. ??? I sleep sushi ??? I give sushi ??? I drink sushi ? Subcategorization (purely syntactic: what set of arguments do words take?) Intransitive verbs (sleep) take only a subject. Transitive verbs (eat) take also one (direct) object. Ditransitive verbs (give) take also one (indirect) object. Selectional preferences (semantic: what types of arguments do words tend to take) The object of eat should be edible. !29 CS447: Natural Language Processing (J. Hockenmaier) A better FSA Noun (Subject) Noun (Object) Transitive Verb (Head) Intransitive Verb (Head) !30 CS447: Natural Language Processing (J. Hockenmaier) Language is recursive the ball the big ball the big, red ball the big, red, heavy ball .... Adjectives can modify nouns. The number of modiﬁers (aka adjuncts) a word can have is (in theory) unlimited. !31 CS447: Natural Language Processing (J. Hockenmaier) Another FSA Determiner Noun Adjective !32 CS447: Natural Language Processing (J. Hockenmaier) Recursion can be more complex the ball the ball in the garden the ball in the garden behind the house the ball in the garden behind the house next to the school .... !33 CS447: Natural Language Processing (J. Hockenmaier) Yet another FSA Det Noun Adj Preposition So, why do we need anything beyond regular (ﬁnite-state) grammars? !34 CS447: Natural Language Processing (J. Hockenmaier) What does this mean? the ball in the garden behind the house !35 There is an attachment ambiguity CS447: Natural Language Processing (J. Hockenmaier) FSAs do not generate hierarchical structure !36 Det Noun Adj Preposition CS447: Natural Language Processing (J. Hockenmaier) [ ] [ ] [ ] I eat sushi with tuna What is the structure of a sentence? Sentence structure is hierarchical: A sentence consists of words (I, eat, sushi, with, tuna) ..which form phrases or constituents: “sushi with tuna” Sentence structure deﬁnes dependencies between words or phrases: !37 [ ] CS447: Natural Language Processing (J. Hockenmaier) Strong vs. weak generative capacity Formal language theory: -deﬁnes language as string sets -is only concerned with generating these strings (weak generative capacity) Formal/Theoretical syntax (in linguistics): -deﬁnes language as sets of strings with (hidden) structure -is also concerned with generating the right structures (strong generative capacity) !38 CS447: Natural Language Processing (J. Hockenmaier) Context-free grammars (CFGs) capture recursion Language has complex constituents (“the garden behind the house”) Syntactically, these constituents behave just like simple ones. (“behind the house” can always be omitted) CFGs deﬁne nonterminal categories to capture equivalent constituents. !39 CS447: Natural Language Processing (J. Hockenmaier) Context-free grammars A CFG is a 4-tuple 〈N, Σ, R, S〉 consisting of: A set of nonterminals N (e.g. N = {S, NP, VP, PP, Noun, Verb, ....}) A set of terminals Σ (e.g. Σ = {I, you, he, eat, drink, sushi, ball, }) A set of rules R R ⊆ {A → β with left-hand-side (LHS) A ∈ N and right-hand-side (RHS) β ∈ (N ∪ Σ)* } A start symbol S ∈ N !40 CS447: Natural Language Processing (J. Hockenmaier) An example DT → {the, a} N → {ball, garden, house, sushi } P → {in, behind, with} NP → DT N NP → NP PP PP → P NP N: noun P: preposition NP: “noun phrase” PP: “prepositional phrase” !41 CS447: Natural Language Processing (J. Hockenmaier) CFGs deﬁne parse trees Correct analysis Incorrect analys eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat eat sus VP N → {sushi, tuna} P → {with} V → {eat} NP → N NP → NP PP PP → P NP VP → V NP !42 CS447: Natural Language Processing (J. Hockenmaier) CFGs and center embedding The mouse ate the corn. The mouse that the snake ate ate the corn. The mouse that the snake that the hawk ate ate ate the corn. .... !43 CS447: Natural Language Processing (J. Hockenmaier) CFGs and center embedding Formally, these sentences are all grammatical, because they can be generated by the CFG that is required for the ﬁrst sentence: S → NP VP NP → NP RelClause RelClause → that NP ate Problem: CFGs are not able to capture bounded recursion. (‘only embed one or two relative clauses’). To deal with this discrepancy between what the model predicts to be grammatical, and what humans consider grammatical, linguists distinguish between a speaker’s competence (grammatical knowledge) and performance (processing and memory limitations) !44 CS447: Natural Language Processing (J. Hockenmaier) CFGs are equivalent to Pushdown automata (PDAs) PDAs are FSAs with an additional stack: Emit a symbol and push/pop a symbol from the stack This is equivalent to the following CFG: S → a X b S → a b X → a X b X → a b Push ‘x’ on stack. Emit ‘a’ !45 Pop ‘x’ from stack. Emit ‘b’ Accept if stack empty. CS447: Natural Language Processing (J. Hockenmaier) Action Stack String 1. Push x on stack. Emit a. x a 2. Push x on stack. Emit a. xx aa 3. Push x on stack. Emit a. xxx aaa 4. Push x on stack. Emit a. xxxx aaaa 5. Pop x off stack. Emit b. xxx aaaab 6. Pop x off stack. Emit b. xx aaaabb 7. Pop x off stack. Emit b. x aaaabbb 8. Pop x off stack. Emit b aaaabbbb Generating anbn !46 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning grammars for natural language !47 CS447: Natural Language Processing (J. Hockenmaier) Two ways to represent structure Correct analysis Incorrect analysis eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks VP Phrase structure trees Dependency trees !48 CS447: Natural Language Processing (J. Hockenmaier) Structure (syntax) corresponds to meaning (semantics) Correct analysis Incorrect analysis eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks eat sushi with chopsticks NP NP NP VP PP V P eat with tuna sushi NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks !49 CS447: Natural Language Processing (J. Hockenmaier) Dependency grammar DGs describe the structure of sentences as a directed acyclic graph. The nodes of the graph are the words The edges of the graph are the dependencies. Typically, the graph is assumed to be a tree. Note: the relationship between DG and CFGs: If a CFG phrase structure tree is translated into DG, the resulting dependency graph has no crossing edges. !50 CS447: Natural Language Processing (J. Hockenmaier) Constituents: Heads and dependents There are different kinds of constituents: Noun phrases: the man, a girl with glasses, Illinois Prepositional phrases: with glasses, in the garden Verb phrases: eat sushi, sleep, sleep soundly Every phrase has a head: Noun phrases: the man, a girl with glasses, Illinois Prepositional phrases: with glasses, in the garden Verb phrases: eat sushi, sleep, sleep soundly The other parts are its dependents. Dependents are either arguments or adjuncts !51 CS447: Natural Language Processing (J. Hockenmaier) Is string α a constituent? Substitution test: Can α be replaced by a single word? He talks [there]. Movement test: Can α be moved around in the sentence? [In class], he talks. Answer test: Can α be the answer to a question? Where does he talk? - [In class]. He talks [in class]. !52 CS447: Natural Language Processing (J. Hockenmaier) Arguments are obligatory Words subcategorize for speciﬁc sets of arguments: Transitive verbs (sbj + obj): [John] likes [Mary] All arguments have to be present: *[John] likes. *likes [Mary]. No argument can be occupied multiple times: *[John] [Peter] likes [Ann] [Mary]. Words can have multiple subcat frames: Transitive eat (sbj + obj): [John] eats [sushi]. Intransitive eat (sbj): [John] eats. !53 CS447: Natural Language Processing (J. Hockenmaier) Adjuncts are optional Adverbs, PPs and adjectives can be adjuncts: Adverbs: John runs [fast]. a [very] heavy book. PPs: John runs [in the gym]. the book [on the table] Adjectives: a [heavy] book There can be an arbitrary number of adjuncts: John saw Mary. John saw Mary [yesterday]. John saw Mary [yesterday] [in town] John saw Mary [yesterday] [in town] [during lunch] [Perhaps] John saw Mary [yesterday] [in town] [during lunch] !54 CS447: Natural Language Processing (J. Hockenmaier) A context-free grammar for a fragment of English !55 CS447: Natural Language Processing (J. Hockenmaier) Noun phrases (NPs) Simple NPs: [He] sleeps. (pronoun) [John] sleeps. (proper name) [A student] sleeps. (determiner + noun) Complex NPs: [A tall student] sleeps. (det + adj + noun) [The student in the back] sleeps. (NP + PP) [The student who likes MTV] sleeps. (NP + Relative Clause) !56 CS447: Natural Language Processing (J. Hockenmaier) The NP fragment NP → Pronoun NP → ProperName NP → Det Noun Det → {a, the, every} Pronoun → {he, she,...} ProperName → {John, Mary,...} Noun → AdjP Noun Noun → N NP → NP PP NP → NP RelClause !57 CS447: Natural Language Processing (J. Hockenmaier) Adjective phrases (AdjP) and prepositional phrases (PP) AdjP → Adj AdjP → Adv AdjP Adj → {big, small, red,...} Adv → {very, really,...} PP → P NP P → {with, in, above,...} !58 CS447: Natural Language Processing (J. Hockenmaier) The verb phrase (VP) He [eats]. He [eats sushi]. He [gives John sushi]. He [eats sushi with chopsticks]. VP → V VP → V NP VP → V NP PP VP → VP PP V → {eats, sleeps gives,...} !59 CS447: Natural Language Processing (J. Hockenmaier) Capturing subcategorization He [eats]. ✔ He [eats sushi]. ✔ He [gives John sushi]. ✔ He [eats sushi with chopsticks]. ✔ *He [eats John sushi]. ??? VP → Vintrans VP → Vtrans NP VP → Vditrans NP NP VP → VP PP Vintrans → {eats, sleeps} Vtrans → {eats} Vtrans → {gives} !60 CS447: Natural Language Processing (J. Hockenmaier) Sentences [He eats sushi]. [Sometimes, he eats sushi]. [In Japan, he eats sushi]. S → NP VP S → AdvP S S → PP S He says [he eats sushi]. VP → Vcomp S Vcomp → {says, think, believes} !61 CS447: Natural Language Processing (J. Hockenmaier) Sentences redeﬁned [He eats sushi]. ✔ *[I eats sushi]. ??? *[They eats sushi]. ??? S → NP3sg VP3sg S → NP1sg VP1sg S → NP3pl VP3pl We need features to capture agreement: (number, person, case,…) !62 CS447: Natural Language Processing (J. Hockenmaier) Complex VPs In English, simple tenses have separate forms: present tense: the girl eats sushi simple past tense: the girl ate sushi Complex tenses, progressive aspect and passive voice consist of auxiliaries and participles: past perfect tense: the girl has eaten sushi future perfect: the girl will have eaten sushi passive voice: the sushi was eaten by the girl progressive: the girl is/was/will be eating sushi !63 CS447: Natural Language Processing (J. Hockenmaier) VPs redeﬁned He [has [eaten sushi]]. The sushi [was [eaten by him]]. VP → Vhave VPpastPart VP → Vbe VPpass VPpastPart → VpastPart NP VPpass → VpastPart PP Vhave→ {has} VpastPart→ {eaten, seen} We need more nonterminals (e.g. VPpastpart). N.B.: We call VPpastPart, VPpass, etc. `untensed’ VPs !64 CS447: Natural Language Processing (J. Hockenmaier) Coordination [He eats sushi] and [she drinks tea] [John] and [Mary] eat sushi. He [eats sushi] and [drinks tea] S → S conj S NP → NP conj NP VP → VP conj VP He says [he eats sushi]. VP → Vcomp S Vcomp → {says, think, believes} !65 CS447: Natural Language Processing (J. Hockenmaier) Relative clauses Relative clauses modify a noun phrase: the girl [that eats sushi] Relative clauses lack a noun phrase, which is understood to be ﬁlled by the NP they modify: ‘the girl that eats sushi’ implies ‘the girl eats sushi’ There are subject and object relative clauses: subject: ‘the girl that eats sushi’ object: ‘the sushi that the girl eats’ !66 CS447: Natural Language Processing (J. Hockenmaier) Yes/No questions Yes/no questions consist of an auxiliary, a subject and an (untensed) verb phrase: does she eat sushi? have you eaten sushi? YesNoQ → Aux NP VPinf YesNoQ → Aux NP VPpastPart !67 CS447: Natural Language Processing (J. Hockenmaier) Wh-questions Subject wh-questions consist of an wh-word, an auxiliary and an (untensed) verb phrase: Who has eaten the sushi? Object wh-questions consist of an wh-word, an auxiliary, an NP and an (untensed) verb phrase: What does Mary eat? !68 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts Natural language syntax Constituents Dependencies Context-free grammar Arguments and modiﬁers Recursion in natural language !69 CS447: Natural Language Processing (J. Hockenmaier) Today’s reading Textbook: Jurafsky and Martin, Chapter 12, sections 1-7 !70 "
265,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 9: The CKY parsing algorithm CS447 Natural Language Processing Last lecture’s key concepts Natural language syntax Constituents Dependencies Context-free grammar Arguments and modiﬁers Recursion in natural language !2 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning grammars for natural language !3 CS447: Natural Language Processing (J. Hockenmaier) An example CFG DT → {the, a} N → {ball, garden, house, sushi } P → {in, behind, with} NP → DT N NP → NP PP PP → P NP N: noun P: preposition NP: “noun phrase” PP: “prepositional phrase” !4 CS447: Natural Language Processing (J. Hockenmaier) Reminder: Context-free grammars A CFG is a 4-tuple 〈N, Σ, R, S〉 consisting of: A set of nonterminals N (e.g. N = {S, NP, VP, PP, Noun, Verb, ....}) A set of terminals Σ (e.g. Σ = {I, you, he, eat, drink, sushi, ball, }) A set of rules R R ⊆ {A → β with left-hand-side (LHS) A ∈ N and right-hand-side (RHS) β ∈ (N ∪ Σ)* } A start symbol S ∈ N !5 CS447: Natural Language Processing (J. Hockenmaier) Constituents: Heads and dependents There are different kinds of constituents: Noun phrases: the man, a girl with glasses, Illinois Prepositional phrases: with glasses, in the garden Verb phrases: eat sushi, sleep, sleep soundly Every phrase has a head: Noun phrases: the man, a girl with glasses, Illinois Prepositional phrases: with glasses, in the garden Verb phrases: eat sushi, sleep, sleep soundly The other parts are its dependents. Dependents are either arguments or adjuncts !6 CS447: Natural Language Processing (J. Hockenmaier) Is string α a constituent? Substitution test: Can α be replaced by a single word? He talks [there]. Movement test: Can α be moved around in the sentence? [In class], he talks. Answer test: Can α be the answer to a question? Where does he talk? - [In class]. He talks [in class]. !7 CS447: Natural Language Processing (J. Hockenmaier) Arguments are obligatory Words subcategorize for speciﬁc sets of arguments: Transitive verbs (sbj + obj): [John] likes [Mary] All arguments have to be present: *[John] likes. *likes [Mary]. No argument can be occupied multiple times: *[John] [Peter] likes [Ann] [Mary]. Words can have multiple subcat frames: Transitive eat (sbj + obj): [John] eats [sushi]. Intransitive eat (sbj): [John] eats. !8 CS447: Natural Language Processing (J. Hockenmaier) Adjuncts are optional Adverbs, PPs and adjectives can be adjuncts: Adverbs: John runs [fast]. a [very] heavy book. PPs: John runs [in the gym]. the book [on the table] Adjectives: a [heavy] book There can be an arbitrary number of adjuncts: John saw Mary. John saw Mary [yesterday]. John saw Mary [yesterday] [in town] John saw Mary [yesterday] [in town] [during lunch] [Perhaps] John saw Mary [yesterday] [in town] [during lunch] !9 CS447 Natural Language Processing Heads, Arguments and Adjuncts in CFGs Heads: We assume that each RHS has one head, e.g. VP → Verb NP (Verbs are heads of VPs) NP → Det Noun (Nouns are heads of NPs) S → NP VP (VPs are heads of sentences) Exception: Coordination, lists: VP → VP conj VP Arguments: The head has a different category from the parent: VP → Verb NP (the NP is an argument of the verb) Adjuncts: The head has the same category as the parent: VP → VP PP (the PP is an adjunct) !10 CS447 Natural Language Processing The right-hand side of a standard CFG can have an arbitrary number of symbols (terminals and nonterminals): VP → ADV eat NP A CFG in Chomsky Normal Form (CNF) allows only two kinds of right-hand sides: – Two nonterminals: VP → ADV VP – One terminal: VP → eat Any CFG can be transformed into an equivalent CNF: VP → ADVP VP1 VP1 → VP2 NP VP2 → eat Chomsky Normal Form !11 VP ADV NP eat VP2 VP ADV NP eat VP1 VP ADV NP eat CS447 Natural Language Processing A note about ε-productions Formally, context-free grammars are allowed to have empty productions (ε = the empty string): VP → V NP NP → DT Noun NP → ε These can always be eliminated without changing the language generated by the grammar: VP → V NP NP → DT Noun NP → ε becomes VP → V NP VP → V ε NP → DT Noun which in turn becomes VP → V NP VP → V NP → DT Noun We will assume that our grammars don’t have ε-productions !12 CS447 Natural Language Processing CKY chart parsing algorithm Bottom-up parsing: start with the words Dynamic programming: save the results in a table/chart re-use these results in ﬁnding larger constituents Complexity: O( n3|G| ) n: length of string, |G|: size of grammar) Presumes a CFG in Chomsky Normal Form: Rules are all either A → B C or A → a (with A,B,C nonterminals and a a terminal) !13 CS447 Natural Language Processing we eat sushi we eat eat sushi sushi eat we S → NP VP VP → V NP V → eat NP → we NP → sushi We eat sushi The CKY parsing algorithm S NP V NP VP !14 To recover the parse tree, each entry needs pairs of backpointers. CS447 Natural Language Processing CKY algorithm 1. Create the chart (an n×n upper triangular matrix for an sentence with n words) – Each cell chart[i][j] corresponds to the substring w(i)…w(j) 2. Initialize the chart (ﬁll the diagonal cells chart[i][i]): For all rules X → w(i), add an entry X to chart[i][i] 3. Fill in the chart: Fill in all cells chart[i][i+1], then chart[i][i+2], …, until you reach chart[1][n] (the top right corner of the chart) – To ﬁll chart[i][j], consider all binary splits w(i)…w(k)|w(k+1)…w(j) – If the grammar has a rule X → YZ, chart[i][k] contains a Y and chart[k+1][j] contains a Z, add an X to chart[i][j] with two backpointers to the Y in chart[i][k] and the Z in chart[k+1][j] 4. Extract the parse trees from the S in chart[1][n]. !15 CS447 Natural Language Processing CKY: ﬁlling the chart !16 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n CS447 Natural Language Processing CKY: ﬁlling one cell !17 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2 w3 w4 w5 w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 CS447 Natural Language Processing V buy VP buy drinks buy drinks with VP buy drinks with milk V, NP drinks drinks with VP, NP drinks with milk P with PP with milk NP milk The CKY parsing algorithm !18 We buy drinks with milk S → NP VP VP → V NP VP → VP PP V → drinks NP → NP PP NP → we NP → drinks NP → milk PP → P NP P → with Each cell may have one entry for each nonterminal CS447 Natural Language Processing we we eat we eat sushi we eat sushi with we eat sushi with tuna eat eat sushi eat sushi with eat sushi with tuna sushi sushi with sushi with tuna with with tuna tuna we we eat we eat sushi we eat sushi with we eat sushi with tuna V eat VP eat sushi eat sushi with VP eat sushi with tuna sushi sushi with NP sushi with tuna with PP with tuna tuna The CKY parsing algorithm !19 We eat sushi with tuna Each cell contains only a single entry for each nonterminal. Each entry may have a list of pairs of backpointers. S → NP VP VP → V NP VP → VP PP V → eat NP → NP PP NP → we NP → sushi NP → tuna PP → P NP P → with CS447: Natural Language Processing (J. Hockenmaier) What are the terminals in NLP? Are the “terminals”: words or POS tags? For toy examples (e.g. on slides), it’s typically the words With POS-tagged input, we may either treat the POS tags as the terminals, or we assume that the unary rules in our grammar are of the form POS-tag → word (so POS tags are the only nonterminals that can be rewritten as words; some people call POS tags “preterminals”) !20 CS447: Natural Language Processing (J. Hockenmaier) Additional unary rules In practice, we may allow other unary rules, e.g. NP → Noun (where Noun is also a nonterminal) In that case, we apply all unary rules to the entries in chart[i][j] after we’ve checked all binary splits (chart[i][k], chart[k+1][j]) Unary rules are ﬁne as long as there are no “loops” that could lead to an inﬁnite chain of unary productions, e.g.: X → Y and Y → X or: X → Y and Y → Z and Z → X !21 CS447 Natural Language Processing CKY so far… Each entry in a cell chart[i][j] is associated with a nonterminal X. If there is a rule X → YZ in the grammar, and there is a pair of cells chart[i][k], chart[k+1][j] with a Y in chart[i][k] and a Z in chart[k+1][j], we can add an entry X to cell chart[i][j], and associate one pair of backpointers with the X in cell chart[i][k] Each entry might have multiple pairs of backpointers. When we extract the parse trees at the end, we can get all possible trees. We will need probabilities to ﬁnd the single best tree! !22 CS447 Natural Language Processing Exercise: CKY parser I eat sushi with chopsticks with you !23 S ⟶NP VP NP ⟶NP PP NP ⟶sushi NP ⟶I NP ⟶chopsticks NP ⟶you VP ⟶VP PP VP ⟶Verb NP Verb ⟶eat PP ⟶Prep NP Prep ⟶with CS447 Natural Language Processing !24 How do you count the number of parse trees for a sentence? 1. For each pair of backpointers (e.g.VP → V NP): multiply #trees of children trees(VPVP → V NP) = trees(V) × trees(NP) 2. For each list of pairs of backpointers (e.g.VP → V NP and VP → VP PP): sum #trees trees(VP) = trees(VPVP→V NP) + trees(VPVP→VP PP) CS447 Natural Language Processing Cocke Kasami Younger (1) w1 ... ... wi ... wn w1 ... ... wi ... wn initChart(n): for i = 1...n: initCell(i,i) initCell(i,i): for c in lex(word[i]): addToCell(cell[i][i], c, null, null) addToCell(Parent,cell,Left, Right) if (cell.hasEntry(Parent)): P = cell.getEntry(Parent) P.addBackpointers(Left, Right) else cell.addEntry(Parent, Left, Right) !25 w1 ... ... wi ... wn w1 ... ... wi ... wn ckyParse(n): initChart(n) fillChart(n) fillChart(n): for span = 1...n-1: for i = 1...n-span: fillCell(i,i+span) fillCell(i,j): for k = i..j-1: combineCells(i, k, j) combineCells(i,k,j): for Y in cell[i][k]: for Z in cell[k +1][j]: for X in Nonterminals: if X →Y Z in Rules: addToCell(cell[i][j],X, Y, Z) w1 ... ... wi ... wn w1 ... Y X wj Z ... ... wn CS447: Natural Language Processing (J. Hockenmaier) Dealing with ambiguity: Probabilistic Context-Free Grammars (PCFGs) !26 CS447 Natural Language Processing Grammars are ambiguous A grammar might generate multiple trees for a sentence: What’s the most likely parse τ for sentence S ? We need a model of P(τ | S) !27 Correct analysis Incorrect analysis eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks NP NP NP VP PP V P eat with tuna sushi NP NP VP PP VP V P eat sushi with tuna Incorrect analysis eat with tuna sushi P sushi eat with chopsticks NP NP VP PP VP V P eat sushi eat sushi wi eat sushi with chopsticks NP NP NP VP PP V P eat with tuna sushi NP NP VP PP VP V P eat sushi eat sushi wit CS447 Natural Language Processing Computing P(τ | S) Using Bayes’ Rule: The yield of a tree is the string of terminal symbols that can be read off the leaf nodes arg max τ P(τ|S) = arg max τ P(τ, S) P(S) = arg max τ P(τ, S) = arg max τ P(τ) if S = yield(τ) !28 Correct analysis eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with tuna eat sushi with chopsticks yield( ) = eat sushi with tuna CS447 Natural Language Processing T is the (inﬁnite) set of all trees in the language: We need to deﬁne P(τ) such that: The set T is generated by a context-free grammar Computing P(τ) !29 ⇤τ ⇥T : 0 ≤P(τ) ≤1 ∑τ⇥T P(τ) = 1 L = {s ⇥Σ∗| ⇤τ ⇥T : yield(τ) = s} S → NP VP VP → Verb NP NP → Det Noun S → S conj S VP → VP PP NP → NP PP S → ..... VP → ..... NP → ..... CS447 Natural Language Processing Probabilistic Context-Free Grammars For every nonterminal X, deﬁne a probability distribution P(X → α | X) over all rules with the same LHS symbol X: !30 S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 CS447 Natural Language Processing Computing P(τ) with a PCFG The probability of a tree τ is the product of the probabilities of all its rules: !31 P(τ) = 0.8 ×0.3 ×0.2 ×1.0 = 0.00384 ×0.23 S NP Noun John VP VP Verb eats NP Noun pie PP P with NP Noun cream S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 CS498JH: Introduction to NLP PCFG parsing (decoding): Probabilistic CKY !32 CS447 Natural Language Processing Probabilistic CKY: Viterbi Like standard CKY, but with probabilities. Finding the most likely tree argmaxτ P(τ,s) is similar to Viterbi for HMMs: Initialization: every chart entry that corresponds to a terminal (entries X in cell[i][i])has a Viterbi probability PVIT(X[i][i] ) = 1 Recurrence: For every entry that corresponds to a non-terminal X in cell[i][j], keep only the highest-scoring pair of backpointers to any pair of children (Y in cell[i][k] and Z in cell[k+1][j]): PVIT(X[i][j]) = argmaxY,Z,k PVIT(Y[i][k]) × PVIT(Z[k+1][j] ) × P(X → Y Z | X ) Final step: Return the Viterbi parse for the start symbol S in the top cell[1][n]. !33 CS447 Natural Language Processing Probabilistic CKY !34 John eats pie with cream N 1.0 John V 1.0 eats N 1.0 pie P 1.0 with N 1.0 cream Input: POS-tagged sentence John_N eats_V pie_N with_P cream_N NP 0.2 VP 0.3 NP 0.2 NP 0.2 S 0.8·0.2·0.3 VP 1·0.3·0.2 = 0.06 PP 1·1·0.2 S 0.8·0.2·0.06 NP 0.2·0.2·0.2 = 0.008 VP max( 1.0 ·0.008·0.3, 0.06·0.2·0.3 ) S 0.2·0.0036·0.8 S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 0.3 0.3 "
266,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 10: Statistical Parsing with PCFGs CS447 Natural Language Processing Where we’re at Previous lecture: Standard CKY (for non-probabilistic CFGs) The standard CKY algorithm ﬁnds all possible parse trees τ for a sentence S = w(1)…w(n) under a CFG G in Chomsky Normal Form. Today’s lecture: Probabilistic Context-Free Grammars (PCFGs) – CFGs in which each rule is associated with a probability CKY for PCFGs (Viterbi): – CKY for PCFGs ﬁnds the most likely parse tree τ* = argmax P(τ | S) for the sentence S under a PCFG. #2 CS447: Natural Language Processing (J. Hockenmaier) Previous Lecture: CKY for CFGs #3 CS447 Natural Language Processing CKY: ﬁlling the chart #4 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n w 1 ... ... wi ... w n w 1 ... .. . wi ... w n CS447 Natural Language Processing CKY: ﬁlling one cell #5 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2 w3 w4 w5 w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 w 1 ... ... wi ... w n w 1 ... .. . wi ... w n chart[2][6]: w1 w2w3w4w5w6 w7 CS447 Natural Language Processing CKY for standard CFGs CKY is a bottom-up chart parsing algorithm that ﬁnds all possible parse trees τ for a sentence S = w(1)…w(n) under a CFG G in Chomsky Normal Form (CNF). – CNF: G has two types of rules: X ⟶ Y Z and X ⟶ w (X, Y, Z are nonterminals, w is a terminal) – CKY is a dynamic programming algorithm – The parse chart is an n×n upper triangular matrix: Each cell chart[i][j] (i ≤ j) stores all subtrees for w(i)…w(j) – Each cell chart[i][j] has at most one entry for each nonterminal X (and pairs of backpointers to each pair of (Y, Z) entry in cells chart[i][k] chart[k+1][j] from which an X can be formed – Time Complexity: O(n3 |G |) #6 CS447: Natural Language Processing (J. Hockenmaier) Dealing with ambiguity: Probabilistic Context- Free Grammars (PCFGs) #7 CS447 Natural Language Processing Probabilistic Context-Free Grammars For every nonterminal X, deﬁne a probability distribution P(X → α | X) over all rules with the same LHS symbol X: #8 S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 CS447 Natural Language Processing Computing P(τ) with a PCFG The probability of a tree τ is the product of the probabilities of all its rules: #9 P(τ) = 0.8 ×0.3 ×0.2 ×1.0 = 0.00384 ×0.23 S NP Noun John VP VP Verb eats NP Noun pie PP P with NP Noun cream S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 CS447 Natural Language Processing Learning the parameters of a PCFG If we have a treebank (a corpus in which each sentence is associated with a parse tree), we can just count the number of times each rule appears, e.g.: S ! NP VP . (count = 1000) S ! S conj S . (count = 220) and then we divide the observed frequency of each rule X → Y Z by the sum of the frequencies of all rules with the same LHS X to turn these counts into probabilities: S ! NP VP . (p = 1000/1220) S ! S conj S . (p = 220/1220) #10 CS447 Natural Language Processing More on probabilities: Computing P(s): If P(τ) is the probability of a tree τ, the probability of a sentence s is the sum of the probabilities of all its parse trees: P(s) = ∑τ:yield(τ) = s P(τ) How do we know that P(L) = ∑τ P(τ) = 1? If we have learned the PCFG from a corpus via MLE, this is guaranteed to be the case. If we just set the probabilities by hand, we could run into trouble, as in the following example: S ! S S (0.9) S ! w (0.1) #11 CS447: Natural Language Processing (J. Hockenmaier) PCFG parsing (decoding): Probabilistic CKY #12 CS447 Natural Language Processing Probabilistic CKY: Viterbi Like standard CKY, but with probabilities. Finding the most likely tree is similar to Viterbi for HMMs: Initialization: – [optional] Every chart entry that corresponds to a terminal (entry w in cell[i][i]) has a Viterbi probability PVIT(w[i][i] ) = 1 (*) – Every entry for a non-terminal X in cell[i][i] has Viterbi probability PVIT(X[i][i] ) = P(X → w | X) [and a single backpointer to w[i][i] (*) ] Recurrence: For every entry that corresponds to a non-terminal X in cell[i][j], keep only the highest-scoring pair of backpointers to any pair of children (Y in cell[i][k] and Z in cell[k+1][j]): PVIT(X[i][j]) = argmaxY,Z,k PVIT(Y[i][k]) × PVIT(Z[k+1][j] ) × P(X → Y Z | X ) Final step: Return the Viterbi parse for the start symbol S in the top cell[1][n]. *this is unnecessary for simple PCFGs, but can be helpful for more complex probability models #13 CS447 Natural Language Processing Probabilistic CKY #14 NP 0.2 John eats pie with cream Noun 1.0 John Verb 1.0 eats Noun 1.0 pie Prep 1.0 with Noun 1.0 cream Input: POS-tagged sentence John_N eats_V pie_N with_P cream_N NP 0.2 VP 0.3 NP 0.2 S 0.8·0.2·0.3 VP 1·0.3·0.2 = 0.06 PP 1·1·0.2 S 0.8·0.2·0.06 NP 0.2·0.2·0.2 = 0.008 VP max( 1.0 ·0.008·0.3, 0.06·0.2·0.3 ) S 0.2·0.0036·0.8 S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 0.3 0.3 Prep NP Prep ⟶ P 1.0 Noun ⟶ N 1.0 Verb ⟶ V 1.0 CS447 Natural Language Processing How do we handle ﬂat rules? #15 S →NP VP 0.8 S →S conj S 0.2 NP →Noun 0.2 NP →Det Noun 0.4 NP →NP PP 0.2 NP →NP conj NP 0.2 VP →Verb 0.4 VP →Verb NP 0.3 VP →Verb NP NP 0.1 VP →VP PP 0.2 PP →P NP 1.0 0.3 0.3 Prep NP S ⟶ S ConjS 0.2 ConjS ⟶ conj S 1.0 Binarize each ﬂat rule by adding dummy nonterminals (ConjS), and setting the probability of the rule with the dummy nonterminal on the LHS to 1 CS447: Natural Language Processing (J. Hockenmaier) Parser evaluation #16 CS447: Natural Language Processing (J. Hockenmaier) Precision and recall Precision and recall were originally developed as evaluation metrics for information retrieval: -Precision: What percentage of retrieved documents are relevant to the query? -Recall: What percentage of relevant documents were retrieved? In NLP, they are often used in addition to accuracy: -Precision: What percentage of items that were assigned label X do actually have label X in the test data? -Recall: What percentage of items that have label X in the test data were assigned label X by the system? Particularly useful when there are more than two labels. #17 CS447: Natural Language Processing (J. Hockenmaier) True vs. false positives, false negatives - True positives: Items that were labeled X by the system, and should be labeled X. - False positives: Items that were labeled X by the system, but should not be labeled X. - False negatives: Items that were not labeled X by the system, but should be labeled X #18 False Negatives (FN) Items labeled X in the gold standard (‘truth’) Items labeled X by the system = TP + FN = TP + FP False Positives (FP) True Positives (TP) CS447: Natural Language Processing (J. Hockenmaier) Precision, recall, f-measure #19 False Positives (FP) False Negatives (FN) True Positives (TP) Items labeled X in the gold standard (‘truth’) = TP + FN Items labeled X by the system = TP + FP Precision: P = TP ∕( TP + FP ) Recall: R = TP ∕( TP + FN ) F-measure: harmonic mean of precision and recall F = (2·P·R)∕(P + R) CS447 Natural Language Processing Evalb (“Parseval”) Measures recovery of phrase-structure trees. Labeled: span and label (NP, PP,...) has to be right. [Earlier variant— unlabeled: span of nodes has to be right] Two aspects of evaluation Precision: How many of the predicted nodes are correct? Recall: How many of the correct nodes were predicted? Usually combined into one metric (F-measure): P = #correctly predicted nodes #predicted nodes R = #correctly predicted nodes #correct nodes F = 2PR P + R #20 CS447 Natural Language Processing Parseval in practice eat sushi with tuna: Precision: 4/5 Recall: 4/5 eat sushi with chopsticks: Precision: 4/5 Recall: 4/5 #21 eat with tuna sushi NP NP VP PP NP V P sushi eat with chopsticks NP NP VP PP VP V P eat sushi with chopsticks NP NP NP VP PP V P eat with tuna sushi NP NP VP PP VP V P Gold standard Parser output N N N N N N N N CS498JH: Introduction to NLP Shortcomings of PCFGs #22 CS447 Natural Language Processing PCFGs make independence assumptions: Only the label of a node determines what children it has. Factors that inﬂuence these assumptions: Shape of the trees: A corpus with ﬂat trees (i.e. few nodes/sentence) results in a model with few independence assumptions. Labeling of the trees: A corpus with many node labels (nonterminals) results in a model with few independence assumptions. #23 How well can a PCFG model the distribution of trees? CS447 Natural Language Processing Example 1: ﬂat trees S I eat sushi with tuna What sentences would a PCFG estimated from this corpus generate? S I eat sushi with chopsticks #24 CS447 Natural Language Processing Example 2: deep trees, few labels S I S eat S sushi S with chopsticks What sentences would a PCFG estimated from this corpus generate? S I S eat S sushi S with tuna #25 CS447 Natural Language Processing Example 3: deep trees, many labels What sentences would a PCFG estimated from this corpus generate? S I S1 eat S2 sushi S3 with tuna S I S1 eat S2 sushi S3 with chopsticks #26 CS447 Natural Language Processing Aside: Bias/Variance tradeoff A probability model has low bias if it makes few independence assumptions. ⇒ It can capture the structures in the training data. This typically leads to a more ﬁne-grained partitioning of the training data. Hence, fewer data points are available to estimate the model parameters. This increases the variance of the model. ⇒ This yields a poor estimate of the distribution. #27 CS447: Natural Language Processing (J. Hockenmaier) Penn Treebank parsing #28 CS447 Natural Language Processing The Penn Treebank The ﬁrst publicly available syntactically annotated corpus Wall Street Journal (50,000 sentences, 1 million words) also Switchboard, Brown corpus, ATIS The annotation: – POS-tagged (Ratnaparkhi’s MXPOST) – Manually annotated with phrase-structure trees – Richer than standard CFG: Traces and other null elements used to represent non-local dependencies (designed to allow extraction of predicate-argument structure) [more on this later in the semester] Standard data set for English parsers #29 CS447 Natural Language Processing The Treebank label set 48 preterminals (tags): – 36 POS tags, 12 other symbols (punctuation etc.) – Simpliﬁed version of Brown tagset (87 tags) (cf. Lancaster-Oslo/Bergen (LOB) tag set: 126 tags) 14 nonterminals: standard inventory (S, NP, VP,...) #30 CS447 Natural Language Processing A simple example #31 Relatively ﬂat structures: – There is no noun level – VP arguments and adjuncts appear at the same level Function tags, e.g. -SBJ (subject), -MNR (manner) CS447 Natural Language Processing A more realistic (partial) example Until Congress acts, the government hasn't any authority to issue new debt obligations of any kind, the Treasury said .... . #32 CS447 Natural Language Processing The Penn Treebank CFG The Penn Treebank uses very ﬂat rules, e.g.: – Many of these rules appear only once. – Many of these rules are very similar. – Can we pool these counts? #33 CS447 Natural Language Processing PCFGs in practice: Charniak (1996) Tree-bank grammars How well do PCFGs work on the Penn Treebank? – Split Treebank into test set (30K words) and training set (300K words). – Estimate a PCFG from training set. – Parse test set (with correct POS tags). – Evaluate unlabeled precision and recall #34 CS447 Natural Language Processing Two ways to improve performance … change the (internal) grammar: - Parent annotation/state splits: Not all NPs/VPs/DTs/… are the same. It matters where they are in the tree … change the probability model: - Lexicalization: Words matter! - Markovization: Generalizing the rules #35 CS447 Natural Language Processing PCFGs assume the expansion of any nonterminal is independent of its parent. But this is not true: NP subjects more likely to be modiﬁed than objects. We can change the grammar by adding the name of the parent node to each nonterminal The parent transformation #36 CS447 Natural Language Processing Markov PCFGs (Collins parser) The RHS of each CFG rule consists of: one head HX, n left sisters Li and m right sisters Ri: Replace rule probabilities with a generative process: For each nonterminal X -generate its head HX (nonterminal or terminal) -then generate its left sisters L1..n and a STOP symbol conditioned on HX -then generate its right sisters R1...n and a STOP symbol conditioned on HX X →Ln...L1 ! ""# $ left sisters HX R1...Rm ! ""# $ right sisters #37 CS447 Natural Language Processing Lexicalization PCFGs can’t distinguish between “eat sushi with chopsticks” and “eat sushi with tuna”. We need to take words into account! P(VPeat → VP PPwith chopsticks | VPeat ) vs. P(VPeat → VP PPwith tuna | VPeat ) Problem: sparse data (PPwith fatty|white|... tuna....) Solution: only take head words into account! Assumption: each constituent has one head word. #38 CS447 Natural Language Processing At the root (start symbol S), generate the head word of the sentence, wS , with P(wS) Lexicalized rule probabilities: Every nonterminal is lexicalized: Xwx Condition rules Xwx → αYβ on the lexicalized LHS Xwx P( Xwx → αYβ | Xwx) Word-word dependencies: For each nonterminal Y in RHS of a rule Xwx → αYβ, condition wY (the head word of Y ) on X and wX: P( wY | Y, X, wX ) Lexicalized PCFGs #39 CS447 Natural Language Processing Dealing with unknown words A lexicalized PCFG assigns zero probability to any word that does not appear in the training data. Solution: Training: Replace rare words in training data with a token ‘UNKNOWN’. Testing: Replace unseen words with ‘UNKNOWN’ #40 CS447 Natural Language Processing Reﬁning the set of categories Unlexicalized Parsing (Klein & Manning ’03) Unlexicalized PCFGs with various transformations of the training data and the model, e.g.: – Parent annotation (of terminals and nonterminals): distinguish preposition IN from subordinating conjunction IN etc. – Add head tag to nonterminals (e.g. distinguish ﬁnite from inﬁnite VPs) – Add distance features Accuracy: 86.3 Precision and 85.1 Recall The Berkeley parser (Petrov et al. ’06, ’07) Automatically learns reﬁnements of the nonterminals Accuracy: 90.2 Precision, 89.9 Recall #41 CS447: Natural Language Processing (J. Hockenmaier) Summary The Penn Treebank has a large number of very ﬂat rules. Accurate parsing requires modiﬁcations to the basic PCFG model: reﬁning the nonterminals, relaxing the independence assumptions by including grandparent information, modeling word-word dependencies, etc. How much of this transfers to other treebanks or languages? #42 "
267,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 11: Penn Treebank Parsing; Dependency Grammars CS447 Natural Language Processing Class Admin !2 CS447 Natural Language Processing Midterm Exam: Friday, Oct 12 The midterm will be during class. Closed book exam: You are not allowed to use any cheat sheets, computers, calculators, phones etc.(you shouldn’t have to anyway) The exam will cover the material from the lectures Format: Short answer questions Review session: Wednesday, Oct 10 in class. Review the material before that class, so that we can clear up any confusions Conﬂict Exam or DRES accommodations: Email me (juliahmr@illinois.edu) asap !3 CS447: Natural Language Processing (J. Hockenmaier) Exam Question types Deﬁne X: Provide a mathematical/formal deﬁnition of X Explain X; Explain what X is/does: Use plain English to deﬁne X and say what X is/does Compute X: Return X; Show the steps required to calculate it Draw X: Draw a ﬁgure of X Show that X is true/is the case/…: This may require a (typically very simple) proof. Discuss/Argue whether … Use your knowledge (of X,Y,Z) to argue your point !4 CS447: Natural Language Processing (J. Hockenmaier) 4th Credit Hour Either a research project (alone or with one other student) or a literature survey (alone) Upcoming deadlines: Fri, Oct 19: Proposal due Fri, Nov 9: Progress report due (Is your paper on track?) Thu, Dec 13: Final report due (Summary of papers) Good places to ﬁnd NLP papers: -ACL anthology http://aclweb.org/anthology covers almost everything published in NLP -JNLE http://journals.cambridge.org/action/displayJournal?jid=NLE is another big NLP journal that is not part of the ACL -Standard machine learning/AI conferences (NIPS, ICML, IJCAI, AAAI) and journals (JMLR, JAIR etc.) are okay as well. -Other venues: check with me that this is actually NLP !5 CS447: Natural Language Processing (J. Hockenmaier) 4th Credit hour: Proposal Upload a one-page PDF to Compass by Oct 19 -written in LaTeX (not MS Word) -with full bibliography of the papers you want to read or base your project on (ideally with links to online versions; add url-ﬁeld to your bibtex ﬁle) -include a motivation of why you have chosen those papers -for a research project: tell me whether you have the data you need, what existing software you will be using, what you will have to implement yourself. -mention any questions/concerns that you may have. !6 CS447 Natural Language Processing Today’s lecture Penn Treebank Parsing Dependency Grammars Dependency Treebanks Dependency Parsing !7 CS447: Natural Language Processing (J. Hockenmaier) Penn Treebank Parsing !8 CS447 Natural Language Processing The Penn Treebank The ﬁrst publicly available syntactically annotated corpus Wall Street Journal (50,000 sentences, 1 million words) also Switchboard, Brown corpus, ATIS The annotation: – POS-tagged (Ratnaparkhi’s MXPOST) – Manually annotated with phrase-structure trees – Richer than standard CFG: Traces and other null elements used to represent non-local dependencies (designed to allow extraction of predicate-argument structure) [more on this later in the semester] Standard data set for English parsers !9 CS447 Natural Language Processing The Treebank label set 48 preterminals (tags): – 36 POS tags, 12 other symbols (punctuation etc.) – Simpliﬁed version of Brown tagset (87 tags) (cf. Lancaster-Oslo/Bergen (LOB) tag set: 126 tags) 14 nonterminals: standard inventory (S, NP, VP,...) !10 CS447 Natural Language Processing A simple example !11 Relatively ﬂat structures: – There is no noun level – VP arguments and adjuncts appear at the same level Function tags, e.g. -SBJ (subject), -MNR (manner) CS447 Natural Language Processing A more realistic (partial) example Until Congress acts, the government hasn't any authority to issue new debt obligations of any kind, the Treasury said .... . !12 CS447 Natural Language Processing The Penn Treebank CFG The Penn Treebank uses very ﬂat rules, e.g.: – Many of these rules appear only once. – Many of these rules are very similar. – Can we pool these counts? !13 CS447 Natural Language Processing PCFGs in practice: Charniak (1996) Tree-bank grammars How well do PCFGs work on the Penn Treebank? – Split Treebank into test set (30K words) and training set (300K words). – Estimate a PCFG from training set. – Parse test set (with correct POS tags). – Evaluate unlabeled precision and recall !14 CS447 Natural Language Processing Two ways to improve performance … change the (internal) grammar: - Parent annotation/state splits: Not all NPs/VPs/DTs/… are the same. It matters where they are in the tree … change the probability model: - Lexicalization: Words matter! - Markovization: Generalizing the rules !15 CS447 Natural Language Processing PCFGs assume the expansion of any nonterminal is independent of its parent. But this is not true: NP subjects more likely to be modiﬁed than objects. We can change the grammar by adding the name of the parent node to each nonterminal The parent transformation !16 CS447 Natural Language Processing Markov PCFGs (Collins parser) The RHS of each CFG rule consists of: one head HX, n left sisters Li and m right sisters Ri: Replace rule probabilities with a generative process: For each nonterminal X -generate its head HX (nonterminal or terminal) -then generate its left sisters L1..n and a STOP symbol conditioned on HX -then generate its right sisters R1...n and a STOP symbol conditioned on HX X →Ln...L1 ! ""# $ left sisters HX R1...Rm ! ""# $ right sisters !17 CS447 Natural Language Processing Lexicalization PCFGs can’t distinguish between “eat sushi with chopsticks” and “eat sushi with tuna”. We need to take words into account! P(VPeat → VP PPwith chopsticks | VPeat ) vs. P(VPeat → VP PPwith tuna | VPeat ) Problem: sparse data (PPwith fatty|white|... tuna....) Solution: only take head words into account! Assumption: each constituent has one head word. !18 CS447 Natural Language Processing At the root (start symbol S), generate the head word of the sentence, wS , with P(wS) Lexicalized rule probabilities: Every nonterminal is lexicalized: Xwx Condition rules Xwx → αYβ on the lexicalized LHS Xwx P( Xwx → αYβ | Xwx) Word-word dependencies: For each nonterminal Y in RHS of a rule Xwx → αYβ, condition wY (the head word of Y ) on X and wX: P( wY | Y, X, wX ) Lexicalized PCFGs !19 CS447 Natural Language Processing Dealing with unknown words A lexicalized PCFG assigns zero probability to any word that does not appear in the training data. Solution: Training: Replace rare words in training data with a token ‘UNKNOWN’. Testing: Replace unseen words with ‘UNKNOWN’ !20 CS447 Natural Language Processing Reﬁning the set of categories Unlexicalized Parsing (Klein & Manning ’03) Unlexicalized PCFGs with various transformations of the training data and the model, e.g.: – Parent annotation (of terminals and nonterminals): distinguish preposition IN from subordinating conjunction IN etc. – Add head tag to nonterminals (e.g. distinguish ﬁnite from inﬁnite VPs) – Add distance features Accuracy: 86.3 Precision and 85.1 Recall The Berkeley parser (Petrov et al. ’06, ’07) Automatically learns reﬁnements of the nonterminals Accuracy: 90.2 Precision, 89.9 Recall !21 CS447: Natural Language Processing (J. Hockenmaier) Summary The Penn Treebank has a large number of very ﬂat rules. Accurate parsing requires modiﬁcations to the basic PCFG model: reﬁning the nonterminals, relaxing the independence assumptions by including grandparent information, modeling word-word dependencies, etc. How much of this transfers to other treebanks or languages? !22 CS447: Natural Language Processing (J. Hockenmaier) Dependency Grammar !23 CS447 Natural Language Processing A dependency parse !24 2 CHAPTER 1. INTRODUCTION Figure 1.1: Dependency structure for an English sentence. The basic assumption underlying all varieties of dependency grammar is the idea that syntactic structure essentially consists of words linked by binary, asymmetrical relations called dependency relations (or dependencies for short). A dependency relation holds between a syntactically subordinate word,called the dependent,and another word on which it depends,called the head.1 This is illustrated in ﬁgure 1.1, which shows a dependency structure for a simple English sentence, where dependency relations are represented by arrows pointing from the head to the dependent.2 Moreover, each arrow has a label, indicating the dependency type. For example, the noun news is a dependent of the verb had with the dependency type subject (SBJ) By contrast the noun effect is a dependent of type object Dependencies are (labeled) asymmetrical binary relations between two lexical items (words). CS447 Natural Language Processing Dependency grammar Word-word dependencies are a component of many (most/all?) grammar formalisms. Dependency grammar assumes that syntactic structure consists only of dependencies. Many variants. Modern DG began with Tesniere (1959). DG is often used for free word order languages. DG is purely descriptive (not a generative system like CFGs etc.), but some formal equivalences are known. !25 CS447 Natural Language Processing Head-argument: eat sushi Arguments may be obligatory, but can only occur once. The head alone cannot necessarily replace the construction. Head-modiﬁer: fresh sushi Modiﬁers are optional, and can occur more than once. The head alone can replace the entire construction. Head-speciﬁer: the sushi Between function words (e.g. prepositions, determiners) and their arguments. Syntactic head ≠ semantic head Coordination: sushi and sashimi Unclear where the head is. Different kinds of dependencies !26 CS447 Natural Language Processing Dependency structures Dependencies form a graph over the words in a sentence. This graph is connected (every word is a node) and (typically) acyclic (no loops). Single-head constraint: Every node has at most one incoming edge. This implies that the graph is a rooted tree. !27 CS447 Natural Language Processing From CFGs to dependencies Assume each CFG rule has one head child (bolded) The other children are dependents of the head. S → NP VP VP is head, NP is a dependent VP → V NP NP NP → DT NOUN NOUN → ADJ N The headword of a constituent is the terminal that is reached by recursively following the head child. (here, V is the head word of S, and N is the head word of NP). If in rule XP → X Y, X is head child and Y dependent, the headword of Y depends on the headword of X. The maximal projection of a terminal w is the highest nonterminal in the tree that w is headword of. Here, Y is a maximal projection. !28 CS447 Natural Language Processing Context-free grammars CFGs capture only nested dependencies The dependency graph is a tree The dependencies do not cross !29 CS447 Natural Language Processing Beyond CFGs: Nonprojective dependencies Dependencies: tree with crossing branches Arise in the following constructions - (Non-local) scrambling (free word order languages) Die Pizza hat Klaus versprochen zu bringen - Extraposition (The guy is coming who is wearing a hat) - Topicalization (Cheeseburgers, I thought he likes) !30 CS447 Natural Language Processing Dependency Treebanks Dependency treebanks exist for many languages: Czech Arabic Turkish Danish Portuguese Estonian .... Phrase-structure treebanks (e.g. the Penn Treebank) can also be translated into dependency trees (although there might be noise in the translation) !31 CS447 Natural Language Processing The Prague Dependency Treebank Three levels of annotation: morphological: [<2M tokens] Lemma (dictionary form) + detailed analysis (15 categories with many possible values = 4,257 tags) surface-syntactic (“analytical”): [1.5M tokens] Labeled dependency tree encoding grammatical functions (subject, object, conjunct, etc.) semantic (“tectogrammatical”): [0.8M tokens] Labeled dependency tree for predicate-argument structure, information structure, coreference (not all words included) (39 labels: agent, patient, origin, effect, manner, etc....) !32 CS447 Natural Language Processing Examples: analytical level !33 CS447 Natural Language Processing Turkish is an agglutinative language with free word order. Rich morphological annotations Dependencies (next slide) are at the morpheme level Very small -- about 5000 sentences METU-Sabanci Turkish Treebank !34 CS447 Natural Language Processing [this and prev. example from Kemal Oﬂazer’s talk at Rochester, April 2007] !35 METU-Sabanci Turkish Treebank CS447 Natural Language Processing Universal Dependencies 37 syntactic relations, intended to be applicable to all languages (“universal”), with slight modiﬁcations for each speciﬁc language, if necessary. http://universaldependencies.org !36 CS447 Natural Language Processing Universal Dependency Relations Nominal core arguments: nsubj (nominal subject), obj (direct object), iobj (indirect object) Clausal core arguments: csubj (clausal subject), ccomp (clausal object [“complement”]) Non-core dependents: advcl (adverbial clause modiﬁer), aux (auxiliary verb), Nominal dependents: nmod (nominal modiﬁer), amod (adjectival modiﬁer), Coordination: cc (coordinating conjunction), conj (conjunct) and many more… !37 CS447 Natural Language Processing Parsing algorithms for DG ‘Transition-based’ parsers: learn a sequence of actions to parse sentences Models: State = stack of partially processed items + queue/buffer of remaining tokens + set of dependency arcs that have been found already Transitions (actions) = add dependency arcs; stack/queue operations ‘Graph-based’ parsers: learn a model over dependency graphs Models: a function (typically sum) of local attachment scores For dependency trees, you can use a minimum spanning tree algorithm !38 CS447 Natural Language Processing Transition-based parsing (Nivre et al.) !39 CS447 Natural Language Processing Transition-based parsing: assumptions This algorithm works for projective dependency trees. Dependency tree: Each word has a single parent (Each word is a dependent of [is attached to] one other word) Projective dependencies: There are no crossing dependencies. For any i, j, k with i < k < j: if there is a dependency between wi and wj, the parent of wk is a word wl between (possibly including) i and j: i ≤ l ≤ j, while any child wm of wk has to occur between (excluding) i and j: i<m<j !40 wi wk wj wi wk wj the parent of wk: one of wi…wj any child of wk: one of wi+1…wj-1 CS447 Natural Language Processing Transition-based parsing Transition-based shift-reduce parsing processes the sentence S = w0w1...wn from left to right. Unlike CKY, it constructs a single tree. Notation: w0 is a special ROOT token. VS = {w0, w1, ..., wn} is the vocabulary of the sentence R is a set of dependency relations The parser uses three data structures: σ: a stack of partially processed words wi ∈ VS β: a buffer of remaining input words wi ∈ VS A: a set of dependency arcs (wi, r, wj) ∈ VS × R ×VS !41 CS447 Natural Language Processing Parser conﬁgurations (σ, β, A) The stack σ is a list of partially processed words We push and pop words onto/off of σ. σ|w : w is on top of the stack. Words on the stack are not (yet) attached to any other words. Once we attach w, w can’t be put back onto the stack again. The buffer β is the remaining input words We read words from β (left-to-right) and push them onto σ w|β : w is on top of the buffer. The set of arcs A deﬁnes the current tree. We can add new arcs to A by attaching the word on top of the stack to the word on top of the buffer, or vice versa. !42 CS447 Natural Language Processing Parser conﬁgurations (σ, β, A) We start in the initial conﬁguration ([w0], [w1,..., wn], {}) (Root token, Input Sentence, Empty tree) We can attach the ﬁrst word (w1) to the root token w0, or we can push w1 onto the stack. (w0 is the only token that can’t get attached to any other word) We want to end in the terminal conﬁguration ([], [], A) (Empty stack, Empty buffer, Complete tree) Success! We have read all of the input words (empty buffer) and have attached all input words to some other word (empty stack) !43 CS447 Natural Language Processing Transition-based parsing We process the sentence S = w0w1...wn from left to right (“incremental parsing”) In the parser conﬁguration (σ|wi, wj|β, A): wi is on top of the stack. wi may have some children wj is on top of the buffer. wj may have some children wi precedes wj ( i < j ) We have to either attach wi to wj, attach wj to wi, or decide that there is no dependency between wi and wj If we reach (σ|wi, wj|β, A), all words wk with i < k < j have already been attached to a parent wm with i ≤ m ≤ j !44 CS447 Natural Language Processing Parser actions (σ, β, A): Parser conﬁguration with stack σ, buffer β, set of arcs A (w, r, w’): Dependency with head w, relation r and dependent w’ SHIFT: Push the next input word wi from the buffer β onto the stack σ (σ, wi|β, A) ⇒ (σ|wi, β, A) LEFT-ARCr: … wi…wj… (dependent precedes the head) Attach dependent wi (top of stack σ) to head wj (top of buffer β) with relation r from wj to wi. Pop wi off the stack. (σ|wi, wj|β, A) ⇒ (σ, wj|β, A ∪ {(wj, r, wi)}) RIGHT-ARCr: …wi…wj … (dependent follows the head) Attach dependent wj (top of buffer β) to head wi (top of stack σ) with relation r from wi to wj. Move wi back to the buffer (σ|wi, wj|β, A) ⇒ (σ, wi|β, A ∪ {(wi, r, wj)}) !45 CS447 Natural Language Processing An example sentence & parse !46 2 CHAPTER 1. INTRODUCTION Figure 1.1: Dependency structure for an English sentence. The basic assumption underlying all varieties of dependency grammar is the idea that syntactic structure essentially consists of words linked by binary, asymmetrical relations called dependency relations (or dependencies for short). A dependency relation holds between a syntactically subordinate word,called the dependent,and another word on which it depends,called the head.1 This is illustrated in ﬁgure 1.1, which shows a dependency structure for a simple English sentence, where dependency relations are represented by arrows pointing from the head to the dependent.2 Moreover, each arrow has a label, indicating the dependency type. For example, the noun news is a dependent of the verb had with the dependency type subject (SBJ) By contrast the noun effect is a dependent of type object CS447 Natural Language Processing !47 Economic news had little effect on financial markets . CS447 Natural Language Processing !48 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !49 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !50 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !51 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !52 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !53 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !54 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !55 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !56 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !57 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !58 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !59 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !60 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !61 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !62 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !63 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !64 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !65 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !66 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !67 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing Transition-based parsing in practice Which action should the parser take under the current conﬁguration? We also need a parsing model that assigns a score to each possible action given a current conﬁguration. -Possible actions: SHIFT, and for any relation r: LEFT-ARCr, or RIGHT-ARCr -Possible features of the current conﬁguration: The top {1,2,3} words on the buffer and on the stack, their POS tags, etc. We can learn this model from a dependency treebank. !68 "
268,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 12: Dependency Parsing; Expressive Grammars CS447: Natural Language Processing (J. Hockenmaier) Dependency Parsing !2 CS447 Natural Language Processing A dependency parse !3 2 CHAPTER 1. INTRODUCTION Figure 1.1: Dependency structure for an English sentence. The basic assumption underlying all varieties of dependency grammar is the idea that syntactic structure essentially consists of words linked by binary, asymmetrical relations called dependency relations (or dependencies for short). A dependency relation holds between a syntactically subordinate word,called the dependent,and another word on which it depends,called the head.1 This is illustrated in ﬁgure 1.1, which shows a dependency structure for a simple English sentence, where dependency relations are represented by arrows pointing from the head to the dependent.2 Moreover, each arrow has a label, indicating the dependency type. For example, the noun news is a dependent of the verb had with the dependency type subject (SBJ) By contrast the noun effect is a dependent of type object Dependencies are (labeled) asymmetrical binary relations between two lexical items (words). CS447 Natural Language Processing Parsing algorithms for DG ‘Transition-based’ parsers: learn a sequence of actions to parse sentences Models: State = stack of partially processed items + queue/buffer of remaining tokens + set of dependency arcs that have been found already Transitions (actions) = add dependency arcs; stack/queue operations ‘Graph-based’ parsers: learn a model over dependency graphs Models: a function (typically sum) of local attachment scores For dependency trees, you can use a minimum spanning tree algorithm !4 CS447 Natural Language Processing Transition-based parsing (Nivre et al.) !5 CS447 Natural Language Processing Transition-based parsing: assumptions This algorithm works for projective dependency trees. Dependency tree: Each word has a single parent (Each word is a dependent of [is attached to] one other word) Projective dependencies: There are no crossing dependencies. For any i, j, k with i < k < j: if there is a dependency between wi and wj, the parent of wk is a word wl between (possibly including) i and j: i ≤ l ≤ j, while any child wm of wk has to occur between (excluding) i and j: i<m<j !6 wi wk wj wi wk wj the parent of wk: one of wi…wj any child of wk: one of wi+1…wj-1 CS447 Natural Language Processing Transition-based parsing Transition-based shift-reduce parsing processes the sentence S = w0w1...wn from left to right. Unlike CKY, it constructs a single tree. Notation: w0 is a special ROOT token. VS = {w0, w1, ..., wn} is the vocabulary of the sentence R is a set of dependency relations The parser uses three data structures: σ: a stack of partially processed words wi ∈ VS β: a buffer of remaining input words wi ∈ VS A: a set of dependency arcs (wi, r, wj) ∈ VS × R ×VS !7 CS447 Natural Language Processing Parser conﬁgurations (σ, β, A) The stack σ is a list of partially processed words We push and pop words onto/off of σ. σ|w : w is on top of the stack. Words on the stack are not (yet) attached to any other words. Once we attach w, w can’t be put back onto the stack again. The buffer β is the remaining input words We read words from β (left-to-right) and push them onto σ w|β : w is on top of the buffer. The set of arcs A deﬁnes the current tree. We can add new arcs to A by attaching the word on top of the stack to the word on top of the buffer, or vice versa. !8 CS447 Natural Language Processing Parser conﬁgurations (σ, β, A) We start in the initial conﬁguration ([w0], [w1,..., wn], {}) (Root token, Input Sentence, Empty tree) We can attach the ﬁrst word (w1) to the root token w0, or we can push w1 onto the stack. (w0 is the only token that can’t get attached to any other word) We want to end in the terminal conﬁguration ([], [], A) (Empty stack, Empty buffer, Complete tree) Success! We have read all of the input words (empty buffer) and have attached all input words to some other word (empty stack) !9 CS447 Natural Language Processing Transition-based parsing We process the sentence S = w0w1...wn from left to right (“incremental parsing”) In the parser conﬁguration (σ|wi, wj|β, A): wi is on top of the stack. wi may have some children wj is on top of the buffer. wj may have some children wi precedes wj ( i < j ) We have to either attach wi to wj, attach wj to wi, or decide that there is no dependency between wi and wj If we reach (σ|wi, wj|β, A), all words wk with i < k < j have already been attached to a parent wm with i ≤ m ≤ j !10 CS447 Natural Language Processing Parser actions (σ, β, A): Parser conﬁguration with stack σ, buffer β, set of arcs A (w, r, w’): Dependency with head w, relation r and dependent w’ SHIFT: Push the next input word wi from the buffer β onto the stack σ (σ, wi|β, A) ⇒ (σ|wi, β, A) LEFT-ARCr: … wi…wj… (dependent precedes the head) Attach dependent wi (top of stack σ) to head wj (top of buffer β) with relation r from wj to wi. Pop wi off the stack. (σ|wi, wj|β, A) ⇒ (σ, wj|β, A ∪ {(wj, r, wi)}) RIGHT-ARCr: …wi…wj … (dependent follows the head) Attach dependent wj (top of buffer β) to head wi (top of stack σ) with relation r from wi to wj. Move wi back to the buffer (σ|wi, wj|β, A) ⇒ (σ, wi|β, A ∪ {(wi, r, wj)}) !11 CS447 Natural Language Processing An example sentence & parse !12 2 CHAPTER 1. INTRODUCTION Figure 1.1: Dependency structure for an English sentence. The basic assumption underlying all varieties of dependency grammar is the idea that syntactic structure essentially consists of words linked by binary, asymmetrical relations called dependency relations (or dependencies for short). A dependency relation holds between a syntactically subordinate word,called the dependent,and another word on which it depends,called the head.1 This is illustrated in ﬁgure 1.1, which shows a dependency structure for a simple English sentence, where dependency relations are represented by arrows pointing from the head to the dependent.2 Moreover, each arrow has a label, indicating the dependency type. For example, the noun news is a dependent of the verb had with the dependency type subject (SBJ) By contrast the noun effect is a dependent of type object CS447 Natural Language Processing !13 Economic news had little effect on financial markets . CS447 Natural Language Processing !14 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !15 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !16 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !17 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !18 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !19 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !20 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !21 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !22 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !23 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !24 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !25 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !26 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !27 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !28 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !29 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !30 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !31 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !32 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing !33 APTER 3. TRANSITION-BASED PARSING Transition Conﬁguration ([root], [Economic, . . . , .], ∅) SH ⇒([root, Economic], [news, . . . , .], ∅) LAatt ⇒([root], [news, . . . , .], A1 = {(news, ATT, Economic)}) SH ⇒([root, news], [had, . . . , .], A1) LAsbj ⇒([root], [had, . . . , .], A2 = A1∪{(had, SBJ, news)}) SH ⇒([root, had], [little, . . . , .], A2) SH ⇒([root, had, little], [effect, . . . , .], A2) LAatt ⇒([root, had], [effect, . . . , .], A3 = A2∪{(effect, ATT, little)}) SH ⇒([root, had, effect], [on, . . . , .], A3) SH ⇒([root, . . . on], [ﬁnancial, markets, .], A3) SH ⇒([root, . . . , ﬁnancial], [markets, .], A3) LAatt ⇒([root, . . . on], [markets, .], A4 = A3∪{(markets, ATT, ﬁnancial)}) RApc ⇒([root, had, effect], [on, .], A5 = A4∪{(on, PC, markets)}) RAatt ⇒([root, had], [effect, .], A6 = A5∪{(effect, ATT, on)}) RAobj ⇒([root], [had, .], A7 = A6∪{(had, OBJ, effect)}) SH ⇒([root, had], [.], A7) RApu ⇒([root], [had], A8 = A7∪{(had, PU, .)}) RApred ⇒([ ], [root], A9 = A8∪{(root, PRED, had)}) SH ⇒([root], [ ], A9) e 3.2: Transition sequence for the English sentence in ﬁgure 1.1 (LAr = Left-Arcr, R t-Arcr, SH = Shift). Economic news had little effect on financial markets . CS447 Natural Language Processing Transition-based parsing in practice Which action should the parser take under the current conﬁguration? We also need a parsing model that assigns a score to each possible action given a current conﬁguration. -Possible actions: SHIFT, and for any relation r: LEFT-ARCr, or RIGHT-ARCr -Possible features of the current conﬁguration: The top {1,2,3} words on the buffer and on the stack, their POS tags, distances between the words, etc. We can learn this model from a dependency treebank. !34 CS447 Natural Language Processing Expressive Grammars !35 CS447 Natural Language Processing Why grammar? Surface string Mary saw John Meaning representation Logical form: saw(Mary,John) Grammar Parsing Generation Pred-arg structure: PRED saw AGENT Mary PATIENT John Dependency graph: saw Mary John !36 CS447 Natural Language Processing Grammar formalisms Formalisms provide a language in which linguistic theories can be expressed and implemented Formalisms deﬁne elementary objects (trees, strings, feature structures) and recursive operations which generate complex objects from simple objects. Formalisms may impose constraints (e.g. on the kinds of dependencies they can capture) !37 CS447 Natural Language Processing How do grammar formalisms differ? Formalisms deﬁne different representations Tree-adjoining Grammar (TAG): Fragments of phrase-structure trees Lexical-functional Grammar (LFG): Annotated phrase-structure trees (c-structure) linked to feature structures (f-structure) Combinatory Categorial Grammar (CCG): Syntactic categories paired with meaning representations Head-Driven Phrase Structure Grammar(HPSG): Complex feature structures (Attribute-value matrices) !38 CS447 Natural Language Processing The dependencies so far: Arguments: Verbs take arguments: subject, object, complements, ... Heads subcategorize for their arguments Adjuncts/Modiﬁers: Adjectives modify nouns, adverbs modify VPs or adjectives, PPs modify NPs or VPs Modiﬁers subcategorize for the head Typically, these are local dependencies: they can be expressed within individual CFG rules VP → Adv Verb NP !39 CS447 Natural Language Processing CFGs capture only nested dependencies The dependency graph is a tree The dependencies do not cross Context-free grammars !40 CS447 Natural Language Processing Dependencies form a tree with crossing branches Beyond CFGs: Nonprojective dependencies !41 CS447 Natural Language Processing Non-projective dependencies (Non-local) scrambling: In a sentence with multiple verbs, the argument of a verb appears in a different clause from that which contains the verb (arises in languages with freer word order than English) Die Pizza hat Klaus versprochen zu bringen The pizza has Klaus promised to bring Klaus has promised to bring the pizza Extraposition: Here, a modiﬁer of the subject NP is moved to the end of the sentence The guy is coming who is wearing a hat Compare with the non-extraposed variant The [guy [who is wearing a hat]] is coming Topicalization: Here, the argument of the embedded verb is moved to the front of the sentence. Cheeseburgers, I [thought [he likes]] !42 CS447 Natural Language Processing Dependencies form a DAG (a node may have multiple incoming edges) Arise in the following constructions: - Control (He has promised me to go), raising (He seems to go) - Wh-movement (the man who you saw yesterday is here again), - Non-constituent coordination (right-node raising, gapping, argument-cluster coordination) Beyond CFGs: Nonlocal dependencies !43 CS447 Natural Language Processing Dependency structures Nested (projective) dependency trees (CFGs) Non-projective dependency trees Non-local dependency graphs !44 CS447 Natural Language Processing Non-local dependencies !45 CS447 Natural Language Processing Bounded long-range dependencies: Limited distance between the head and argument Unbounded long-range dependencies: Arbitrary distance (within the same sentence) between the head and argument Unbounded long-range dependencies cannot (in general) be represented with CFGs. Chomsky’s solution: Add null elements (and co-indexation) Long-range dependencies !46 CS447 Natural Language Processing Unbounded nonlocal dependencies Wh-questions and relative clauses contain unbounded nonlocal dependencies, where the missing NP may be arbitrarily deeply embedded: ‘the sushi that [you told me [John saw [Mary eat]]]’ ‘what [did you tell me [John saw [Mary eat]]]?’ Linguists call this phenomenon wh-extraction (wh-movement). !47 CS447 Natural Language Processing Non-local dependencies in wh-extraction !48 NP NP SBAR S IN VP NP S VP NP V V the sushi that you told NP me John saw S VP NP V Mary eat CS447 Natural Language Processing The trace analysis of wh-extraction !49 NP NP NP SBAR S IN VP NP S VP NP V V the sushi that you told NP me John saw S VP NP V Mary eat *T* trace CS447 Natural Language Processing Slash categories for wh-extraction Because only one element can be extracted, we can use slash categories. This is still a CFG: the set of nonterminals is ﬁnite. Generalized Phrase Structure Grammar (GPSG), Gazdar et al. (1985) !50 NP NP SBAR S/NP IN VP/NP NP S/NP VP/NP NP V V the sushi that you told NP me John saw S/NP VP/NP NP V Mary eat CS447 Natural Language Processing German: center embedding ...daß ich [Hans schwimmen] sah ...that I Hans swim saw ...that I saw [Hans swim] ...daß ich [Maria [Hans schwimmen] helfen] sah ...that I Maria Hans swim help saw ...that I saw [Mary help [Hans swim]] ...daß ich [Anna [Maria [Hans schwimmen] helfen] lassen] sah ...that I Anna Maria Hans swim help let saw ...that I saw [Anna let [Mary help [Hans swim]]] !51 CS447 Natural Language Processing Dutch: cross-serial dependencies ...dat ik Hans zag zwemmen ...that I Hans saw swim ...that I saw [Hans swim] ...dat ik Maria Hans zag helpen zwemmen ...that I Maria Hans saw help swim ...that I saw [Mary help [Hans swim]] ...dat ik Anna Maria Hans zag laten helpen zwemmen ...that I Anna Maria Hans saw let help swim ...that I saw [Anna let [Mary help [Hans swim]]] Such cross-serial dependencies require mildly context-sensitive grammars !52 CS447 Natural Language Processing Two mildly context-sensitive formalisms: TAG and CCG !53 CS447 Natural Language Processing Recursively enumerable The Chomsky Hierarchy Context-sensitive Mildly context-sensitive Context-free Regular !54 CS447 Natural Language Processing Mildly context-sensitive grammars Contain all context-free grammars/languages Can be parsed in polynomial time (TAG/CCG: O(n6)) (Strong generative capacity) capture certain kinds of dependencies: nested (like CFGs) and cross-serial (like the Dutch example), but not the MIX language: MIX: the set of strings w ∈ {a, b, c}* that contain equal numbers of as, bs and cs Have the constant growth property: the length of strings grows in a linear way The power-of-2 language {a2n} does not have the constant growth propery. !55 CS447 Natural Language Processing TAG and CCG are lexicalized formalisms The lexicon: -pairs words with elementary objects -speciﬁes all language-speciﬁc information (e.g. subcategorization information) The grammatical operations: -are universal -deﬁne (and impose constraints on) recursion. !56 CS447 Natural Language Processing A (C)CG derivation !57 CCG categories are deﬁned recursively: - Categories are atomic (S, NP) or complex (S\NP, (S\NP)/NP) - Complex categories (X/Y or X\Y) are functions: X/Y combines with an adjacent argument to its right of category Y to return a result of category X. Function categories can be composed, giving more expressive power than CFGs More on CCG in one of our Semantics lectures! CS447 Natural Language Processing Tree-Adjoining Grammar !58 CS447 Natural Language Processing (Lexicalized) Tree-Adjoining Grammar AK Joshi and Y Schabes (1996) Tree Adjoining Grammars. In G. Rosenberg and A. Salomaa, Eds., Handbook of Formal Languages TAG is a tree-rewriting formalism: TAG deﬁnes operations (substitution, adjunction) on trees. The elementary objects in TAG are trees (not strings) TAG is lexicalized: Each elementary tree is anchored to a lexical item (word) “Extended domain of locality”: The elementary tree contains all arguments of the anchor. TAG requires a linguistic theory which speciﬁes the shape of these elementary trees. TAG is mildly context-sensitive: can capture Dutch cross-serial dependencies but is still efﬁciently parseable !59 CS447 Natural Language Processing Extended domain of locality S NP VP VBZ NP eats We want to capture all arguments of a word in a single elementary object. We also want to retain certain syntactic structures (e.g. VPs). Our elementary objects are tree fragments: !60 CS447 Natural Language Processing TAG substitution (arguments) Substitute X Y X↓ Y↓ α1: X α2: Y α3: α2 α3 α1 Derivation tree: Derived tree: !61 CS447 Natural Language Processing ADJOIN TAG adjunction X X* X X X* Auxiliary tree Foot node α1: β1: α1 β1 Derived tree: Derivation tree: !62 CS447 Natural Language Processing The effect of adjunction TIG: sister adjunction TAG: wrapping adjunction No adjunction: TSG (Tree substitution grammar) TSG is context-free Sister adjunction: TIG (Tree insertion grammar) TIG is also context-free, but has a linguistically more adequate treatment of modiﬁers Wrapping adjunction: TAG (Tree-adjoining grammar) TAG is mildy context-sensitive !63 CS447 Natural Language Processing A small TAG lexicon S NP VP VBZ NP eats α1: NP John α2: VP RB VP* always β1: NP tapas α3: !64 CS447 Natural Language Processing A TAG derivation S NP VP VBZ NP eats NP John NP tapas VP RB VP* always NP NP NP NP α2: α1: β1: α3: α1 α3 α2 !65 CS447 Natural Language Processing A TAG derivation S NP VP VBZ NP eats tapas VP RB VP* always John VP VP α1 α3 α2 β1 β1 !66 CS447 Natural Language Processing A TAG derivation S NP VBZ VP NP eats tapas VP RB VP* always John !67 CS447 Natural Language Processing anbn: Cross-serial dependencies Elementary trees: Deriving aabb S a b S S* S a b S S a b S S a b S S* S a b S S S a b S S !68 CS447 Natural Language Processing Feature Structure Grammars !69 CS447 Natural Language Processing Simple grammars overgenerate This generates ungrammatical sentences like “these student eats a cakes” We need to capture (number/person) agreement S → NP VP VP → Verb NP NP → Det Noun Det → the | a | these Verb → eat |eats Noun → cake |cakes | student | students !70 CS447 Natural Language Processing Reﬁning the nonterminals This yields very large grammars. What about person, case, ...? Difﬁcult to capture generalizations. Subject and verb have to have number agreement NPsg, NPpl and NP are three distinct nonterminals S → NPsg VPsg S → NPpl VPpl VPsg → VerbSg NP VPpl → VerbPl NP NPsg → DetSg NounSg DetSg → the | a ... ... ... !71 CS447 Natural Language Processing Replace atomic categories with feature structures: A feature structure is a list of features (= attributes), e.g. CASE, and values (eg NOM). We often represent feature structures as attribute value matrices (AVM) Usually, values are typed (to avoid CASE:SG) Feature structures !72 CS447 Natural Language Processing Feature structures as directed graphs !73 = NP Sg 3 PERS Nom CASE NUM CAT CS447 Natural Language Processing Complex feature structures We distinguish between atomic and complex feature values. A complex value is a feature structure itself. This allows us to capture better generalizations. Only atomic values: !74 Complex values: CS447 Natural Language Processing Feature paths A feature path allows us to identify particular values in a feature structure: 〈NP CAT〉 = NP 〈NP AGR CASE〉 = NOM !75 NP: CS447 Natural Language Processing Two feature structures A and B unify ( A ⊔ B) if they can be merged into one consistent feature structure C: Otherwise, uniﬁcation fails: Uniﬁcation !76 CS447 Natural Language Processing CFG rules are augmented with constraints: A0 → A1 ... An {set of constraints} There are two kinds of constraints: Uniﬁcation constraints: 〈Ai feature-path〉 = 〈 Aj feature-path〉 Value constraints: 〈Ai feature-path〉 = atomic value PATR-II style feature structures !77 CS447 Natural Language Processing Lexical entry Constraints Grammar rule Constraints Grammar rule Constraints S →NP VP 〈NP NUM〉 = 〈VP NUM〉 〈NP CASE〉 = nom NP →DT NOUN 〈NP NUM〉 = 〈NOUN NUM〉 〈NP CASE〉 = 〈NOUN CASE〉 NOUN →cake 〈NOUN NUM〉 = sg A grammar with feature structures !78 CS447 Natural Language Processing Lexical entry Constraints Grammar rule Constraints Grammar rule Constraints S →NP VP 〈NP AGR〉 = 〈VP AGR〉 〈NP CASE〉 = nom NP →DT NOUN 〈NP AGR〉 = 〈NOUN AGR〉 NOUN →cake 〈NOUN AGR NUM〉 = sg With complex feature structures !79 Complex feature structures capture better generalizations (and hence require fewer constraints) — cf. the previous slide CS447 Natural Language Processing Attribute-Value Grammars and CFGs If every feature can only have a ﬁnite set of values, any attribute-value grammar can be compiled out into a (possibly huge) context-free grammar !80 CS447 Natural Language Processing Going beyond CFGs The power-of-2 language: L2 = {wi | i is a power of 2} L2 is a (fully) context-sensitive language. (Mildly context-sensitive languages have the constant growth property (the length of words always increases by a constant factor c)) Here is a feature grammar which generates L2: !81 A ! a hA Fi = 1 A ! A1 A2 hA Fi = hA1i hA Fi = hA2i CS447 Natural Language Processing Today’s key concepts Transition-based dependency parsing for projective dependency trees Going beyond projective dependencies: non-projective dependencies non-local dependencies Expressive Grammars TAG CCG Feature-Structure Grammars !82 "
269,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 15: Compositional Semantics CS447: Natural Language Processing (J. Hockenmaier) Admin !2 CS447: Natural Language Processing (J. Hockenmaier) 0.00 5.00 10.00 15.00 20.00 25.00 0.0 5.0 10.0 15.0 20.0 25.0 Midterm results !3 Undergrads (black) Grads (blue) 100% = 22 points Ugrad median: 17.9 points ≈ B (Same scale for grads) x x CS447: Natural Language Processing (J. Hockenmaier) Midterm results Converting points to percentages: - 22 out of 25 points = 100% (you will soon see both in Compass) Converting points/percentages to letter grades: The ﬁnal conversion will be based on the total percentage at the end of the semester (MPs, midterm, ﬁnal, (project)) I use the undergrads’ performance as yardstick for everybody If I had to give letter grades for this midterm, here is a rough scale: - You would need 19 points (~86%) or more to get an A - The undergrad median (17.9 points = 81.4%) would correspond to a B letter grade - You would need at least 40% (9 points) to pass the class. !4 CS447: Natural Language Processing Regrade requests We will post solutions on the class website (securely). We will accept regrade requests until Nov 9. !5 CS447: Natural Language Processing (J. Hockenmaier) How can you do better? Come to class, and participate. Spend time with the material after each lecture. Read the textbook. Use Piazza. Come to ofﬁce hours. Let us know if you struggle. !6 CS447: Natural Language Processing (J. Hockenmaier) 4th Credit hour: Proposal Upload a one-page PDF to Compass by this Friday -written in LaTeX (not MS Word) -with full bibliography of the papers you want to read or base your project on (ideally with links to online versions; add url-ﬁeld to your bibtex ﬁle) -include a motivation of why you have chosen those papers -for a research project: tell me whether you have the data you need, what existing software you will be using, what you will have to implement yourself. -mention any questions/concerns that you may have -include your names and NetId/Illinois emails -one proposal per project is ﬁne. !7 CS447: Natural Language Processing Back to the material… !8 CS447: Natural Language Processing Semantics In order to understand language, we need to know its meaning. -What is the meaning of a word? (Lexical semantics) -What is the meaning of a sentence? ([Compositional] semantics) -What is the meaning of a longer piece of text? (Discourse semantics) !9 CS447: Natural Language Processing Natural language conveys information about the world We can compare statements about the world with the actual state of the world: Champaign is in California. (false) We can learn new facts about the world from natural language statements: The earth turns around the sun. We can answer questions about the world: Where can I eat Korean food on campus? !10 CS447: Natural Language Processing We draw inferences from natural language statements Some inferences are purely linguistic: All blips are foos. Blop is a blip. ____________ Blop is a foo (whatever that is). Some inferences require world knowledge. Mozart was born in Salzburg. Mozart was born in Vienna. _______________________ No, that can’t be - these are different cities. !11 CS447: Natural Language Processing Today’s lecture Our initial question: What is the meaning of (declarative) sentences? Declarative sentences: “John likes coffee”. (We won’t deal with questions (“Who likes coffee?”) and imperative sentences (commands: “Drink up!”)) Follow-on question 1: How can we represent the meaning of sentences? Follow-on question 2: How can we map a sentence to its meaning representation? !12 CS447: Natural Language Processing What do nouns and verbs mean? In the simplest case, an NP is just a name: John Names refer to entities in the world. Verbs deﬁne n-ary predicates: depending on the arguments they take (and the state of the world), the result can be true or false. !13 CS447: Natural Language Processing What do sentences mean? Declarative sentences (statements) can be true or false, depending on the state of the world: John sleeps. In the simplest case, the consist of a verb and one or more noun phrase arguments. Principle of compositionality (Frege): The meaning of an expression depends on the meaning of its parts and how they are put together. !14 CS447: Natural Language Processing First-order predicate logic (FOL) as a meaning representation language !15 CS447: Natural Language Processing Predicate logic expressions Terms: refer to entities Variables: x, y, z Constants: John’, Urbana’ Functions applied to terms (fatherOf(John’)’) Predicates: refer to properties of, or relations between, entities tall’(x), eat’(x,y), … Formulas: can be true or false Atomic formulas: predicates, applied to terms: tall’(John’) Complex formulas: constructed recursively via logical connectives and quantiﬁers !16 CS447: Natural Language Processing Formulas Atomic formulas are predicates, applied to terms: book(x), eat(x,y) Complex formulas are constructed recursively by ...negation (¬): ¬book(John’) ...connectives (⋀,⋁,→): book(y) ⋀ read(x,y) conjunction (and): φ⋀ψ disjunction (or): φ⋁ψ implication (if): φ→ψ ...quantiﬁers (∀x, ∃x) universal (typically with implication) ∀x[φ(x) →ψ(x)] existential (typically with conjunction) ∃x[φ(x)], ∃x[φ(x) ⋀ψ(x)] Interpretation: formulas are either true or false. !17 CS447: Natural Language Processing The syntax of FOL expressions Term ⇒ Constant | Variable | Function(Term,...,Term) Formula ⇒ Predicate(Term, ...Term) | ¬ Formula | ∀ Variable Formula | ∃ Variable Formula | Formula ∧ Formula | Formula ∨ Formula | Formula → Formula !18 CS447: Natural Language Processing Some examples !19 John is a student: student(john) All students take at least one class: ∀x student(x) ⟶ ∃y(class(y) ∧ takes(x,y)) There is a class that all students take: ∃y(class(y) ∧ ∀x (student(x) ⟶ takes(x,y)) CS447: Natural Language Processing FOL is sufﬁcient for many Natural Language inferences All blips are foos. ∀x blip(x) → foo(x) Blop is a blip. blip(blop) ____________ ____________ Blop is a foo foo(blop) Some inferences require world knowledge. Mozart was born in Salzburg. bornIn(Mozart, Salzburg) Mozart was born in Vienna. bornIn(Mozart, Vienna) ______________________ ______________________ No, that can’t be- bornIn(Mozart, Salzburg) these are different cities ∧¬bornIn(Mozart, Salzburg) !20 CS447: Natural Language Processing Not all of natural language can be expressed in FOL: Tense: It was hot yesterday. I will go to Chicago tomorrow. Modals: You can go to Chicago from here. Other kinds of quantiﬁers: Most students hate 8:00am lectures. !21 CS447: Natural Language Processing λ-Expressions We often use λ-expressions to construct complex logical formulas: -λx.φ(..x...) is a function where x is a variable, and φ some FOL expression. -β-reduction (called λ-reduction in textbook): Apply λx.φ(..x...) to some argument a: (λx.φ(..x...) a) ⇒ φ(..a...) Replace all occurrences of x in φ(..x...) with a -n-ary functions contain embedded λ-expressions: λx.λy.λz.give(x,y,z) !22 CS447 Natural Language Processing CCG: the machinery Categories: specify subcat lists of words/constituents. Combinatory rules: specify how constituents can combine. The lexicon: speciﬁes which categories a word can have. Derivations: spell out process of combining constituents. !23 CS447: Natural Language Processing (Combinatory) Categorial Grammar !24 CS447 Natural Language Processing CCG categories Simple (atomic) categories: NP, S, PP Complex categories (functions): Return a result when combined with an argument VP, intransitive verb S\NP Transitive verb (S\NP)/NP Adverb (S\NP)\(S\NP) Prepositions ((S\NP)\(S\NP))/NP (NP\NP)/NP PP/NP !25 CS447 Natural Language Processing Forward application (>): (S\NP)/NP NP ⇒> S\NP eats tapas eats tapas Backward application (<): NP S\NP ⇒< S John eats tapas John eats tapas Function application Used in all variants of categorial grammar !26 CS447 Natural Language Processing A (C)CG derivation !27 CS447: Natural Language Processing Function application Combines a function X/Y or X\Y with its argument Y to yield the result X: (S\NP)/NP NP -> S\NP eats tapas eats tapas NP S\NP -> S John eats tapas John eats tapas !28 CS447: Natural Language Processing Type-raising and composition Type-raising: X → T/(T\X) Turns an argument into a function. NP → S/(S\NP) (subject) NP → (S\NP)\((S\NP)/NP) (object) Harmonic composition: X/Y Y/Z → X/Z Composes two functions (complex categories) (S\NP)/PP PP/NP → (S\NP)/NP S/(S\NP) (S\NP)/NP → S/NP Crossing function composition: X/Y Y\Z → X\Z Composes two functions (complex categories) (S\NP)/S S\NP → (S\NP)\NP !29 CS447: Natural Language Processing Type-raising and composition !30 Wh-movement (relative clause): Right-node raising: CS447: Natural Language Processing An example !31 John sees Mary NP (S\NP)/NP NP > S\NP < S CS447: Natural Language Processing Using Combinatory Categorial Grammar (CCG) to map sentences to predicate logic !32 CS447: Natural Language Processing CCG semantics Every syntactic constituent has a semantic interpretation: Every lexical entry maps a word to a syntactic category and a corresponding semantic type: John=(NP, john’ ) Mary= (NP, mary’ ) loves: ((S\NP)/NP λx.λy.loves(x,y)) Every combinatory rule has a syntactic and a semantic part: Function application: X/Y:λx.f(x) Y:a → X:f(a) Function composition: X/Y:λx.f(x) Y/Z:λy.g(y) → X/Z:λz.f(λy.g(y).z) Type raising: X:a → T/(T\X) λf.f(a) !33 CS447: Natural Language Processing An example with semantics !34 John sees Mary NP : John (S\NP)/NP : λx.λy.sees(x,y) NP : Mary > S\NP : λy.sees(Mary,y) < S : sees(Mary,John) CS447: Natural Language Processing Supplementary material: quantiﬁer scope ambiguities in CCG !35 CS447: Natural Language Processing Quantiﬁer scope ambiguity “Every chef cooks a meal” -Interpretation A: For every chef, there is a meal which he cooks. -Interpretation B: There is some meal which every chef cooks. !36 ⇤y[meal(y)⌅⇥x[chef(x) →cooks(y,x)]] ⇥x[chef(x) →⇤y[meal(y)⌅cooks(y,x)]] CS447: Natural Language Processing !37 Every chef cooks a meal (S/(S\NP))/N N (S\NP)/NP ((S\NP)\((S\NP)/NP))/N N λPλQ.⇤x[Px ⇥Qx] λz.chef(z) λu.λv.cooks(u,v) λPλQ⌅y[Py⇧Qy] λz.meal(z) > > S/(S\NP) (S\NP)\((S\NP)/NP) λQ.⇤x[λz.chef(z)x ⇥Qx] λQ⌅y[λz.meal(z)y⇧Qy] ≡λQ.⇤x[chef(x) ⇥Qx] ≡λQλw.⌅y[meal(y)⇧Qyw] < S\NP λw.⌅y[meal(y)⇧λuλv.cooks(u,v)yw] ≡λw.⌅y[meal(y)⇧cooks(y,w)] > S : ⇤x[chef(x) ⇥λw.⌅y[meal(y)⇧cooks(y,w)]x] ≡⇤x[chef(x) ⇥⌅y[meal(y)⇧cooks(y,x)]] Interpretation A CS447: Natural Language Processing !38 Every chef cooks a meal (S/(S\NP))/N N (S\NP)/NP (S\(S/NP))/N N λPλQ.⇤x[Px ⇥Qx] λz.chef(z) λu.λv.cooks(u,v) λPλQ⌅y[Py⇧Qy] λz.meal(z) > > S/(S\NP) S\(S/NP) λQ⇤x[λz.chef(z)x ⇥Qx] λQ⌅y[λz.meal(z)y⇧Qy] ≡λQ⇤x[chef(x) ⇥Qx] ≡λQ⌅y[meal(y)⇧Qy] >B S/NP λw.⇤x[chef(x) ⇥λuλv.cooks(u,v)wx] ≡λw.⇤x[chef(x) ⇥cooks(w,x)] < S⌅y[meal(y)⇧λw.⇤x[chef(x) ⇥cooks(y,w)]x] ≡⌅y[meal(y)⇧⇤x[chef(x) ⇥cooks(y,x)]] Interpretation B CS447: Natural Language Processing !39 Additional topics Representing events and temporal relations: - Add event variables e to represent the events described by verbs, and temporal variables t to represent the time at which an event happens. Other quantiﬁers: - What about “most | at least two | … chefs”? Underspeciﬁed representations: - Which interpretation of “Every chef cooks a meal” is correct? This might depend on context. Let the parser generate an underspeciﬁed representation from which both readings can be computed. Going beyond single sentences: - How do we combine the interpretations of single sentences? "
27,"School of Computer Science © Eric Xing @ CMU, 2005-2014 1 Probabilistic Graphical Models Approximate Inference: Parallel MCMC Eric Xing Lecture XX, April 23rd, 2014 Recap of MCMC l Markov Chain Monte Carlo methods use adaptive proposals Q(x’|x) to sample from the true distribution P(x) l Metropolis-Hastings allows you to specify any proposal Q(x’|x) l But choosing a good Q(x’|x) requires care l Gibbs sampling sets the proposal Q(x’|x) to the conditional distribution P(x’|x) l Acceptance rate always 1! © Eric Xing @ CMU, 2005-2014 2 Parallel MCMC for Large Scales l Datasets and models can be very large l Millions to billions of data points l Millions to billions of random variables l Compute time measured in CPU-years l Need GBs to TBs of memory l E.x. Yahoo web graph has ~1.4 billion nodes and 6.6 billion edges l Imagine doing a Markov Random Field on that network l Without parallelism, we cannot use large datasets and models! l Today: how to use multiple CPUs and machines in MCMC © Eric Xing @ CMU, 2005-2014 3 Taking Multiple Chains l Proper use of MCMC actually requires parallelism l To determine convergence, you need to take multiple MCMC chains l Chains are independent, so you can run one chain per CPU l Once converged, you can combine samples from all chains © Eric Xing @ CMU, 2005-2014 4 Chain on core 1 Chain on core 2 Chain on core 3 Not converged Converged Taking Multiple Chains l Taking multiple chains doesn’t solve all issues, though l If burn-in is long, then all chains will take a long time to converge! l We need a way to take each sample faster… © Eric Xing @ CMU, 2005-2014 5 Chain on core 1 Chain on core 2 Chain on core 3 Not converged Converged Idea: Run Gibbs Sampling in Parallel? l Recall the alarm network l Initialize all variables at t = 0 to False l Idea: parallel Gibbs sample all variables at step t conditioned on t-1 6 t B E A J M 0 F F F F F 1 2 3 4 © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Sampling P(B|A,E) at t = 1: Using Bayes Rule, l (A,E) = (F,F), so we compute the following, and sample B = F 7 t B E A J M 0 F F F F F 1 F 2 3 4 ) ( ) , | ( ) , | ( B P E B A P E A B P ∝ 9980 . 0 ) 999 . 0 )( 999 . 0 ( ) , | ( 0006 . 0 ) 01 . 0 )( 06 . 0 ( ) , | ( = ∝ = = = = ∝ = = = F E F A F B P F E F A T B P © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Sampling P(E|A,B): Using Bayes Rule, l (A,B) = (F,F), so we compute the following, and sample E = T 8 t B E A J M 0 F F F F F 1 F T 2 3 4 ) ( ) , | ( ) , | ( E P E B A P B A E P ∝ 9970 . 0 ) 998 . 0 )( 999 . 0 ( ) , | ( 0142 . 0 ) 02 . 0 )( 71 . 0 ( ) , | ( = ∝ = = = = ∝ = = = F B F A F E P F B F A T E P © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Notice the difference l Normal Gibbs sampling: compute P(E|A,B) based on Bt=1, At=0 l Naïve Parallel GS: compute P(E|A,B) based on Bt=0, At=0 l At step t, always condition on t-1 instead of most recently sampled value 9 t B E A J M 0 F F F F F 1 F T 2 3 4 © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Sampling P(A|B,E,J,M): Using Bayes Rule, l (B,E,J,M) = (F,F,F,F), so we compute the following, and sample A = F 10 t B E A J M 0 F F F F F 1 F T F 2 3 4 ) , | ( ) | ( ) | ( ) , , , | ( E B A P A M P A J P M J E B A P ∝ 9396 . 0 ) 999 . 0 )( 99 . 0 )( 95 . 0 ( ) , , , | ( 00003 . 0 ) 001 . 0 )( 3 . 0 )( 1 . 0 ( ) , , , | ( = ∝ = = = = = = ∝ = = = = = F M F J F E F B F A P F M F J F E F B T A P © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Sampling P(J|A): No need to apply Bayes Rule l A = F, so we compute the following, and sample J = T 11 t B E A J M 0 F F F F F 1 F T F T 2 3 4 95 . 0 ) | ( 05 . 0 ) | ( ∝ = = ∝ = = F A F J P F A T J P © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l Sampling P(M|A): No need to apply Bayes Rule l A = F, so we compute the following, and sample M = F 12 t B E A J M 0 F F F F F 1 F T F T F 2 3 4 99 . 0 ) | ( 01 . 0 ) | ( ∝ = = ∝ = = F A F M P F A T M P © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l We just finished sampling variables t=1 l Why is the sampling parallelizable? l We only conditioned on variable state at t=0, which is known in advance! l We can sample B,E,A,J,M on separate processors, without having to send information between processors 13 t B E A J M 0 F F F F F 1 F T F T F 2 3 4 © Eric Xing @ CMU, 2005-2014 Naïve Parallel Gibbs Sampling l In practice, works very well for some graphical models l E.g. collapsed Gibbs Sampling for LDA l Just assign different zi’s to different processors or machines l But there’s a problem… © Eric Xing @ CMU, 2005-2014 14 Where Naïve Parallel GS Fails l Naïve Parallel GS may not converge to the stationary distribution l Consider the following Bayes Net: l Essentially an XOR relation between (A,B) and (A,C) l Joint distribution P(A,B,C) has only 8 states, so we can compute the stationary distribution. It is dominated by 2 equally-probable states: l (A,B,C) = (T,F,T) and (A,B,C) = (F,T,F) © Eric Xing @ CMU, 2005-2014 15 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l Let’s initialize (A,B,C) = (F,F,F) and see what happens when we naively Gibbs sample in parallel… 16 t A B C 0 F F F 1 2 3 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l Sampling P(A|B,C): l (B,C) = (F,F) so we sample A = T 17 t A B C 0 F F F 1 T 2 3 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 ) | ( ) | ( ) , | ( A C P A B P C B A P ∝ 0 ) 001 . 0 )( 001 . 0 ( ) , | ( 1 ) 999 . 0 )( 999 . 0 ( ) , | ( ≈ ∝ = = = ≈ ∝ = = = F C F B F A P F C F B T A P Where Naïve Parallel GS Fails l Sampling P(B|A): No need to apply Bayes Rule l A = F so we sample B = T 18 t A B C 0 F F F 1 T T 2 3 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 0 ) 001 . 0 ( ) | ( 1 ) 999 . 0 ( ) | ( ≈ ∝ = = ≈ ∝ = = F A F B P F A T B P Where Naïve Parallel GS Fails l Sampling P(C|A): No need to apply Bayes Rule l A = F so we sample C = T 19 t A B C 0 F F F 1 T T T 2 3 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 0 ) 001 . 0 ( ) | ( 1 ) 999 . 0 ( ) | ( ≈ ∝ = = ≈ ∝ = = F A F C P F A T C P Where Naïve Parallel GS Fails l Easy to see that at t=2, we will get (A,B,C) = (F,F,F) 20 t A B C 0 F F F 1 T T T 2 F F F 3 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l Easy to see that at t=2, we will get (A,B,C) = (F,F,F) l At t=3, (A,B,C) = (T,T,T) 21 t A B C 0 F F F 1 T T T 2 F F F 3 T T T 4 © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l Easy to see that at t=2, we will get (A,B,C) = (F,F,F) l At t=3, (A,B,C) = (T,T,T) l At t=4, (A,B,C) = (F,F,F) 22 t A B C 0 F F F 1 T T T 2 F F F 3 T T T 4 F F F © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l Easy to see that at t=2, we will get (A,B,C) = (F,F,F) l At t=3, (A,B,C) = (T,T,T) l At t=4, (A,B,C) = (F,F,F) l Can you see the problem? 23 t A B C 0 F F F 1 T T T 2 F F F 3 T T T 4 F F F © Eric Xing @ CMU, 2005-2014 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Where Naïve Parallel GS Fails l We know the stationary distribution is [(F,T,F), (T,F,T)] l But naïve parallel GS gets stuck in [(T,T,T), (F,F,F)] l Naïve parallel GS performs poorly on near-discrete distributions l What is the correct way to Gibbs sample in parallel? © Eric Xing @ CMU, 2005-2014 24 B A C A P(B) T 0.0001 F 0.9999 A P(C) T 0.0001 F 0.9999 Correct Parallel Gibbs Sampling l Recall that in MRFs, we Gibbs sample by sampling from P(x| MB(x)), the conditional distribution of x given its Markov Blanket MB(x) l For MRFs, the Markov Blanket of x is just its neighbors l In the MRF below, the red node’s Markov Blanket consists of the blue nodes © Eric Xing @ CMU, 2005-2014 25 Correct Parallel Gibbs Sampling l Observe that we can correctly Gibbs sample the two green nodes simultaneously l Neither node is part of the other’s Markov Blanket, so their conditional distributions do not depend on each other l Sampling one of the green nodes doesn’t change the conditional distribution of the other node! © Eric Xing @ CMU, 2005-2014 26 Correct Parallel Gibbs Sampling l How do we generalize this idea to the whole graph? l Find subsets of nodes, such that all nodes in a given subset are not in each other’s Markov Blankets, and the subsets cover the whole graph l The subsets should be as large as possible § Because we can Gibbs sample all nodes in a subset at the same time l At the same time, we want as few subsets as possible § The Markov Blankets of different subsets overlap, so they cannot be sampled at the same time. We must process the subsets sequentially. © Eric Xing @ CMU, 2005-2014 27 Correct Parallel Gibbs Sampling l We can find these covering subsets with k-coloring algorithms (Gonzales et al., 2011) l A k-coloring algorithm colors a graph using k colors, such that: l Every node gets one color l No edge has two nodes of the same color l Trees always admit a 2-coloring (e.g. below) l Assign one color to some node, and alternate colors as you move away © Eric Xing @ CMU, 2005-2014 28 Correct Parallel Gibbs Sampling l Bipartite graphs are always 2-colorable l Color each side of the bipartite graph with opposite colors l e.x. Latent Dirichlet Allocation model is bipartite l However, not all graphs have k-colorings for all k ≥ 2 l In the worst case, a graph with n nodes can require n colors l The full clique is one such graph l Determining if a graph is k-colorable for k > 2 is NP-complete l In practice, we employ heuristics to find k-colorings l Instead of using k-colorings, why not just Gibbs sample all variables at the same time? l The Markov Chain may become non-ergodic, and is no longer guaranteed to converge to the stationary distribution! © Eric Xing @ CMU, 2005-2014 29 Online Parallel MCMC l In “online” algorithms, we need to process new data points one-at-a-time l Moreover, we have to “forget” older data points because memory is finite l For such applications to be viable, we can only afford constant time work per new data point l Otherwise we will reach a point where new data can no longer be processed in a reasonable amount of time l We also want the algorithm to be parallel for scaling up l What MCMC techniques can we use to make an online parallel algorithm? © Eric Xing @ CMU, 2005-2014 30 Sequential Monte Carlo l SMC is a generalization of Particle Filters l Recall that PFs incrementally sample P(Xt|Y1:t), where the Xs are latent r.v.s and the Ys are observations under a state-space model l SMC does not assume the GM is a state-space model, or has any particular structure at all l Suppose we have n r.v.s x1,…,xn l SMC first draws samples from the marginal distribution P(x1), then P(x1:2), and so on until P(x1:n) l Key idea: Construct proposals such that we sample from P(x1:k+1) in constant time, given samples from P(x1:k) l Like other MCMC algorithms, we only require that we can evaluate P’(x1:n) = aP(x1:n) for some unknown a © Eric Xing @ CMU, 2005-2014 31 Sequential Importance Sampling l SIS is the foundation of Sequential Monte Carlo l It allows new variables to be sampled in constant time, without resampling older variables l SIS uses proposal distributions with the following structure: l Notice we can propose xk+1 if we’ve already drawn x1:k, without having to redraw x1:k © Eric Xing @ CMU, 2005-2014 32 ∏ = − − − − = = n k k k k n n n n n n n x x q x q x x q x q x q 2 1 : 1 1 1 1 : 1 1 : 1 1 : 1 ) | ( ) ( ) | ( ) ( ) ( Sequential Importance Sampling l In normalized importance sampling, recall how the sample weights wi are defined: l In SIS, the unnormalized weights r can be rewritten as a telescoping product: © Eric Xing @ CMU, 2005-2014 33 ∑ = i i i P w x f X f ) ( ) ( ) ( ) ( i i i x Q x P r ʹ′ = ∑ = j j i i r r w where and ∏ = − − − − − − − − − = = ʹ′ ʹ′ ʹ′ = ʹ′ = n k k k n n n n n n n n n n n n n n n n n n n n x x r x x r x x q x P x P x q x P x q x P x r 2 : 1 1 1 : 1 1 : 1 1 1 : 1 1 : 1 1 : 1 1 : 1 1 1 : 1 1 : 1 : 1 : 1 ) ( ) ( ) ( ) ( ) | ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( α α ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 : 1 − − − ʹ′ ʹ′ = n n n n n n n n n x x q x P x P x α where Sequential Importance Sampling l This means the unnormalized weights r can be computed incrementally l Compute αn and use it to update r(x1:n-1) to r(x1:n) § NB: For this update to be constant time, we also require P’n(x1:n) to be computable from P’n-1(x1:n-1) in constant time l We remember the unnormalized weights r at each iteration, and compute the normalized weights w as needed from r l Thus, we can sample x AND compute the normalized weights w using constant time per new variable xn l So SIS meets the requirements for an online inference algorithm! l Even better, the samples don’t depend on each other l Assign one CPU core per sample to make the SIS algorithm parallel! © Eric Xing @ CMU, 2005-2014 34 ∏ = = n k k k n x x r x r 2 : 1 1 1 : 1 ) ( ) ( ) ( α ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 : 1 − − − ʹ′ ʹ′ = n n n n n n n n n x x q x P x P x α where Sequential Importance Sampling l SIS algorithm: l At time n = 1 l Parallel draw samples xi 1 ~ q1(x1) l Parallel compute unnormalized weights l Compute normalized weights wi 1 by normalizing ri 1 § Although this step is sequential, it takes almost no time to perform l At time n ≥ 2 l Parallel draw samples xi n ~ qn(xn|xi 1:n-1) l Parallel compute unnorm. wgts. l Compute normalized weights wi n by normalizing ri n § Although this step is sequential, it takes almost no time to perform © Eric Xing @ CMU, 2005-2014 35 ) ( / ) ( 1 1 1 1 1 i i i x q x P r ʹ′ = ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 1 : 1 1 i n i n n i n n i n n i n i n n i n i n x x q x P x P r x r r − − − − − ʹ′ ʹ′ = = α Sequential Importance Sampling l But we are not done yet! l Unfortunately, SIS suffers from a severe drawback: the variance of the samples increases exponentially with n! l See eq (31) of Doucet’s SMC tutorial for an example l Resampling at each iteration will decrease the sample variance! l Similar to weighted resampling from the first MC lecture! © Eric Xing @ CMU, 2005-2014 36 Multinomial Resampling l Suppose we have m samples x1,…,xm with corresponding importance weights w1,…,wm l Construct a categorical distribution from these samples: l This distribution has m categories (choices) l The probability of drawing category k is wk l Drawing category k gets us xk l To resample, just draw N times from this distribution l Note that N can be greater/less than m! l For more advanced strategies such as systematic and residual resampling, refer to page 13 of Doucet’s SMC tutorial © Eric Xing @ CMU, 2005-2014 37 Why Resample? l Apart from decreasing variance, there are other reasons… l Resampling removes samples xk with low weights wk l Low-weight samples come from low-probability regions of P(x) l We want to focus computation on high-probability regions of P(x) l Notice that each sample gets an equal amount of computation, regardless of its weight wk l Resampling ensures that more computation is spent on samples xk that come from high-probability regions of P(x) l Resampling prevents a small number of samples xk from dominating the empirical distribution l Resampling resets all weights wk to 1/N l This prevents sample weights wk from growing until they reach 1 © Eric Xing @ CMU, 2005-2014 38 Sequential Monte Carlo l The SMC algorithm is just SIS with resampling: l At time n = 1 l Parallel draw samples xi 1 ~ q1(x1) l Parallel compute unnormalized weights l Compute normalized weights wi 1 by normalizing ri 1 l Parallel resample wi 1, xi 1 into N equally-weighted particles xi 1 l At time n ≥ 2 l Parallel draw samples xi n ~ qn(xn|xi 1:n-1) l Parallel compute unnorm. wgts. l Compute normalized weights wi n by normalizing ri n l Parallel resample wi n,xi 1:n into N equally-weighted particles xi 1:n © Eric Xing @ CMU, 2005-2014 39 ) ( / ) ( 1 1 1 1 1 i i i x q x P r ʹ′ = ) | ( ) ( ) ( ) ( 1 : 1 1 : 1 1 : 1 1 : 1 1 i n i n n i n n i n n i n i n n i n i n x x q x P x P r x r r − − − − − ʹ′ ʹ′ = = α Summary l Parallel Gibbs sampling l Naïve strategy: sample all variables at the same time l Correct strategy: perform graph colorings and sample same-colored nodes in parallel l Sequential Monte Carlo l Uses incremental proposal distributions l Provides a framework for designing online, parallel MCMC algorithms © Eric Xing @ CMU, 2005-2014 40 Parallel Inference for Bayesian Nonparametric l Dirichlet Process Mixture Model (recap) l Inference schemes (recap) l Parallel inference schemes l Results © Eric Xing @ CMU, 2005-2014 41 Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 42 People sit on the table with the most preferred dish/color Finite Mixture Model:- Restaurant Perspective l Table: l Cluster l People: l Items to be clustered l Parameters: l Dish/color on each table l Center of each cluster l Hidden Variable: l Assignment of people to each table © Eric Xing @ CMU, 2005-2014 43 Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 44 People sit on the table with the most preferred dish/color Which clustering algorithm will it lead to? Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 45 People sit on the table with the most preferred dish/color Which clustering algorithm will it lead to? Hard Kmeans Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 46 People sit on the table proportional to appreciation of dish/color Which clustering algorithm will it lead to? Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 47 People sit on the table proportional to appreciation of dish/color Which clustering algorithm will it lead to? Soft Kmeans Soft Kmeans Generative Model © Eric Xing @ CMU, 2005-2014 48 Zi Xi i = 1, … N ηk H k = 1, … K for k=1, … K ηk ~ H for i=1, … N Zi ~ U(1,K) Xi ~ f(ηzi) appreciation of dish/color Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 49 People sit on the table proportional to appreciation of dish/color and number of people sitting on the table Which clustering algorithm will it lead to? Dirichlet Distribution Mixture Model Finite MM Generative Model © Eric Xing @ CMU, 2005-2014 50 Zi Xi i = 1, … N ηk H k = 1, … K for k=1, … K ηk ~ H θ ~ Dir(α) for i=1, … N Zi ~ Mul(θ) Xi ~ f(ηzi) θ α Finite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 51 People sit on the table proportional to appreciation of dish/color and number of people sitting on the table Which clustering algorithm will it lead to? Dirichlet Distribution Mixture Model Infinite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 52 People sit on the table proportional to appreciation of dish/color and number of people sitting on the table Infinite Mixture Model:- Restaurant Perspective © Eric Xing @ CMU, 2005-2014 53 People sit on the table proportional to appreciation of dish/color and number of people sitting on the table Turning the definition © Eric Xing @ CMU, 2005-2014 54 Proportional to selecting a table Dish on the table Stick Breaking Construction © Eric Xing @ CMU, 2005-2014 55 Take a stick of unit length Step 2:- Break it into two parts Step 3:- Choose a dish Step 1:- Step 4:- Go to step 2 Proportional to selecting a table Dish Stick Breaking Construction © Eric Xing @ CMU, 2005-2014 56 Proportional to selecting a table Dish on the table Graphical Model Representation © Eric Xing @ CMU, 2005-2014 57 Proportional to number of customer sitting on the table Which table each customer sit at Which dish is selected at each table Dirichlet Process Mixture Model Inference l Gibbs Sampling:- l Sample each of the variable given the rest. l Variables to sample are table proportion Vk , table assignment to each customer (Z) and dish at each table η © Eric Xing @ CMU, 2005-2014 58 Inference l Gibbs Sampling:- l Sample each of the variable given the rest. l Variables to sample are table proportion Vk , table assignment to each customer (Z) and dish at each table η l Parallel inference: Easy © Eric Xing @ CMU, 2005-2014 59 Inference l Gibbs Sampling:- l Sample each of the variable given the rest. l Variables to sample are table proportion Vk , table assignment to each customer (Z) and dish at each table η l Parallel inference: Easy l Poor mixing © Eric Xing @ CMU, 2005-2014 60 Inference l Collapsed Gibbs Sampler:- l Integrate out Vk and ηk l Leads to better mixing l Parallel inference: Hard © Eric Xing @ CMU, 2005-2014 61 Inference l Collapsed Gibbs suffer from large computational cost © Eric Xing @ CMU, 2005-2014 62 Running Example: 10 million data points to be clustered. Inference l Variational Inference l Approximate the posterior with a distribution belonging to a more manageable family of distribution l Parallel inference: Easy l Search within a restricted class of models, looses the expressiveness l Typically less accuracy than MCMC methods © Eric Xing @ CMU, 2005-2014 63 Inference l Sequential Monte Carlo Method:- l Keep a pools of particles, approximate the distribution using weighted combination of the pool l Parallel inference: Easy l High variance for naïve implementation, needs resampling (MCMC ) © Eric Xing @ CMU, 2005-2014 64 Parallel MCMC l Naïve l Run collapsed sampler on individual core l Combine the result approximately !! © Eric Xing @ CMU, 2005-2014 65 Restaurant 1 Restaurant P Parallel MCMC l Naïve l Run collapsed sampler on individual core l Combine the result approximately !! l How l Why should two newly discovered clustered in two different processor be the same? © Eric Xing @ CMU, 2005-2014 66 Parallel MCMC l Idea: Dirichlet Mixture of Dirichlet processes are Dirichlet processes l Skeptic (proof coming) © Eric Xing @ CMU, 2005-2014 67 Parallel MCMC l Idea: Dirichlet Mixture of Dirichlet processes are Dirichlet processes © Eric Xing @ CMU, 2005-2014 68 Restaurant 1 Restaurant P Parallel MCMC l Idea: Dirichlet Mixture of Dirichlet processes are Dirichlet processes © Eric Xing @ CMU, 2005-2014 69 Restaurant 1 Restaurant P Parallel MCMC l Idea: Dirichlet Mixture of Dirichlet processes are Dirichlet processes © Eric Xing @ CMU, 2005-2014 70 Restaurant 1 Restaurant P Auxiliary Variable Model For DP l The generative process is as follows :- © Eric Xing @ CMU, 2005-2014 71 Proof l If and Then posterior distribution is given by: l If and , Then © Eric Xing @ CMU, 2005-2014 72 Inference l Conditioned on the Restaurant allocation data are distributed according to P independent Dirichlet process l Perform local collapsed gibbs sampling on the independent DPs l For the global parameters perform MH l Select a cluster ‘c’ and a processor ‘p’ l Propose: move ‘c’ to ‘p’ l Acceptance ratio depends on cluster size l Can pass the indices of the cluster item. l Can be done asynchronously without affecting the performance. © Eric Xing @ CMU, 2005-2014 73 Result © Eric Xing @ CMU, 2005-2014 74 Extension to HDP © Eric Xing @ CMU, 2005-2014 75 Take home message l Naïve parallel inference scheme does not always work l Utilize structure of the problem: Conditional independence l Exact parallel inference or bound on error © Eric Xing @ CMU, 2005-2014 76 "
270,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 16: More on Compositional Semantics, Verb Semantics CS447: Natural Language Processing Admin Midterm: Regrade requests for midterm accepted until Nov 9 Points available on Compass. 22 points = 100% Project/Literature review proposals: Due at the end of day on Monday on Compass One page PDF (in LaTeX, not Word) is sufﬁcient Include your names and NetIDs Include all references (ideally with hyperlinks) Explain what you want to do and why. Include a to-do list For projects: describe what resources you have or need. (Use existing datasets, don’t annotate your own data) !2 CS447: Natural Language Processing Combinatory Categorial Grammar (CCG) !3 CS447 Natural Language Processing CCG categories Simple (atomic) categories: NP, S, PP Complex categories (functions): Return a result when combined with an argument VP, intransitive verb S\NP Transitive verb (S\NP)/NP Adverb (S\NP)\(S\NP) Prepositions ((S\NP)\(S\NP))/NP (NP\NP)/NP PP/NP !4 CCG categories are functions CCG has a few atomic categories, e.g S, NP , PP All other CCG categories are functions: ""5 S Result NP Argument / Dir. Rules: Function application S/NP Function NP Argument ""6 Result S x y · y = x Rules: Function application S\NP Function NP Argument ""7 Result S y · x y = x Rules: Function application (S\NP)/NP Function NP Argument ""8 Result S\NP x y · y = x CS447 Natural Language Processing A (C)CG derivation !9 Rules: Function Composition S/S 1st Function S\NP 2nd Function ""10 S\NP x y · y z = x z Rules: Type-Raising NP ""11 S/(S\NP) y = x x · y = x ! x y "" CS447: Natural Language Processing Type-raising and composition Type-raising: X → T/(T\X) Turns an argument into a function. NP → S/(S\NP) (subject) NP → (S\NP)\((S\NP)/NP) (object) Harmonic composition: X/Y Y/Z → X/Z Composes two functions (complex categories) (S\NP)/PP PP/NP → (S\NP)/NP S/(S\NP) (S\NP)/NP → S/NP Crossing function composition: X/Y Y\Z → X\Z Composes two functions (complex categories) (S\NP)/S S\NP → (S\NP)\NP !12 CS447: Natural Language Processing Type-raising and composition !13 Wh-movement (relative clause): Right-node raising: CS447: Natural Language Processing Using Combinatory Categorial Grammar (CCG) to map sentences to predicate logic !14 CS447: Natural Language Processing λ-Expressions We often use λ-expressions to construct complex logical formulas: -λx.φ(..x...) is a function where x is a variable, and φ some FOL expression. -β-reduction (called λ-reduction in textbook): Apply λx.φ(..x...) to some argument a: (λx.φ(..x...) a) ⇒ φ(..a...) Replace all occurrences of x in φ(..x...) with a -n-ary functions contain embedded λ-expressions: λx.λy.λz.give(x,y,z) !15 CS447: Natural Language Processing CCG semantics Every syntactic constituent has a semantic interpretation: Every lexical entry maps a word to a syntactic category and a corresponding semantic type: John=(NP, john’ ) Mary= (NP, mary’ ) loves: ((S\NP)/NP λx.λy.loves(x,y)) Every combinatory rule has a syntactic and a semantic part: Function application: X/Y:λx.f(x) Y:a → X:f(a) Function composition: X/Y:λx.f(x) Y/Z:λy.g(y) → X/Z:λz.f(λy.g(y).z) Type raising: X:a → T/(T\X) λf.f(a) !16 CS447: Natural Language Processing An example with semantics !17 John sees Mary NP : John (S\NP)/NP : λx.λy.sees(x,y) NP : Mary > S\NP : λy.sees(Mary,y) < S : sees(Mary,John) CS447: Natural Language Processing Supplementary material: quantiﬁer scope ambiguities in CCG !18 CS447: Natural Language Processing Quantiﬁer scope ambiguity “Every chef cooks a meal” -Interpretation A: For every chef, there is a meal which he cooks. -Interpretation B: There is some meal which every chef cooks. !19 ⇤y[meal(y)⌅⇥x[chef(x) →cooks(y,x)]] ⇥x[chef(x) →⇤y[meal(y)⌅cooks(y,x)]] CS447: Natural Language Processing !20 Every chef cooks a meal (S/(S\NP))/N N (S\NP)/NP ((S\NP)\((S\NP)/NP))/N N λPλQ.⇤x[Px ⇥Qx] λz.chef(z) λu.λv.cooks(u,v) λPλQ⌅y[Py⇧Qy] λz.meal(z) > > S/(S\NP) (S\NP)\((S\NP)/NP) λQ.⇤x[λz.chef(z)x ⇥Qx] λQ⌅y[λz.meal(z)y⇧Qy] ≡λQ.⇤x[chef(x) ⇥Qx] ≡λQλw.⌅y[meal(y)⇧Qyw] < S\NP λw.⌅y[meal(y)⇧λuλv.cooks(u,v)yw] ≡λw.⌅y[meal(y)⇧cooks(y,w)] > S : ⇤x[chef(x) ⇥λw.⌅y[meal(y)⇧cooks(y,w)]x] ≡⇤x[chef(x) ⇥⌅y[meal(y)⇧cooks(y,x)]] Interpretation A CS447: Natural Language Processing !21 Every chef cooks a meal (S/(S\NP))/N N (S\NP)/NP (S\(S/NP))/N N λPλQ.⇤x[Px ⇥Qx] λz.chef(z) λu.λv.cooks(u,v) λPλQ⌅y[Py⇧Qy] λz.meal(z) > > S/(S\NP) S\(S/NP) λQ⇤x[λz.chef(z)x ⇥Qx] λQ⌅y[λz.meal(z)y⇧Qy] ≡λQ⇤x[chef(x) ⇥Qx] ≡λQ⌅y[meal(y)⇧Qy] >B S/NP λw.⇤x[chef(x) ⇥λuλv.cooks(u,v)wx] ≡λw.⇤x[chef(x) ⇥cooks(w,x)] < S⌅y[meal(y)⇧λw.⇤x[chef(x) ⇥cooks(y,w)]x] ≡⌅y[meal(y)⇧⇤x[chef(x) ⇥cooks(y,x)]] Interpretation B CS447: Natural Language Processing To summarize… !22 CS447: Natural Language Processing Understanding sentences “Every chef cooks a meal” ⇥x[chef(x) →⇤y[meal(y)⌅cooks(y,x)]] ⇤y[meal(y)⌅⇥x[chef(x) →cooks(y,x)]] We translate sentences into (ﬁrst-order) predicate logic. Every (declarative) sentence corresponds to a proposition, which can be true or false. !23 CS447: Natural Language Processing But… … what can we do with these representations? Being able to translate a sentence into predicate logic is not enough, unless we also know what these predicates mean. Semantics joke (B. Partee): The meaning of life is life’ Compositional formal semantics tells us how to ﬁt together pieces of meaning, but doesn’t have much to say about the meaning of the basic pieces (i.e. lexical semantics) … how do we put together meaning representations of multiple sentences? We need to consider discourse (there are approaches within formal semantics, e.g. Discourse Representation Theory) … Do we really need a complete analysis of each sentence? This is pretty brittle (it’s easy to make a parsing mistake) Can we get a more shallow analysis? !24 CS447: Natural Language Processing Semantic Role Labeling/ Verb Semantics !25 CS447: Natural Language Processing What do verbs mean? Verbs describe events or states (‘eventualities’): Tom broke the window with a rock. The window broke. The window was broken by Tom/by a rock. We want to translate verbs to predicates. But: a naive translation (e.g. subject = ﬁrst argument, object = second argument, etc.) does not capture the differences in meaning break(Tom, window, rock) break(window) break(window, Tom) break(window, rock) !26 CS447: Natural Language Processing Semantic/Thematic roles Verbs describe events or states (‘eventualities’): Tom broke the window with a rock. The window broke. The window was broken by Tom/by a rock. Thematic roles refer to participants of these events: Agent (who performed the action): Tom Patient (who was the action performed on): window Tool/Instrument (what was used to perform the action): rock Semantic/thematic roles (agent, patient) are different from grammatical roles (subject or object). !27 CS447: Natural Language Processing The inventory of thematic roles We need to deﬁne an inventory of thematic roles To create systems that can identify thematic roles automatically, we need to create labeled training data. It is difﬁcult to give a formal deﬁnition of thematic roles that generalizes across all verbs. !28 CS447: Natural Language Processing PropBank and FrameNet Proposition Bank (PropBank): Very coarse argument roles (arg0, arg1,…), used for all verbs (but interpretation depends on the speciﬁc verb) Arg0 = proto-agent Arg1 = proto-patient Arg2...: speciﬁc to each verb ArgM-TMP/LOC/...: temporal/locative/... modiﬁers FrameNet: Verbs fall into classes that deﬁne different kinds of frames (change-position-on-a-scale frame: rise, increase,...). Each frame has its own set of “frame elements” (thematic roles) !29 CS447: Natural Language Processing PropBank agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing [Arg0 The group] agreed [Arg1 it wouldn’t make an offer] [Arg0 John] agrees with [Arg2 Mary] fall.01 Arg1: patient/thing falling Arg2: extent/amount fallen Arg3: start point Arg4: end point [Arg1 Sales] fell [Arg4 to $251 million] [Arg1 Junk bonds] fell [Arg2 by 5%] Semantic role labeling: Recover the semantic roles of verbs (nowadays typically PropBank-style) Machine learning; trained on PropBank Syntactic parses provide useful information !30 CS447: Natural Language Processing Diathesis Alternations Active/passive alternation: Tom broke the window with a rock. (active voice) The window was broken by Tom/by a rock. (passive voice) Causative alternation: Tom broke the window. (‘causative’; active voice) The window broke. (‘anticausative’/‘inchoative’; active voice) Dative alternation Tom gave the gift to Mary. Tom gave Mary the gift. Locative alternation: Jessica loaded boxes into the wagon. Jessica loaded the wagon with boxes. !31 CS447: Natural Language Processing Verb classes Verbs with similar meanings undergo the same syntactic alternations, and have the same set of thematic roles (Beth Levin, 1993) VerbNet (verbs.colorado.edu; Kipper et al., 2008) A large database of verbs, their thematic roles and their alternations !32 "
271,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 17: Vector-space semantics (distributional similarities) CS447: Natural Language Processing (J. Hockenmaier) Where we’re at We have looked at how to obtain the meaning of sentences from the meaning of their words (represented in predicate logic). Now we will look at how to represent the meaning of words (although this won’t be in predicate logic) We will consider different tasks: -Computing the semantic similarity of words by representing them in a vector space -Finding groups of similar words by inducing word clusters -Identifying different meanings of words by word sense disambiguation ""2 CS447: Natural Language Processing (J. Hockenmaier) What we’re going to cover today Pointwise mutual information A very useful metric to identify events that frequency co-occur Distributional (Vector-space) semantics: Measure the semantic similarity of words in terms of the similarity of the contexts in which the words appear -The distributional hypothesis -Representing words as (sparse) vectors -Computing word similarities ""3 CS447: Natural Language Processing (J. Hockenmaier) Using PMI to identify words that “go together” ""4 CS447: Natural Language Processing (J. Hockenmaier) Discrete random variables A discrete random variable X can take on values {x1,…, xn} with probability p(X = xi) A note on notation: p(X) refers to the distribution, while p(X = xi) refers to the probability of a speciﬁc value xi. p(X = xi) also written as p(xi) In language modeling, the random variables correspond to words W or to sequences of words W(1)…W(n). Another note on notation: We’re often sloppy in making the distinction between the i-th word [token] in a sequence/string, and the i-th word [type] in the vocabulary clear. ""5 CS447: Natural Language Processing (J. Hockenmaier) Mutual information I(X;Y) Two random variables X, Y are independent iff their joint distribution is equal to the product of their individual distributions: p( X, Y ) = p( X )p( Y ) That is, for all outcomes x, y: p( X=x, Y=x ) = p( X=x )p( Y=y ) I(X;Y), the mutual information of two random variables X and Y is deﬁned as ""6 I(X; Y ) = X X,Y p(X = x, Y = y) log p(X = x, Y = y) p(X = x)p(Y = y) CS447: Natural Language Processing (J. Hockenmaier) Pointwise mutual information (PMI) Recall that two events x, y are independent if their joint probability is equal to the product of their individual probabilities: x,y are independent iff p(x,y) = p(x)p(y) x,y are independent iff p(x,y)∕p(x)p(y) = 1 In NLP, we often use the pointwise mutual information (PMI) of two outcomes/events (e.g. words): ""7 PMI(x, y) = log p(X = x, Y = y) p(X = x)p(Y = y) CS447: Natural Language Processing (J. Hockenmaier) Using PMI to ﬁnd related words Find pairs of words wi, wj that have high pointwise mutual information: Different ways of deﬁning p(wi, wj) give different answers. ""8 PMI (wi, wj) = log p(wi, wj) p(wi)p(wj) CS447: Natural Language Processing (J. Hockenmaier) Using PMI to ﬁnd “sticky pairs” p(wi, wj): probability that wi, wj are adjacent Deﬁne p(wi, wj) = p(“wiwj”) High PMI word pairs under this deﬁnition: Humpty Dumpty, Klux Klan, Ku Klux, Tse Tung, avant garde, gizzard shad, Bobby Orr, mutatis mutandis, Taj Mahal, Pontius Pilate, ammonium nitrate, jiggery pokery, anciens combattants, fuddle duddle, helter skelter, mumbo jumbo (and a few more) ""9 CS447: Natural Language Processing (J. Hockenmaier) Back to lexical semantics… ""10 CS447: Natural Language Processing (J. Hockenmaier) Different approaches to lexical semantics Lexicographic tradition: -Use lexicons, thesauri, ontologies -Assume words have discrete word senses: bank1 = ﬁnancial institution; bank2 = river bank, etc. -May capture explicit relations between word (senses): “dog” is a “mammal”, etc. Distributional tradition: -Map words to (sparse) vectors that capture corpus statistics -Contemporary variant: use neural nets to learn dense vector “embeddings” from very large corpora (this is a prerequisite for most neural approaches to NLP) -This line of work often ignores the fact that words have multiple senses or parts-of-speech ""11 CS447: Natural Language Processing (J. Hockenmaier) Vector representations of words “Traditional” distributional similarity approaches represent words as sparse vectors [today’s lecture] -Each dimension represents one speciﬁc context -Vector entries are based on word-context co-occurrence statistics (counts or PMI values) Alternative, dense vector representations: -We can use Singular Value Decomposition to turn these sparse vectors into dense vectors (Latent Semantic Analysis) -We can also use neural models to explicitly learn a dense vector representation (embedding) (word2vec, Glove, etc.) Sparse vectors = most entries are zero Dense vectors = most entries are non-zero ""12 CS447: Natural Language Processing (J. Hockenmaier) Distributional Similarities Measure the semantic similarity of words in terms of the similarity of the contexts in which the words appear Represent words as vectors ""13 CS447: Natural Language Processing (J. Hockenmaier) Why do we care about word similarity? Question answering: Q: “How tall is Mt. Everest?” Candidate A: “The ofﬁcial height of Mount Everest is 29029 feet” “tall” is similar to “height” ""14 CS447: Natural Language Processing (J. Hockenmaier) Why do we care about word similarity? Plagiarism detection ""15 CS447: Natural Language Processing (J. Hockenmaier) Why do we care about word contexts? What is tezgüino? A bottle of tezgüino is on the table. Everybody likes tezgüino. Tezgüino makes you drunk. We make tezgüino out of corn. (Lin, 1998; Nida, 1975) The contexts in which a word appears tells us a lot about what it means. ""16 CS447: Natural Language Processing (J. Hockenmaier) The Distributional Hypothesis Zellig Harris (1954): “oculist and eye-doctor … occur in almost the same environments” “If A and B have almost identical environments we say that they are synonyms.” John R. Firth 1957: You shall know a word by the company it keeps. The contexts in which a word appears tells us a lot about what it means. Words that appear in similar contexts have similar meanings ""17 CS447: Natural Language Processing (J. Hockenmaier) Exploiting context for semantics Distributional similarities (vector-space semantics): Use the set of contexts in which words (= word types) appear to measure their similarity Assumption: Words that appear in similar contexts (tea, coffee) have similar meanings. Word sense disambiguation (future lecture) Use the context of a particular occurrence of a word (token) to identify which sense it has. Assumption: If a word has multiple distinct senses (e.g. plant: factory or green plant), each sense will appear in different contexts. ""18 CS447: Natural Language Processing (J. Hockenmaier) Distributional similarities ""19 CS447: Natural Language Processing (J. Hockenmaier) Distributional similarities Distributional similarities use the set of contexts in which words appear to measure their similarity. They represent each word w as a vector w w = (w1, …, wN) ∈ RN in an N-dimensional vector space. -Each dimension corresponds to a particular context cn -Each element wn of w captures the degree to which the word w is associated with the context cn. - wn depends on the co-occurrence counts of w and cn The similarity of words w and u is given by the similarity of their vectors w and u ""20 CS447: Natural Language Processing (J. Hockenmaier) Documents as contexts Let’s assume our corpus consists of a (large) number of documents (articles, plays, novels, etc.) In that case, we can deﬁne the contexts of a word as the sets of documents in which it appears. Conversely, we can represent each document as the (multi)set of words which appear in it. -Intuition: Documents are similar to each other if they contain the same words. -This is useful for information retrieval, e.g. to compute the similarity between a query (also a document) and any document in the collection to be searched. ""21 CS447: Natural Language Processing (J. Hockenmaier) Term-Document Matrix A Term-Document Matrix is a 2D table: -Each cell contains the frequency (count) of the term (word) t in document d: tft,d -Each column is a vector of counts over words, representing a document -Each row is a vector of counts over documents, representing a word ""22 As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 CS447: Natural Language Processing (J. Hockenmaier) Term-Document Matrix Two documents are similar if their vectors are similar Two words are similar if their vectors are similar ""23 As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 CS447: Natural Language Processing (J. Hockenmaier) What is a ‘context’? There are many different deﬁnitions of context that yield different kinds of similarities: Contexts deﬁned by nearby words: How often does w appear near the word drink? Near = “drink appears within a window of ±k words of w”, or “drink appears in the same document/sentence as w” This yields fairly broad thematic similarities. Contexts deﬁned by grammatical relations: How often is (the noun) w used as the subject (object) of the verb drink? (Requires a parser). This gives more ﬁne-grained similarities. ""24 CS447: Natural Language Processing (J. Hockenmaier) Using nearby words as contexts -Decide on a ﬁxed vocabulary of N context words c1..cN Context words should occur frequently enough in your corpus that you get reliable co-occurrence counts, but you should ignore words that are too common (‘stop words’: a, the, on, in, and, or, is, have, etc.) -Deﬁne what ‘nearby’ means For example: w appears near c if c appears within ±5 words of w -Get co-occurrence counts of words w and contexts c -Deﬁne how to transform co-occurrence counts of words w and contexts c into vector elements wn For example: compute (positive) PMI of words and contexts -Deﬁne how to compute the similarity of word vectors For example: use the cosine of their angles. ""25 CS447: Natural Language Processing (J. Hockenmaier) Deﬁning and counting co-occurrence Deﬁning co-occurrences: -Within a ﬁxed window: vi occurs within ±n words of w -Within the same sentence: requires sentence boundaries -By grammatical relations: vi occurs as a subject/object/modiﬁer/… of verb w (requires parsing - and separate features for each relation) Counting co-occurrences: -fi as binary features (1,0): w does/does not occur with vi -fi as frequencies: w occurs n times with vi -fi as probabilities: e.g. fi is the probability that vi is the subject of w. ""26 CS447: Natural Language Processing (J. Hockenmaier) Getting co-occurrence counts Co-occurrence as a binary feature: Does word w ever appear in the context c? (1 = yes/0 = no) Co-occurrence as a frequency count: How often does word w appear in the context c? (0…n times) Typically: 10K-100K dimensions (contexts), very sparse vectors ""27 arts boil data function large sugar water apricot 0 1 0 0 1 1 1 pineapple 0 1 0 0 1 1 1 digital 0 0 1 1 1 0 0 information 0 0 1 1 1 0 0 arts boil data function large sugar water apricot 0 1 0 0 5 2 7 pineapple 0 2 0 0 10 8 5 digital 0 0 31 8 20 0 0 information 0 0 35 23 5 0 0 CS447: Natural Language Processing (J. Hockenmaier) Counts vs PMI Sometimes, low co-occurrences counts are very informative, and high co-occurrence counts are not: -Any word is going to have relatively high co-occurrence counts with very common contexts (e.g. “it”, “anything”, “is”, etc.), but this won’t tell us much about what that word means. -We need to identify when co-occurrence counts are more likely than we would expect by chance. We therefore want to use PMI values instead of raw frequency counts: But this requires us to deﬁne p(w, c), p(w) and p(c) ""28 PMI(w, c) = log p(w, c) p(w)p(c) CS447: Natural Language Processing (J. Hockenmaier) Word-Word Matrix Context: ± 7 words Resulting word-word matrix: f(w, c) = how often does word w appear in context c: “information” appeared six times in the context of “data” ""29 aardvark computer data pinch result sugar … apricot 0 0 0 1 0 1 pineapple 0 0 0 1 0 1 digital 0 2 1 0 1 0 information 0 1 6 0 4 0 CS447: Natural Language Processing (J. Hockenmaier) ""30 p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 pij = fij fij j=1 C ∑ i=1 W ∑ p(wi ) = fij j=1 C ∑ N p(cj ) = fij i=1 W ∑ N p(w=information, c=data) = 6/19 = .32 p(w=information) = 11/19 = .58 p(c=data) = 7/19 = .37 p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 CS447: Natural Language Processing (J. Hockenmaier) Computing PMI of w and c: Using a ﬁxed window of ± k words N: How many tokens does the corpus contain? f(w) ≤ N: How often does w occur? f(w, c) ≤ f(w,): How often does w occur with c in its window? f(c) = ∑wf(w, c) ≤ N: How many tokens have c in their window? p(w) = f(w)/N p(c) = f(c)/N p(w, c) = f(w, c)/N ""31 PMI(w, c) = log p(w, c) p(w)p(c) CS447: Natural Language Processing (J. Hockenmaier) Computing PMI of w and c: w and c in the same sentence N: How many sentences does the corpus contain? f(w) ≤ N: How many sentences contain w? f(w, c) ≤ f(w): How many sentences contain w and c? f(c) ≤ N: How many sentences contain c? p(w) = f(w)/N p(c) = f(c)/N p(w, c) = f(w, c)/N ""32 PMI(w, c) = log p(w, c) p(w)p(c) CS447: Natural Language Processing (J. Hockenmaier) Using grammatical features Observation: verbs have ‘selectional preferences’: E.g. “eat” takes edible things as objects and animate entities as subjects. Exceptions: metonymy (“The VW honked at me” ) and metaphors: “Skype ate my credit” This allows us to induce noun classes: Edible things occur as objects of “eat”. In general, nouns that occur as subjects/objects of speciﬁc verbs tend to be similar. This also allows us to induce verb classes: Verbs that take the same class of nouns as arguments tend to be similar/related. ""33 CS447: Natural Language Processing (J. Hockenmaier) Example: frequencies of grammatical relations 64M word corpus, parsed with Minipar (Lin, 1998) ""34 cell sbj of absorb 1 sbj of adapt 1 sbj of behave 1 ... ... mod of abnormality 3 mod of anemia 8 ... obj of attack 6 obj of call 11 ... CS447: Natural Language Processing (J. Hockenmaier) Measuring association with context -Every element fi of the co-occurrence vector corresponds to some word w’ (and possibly a relation r ): e.g. (r,w’)= (obj-of, attack) -The value of fi should indicate the association strength between (r, w’ ) and w. -What value should feature fi for word w have? Probability P(fi | w): fi will be high for any frequent feature (regardless of w) ""35 CS447: Natural Language Processing (J. Hockenmaier) Frequencies vs. PMI ""36 Count PMI bunch beer 2 12.34 tea 2 11.75 liquid 2 10.53 champagne 4 11.75 anything 3 5.15 it 3 1.25 Objects of ‘drink’ (Lin, 1998) CS447: Natural Language Processing (J. Hockenmaier) Positive Pointwise Mutual Information PMI is negative when words co-occur less than expected by chance. This is unreliable without huge corpora: With P(w1) ≈ P(w2) ≈ 10-6, we can’t estimate whether P(w1,w2) is signiﬁcantly different from 10-12 We often just use positive PMI values, and replace all PMI values < 0 with 0: Positive Pointwise Mutual Information (PPMI): PPMI(w,c) = PMI(w,c) if PMI(w,c) > 0 = 0 if PMI(w,c) ≤ 0 ""37 CS447: Natural Language Processing (J. Hockenmaier) PMI and smoothing PMI is biased towards infrequent events: If P(w, c) = P(w) = P(c), then PMI(w,c) = log(1/P(w)) So PMI(w, c) is larger for rare words w with low P(w). Simple remedy: Add-k smoothing of P(w, c), P(w), P(c) pushes all PMI values towards zero. Add-k smoothing affects low-probability events more, and will therefore reduce the bias of PMI towards infrequent events. (Pantel & Turney 2010) ""38 CS447: Natural Language Processing (J. Hockenmaier) Vector similarity In distributional models, every word is a point in n-dimensional space. How do we measure the similarity between two points/vectors? In general: -Manhattan distance (Levenshtein distance, L1 norm) -Euclidian distance (L2 norm) ""39 distL1(⌅ x, ⌅ y) = N ! i=1 |xi −yi| distL2(⌅ x, ⌅ y) = ⌅ ⇤ ⇤ ⇥ N $ i=1 (xi −yi)2 X Y L1 L2 CS447: Natural Language Processing (J. Hockenmaier) Dot product as similarity If the vectors consist of simple binary features (0,1), we can use the dot product as similarity metric: The dot product is a bad metric if the vector elements are arbitrary features: it prefers long vectors - If one xi is very large (and yi nonzero), sim(x,y) gets very large If the number of nonzero xi and yi s is very large, sim(x,y) gets very large. - Both can happen with frequent words. ""40 simdot−prod(⌅ x, ⌅ y) = N ! i=1 xi × yi length of ⇥ x : |⇥ x| = ⌅ ⇤ ⇤ ⇥ N $ i=1 x2 i CS447: Natural Language Processing (J. Hockenmaier) Vector similarity: Cosine One way to deﬁne the similarity of two vectors is to use the cosine of their angle. The cosine of two vectors is their dot product, divided by the product of their lengths: sim(w, u) = 1: w and u point in the same direction sim(w, u) = 0: w and u are orthogonal sim(w, u) = −1: w and u point in the opposite direction ""41 simcos(⌅ x, ⌅ y) = !N i=1 xi ⇥yi ⇥!N i=1 x2 i ⇥!N i=1 y2 i = ⌅ x · ⌅ y |⌅ x||⌅ y| CS447: Natural Language Processing (J. Hockenmaier) Kullback-Leibler divergence When the vectors x are probabilities, i.e. xi = P( fi | wx), we can measure the distance between the two distributions P and Q The standard metric is Kullback-Leibler divergence D(P||Q) But KL divergence is not very good because it is -Undeﬁned if P(x)=0 and Q(x) ≠ 0. -Asymmetric: D(P||Q) ≠ D(Q||P ) ""42 D(P||Q) = ! x P(x) log P(x) Q(x) CS447: Natural Language Processing (J. Hockenmaier) Jensen/Shannon divergence Instead, we use the Jensen/Shannon divergence: the distance of each distribution from their average. -Average of P and Q: -Jensen/Shannon divergence of P and Q: -As a distance measure between x,y (with xi = P( fi | wx ) ) ""43 JS(P||Q) = D(P||AvgP,Q) + D(Q||AvgP,Q) AvgP,Q(x) = P(x) + Q(x) 2 distJS(⇤ x,⇤ y) = ∑ i xi log2 ! xi (xi +yi)/2 ⇥ +yi log2 ! yi (xi +yi)/2 ⇥ CS447: Natural Language Processing (J. Hockenmaier) More recent developments ""44 CS447: Natural Language Processing (J. Hockenmaier) Neural embeddings There is a lot of recent work on neural-net based word embeddings: word2vec,https://code.google.com/p/word2vec/ Glove http://nlp.stanford.edu/projects/glove/ etc. Using the vectors produced by these word embeddings instead of the raw words themselves can be very beneﬁcial for many tasks. This is currently a very active area of research. ""45 CS447: Natural Language Processing (J. Hockenmaier) Analogies It can be shown that for some of these embeddings, the learned word vectors can capture analogies: Queen::King = Woman::Man In the vector representation: queen ≈ king − man + woman Similar results for e.g. countries and capitals: Germany::Berlin = France::Paris ""46 CS447: Natural Language Processing (J. Hockenmaier) “Semantic spaces”? Does this mean that these vector spaces represent semantics? Yes, but only to some extent. -Different context deﬁnitions (or embeddings) give different vector spaces with different similarities -Often, antonyms (hot/cold, etc.) have very similar vectors. -Vector spaces are not well-suited to capturing hypernym relations (every dog is an animal) We will get back to that when we talk more about lexical semantics. Another open problem: how to get from words to the semantics of sentences ""47 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts Distributional hypothesis Distributional similarities: word-context matrix representing words as vectors positive PMI computing the similarity of word vectors ""48 "
272,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 18: Word Sense CS447: Natural Language Processing (J. Hockenmaier) Next week Julia is away. Wednesday: The TAs will be available (in DCL 1320) to discuss projects. Friday: The TAs will give an introductory lecture on neural networks !2 CS447: Natural Language Processing (J. Hockenmaier) Last Wednesday’s key concepts Distributional hypothesis Distributional similarities: word-context matrix representing words as vectors positive PMI computing the similarity of word vectors !3 CS447: Natural Language Processing Word senses What does ‘bank’ mean? -a ﬁnancial institution (US banks have raised interest rates) -a particular branch of a ﬁnancial institution (the bank on Green Street closes at 5pm) -the bank of a river (In 1927, the bank of the Mississippi ﬂooded) -a ‘repository’ (I donate blood to a blood bank) !4 CS447: Natural Language Processing Lexicon entries !5 lemmas senses CS447: Natural Language Processing Some terminology Word forms: runs, ran, running; good, better, best Any, possibly inﬂected, form of a word (i.e. what we talked about in morphology) Lemma (citation/dictionary form): run A basic word form (e.g. inﬁnitive or singular nominative noun) that is used to represent all forms of the same word. (i.e. the form you’d search for in a dictionary) Lexeme: RUN(V), GOOD(A), BANK1(N), BANK2(N) An abstract representation of a word (and all its forms), with a part-of-speech and a set of related word senses. (Often just written (or referred to) as the lemma, perhaps in a different FONT) Lexicon: A (ﬁnite) list of lexemes !6 CS447: Natural Language Processing Trying to make sense of senses Polysemy: A lexeme is polysemous if it has different related senses bank = financial institution or building Homonyms: Two lexemes are homonyms if their senses are unrelated, but they happen to have the same spelling and pronunciation bank = (financial) bank or (river) bank !7 CS447: Natural Language Processing Relations between senses Symmetric relations: Synonyms: couch/sofa Two lemmas with the same sense Antonyms: cold/hot, rise/fall, in/out Two lemmas with the opposite sense Hierarchical relations: Hypernyms and hyponyms: pet/dog The hyponym (dog) is more speciﬁc than the hypernym (pet) Holonyms and meronyms: car/wheel The meronym (wheel) is a part of the holonym (car) !8 CS447: Natural Language Processing (J. Hockenmaier) WordNet !9 CS447: Natural Language Processing WordNet Very large lexical database of English: 110K nouns, 11K verbs, 22K adjectives, 4.5K adverbs (WordNets for many other languages exist or are under construction) Word senses grouped into synonym sets (“synsets”) linked into a conceptual-semantic hierarchy 81K noun synsets, 13K verb synsets, 19K adj. synsets, 3.5K adv synsets Avg. # of senses: 1.23 nouns, 2.16 verbs, 1.41 adj, 1.24 adverbs Conceptual-semantic relations: hypernym/hyponym also holonym/meronym Also lexical relations, in particular lemmatization Available at http://wordnet.princeton.edu !10 CS447: Natural Language Processing !11 A WordNet example CS447: Natural Language Processing Hypernym/hyponym (between concepts) The more general ‘meal’ is a hypernym of the more speciﬁc ‘breakfast’ Instance hypernym/hyponym (between concepts and instances) Austen is an instance hyponym of author Member holonym/meronym (groups and members) professor is a member meronym of (a university’s) faculty Part holonym/meronym (wholes and parts) wheel is a part meronym of (is a part of) car. Substance meronym/holonym (substances and components) ﬂour is a substance meronym of (is made of) bread !12 Hierarchical synset relations: nouns CS447: Natural Language Processing Hypernym/troponym (between events): travel/ﬂy, walk/stroll Flying is a troponym of traveling: it denotes a speciﬁc manner of traveling Entailment (between events): snore/sleep Snoring entails (presupposes) sleeping !13 Hierarchical synset relations: verbs CS447: Natural Language Processing WordNet Hypernyms and Hyponyms !14 CS447: Natural Language Processing (J. Hockenmaier) Thesaurus-based similarity !15 CS447: Natural Language Processing (J. Hockenmaier) Thesaurus-based word similarity Instead of using distributional methods, rely on a resource like WordNet to compute word similarities. Problem: each word may have multiple entries in WordNet, depending on how many senses it has. We often just assume that the similarity of two words is equal to the similarity of their two most similar senses. NB: There are a few recent attempts to combine neural embeddings with the information encoded in resources like WordNet. Here, we’ll just go quickly over some classic approaches. !16 CS447: Natural Language Processing (J. Hockenmaier) Thesaurus-based word similarity Basic idea: A thesaurus like WordNet contains all the information needed to compute a semantic distance metric. Simplest instance: compute distance in WordNet sim(s, s’) = -log pathlen(s, s’) pathlen(s,s’): number of edges in shortest path between s and s’ Note: WordNet nodes are synsets (=word senses). Applying this to words w, w’: sim(w, w’) = max sim(s, s’) s ∈ Senses(w) s’∈ Senses(w’) !17 CS447: Natural Language Processing (J. Hockenmaier) WordNet path lengths The path length (distance) pathlen(s, s’) between two senses s, s’ is the length of the (shortest) path between them !18 standard currency coinage coin dime money fund scale Richter scale medium of exchange nickel budget CS447: Natural Language Processing (J. Hockenmaier) The lowest common subsumer The lowest common subsumer (ancestor) LCS(s, s’) of two senses s, s’ is the lowest common ancestor node in the hierarchy !19 standard currency coinage coin dime money fund scale Richter scale nickel budget medium of exchange CS447: Natural Language Processing (J. Hockenmaier) WordNet path lengths A few examples: pathlen(nickel, dime) = 2 pathlen(nickel, money) = 5 pathlen(nickel, budget) = 7 But do we really want the following? pathlen(nickel, coin) < pathlen(nickel, dime) pathlen(nickel, Richter scale) = pathlen(nickel, budget) !20 standard medium of exchange currency coinage coin nickel dime money fund budget scale Richter scale CS447: Natural Language Processing (J. Hockenmaier) Information-content similarity Basic idea: Add corpus statistics to thesaurus hierarchy For each concept/sense s (synset node in WordNet), deﬁne: -words(s): the set of words subsumed by (=below) s. All words will be subsumed by the root of the hierarchy -P(s): the probability that a random word in the corpus is an instance of s (Either use a sense-tagged corpus, or count each word as one instance of each of its possible senses) -This deﬁnes the Information content of a sense s: IC(s) = −log P(s) !21 P(s) = ∑w∈words(s) c(w) N CS447: Natural Language Processing (J. Hockenmaier) P(s) and IC(s): examples !22 entity p=0.395 IC=1.3 hill p=.0000189 IC=15.7 coast p=.0000216 IC=15.5 geological formation p=0.00176 IC=9.15 CS447: Natural Language Processing (J. Hockenmaier) Using LCS to compute similarity Resnik (1995)’s similarity metric: simResnik(s,s’) = −log P( LCS(s, s’) ) The underlying intuition: -If sLCS = LCS(s,s’) is the root of the hierarchy, P(sLCS)=1 -The lower sLCS is in the hierarchy, the more speciﬁc it is, and the lower P(sLCS) will be. LCS(car, banana) = physical entity LCS(nickel, dime) = coin Problem: this does not take into account how different s,s’ are LCS(thing, object) = physical entity = LCS(car, banana) !23 CS447: Natural Language Processing (J. Hockenmaier) Better similarity metrics Lin (1998)’s similarity: simLin(s,s’) = 2× log P(sLCS) / [ log P(s) + logP(s’) ] Jiang & Conrath (1997) ’s distance distJC(s,s’) = 2× log P(sLCS) − [ log P(s) + log P(s’) ] simJC(s,s’) = 1/distJC(s, s’) (NB: you don’t have to memorize these for the exam…) !24 CS447: Natural Language Processing (J. Hockenmaier) Problems with thesaurus-based similarity We need to have a thesaurus! (not available for all languages) We need to have a thesaurus that contains the words we’re interested in. We need a thesaurus that captures a rich hierarchy of hypernyms and hyponyms. Most thesaurus-based similarities depend on the speciﬁcs of the hierarchy that is implement in the thesaurus. !25 CS447: Natural Language Processing (J. Hockenmaier) Learning hyponym relations If we don’t have a thesaurus, can we learn that Corolla is a kind of car? Certain phrases and patterns indicate hyponym relations: Hearst(1992) Enumerations: cars such as the Corolla, the Civic, and the Vibe, Appositives: the Corolla , a popular car… We can also learn these patterns if we have some seed examples of hyponym relations (e.g. from WordNet): 1. Take all hyponym/hypernym pairs from WordNet (e.g. car/vehicle) 2. Find all sentences that contain both, and identify patterns 3. Apply these patterns to new data to get new hyponym/hypernym pairs !26 CS447: Natural Language Processing Word Sense Disambiguation !27 CS447: Natural Language Processing What does this word mean? !28 This plant needs to be watered each day. ⇒ living plant This plant manufactures 1000 widgets each day. ⇒ factory Word Sense Disambiguation (WSD): Identify the sense of content words (nouns, verbs, adjectives) in context (assuming a ﬁxed inventory of word senses) Applications: machine translation, question answering, information retrieval, text classiﬁcation CS447: Natural Language Processing The data !29 CS447: Natural Language Processing WSD evaluation Evaluation metrics: -Accuracy: How many instances of the word are tagged with their correct sense? -Precision and recall: How many instances of each sense did we predict/recover correctly? Baseline accuracy: -Choose the most frequent sense per word WordNet: take the ﬁrst (=most frequent) sense -Lesk algorithm (see below) Upper bound accuracy: -Inter-annotator agreement: how often do two people agree ~75-80% for all words task with WordNet, ~90% for simple binary tasks -Pseudo-word task: Replace all occurrences of words wa and wb (door, banana) with a nonsense word wab (banana-door). !30 CS447: Natural Language Processing Dictionary-based WSD: Lesk algorithm (Lesk 1986) !31 CS447: Natural Language Processing Dictionary-based methods We often don’t have a labeled corpus, but we might have a dictionary/thesaurus that contains glosses and examples: bank1 Gloss: a financial institution that accepts deposits and channels the money into lending activities Examples: “he cashed the check at the bank”, “that bank holds the mortgage on my home” bank2 Gloss: sloping land (especially the slope beside a body of water) Examples: “they pulled the canoe up on the bank”, “he sat on the bank of the river and watched the current” !32 CS447: Natural Language Processing The Lesk algorithm Basic idea: Compare the context with the dictionary deﬁnition of the sense. Assign the dictionary sense whose gloss and examples are most similar to the context in which the word occurs. Compare the signature of a word in context with the signatures of its senses in the dictionary Assign the sense that is most similar to the context Signature = set of content words (in examples/gloss or in context) Similarity = size of intersection of context signature and sense signature !33 CS447: Natural Language Processing bank1: Gloss: a financial institution that accepts deposits and channels the money into lending activities Examples: “he cashed the check at the bank”, “that bank holds the mortgage on my home” Signature(bank1) = {financial, institution, accept, deposit, channel, money, lend, activity, cash, check, hold, mortgage, home} bank2: Gloss: sloping land (especially the slope beside a body of water) Examples: “they pulled the canoe up on the bank”, “he sat on the bank of the river and watched the current” Signature(bank2) = {slope, land, body, water, pull, canoe, sit, river, watch, current} Sense signatures (dictionary) !34 CS447: Natural Language Processing Signature of target word Test sentence: “The bank refused to give me a loan.” Simpliﬁed Lesk: Overlap between sense signature and (simple) signature of the target word: Target signature = words in context: {refuse, give, loan} Original Lesk: Overlap between sense signature and augmented signature of the target word Augmented target signature with signatures of words in context {refuse, reject, request,... , give, gift, donate,... loan, money, borrow,...} !35 CS447: Natural Language Processing (J. Hockenmaier) Lesk algorithm: Summary The Lesk algorithm requires an electronic dictionary of word senses (e.g. WordNet) and a lemmatizer. It does not use any machine learning, but it is still a useful baseline. !36 CS447: Natural Language Processing WSD as a learning problem !37 CS447: Natural Language Processing WSD as a learning problem Supervised: -You have a (large) corpus annotated with word senses -Here, WSD is a standard supervised learning task Semi-supervised (bootstrapping) approaches: -You only have very little annotated data (and a lot of raw text) -Here, WSD is a semi-supervised learning task !38 CS447: Natural Language Processing (J. Hockenmaier) WSD as a (binary) classiﬁcation task If w has two different senses, we can treat WSD for w as a binary classiﬁcation problem: Does this occurrence of w have sense A or sense B? If w has multiple senses, we are dealing with a multiclass classiﬁcation problem. We can use labeled training data to train a classiﬁer. Labeled = each instance of w is marked as A or B. This is a kind of supervised learning !39 CS447: Natural Language Processing (J. Hockenmaier) Designing a WSD classiﬁer We represent each occurrence of the word w as a feature vector w Now the elements of w capture the speciﬁc context of the token w In distributional similarities, w provides a summary of all the contexts in which w occurs in the training corpus. !40 CS447: Natural Language Processing Implementing a WSD classiﬁer Basic insight: The sense of a word in a context depends on the words in its context. Features: -Which words in context: all words, all/some content words -How large is the context? sentence, prev/following 5 words -Do we represent context as bag of words (unordered set of words) or do we care about the position of words (preceding/ following word)? -Do we care about POS tags? -Do we represent words as they occur in the text or as their lemma (dictionary form)? !41 CS447: Natural Language Processing A decision list is an ordered list of yes-no questions bass1 = fish vs. bass2 = music: 1. Does ‘fish’ occur in window? - Yes. => bass1 2. Is the previous word ‘striped ’? - Yes. => bass1 3. Does ‘guitar’ occur in window? - Yes. => bass2 4. Is the following word ‘player’? - Yes. => bass2 Learning a decision list for a word with two senses: - Deﬁne a feature set: what kind of questions do you want to ask? - Enumerate all features (questions) the training data gives answers for - Score each feature: Decision lists !42 score(fi) = ! ! ! !log ⇥P(sense1|fi) P(sense2|fi) ⇤! ! ! ! CS447: Natural Language Processing Semi-supervised: Yarowsky algorithm The task: Learn a decision list classiﬁer for each ambiguous word (e.g. “plant”: living/factory?) from lots of unlabeled sentences. Features used by the classiﬁer: - Collocations: “plant life”, “manufacturing plant” - Nearby (± 2-10) words: “animal ”, “automate” Assumption 1: One-sense-per-collocation “plant” in “plant life” always refers to living plants Assumption 2: One-sense-per-discourse A text talks either about living plants or about factories. !43 CS447: Natural Language Processing Yarowsky’s training regime 1. Initialization: - Label a few seed examples. - Train an initial classiﬁer on these seed examples 2. Relabel: - Label all examples with current classiﬁer. - Put all examples that are labeled with high conﬁdence into a new labeled data set. - Optional: apply one-sense-per-discourse to correct mistakes and get additional labels 3. Retrain: - Train a new classiﬁer on the new labeled data set. 4. Repeat 2. and 3. until convergence. !44 CS447: Natural Language Processing Initial state: few labels !45 CS447: Natural Language Processing The initial decision list !46 CS447: Natural Language Processing Intermediate state: more labels !47 CS447: Natural Language Processing Final state: almost everything labeled !48 CS447: Natural Language Processing Initial vs. ﬁnal decision lists !49 CS447: Natural Language Processing (J. Hockenmaier) Summary: Yarowsky algorithm Semi-supervised approach for WSD. Basic idea: -start with some minimal seed knowledge to get a few labeled examples as training data -train a classiﬁer -apply this classiﬁer to new examples -add the most conﬁdently classiﬁed examples to the training data -use heuristics (one-sense-per-discourse) to add even more labeled examples to the training data -retrain the classiﬁer, …. !50 CS447: Natural Language Processing (J. Hockenmaier) Today’s key concepts Word senses polysemy, homonyms hypernyms, hyponyms holonyms, meronyms WordNet as a resource to compute thesaurus-based similarities Word Sense disambiguation Lesk algorithm As a classiﬁcation problem Yarowsky algorithm !51 "
273,"Lecture 20: Neural Networks for NLP Zubin Pahuja zpahuja2@illinois.edu CS447: Natural Language Processing 1 courses.engr.illinois.edu/cs447 Today’s Lecture • Feed-forward neural networks as classifiers • simple architecture in which computation proceeds from one layer to the next • Application to language modeling • assigning probabilities to word sequences and predicting upcoming words CS447: Natural Language Processing 2 Supervised Learning Two kinds of prediction problems: • Regression • predict results with continuous output • e.g. price of a house from its size, number of bedrooms, zip code, etc. • Classification • predict results in a discrete output • e.g. whether user will click on an ad CS447: Natural Language Processing 3 What’s a Neural Network? CS447: Natural Language Processing 4 Why is deep learning taking off? • Unprecedented amount of data • performance of traditional learning algorithms such as SVM, logistic regression plateaus • Faster computation • GPU acceleration • algorithms that train faster and deeper • using ReLU over sigmoid activation • gradient descent optimizers, like Adam • End-to-end learning • model directly converts input data into output prediction bypassing intermediate steps in a traditional pipeline CS447: Natural Language Processing 5 McCulloch-Pitts Neuron CS447: Natural Language Processing 6 They are called neural because their origins lie in But the modern use in language processing no longer draws on these early biological inspirations Neural Units • Building blocks of a neural network • Given a set of inputs x1...xn, a unit has a set of corresponding weights w1...wn and a bias b, so the weighted sum z can be represented as: or, z = w · x + b using dot-product CS447: Natural Language Processing 7 Neural Units • Apply non-linear function f (or g) to z to compute activation a: • since we are modeling a single unit, the activation is also the final output y CS447: Natural Language Processing 8 Activation Functions: Sigmoid • Sigmoid (σ) • maps output into the range [0,1] • differentiable CS447: Natural Language Processing 9 Activation Functions: Tanh • Tanh • maps output into the range [-1, 1] • better than sigmoid • smoothly differentiable and maps outlier values towards the mean CS447: Natural Language Processing 10 Activation Functions: ReLU • Rectified Linear Unit (ReLU) y = max(x, 0) • High values of z in sigmoid/ tanh result in values of y that are close to 1 which causes problems for learning CS447: Natural Language Processing 11 XOR Problem • Minsky-Papert proved perceptron can’t compute XOR logical operation CS447: Natural Language Processing 12 XOR Problem • Perceptron can compute the logical AND and OR functions easily • But it’s not possible to build a perceptron to compute logical XOR! CS447: Natural Language Processing 13 XOR Problem • Perceptron is a linear classifier but XOR is not linearly separable • for a 2D input x0 and x1, the perceptron equation: w1x1 + w2x2 + b = 0 is the equation of a line CS447: Natural Language Processing 14 XOR Problem: Solution CS447: Natural Language Processing 15 • XOR function can be computed using two layers of ReLU-based units • XOR problem demonstrates need for multi-layer networks XOR Problem: Solution CS447: Natural Language Processing 16 • Hidden layer forms a linearly separable representation for the input In this example, we stipulated the weights but in real applications, the weights for neural networks are learned automatically using the error back-propagation algorithm Why do we need non-linear activation functions? • Network of simple linear (perceptron) units cannot solve XOR problem • a network formed by many layers of purely linear units can always be reduced to a single layer of linear units a[1] = z[1] = W[1] · x + b[1] a[2] = z[2] = W[2] · a [1] + b[2] = W[2] · (W[1] · x + b[1]) + b[2] = (W[2] · W[1]) · x + (W[2] · b[1] + b[2]) = W’ · x + b’ … no more expressive than logistic regression! • we’ve already shown that a single unit cannot solve the XOR problem CS447: Natural Language Processing 17 Feed-Forward Neural Networks • Each layer is fully-connected • Represent parameters for hidden layer by combining weight vector wi and bias bi for each unit i into a single weight matrix W and a single bias vector b for the whole layer ![#] = &[#]' + )[#] ℎ= +[#] = ,(![#]) where & ∈ℝ12×14 and ), ℎ∈ℝ12 CS447: Natural Language Processing 18 a.k.a. multi-layer perceptron (MLP), though it’s a misnomer Feed-Forward Neural Networks • Output could be real-valued number (for regression), or a probability distribution across the output nodes (for multinomial classification) ![#] = &[#]ℎ+ )[#], such that ![#] ∈ℝ,-, &[#] ∈ℝ,-×,/ • We apply softmax function to encode ![#] as a probability distribution • So a neural network is like logistic regression over induced feature representations from prior layers of the network rather than forming features using feature templates CS447: Natural Language Processing 19 Recap: 2-layer Feed-Forward Neural Network ![#] = &[#]'[(] + *[#] '[#] = ℎ= ,[#](![#]) ![/] = &[/]'[#] + *[/] '[/] = ,[/](![/]) 0 1 = '[/] We use '[(] to stand for input 2, 0 1 for predicted output, 1 for ground truth output and g(⋅) for activation function. ,[/] might be softmax for multinomial classification or sigmoid for binary classification, while ReLU or tanh might be activation function ,(⋅) at the internal layers. CS447: Natural Language Processing 20 N-layer Feed-Forward Neural Network for i in 1..n: ![#] = &[#]'[#()] + +[#] '[#] = ,[#](![#]) / 0 = '[1] CS447: Natural Language Processing 21 Training Neural Nets: Loss Function • Models the distance between the system output and the gold output • Same as logistic regression, the cross-entropy loss • for binary classification • for multinomial classification CS447: Natural Language Processing 22 Training Neural Nets: Gradient Descent • To find parameters that minimize loss function, we use gradient descent • But it’s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer • we use error back-propagation to partial out loss over intermediate layers • builds on notion of computation graphs CS447: Natural Language Processing 23 Training Neural Nets: Computation Graphs Computation is broken down into separate operations, each of which is modeled as a node in a graph Consider ! "", $, % = % "" + 2$ CS447: Natural Language Processing 24 Training Neural Nets: Backward Differentiation • Uses chain rule from calculus For f(x) = u(v(x)), we have • For our function ! = #(% + 2(), we need the derivatives: • Requires the intermediate derivatives: CS447: Natural Language Processing 25 Training Neural Nets: Backward Pass • Compute from right to left • For each node: 1. compute local partial derivative with respect to the parent 2. multiply it by the partial that is passed down from the parent 3. then pass it to the child • Also requires derivatives of activation functions CS447: Natural Language Processing 26 Training Neural Nets: Best Practices • Non-convex optimization problem 1. initialize weights with small random numbers, preferably gaussians 2. regularize to prevent over-fitting, e.g. dropout • Optimization techniques for gradient descent • momentum, RMSProp, Adam, etc. CS447: Natural Language Processing 27 Parameters vs Hyperparameters • Parameters are learned by gradient descent • e.g. weights matrix W and biases b • Hyperparameters are set prior to learning • e.g. learning rate, mini-batch size, model architecture (number of layers, number of hidden units per layer, choice of activation functions), regularization technique • require to be tuned CS447: Natural Language Processing 28 Neural Language Models Predicting upcoming words from prior word context CS447: Natural Language Processing 29 Neural Language Models • Feed-forward neural LM is a standard feedforward network that takes as input at time t a representation of some number of previous words (wt−1,wt−2…) and outputs probability distribution over possible next words • Advantages • don’t need smoothing • can handle much longer histories • generalize over context of similar words • higher predictive accuracy • Uses include machine translation, dialog, language generation CS447: Natural Language Processing 30 Embeddings • Mapping from words in vocabulary V to vectors of real numbers e • Each word may be represented as one hot-vector of length |V| • Concatenate each of N context vectors for preceding words • Long, sparse, hard to generalize. Can we learn a concise representation? CS447: Natural Language Processing 31 Embeddings • Allow neural n-gram LM to generalize to unseen data better “I have to make sure when I get home to feed the cat.” If we’ve never seen the word “dog” after “feed the”, n-gram LM will predict “cat” given the prefix. But neural LM makes use of similarity of embeddings to assign a reasonably high probability to both dog and cat CS447: Natural Language Processing 32 Embeddings Moving window at time t with pre-trained embedding vector, say using word2vec for each of three previous words wt−1, wt−2, and wt−3, concatenated to produce input CS447: Natural Language Processing 33 Learning Embeddings for Neural n-gram LM • Task may place strong constraints on what makes a good representation • To learn embeddings, add an extra layer to the network and propagate errors all the way back to the embedding vectors • Represent each of N previous words as one hot-vector of length |V|, and learn an embedding matrix E ∈ℝ$×& such that for one-hot column vector '( for word ) (, the projection layer is *'( = ,( CS447: Natural Language Processing 34 Learning Embeddings: Forward Pass ![#] = & = '(), '(+, … , '(- .[)] = /[)]![#] + 1[)] ![)] = 2[)](.[)]) .[+] = /[+]![)] + 1[+] 5 6 = ![+] = 2[+](.[+]) Each node i in 5 6 estimates probability 7 89_; 89<), 89<+, 89<=) CS447: Natural Language Processing 35 Training the Neural Language Model • To set all the parameters θ = E,W,U,b, we do gradient descent using error back propagation on the computation graph to compute gradient • Loss Function: cross-entropy (negative log likelihood) L = −log p &'( &')*, &'),, &')-.*) Training the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings E CS447: Natural Language Processing 36 Summary • Neural networks are built out of neural units, which take weighted sum of inputs and apply a non-linear activation function such as sigmoid, tanh, ReLU • In a fully-connected feed-forward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles • Power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network • Neural networks are trained by optimization algorithms like gradient descent using error back-propagation on a computation graph • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling CS447: Natural Language Processing 37 "
274,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 21: Machine Translation CS447 Natural Language Processing Machine Translation in 2018 !2 Google Translate translate.google.com CS447 Natural Language Processing Machine Translation in 2012 !3 Google Translate translate.google.com CS447 Natural Language Processing Why is MT difﬁcult? !4 CS447 Natural Language Processing Some examples John loves Mary. Jean aime Marie. John told Mary a story. Jean a raconté une histoire à Marie. John is a computer scientist. Jean est informaticien. John swam across the lake. Jean a traversé le lac à la nage. !5 CS447 Natural Language Processing John loves Mary. Jean aime Marie. John told Mary a story. Jean [a raconté ] une histoire [à Marie]. John is a [computer scientist]. Jean est informaticien. John [swam across] the lake. Jean [a traversé] le lac [à la nage]. Correspondences !6 CS447 Natural Language Processing Correspondences One-to-one: John = Jean, aime = loves, Mary=Marie One-to-many/many-to-one: Mary = [à Marie] [a computer scientist] = informaticien Many-to-many: [swam across ] = [a traversé à la nage] Reordering required: told Mary1 [a story]2 = a raconté [une histoire]2 [à Marie]1 !7 CS447 Natural Language Processing Lexical divergences The different senses of homonymous words generally have different translations: English-German: (river) bank - Ufer (ﬁnancial) bank - Bank The different senses of polysemous words may also have different translations: I know that he bought the book: Je sais qu’il a acheté le livre. I know Peter: Je connais Peter. I know math: Je m’y connais en maths. !8 CS447 Natural Language Processing Lexical divergences Lexical speciﬁcity German Kürbis = English pumpkin or (winter) squash English brother = Chinese gege (older) or didi (younger) Morphological divergences English: new book(s), new story/stories French: un nouveau livre (sg.m), une nouvelle histoire (sg.f), des nouveaux livres (pl.m), des nouvelles histoires (pl.f) -How much inﬂection does a language have? (cf. Chinese vs.Finnish) -How many morphemes does each word have? -How easily can the morphemes be separated? !9 CS447 Natural Language Processing Syntactic divergences Word order: ﬁxed or free? If ﬁxed, which one? [SVO (Sbj-Verb-Obj), SOV, VSO,… ] Head-marking vs. dependent-marking Dependent-marking (English) the man’s house Head-marking (Hungarian) the man house-his Pro-drop languages can omit pronouns: Italian (with inﬂection): I eat = mangio; he eats = mangia Chinese (without inﬂection): I/he eat: chīfàn !10 CS447 Natural Language Processing Syntactic divergences: negation !11 Normal Negated English I drank coffee. I didn’t drink (any) coffee. do-support, any French J’ai bu du café Je n’ai pas bu de café. ne..pas du → de German Ich habe Kaffee getrunken Ich habe keinen Kaffee getrunken keinen Kaffee = ‘no coffee’ CS447 Natural Language Processing Semantic differences Aspect: -English has a progressive aspect: ‘Peter swims’ vs. ‘Peter is swimming’ -German can only express this with an adverb: ‘Peter schwimmt’ vs. ‘Peter schwimmt gerade’ (‘swims currently’) Motion events have two properties: -manner of motion (swimming) -direction of motion (across the lake) Languages express either the manner with a verb and the direction with a ‘satellite’ or vice versa (L. Talmy): English (satellite-framed): He [swam]MANNER [across]DIR the lake French (verb-framed): Il a [traversé ]DIR le lac [à la nage ]MANNER !12 CS447 Natural Language Processing An exercise !13 CS447 Natural Language Processing Knight’s Centauri and Arctuan 1a. ok-voon ororok sprok. 1b. at-voon bichat dat. 2a. ok-drubel ok-voon anok plok sprok. 2b. at-drubel at-voon pippat rrat dat. 3a. erok sprok izok hihok ghirok. 3b. totat dat arrat vat hilat. 4a. ok-voon anok drok brok jok. 4b. at-voon krat pippat sat lat. 5a. wiwok farok izok stok. 5b. totat jjat quat cat. 6a. lalok sprok izok jok stok. 6b. wat dat krat quat cat. 7a. lalok farok ororok lalok sprok izok enemok. 7b. wat jjat bichat wat dat vat eneat. 8a. lalok brok anok plok nok. 8b. iat lat pippat rrat nnat. 9a. wiwok nok izok kantok ok-yurp. 9b. totat nnat quat oloat at-yurp. 10a. lalok mok nok yorok ghirok clok. 10b. wat nnat gat mat bat hilat. 11a. lalok nok crrrok hihok yorok zanzanok. 11b. wat nnat arrat mat zanzanat. 12a. lalok rarok nok izok hihok mok. 12b. wat nnat forat arrat vat gat. !14 CS447 Natural Language Processing The original corpus 1a. Garcia and associates. 1b. Garcia y asociados. 2a. Carlos Garcia has three associates. 2b. Carlos Garcia tiene tres asociados. 3a. his associates are not strong. 3b. sus asociados no son fuertes. 4a. Garcia has a company also. 4b. Garcia tambien tiene una empresa. 5a. its clients are angry. 5b. sus clientes están enfadados. 6a. the associates are also angry. 6b. los asociados tambien están enfadados. 7a. the clients and the associates are enemies. 7b. los clientes y los asociados son enemigos. 8a. the company has three groups. 8b. la empresa tiene tres grupos. 9a. its groups are in Europe. 9b. sus grupos están en Europa. 10a. the modern groups sell strong pharmaceuticals. 10b. los grupos modernos venden medicinas fuertes. 11a. the groups do not sell zanzanine. 11b. los grupos no venden zanzanina. 12a. the small groups are not modern. 12b. los grupos pequeños no son modernos. !15 CS447 Natural Language Processing 1a. Garcia and associates. 1b. Garcia y asociados. 2a. Carlos Garcia has three associates. 2b. Carlos Garcia tiene tres asociados. 3a. his associates are not strong. 3b. sus asociados no son fuertes. 4a. Garcia has a company also. 4b. Garcia tambien tiene una empresa. 5a. its clients are angry. 5b. sus clientes están enfadados. 6a. the associates are also angry. 6b. los asociados tambien están enfadados. 7a. the clients and the associates are enemies. 7b. los clientes y los asociados son enemigos. 8a. the company has three groups. 8b. la empresa tiene tres grupos. 9a. its groups are in Europe. 9b. sus grupos están en Europa. 10a. the modern groups sell strong pharmaceuticals 10b. los grupos modernos venden medicinas fuertes 11a. the groups do not sell zanzanine. 11b. los grupos no venden zanzanina. 12a. the small groups are not modern. 12b. los grupos pequeños no son modernos. !16 1a. ok-voon ororok sprok. 1b. at-voon bichat dat. 2a. ok-drubel ok-voon anok plok sprok. 2b. at-drubel at-voon pippat rrat dat. 3a. erok sprok izok hihok ghirok. 3b. totat dat arrat vat hilat. 4a. ok-voon anok drok brok jok. 4b. at-voon krat pippat sat lat. 5a. wiwok farok izok stok. 5b. totat jjat quat cat. 6a. lalok sprok izok jok stok. 6b. wat dat krat quat cat. 7a. lalok farok ororok lalok sprok izok enemok 7b. wat jjat bichat wat dat vat eneat. 8a. lalok brok anok plok nok. 8b. iat lat pippat rrat nnat. 9a. wiwok nok izok kantok ok-yurp. 9b. totat nnat quat oloat at-yurp. 10a. lalok mok nok yorok ghirok clok. 10b. wat nnat gat mat bat hilat. 11a. lalok nok crrrok hihok yorok zanzanok. 11b. wat nnat arrat mat zanzanat. 12a. lalok rarok nok izok hihok mok. 12b. wat nnat forat arrat vat gat. CS447 Natural Language Processing Machine translation approaches !17 CS447 Natural Language Processing !18 CS447 Natural Language Processing The Rosetta Stone Three different translations of the same text: -Hieroglyphic Egyptian (used by priests) -Demotic Egyptian (used for daily purposes) -Classical Greek (used by the administration) Instrumental in our understanding of ancient Egyptian This is an instance of parallel text: The Greek inscription allowed scholars to decipher the hieroglyphs !19 CS447 Natural Language Processing MT History WW II: Code-breaking efforts at Bletchley Park, England (Alan Turing) 1948: Shannon/Weaver: Information theory 1949: Weaver’s memorandum deﬁnes the task 1954: IBM/Georgetown demo: 60 sentences Russian-English 1960: Bar-Hillel: MT to difﬁcult 1966: ALPAC report: human translation is far cheaper and better: kills MT for a long time 1980s/90s: Transfer and interlingua-based approaches 1990: IBM’s CANDIDE system (ﬁrst modern statistical MT system) 2000s: Huge interest and progress in wide-coverage statistical MT: phrase-based MT, syntax-based MT, open-source tools Now: Neural machine translation !20 CS447 Natural Language Processing Words Syntax Semantics Syntactic transfer Semantic transfer Direct transfer The Vauquois triangle !21 Source Target Words Syntax Semantics Interlingua Generation Transfer Analysis CS447 Natural Language Processing Direct translation Maria non dió una bofetada a la bruja verde. 1. Morphological analysis of source string Maria nonNeg dar3sgF-Past una bofetada a la bruja verde (usually, a complete morphological analysis) 2. Lexical transfer (using a translation dictionary): Mary not slap3sgF-Past to the witch green. 3. Local reordering: Mary not slap3sgF-Past the green witch. 4. Morphology: Mary did not slap the green witch. !22 CS447 Natural Language Processing Adverb placement in German: The green witch is at home this week. Diese Woche ist die grüne Hexe zuhause. Japanese SOV order: He adores listening to music Kare ha ongaku wo kiku no ga daisuki desu PPs in Chinese: Jackie Cheng went to Hong Kong Cheng Long dao Xianggang qu Limits of direct translation: Phrasal reordering !23 CS447 Natural Language Processing Requires a syntactic parse of the source language, followed by reordering of the tree Local reordering: Nonlocal reordering: Syntactic transfer !24 S PP diese Woche V ist NP die gr¨ une Hexe PP zuhause S NP The green witch VP V is PP at home PP this week Noun Adj green N witch Noun N bruja Adj verde CS447 Natural Language Processing Semantic transfer Done at the level of predicate-argument structure (some people call this syntactic transfer too…): or at the level of semantic representations (e.g. DRSs): !25 Dorna et al. 1998 CS447 Natural Language Processing Based on the assumption that there is one common meaning representation (e.g. predicate logic) that abstracts away from any difference in surface realization. Semantic transfer: each language produces its own meaning representation Was thought useful for multilingual translation Interlingua approaches !26 Leavitt et al. 1994 CS447 Natural Language Processing Statistical Machine Translation !27 CS447 Natural Language Processing Statistical Machine Translation We want the best (most likely) [English] translation for the [Chinese] input: argmaxEnglish P( English | Chinese ) We can either model this probability directly, or we can apply Bayes Rule. Using Bayes Rule leads to the “noisy channel” model. As with sequence labeling, Bayes Rule simpliﬁes the modeling task, so this was the ﬁrst approach for statistical MT. !28 CS447 Natural Language Processing Decoder (Translating to English) Î = argmaxI P(O|I)P(I) The noisy channel model !29 Translating from Chinese to English: argmaxEngP(Eng|Chin) = argmaxEng P(Chin|Eng) ⇤ ⇥# ⌅ Translation Model × P(Eng) ⇤⇥# ⌅ LanguageModel Foreign Output O Noisy Channel P(O | I) English Input I Guess of English Input Î CS447 Natural Language Processing The noisy channel model This is really just an application of Bayes’ rule: The translation model P(F | E) is intended to capture the faithfulness of the translation. It needs to be trained on a parallel corpus The language model P(E) is intended to capture the ﬂuency of the translation. It can be trained on a (very large) monolingual corpus !30 ˆ E = arg max E P(E|F) = arg max E P(F|E) ⇥P(E) P(F) = arg max E P(F|E) | {z } Translation Model ⇥ P(E) | {z } Language Model CS447 Natural Language Processing Statistical MT !31 Translation Model Ptr(早晨 | morning) Language Model Plm(honorable | good morning) MOTION: PRESIDENT (in Cantonese): Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the Parallel corpora Monolingual corpora Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Decoding algorithm Input 主席：各位議 員，早晨。 Translation President: Good morning, Honourable Members. CS447 Natural Language Processing Size of models Effect on translation quality With training on data from the web and clever parallel processing (MapReduce/Bloom ﬁlters), n can be quite large -Google (2007) uses 5-grams to 7-grams, -This results in huge models, but the effect on translation quality levels off quickly: n-gram language models for MT !32 CS447 Natural Language Processing Translation probability P(fpi | epi ) Phrase translation probabilities can be obtained from a phrase table: This requires phrase alignment on a parallel corpus. !33 EP FP count green witch grüne Hexe … at home zuhause 10534 at home daheim 9890 is ist 598012 this week diese Woche …. CS447 Natural Language Processing Creating parallel corpora A parallel corpus consists of the same text in two (or more) languages. Examples: Parliamentary debates: Canadian Hansards; Hong Kong Hansards, Europarl; Movie subtitles (OpenSubtitles) In order to train translation models, we need to align the sentences (Church & Gale ’93) !34 CS447 Natural Language Processing Today’s key concepts Why is machine translation hard? Linguistic divergences: morphology, syntax, semantics Different approaches to machine translation: Vauquois triangle Statistical MT (more on this next time) !35 "
275,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 22: Statistical Machine Translation CS447 Natural Language Processing Projects and Literature Reviews First report due Nov 26 (PDF written in LaTeX; no length restrictions; submission through Compass) Purpose of this ﬁrst report: Check-in to make sure that you’re on track (or, if not, that we can spot problems) Rubrics for the ﬁnal reports (due on Reading Day): https://courses.engr.illinois.edu/CS447/LiteratureReviewRubric.pdf https://courses.engr.illinois.edu/CS447/FinalProjectRubric.pdf ""2 CS447 Natural Language Processing Projects and Literature Reviews Guidelines for ﬁrst Project Report: What is your project about? What are the relevant papers you are building on? What data are you using? What evaluation metric will you be using? What models will you implement/evaluate? What is your to-do list? Guidelines for ﬁrst Literature Review Report: What is your literature review about? (What task or what kind of models? Do you have any speciﬁc questions or focus?) What are the papers you will review? (If you already have it, give a brief summary of each of them) What’s your to-do list? ""3 CS447 Natural Language Processing Statistical Machine Translation ""4 CS447 Natural Language Processing Statistical Machine Translation We want the best (most likely) [English] translation for the [Chinese] input: argmaxEnglish P( English | Chinese ) We can either model this probability directly, or we can apply Bayes Rule. Using Bayes Rule leads to the “noisy channel” model. As with sequence labeling, Bayes Rule simpliﬁes the modeling task, so this was the ﬁrst approach for statistical MT. ""5 CS447 Natural Language Processing Decoder (Translating to English) Î = argmaxI P(O|I)P(I) The noisy channel model ""6 Translating from Chinese to English: argmaxEngP(Eng|Chin) = argmaxEng P(Chin|Eng) ⇤ ⇥# ⌅ Translation Model × P(Eng) ⇤⇥# ⌅ LanguageModel Foreign Output O Noisy Channel P(O | I) English Input I Guess of English Input Î CS447 Natural Language Processing The noisy channel model This is really just an application of Bayes’ rule: The translation model P(F | E) is intended to capture the faithfulness of the translation. It needs to be trained on a parallel corpus The language model P(E) is intended to capture the ﬂuency of the translation. It can be trained on a (very large) monolingual corpus ""7 ˆ E = arg max E P(E|F) = arg max E P(F|E) ⇥P(E) P(F) = arg max E P(F|E) | {z } Translation Model ⇥ P(E) | {z } Language Model CS447 Natural Language Processing Statistical MT with the noisy channel model ""8 Translation Model Ptr(早晨 | morning) Language Model Plm(honorable | good morning) MOTION: PRESIDENT (in Cantonese): Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the Parallel corpora Monolingual corpora Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Good morning, Honourable Members. We will now start the meeting. First of all, the motion on the ""Appointment of the Chief Justice of the Court of Final Appeal of the Hong Kong Special Administrative Region"". Secretary for Justice. Decoding algorithm Input 主席：各位議 員，早晨。 Translation President: Good morning, Honourable Members. CS447 Natural Language Processing Size of models Effect on translation quality With training on data from the web and clever parallel processing (MapReduce/Bloom ﬁlters), n can be quite large -Google (2007) uses 5-grams to 7-grams, -This results in huge models, but the effect on translation quality levels off quickly: n-gram language models for MT ""9 CS447 Natural Language Processing Translation probability P(fpi | epi ) Phrase translation probabilities can be obtained from a phrase table: This requires phrase alignment on a parallel corpus. ""10 EP FP count green witch grüne Hexe … at home zuhause 10534 at home daheim 9890 is ist 598012 this week diese Woche …. CS447 Natural Language Processing Getting translation probabilities A parallel corpus consists of the same text in two (or more) languages. Examples: Parliamentary debates: Canadian Hansards; Hong Kong Hansards, Europarl; Movie subtitles (OpenSubtitles) In order to train translation models, we need to align the sentences (Church & Gale ’93) We can learn word and phrase alignments from these aligned sentences ""11  CS447 Natural Language Processing IBM models First statistical MT models, based on noisy channel: Translate from source f to target e via a translation model P(f | e) and a language model P(e) The translation model goes from target e to source f via word alignments a: P(f | e) = ∑a P(f, a | e) Original purpose: Word-based translation models Today: Can be used to obtain word alignments, which are then used to obtain phrase alignments for phrase-based translation models Sequence of 5 translation models Model 1 is too simple to be used by itself, but can be trained very easily on parallel data. ""12  CS447 Natural Language Processing IBM translation models: assumptions The model “generates” the ‘foreign’ source sentence f conditioned on the ‘English’ target sentence e by the following stochastic process: 1. Generate the length of the source f with probability p = ... 2. Generate the alignment of the source f to the target e with probability p = ... 3. Generate the words of the source f with probability p = ... ""13  CS447 Natural Language Processing Word alignments in the IBM models ""14  CS447 Natural Language Processing Word alignment ""15 Jean aime Marie John loves Mary dass John Maria liebt that John loves Mary John loves Mary. … that John loves Mary. Jean aime Marie. … dass John Maria liebt.  CS447 Natural Language Processing Word alignment ""16 Maria no dió una bofetada a la bruja verde Mary did not slap the green witch  CS447 Natural Language Processing Word alignment ""17 Marie a traversé le lac à la nage Mary swam across the lake  CS447 Natural Language Processing Word alignment ""18 Target Source Marie a traversé le lac à la nage Mary swam across the lake One target word can be aligned to many source words.  CS447 Natural Language Processing Word alignment ""19 Target Source Marie a traversé le lac à la nage Mary swam across the lake One target word can be aligned to many source words. But each source word can only be aligned to one target word. This allows us to model P(source | target)  CS447 Natural Language Processing Word alignment ""20 Some source words may not align to any target words. Target Source Marie a traversé le lac à la nage Mary swam across the lake  CS447 Natural Language Processing Some source words may not align to any target words. Word alignment ""21 Target Source Marie a traversé le lac à la nage NULL Mary swam across the lake To handle this we assume a NULL word in the target sentence.  CS447 Natural Language Processing Representing word alignments ""22 1 2 3 4 5 6 7 8 Marie a traversé le lac à la nage 0 NULL 1 Mary 2 swam 3 across 4 the 5 lake Position 1 2 3 4 5 6 7 8 Foreign Marie a traversé le lac à la nage Alignment 1 3 3 4 5 0 0 2 Every source word f[i] is aligned to one target word e[j] (incl. NULL). We represent alignments as a vector a (of the same length as the source) with a[i] = j  CS447 Natural Language Processing The IBM alignment models ""23  CS447 Natural Language Processing Use the noisy channel (Bayes rule) to get the best (most likely) target translation e for source sentence f: The translation model P(f | e) requires alignments a Generate f and the alignment a with P(f, a | e): m = #words in fj marginalize (=sum) over all alignments a The IBM models ""24 noisy channel arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1.. ⇧ ⌅⇤ Translati arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1..j−1, e, ⇧ ⌅⇤ Translation fj probability of alignment aj probability of word fj arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1..j−1, e, m) ⇧ ⌅⇤ ⌃ Translation fj  CS447 Natural Language Processing Model parameters Length probability P(m | n): What’s the probability of generating a source sentence of length m given a target sentence of length n? Count in training data Alignment probability: P(a | m, n): Model 1 assumes all alignments have the same probability: For each position a1...am, pick one of the n+1 target positions uniformly at random Translation probability: P(fj = lac | aj = i, ei = lake): In Model 1, these are the only parameters we have to learn. ""25  CS447 Natural Language Processing Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2 0 1 2 3 4 5 NULL Mary swam across the lake IBM model 1: Generative process For each target sentence e = e1..en of length n: 1. Choose a length m for the source sentence (e.g m = 8) 2. Choose an alignment a = a1...am for the source sentence Each aj corresponds to a word ei in e: 0 ≤ aj ≤ n 3. Translate each target word eaj into the source language ""26 0 1 2 3 4 5 NULL Mary swam across the lake Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2 Translation Marie a traversé le lac à la nage Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2  CS447 Natural Language Processing IBM model 1: details The length probability is constant: P(m | e) = ε The alignment probability is uniform (n = length of target string): P(ai | e) = 1/(n+1) The translation probability depends only on eai (the corresponding target word): P(fi | eai) ""27 P(f, a|e) = P(m|e) ⌅⇤⇥⇧ Length: |f|=m m % j=1 P(aj|a1..j−1, f1..j−1, m, e) ⌅ ⇤⇥ ⇧ Word alignment aj P(fj|a1..jf1..j−1, e, m) ⌅ ⇤⇥ ⇧ Translation fj = ϵ m % j=1 1 n + 1P(fj|eaj) = ϵ (n + 1)m m % j=1 P(fj|eaj)  CS447 Natural Language Processing Finding the best alignment How do we ﬁnd the best alignment between e and f? ""28 ˆ a = arg max a P(f, a|e) = arg max a ϵ (n + 1)m m ! j=1 P(fj|eaj) = arg max a m ! j=1 P(fj|eaj) ˆ aj = arg max aj P(fj|eaj)  CS447 Natural Language Processing Learning translation probabilities The only parameters that need to be learned are the translation probabilities P(f | e) P(fj = lac | ei = lake) If the training corpus had word alignments, we could simply count how often ‘lake’ is aligned to ‘lac’: P( lac | lake) = count(lac, lake) ⁄ ∑w count(w, lake) But we don’t have gold word alignments. So, instead of relative frequencies, we have to use expected relative frequencies: P( lac | lake) = 〈count(lac, lake)〉 ⁄ 〈∑w count(w, lake)〉 ""29  CS447 Natural Language Processing Training Model 1 with EM The only parameters that need to be learned are the translation probabilities P(f | e) We use the EM algorithm to estimate these parameters from a corpus with S sentence pairs s = 〈 f (s), e(s)〉 with alignments A(f (s), e(s)) -Initialization: guess P(f | e) -Expectation step: compute expected counts -Maximization step: recompute probabilities P(f |e) ""30 ˆ P(f|e) = ⟨c(f, e)⇥ ! f ′⟨c(f ′, e)⇥ ⟨c(f, e)⇥= ! s∈S ⟨c(f, e|e(s), f (s))⇥  CS447 Natural Language Processing Expectation-Maximization (EM) 1. Initialize a ﬁrst model, M0 2. Expectation (E) step: Go through training data to gather expected counts 〈count(lac, lake)〉 3. Maximization (M) step: Use expected counts to compute a new model Mi+1 Pi+1( lac | lake) = 〈count(lac, lake)〉 ⁄ 〈∑w count(w, lake)〉 4.Check for convergence: Compute log-likelihood of training data with Mi+1 If the difference between new and old log-likelihood smaller than a threshold, stop. Else go to 2. ""31  CS447 Natural Language Processing The E-step ""32 Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) We need to know , the probability that word fj is aligned to word eaj under the alignment a ute the expected count ⇥c(f, e|f, e)⇤: (f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇤ ⇥ j P(fj|eaj)  CS447 Natural Language Processing Other translation models Model 1 is a very simple (and not very good) translation model. IBM models 2-5 are more complex. They take into account: -“fertility”: the number of foreign words generated by each target word -the word order and string position of the aligned words ""33 CS447 Natural Language Processing Today’s key concepts Why is machine translation hard? Linguistic divergences: morphology, syntax, semantics Different approaches to machine translation: Vauquois triangle Statistical MT: Noisy Channel, IBM Model 1 (more on this next time) ""34 "
276,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 23: Phrase-based MT (corrected) CS447: Natural Language Processing (J. Hockenmaier) Recap: IBM models for MT ""2  CS447 Natural Language Processing Use the noisy channel (Bayes rule) to get the best (most likely) target translation e for source sentence f: The translation model P(f | e) requires alignments a Generate f and the alignment a with P(f, a | e): m = #words in fj marginalize (=sum) over all alignments a The IBM models ""3 noisy channel arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1.. ⇧ ⌅⇤ Translati arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1..j−1, e, ⇧ ⌅⇤ Translation fj probability of alignment aj probability of word fj arg max e P(e|f) = arg max e P(f|e)P(e) P(f|e) = ! a⇥A(e,f) P(f, a|e) P(f, a|e) = P(m|e) ⇧⌅⇤⌃ Length: |f|=m m ⇥ j=1 P(aj|a1..j−1, f1..j−1, m, e) ⇧ ⌅⇤ ⌃ Word alignment aj P(fj|a1..jf1..j−1, e, m) ⇧ ⌅⇤ ⌃ Translation fj  CS447 Natural Language Processing Representing word alignments ""4 1 2 3 4 5 6 7 8 Marie a traversé le lac à la nage 0 NULL 1 Mary 2 swam 3 across 4 the 5 lake Position 1 2 3 4 5 6 7 8 Foreign Marie a traversé le lac à la nage Alignment 1 3 3 4 5 0 0 2 Every source word f[i] is aligned to one target word e[j] (incl. NULL). We represent alignments as a vector a (of the same length as the source) with a[i] = j  CS447 Natural Language Processing Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2 0 1 2 3 4 5 NULL Mary swam across the lake IBM model 1: Generative process For each target sentence e = e1..en of length n: 1. Choose a length m for the source sentence (e.g m = 8) 2. Choose an alignment a = a1...am for the source sentence Each aj corresponds to a word ei in e: 0 ≤ aj ≤ n 3. Translate each target word eaj into the source language ""5 0 1 2 3 4 5 NULL Mary swam across the lake Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2 Translation Marie a traversé le lac à la nage Position 1 2 3 4 5 6 7 8 Alignment 1 3 3 4 5 0 0 2  CS447 Natural Language Processing Expectation-Maximization (EM) 1. Initialize a ﬁrst model, M0 2. Expectation (E) step: Go through training data to gather expected counts 〈count(lac, lake)〉 3. Maximization (M) step: Use expected counts to compute a new model Mi+1 Pi+1( lac | lake) = 〈count(lac, lake)〉 ⁄ 〈∑w count(w, lake)〉 4.Check for convergence: Compute log-likelihood of training data with Mi+1 If the difference between new and old log-likelihood smaller than a threshold, stop. Else go to 2. ""6  CS447 Natural Language Processing The E-step ""7 Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) Compute the expected count ⇥c(f, e|f, e)⇤: ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) P(a|f, e) · c(f, e|a, e, f) ⌥ ⌃⇧ % How often are f,e aligned in a? P(a|f, e) = P(a, f|e) P(f|e) = P(a, f|e) & a′ P(a′, f|e) P(a, f|e) = ⌅ j P(fj|eaj) ⇥c(f, e|f, e)⇤ = ⇤ a⇥A(f,e) ⇥ j P(fj|eaj) & a′ ⇥ j P(fj|ea′ j) · c(f, e|a, e, f) CS447: Natural Language Processing (J. Hockenmaier) Phrase-based translation models ""8 CS447: Natural Language Processing (J. Hockenmaier) Phrase-based translation models Assumption: fundamental units of translation are phrases: Phrase-based model of P(F | E): 1. Split target sentence deterministically into phrases ep1...epn 2. Translate each target phrase epi into source phrase fpi with translation probability φ(fpi |epi) 3. Reorder foreign phrases with distortion probability d(ai-bi-1) = c|ai-bi-1 -1| ai = start position of source phrase generated by ei bi-1 = end position of source phrase generated by ei-1 ""9 主席：各位議員，早晨。 President (in Cantonese): Good morning, Honourable Members. CS447: Natural Language Processing (J. Hockenmaier) Phrase-based models of P( f | e) Split target sentence e=e1..n into phrases ep1..epN: [The green witch] [is] [at home] [this week] Translate each target phrase epi into source phrase fpi with translation probability P(fpi |epi): [The green witch] = [die grüne Hexe], ... Arrange the set of source phrases { fpi } to get s with distortion probability P( fp |{ fpi }): [Diese Woche] [ist] [die grüne Hexe] [zuhause] ""10 P(f|e = ⇤ep1, ..., epl) = ! i P(fpi|epi)P(fp|{fpi}) CS447: Natural Language Processing (J. Hockenmaier) Translation probability P(fpi | epi) Phrase translation probabilities can be obtained from a phrase table: This requires phrase alignment ""11 EP FP count green witch grüne Hexe … at home zuhause 10534 at home daheim 9890 is ist 598012 this week diese Woche …. CS447: Natural Language Processing (J. Hockenmaier) Word alignment ""12 Diese Woche ist die grüne Hexe zuhause The green witch is at home this week CS447: Natural Language Processing (J. Hockenmaier) Phrase alignment ""13 Diese Woche ist die grüne Hexe zuhause The green witch is at home this week CS447: Natural Language Processing (J. Hockenmaier) Obtaining phrase alignments We’ll skip over details, but here’s the basic idea: For a given parallel corpus (F-E) 1. Train two word aligners, (F→E and E→F) 2. Take the intersection of these alignments to get a high-precision word alignment 3. Grow these high-precision alignments until all words in both sentences are included in the alignment. Consider any pair of words in the union of the alignments, and incrementally add them to the existing alignments 4. Consider all phrases that are consistent with this improved word alignment ""14 CS447: Natural Language Processing (J. Hockenmaier) Decoding (for phrase-based MT) ""15 CS447: Natural Language Processing (J. Hockenmaier) Phrase-based models of P( f | e) Split target sentence e=e1..n into phrases ep1..epN: [The green witch] [is] [at home] [this week] Translate each target phrase epi into source phrase fpi with translation probability P(fpi |epi): [The green witch] = [die grüne Hexe], ... Arrange the set of source phrases { fpi } to get s with distortion probability P( fp |{ fpi }): [Diese Woche] [ist] [die grüne Hexe] [zuhause] ""16 P(f|e = ⇤ep1, ..., epl) = ! i P(fpi|epi)P(fp|{fpi}) CS447: Natural Language Processing (J. Hockenmaier) Translating How do we translate a foreign sentence (e.g. “Diese Woche ist die grüne Hexe zuhause” ) into English? -We need to ﬁnd ê = argmaxe P(f | e)P(e) -There is an exponential number of candidate translations e - But we can look up phrase translations ep and P( fp | ep ) in the phrase table: ""17 diese Woche ist die grüne Hexe zuhause this 0.2 week 0.7 is 0.8 the 0.3 green 0.3 witch 0.5 at home 1.00.5 these 0.5 the green 0.4 sorceress 0.6 this week 0.6 green witch 0.7 is this week 0.4 the green witch 0.7 CS447: Natural Language Processing (J. Hockenmaier) Generating a (random) translation 1. Pick the ﬁrst Target phrase ep1 from the candidate list. P := PLM(<s> ep1 )PTrans(fp1 | ep1 ) E = the, F= <….die…> 2. Pick the next target phrase ep2 from the candidate list P := P × PLM(ep2 | ep1)PTrans(fp2 | ep2 ) E = the green witch, F = <….die grüne Hexe...> 3. Keep going: pick target phrases epi until the entire source sentence is translated P := P × PLM(epi | ep1…i-1)PTrans(fpi | epi ) E = the green witch is, F = <….ist die grüne Hexe...> ""18 diese Woche ist die grüne Hexe zuhause this 0.2 week 0.7 is 0.8 the 0.3 green 0.3 witch 0.5 at home 0.5 these 0.5 the green 0.4 sorceress 0.6 this week 0.6 green witch 0.7 is this week 0.4 the green witch 0.7 1 4 2 3 5 CS447: Natural Language Processing (J. Hockenmaier) Finding the best translation How can we ﬁnd the best translation efﬁciently? There is an exponential number of possible translations. We will use a heuristic search algorithm We cannot guarantee to ﬁnd the best (= highest-scoring) translation, but we’re likely to get close. We will use a “stack-based” decoder (If you’ve taken Intro to AI: this is A* (“A-star”) search) We will score partial translations based on how good we expect the corresponding completed translation to be. Or, rather: we will score partial translations on how bad we expect the corresponding complete translation to be. That is, our scores will be costs (high=bad, low=good) ""19 CS447: Natural Language Processing (J. Hockenmaier) Scoring partial translations Assign expected costs to partial translations (E, F): expected_cost(E,F) = current_cost(E,F) + future_cost(E,F) The current cost is based on the score of the partial translation (E, F) e.g. current_cost(E,F) = logP(E)P(F | E) The (estimated) future cost is a lower bound on the actual cost of completing the partial translation (E, F): true_cost(E,F) (= current_cost(E,F) + actual_future_cost(E,F)) ≥ expected_cost(E,F) (= current_cost(E,F) + est_future_cost(E,F)) because actual_future_cost(E,F) ≥ est_future_cost(E,F) (The estimated future cost ignores the distortion cost) ""20 CS447: Natural Language Processing (J. Hockenmaier) Stack-based decoding Maintain a priority queue (=’stack’) of partial translations (hypotheses) with their expected costs. Each element on the stack is open (we haven’t yet pursued this hypothesis) or closed (we have already pursued this hypothesis) At each step: -Expand the best open hypothesis (the open translation with the lowest expected cost) in all possible ways. -These new translations become new open elements on the stack. -Close the best open hypothesis. Additional Pruning (n-best / beam search): Only keep the n best open hypotheses around ""21 CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""22 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: current translation F: which words in F F: have we covered? CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""23 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: F: ******* Cost: 999 We’re done with this node now (all continuations have a lower cost) CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""24 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: F: ******* Cost: 999 Expand one of these new yellow nodes next CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""25 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: F: ******* Cost: 999 E: the at home F: ***d*H* Cost: 983 E: the F: ***d*** Cost: 500 Expand the yellow node with the lowest cost CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""26 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: the at home F: ***d*H* Cost: 983 E: F: ******* Cost: 999 E: the F: ***d*** Cost: 500 E: the green witch F: ***dgH* Cost: 560 Expand the next node with the lowest cost CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""27 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: the at home F: ***d*H* Cost: 983 E: F: ******* Cost: 999 E: the F: ***d*** Cost: 500 E: the green witch F: ***dgH* Cost: 560 CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""28 Cost: 852 E: the F: ***d*** Cost: 500 Cost: 993 ... ... Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... Cost: 983 Cost: 999 Cost: 500 Cost: 560 Cost: 732 Cost: 705 Cost: 800 We always expand the best (lowest-cost) node, even if it’s not the last one introduced CS447: Natural Language Processing (J. Hockenmaier) MT evaluation ""29 CS447: Natural Language Processing (J. Hockenmaier) Evaluate candidate translations against several reference translations. C1: It is a guide to action which ensures that the military always obeys the commands of the party. C2: It is to insure the troops forever hearing the activity guidebook that party direct R1: It is a guide to action that ensures that the military will forever heed Party commands. R2: It is the guiding principle which guarantees the military forces always being under the command of the Party. R3: It is the practical guide for the army always to heed the directions of the party. The BLEU score is based on N-gram precision: How many n-grams in the candidate translation occur also in one of the reference translation? ""30 Automatic evaluation: BLEU CS447: Natural Language Processing (J. Hockenmaier) BLEU details For n ∈ {1,…,4}, compute the (modiﬁed) precision of all n-grams: MaxFreqref(‘the party’) = max. count of ‘the party’ in one reference translation. Freqc(‘the party’) = count of ‘the party’ in candidate translation c. Penalize short candidate translations by a brevity penalty BP c = length (number of words) of the whole candidate translation corpus r = Pick for each candidate the reference translation that is closest in length; sum up these lengths. Brevity penalty BP = exp(1-c/r) for c ≤ r; BP = 1 for c>r (BP ranges from e for c=0 to 1 for c=r) ""31 Precn = P c2C P n-gram2c MaxFreqref(n-gram) P c2C P -gram2c Freqc(n-gram) CS447: Natural Language Processing (J. Hockenmaier) BLEU score The BLEU score is the geometric mean of the precision of the unigrams, bigrams, trigrams, quadrigrams, weighted by the brevity penalty BP. ""32 BLEU = BP ⇥exp 1 N N X n=1 log Precn ! CS447: Natural Language Processing (J. Hockenmaier) Human evaluation We want to know whether the translation is “good” English, and whether it is an accurate translation of the original. - Ask human raters to judge the ﬂuency and the adequacy of the translation (e.g. on a scale of 1 to 5) - Correlated with ﬂuency is accuracy on cloze task: Give rater the sentence with one word replaced by blank. Ask rater to guess the missing word in the blank. - Similar to adequacy is informativeness Can you use the translation to perform some task (e.g. answer multiple-choice questions about the text) ""33 CS498JH: Introduction to NLP Summary: Machine Translation ""34 CS447: Natural Language Processing (J. Hockenmaier) Machine translation models Current MT models all rely on statistics. Many current models do estimate P(E | F) directly, but may use features based on language models (capturing P(E)) and IBM-style translation models (P(F | E)) internally. There are a number of syntax-based models, e.g. using synchronous context-free grammars, which consist of pairs of rules for the two languages in which each RHS NT in language A corresponds to a RHS NT in language B: Language A: XP → YP ZP Language B: XP → ZP YP ""35 CS447: Natural Language Processing (J. Hockenmaier) More recent developments Neural network-based approaches: Recurrent neural networks (RNN) can model sequences (e.g. strings, sentences, etc.) Use one RNN (the encoder) to process the input in the source language Pass its output to another RNN (the decoder) to generate the output in the target language See e.g. http://www.tensorﬂow.org/tutorials/seq2seq/ index.md#sequence-to-sequence_basics ""36 "
277,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 24: A very brief introduction to discourse CS447 Natural Language Processing Projects and Literature Reviews First report due Nov 26 (PDF written in LaTeX; no length restrictions; submission through Compass) Purpose of this ﬁrst report: Check-in to make sure that you’re on track (or, if not, that we can spot problems) Rubrics for the ﬁnal reports (due on Reading Day): https://courses.engr.illinois.edu/CS447/LiteratureReviewRubric.pdf https://courses.engr.illinois.edu/CS447/FinalProjectRubric.pdf ""2 CS447 Natural Language Processing Projects and Literature Reviews Guidelines for ﬁrst Project Report: What is your project about? What are the relevant papers you are building on? What data are you using? What evaluation metric will you be using? What models will you implement/evaluate? What is your to-do list? Guidelines for ﬁrst Literature Review Report: What is your literature review about? (What task or what kind of models? Do you have any speciﬁc questions or focus?) What are the papers you will review? (If you already have it, give a brief summary of each of them) What’s your to-do list? ""3 CS447: Natural Language Processing (J. Hockenmaier) Outlook Lectures 25—27: Neural approaches to NLP Lecture 28 (Wed, Dec 12): Final exam (in-class, closed book, only materials after midterm) ""4 CS447: Natural Language Processing Fixing my bug from the last lecture… ""5 CS447: Natural Language Processing (J. Hockenmaier) Finding the best translation How can we ﬁnd the best translation efﬁciently? There is an exponential number of possible translations. We will use a heuristic search algorithm We cannot guarantee to ﬁnd the best (= highest-scoring) translation, but we’re likely to get close. We will use a “stack-based” decoder (If you’ve taken Intro to AI: this is A* (“A-star”) search) We will score partial translations based on how good we expect the corresponding completed translation to be. Or, rather: we will score partial translations on how bad we expect the corresponding complete translation to be. That is, our scores will be costs (high=bad, low=good) ""6 CS447: Natural Language Processing (J. Hockenmaier) Scoring partial translations Assign expected costs to partial translations (E, F): expected_cost(E,F) = current_cost(E,F) + future_cost(E,F) The current cost is based on the score of the partial translation (E, F) e.g. current_cost(E,F) = logP(E)P(F | E) The (estimated) future cost is a lower bound on the actual cost of completing the partial translation (E, F): true_cost(E,F) (= current_cost(E,F) + actual_future_cost(E,F)) ≥ expected_cost(E,F) (= current_cost(E,F) + est_future_cost(E,F)) because actual_future_cost(E,F) ≥ est_future_cost(E,F) (The estimated future cost ignores the distortion cost) ""7 CS447: Natural Language Processing (J. Hockenmaier) Stack-based decoding Maintain a priority queue (=’stack’) of partial translations (hypotheses) with their expected costs. Each element on the stack is open (we haven’t yet pursued this hypothesis) or closed (we have already pursued this hypothesis) At each step: -Expand the best open hypothesis (the open translation with the lowest expected cost) in all possible ways. -These new translations become new open elements on the stack. -Close the best open hypothesis. Additional Pruning (n-best / beam search): Only keep the n best open hypotheses around ""8 CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""9 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: current translation F: which words in F F: have we covered? CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""10 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: F: ******* Cost: 999 We’re done with this node now (all continuations have a lower cost) CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""11 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: F: ******* Cost: 999 Expand one of these new yellow nodes next CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""12 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: F: ******* Cost: 999 E: the at home F: ***d*H* Cost: 983 E: the F: ***d*** Cost: 500 Expand the yellow node with the lowest cost CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""13 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: the at home F: ***d*H* Cost: 983 E: F: ******* Cost: 999 E: the F: ***d*** Cost: 500 E: the green witch F: ***dgH* Cost: 560 Expand the next node with the lowest cost CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""14 E: these F: d****** Cost: 852 E: the F: ***d*** Cost: 500 E: at home F: ******z Cost: 993 ... ... E: the witch F: ***d*H* Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... E: the at home F: ***d*H* Cost: 983 E: F: ******* Cost: 999 E: the F: ***d*** Cost: 500 E: the green witch F: ***dgH* Cost: 560 CS447: Natural Language Processing (J. Hockenmaier) E: F: ******* Cost: 999 Stack-based decoding ""15 Cost: 852 E: the F: ***d*** Cost: 500 Cost: 993 ... ... Cost: 700 E: the green witch F: ***dgH* Cost: 560 ... ... Cost: 983 Cost: 999 Cost: 500 Cost: 560 Cost: 732 Cost: 705 Cost: 800 We always expand the best (lowest-cost) node, even if it’s not the last one introduced CS447: Natural Language Processing Discourse ""16 CS447: Natural Language Processing What is discourse? On Monday, John went to Einstein’s. He wanted to buy lunch. But the cafe was closed. That made him angry, so the next day he went to Green Street instead. ‘Discourse’: any linguistic unit that consists of multiple sentences Speakers describe “some situation or state of the real or some hypothetical world” (Webber, 1983) Speakers attempt to get the listener to construct a similar model of the situation. ""17 CS498JH: Introduction to NLP Why study discourse? For natural language understanding: Most information is not contained in a single sentence. The system has to aggregate information across paragraphs or entire documents. For natural language generation: When systems generate text, that text needs to be easy to understand — it has to be coherent. What makes text coherent? ""18 CS447: Natural Language Processing How can we understand discourse? On Monday, John went to Einstein’s. He wanted to buy lunch. But the cafe was closed. That made him angry, so the next day he went to Green Street instead. Understanding discourse requires (among other things): 1) doing coreference resolution: ‘the cafe’ and ‘Einstein’s’ refer to the same entity He and John refer to the same person. That refers to ‘the cafe was closed’. 2) identifying discourse (‘coherence’) relations: ‘He wanted to buy lunch’ is the reason for ‘John went to Bevande.’ ""19 CS447: Natural Language Processing Discourse models An explicit representation of: - the events and entities that a discourse talks about - the relations between them (and to the real world). This representation is often written in some form of logic. What does this logic need to capture? ""20 CS447: Natural Language Processing Discourse models should capture... Physical entities: John, Einstein’s, lunch Events: On Monday, John went to Einstein’s involve entities, take place at a point in time States: It was closed. involve entities and hold for a period of time Temporal relations: afterwards between events and states Rhetorical (‘discourse’) relations: ... so ... instead between events and states ""21 CS447: Natural Language Processing Rhetorical (Discourse) relations ""22 CS447: Natural Language Processing Rhetorical relations Discourse 1: John hid Bill’s car keys. He was drunk. Discourse 2: John hid Bill’s car keys. He likes spinach. Discourse 1 is more coherent than Discourse 2 because “He(=Bill) was drunk” provides an explanation for “John hid Bill’s car keys” What kind of relations between two consecutive utterances (=sentences, clauses, paragraphs,…) make a discourse coherent? Rhetorical Structure Theory; also lots of recent work on discourse parsing (Penn Discourse Treebank) ""23 CS447: Natural Language Processing Example: The Result relation The reader can infer that the state/event described in S0 causes (or: could cause) the state/event asserted in S1: S0: The Tin Woodman was caught in the rain. S1: His joints rusted. This can be rephrased as: “S0. As a result, S1” ""24 CS447: Natural Language Processing Example: The Explanation relation The reader can infer that the state/event in S1 provides an explanation (reason) for the state/event in S0: S0: John hid Bill’s car keys. S1: He was drunk. This can be rephrased as: “S0 because S1” ""25 CS447: Natural Language Processing Rhetorical Structure Theory (RST) RST (Mann & Thompson, 1987) describes rhetorical relations between utterances: Evidence, Elaboration, Attribution, Contrast, List,… Different variants of RST assume different sets of relations. Most relations hold between a nucleus (N) and a satellite (S). Some relations (e.g. List) have multiple nuclei (and no satellite). Every relation imposes certain constraints on its arguments (N,S), that describe the goals and beliefs of the reader R and writer W, and the effect of the utterance on the reader. ""26 CS447: Natural Language Processing Discourse structure is hierarchical RST website: http://www.sfu.ca/rst/ ""27 CS447: Natural Language Processing Referring expressions and coreference resolution ""28 CS447: Natural Language Processing How do we refer to entities? ‘a book’, ‘it’, ‘ book’ ""29 ‘this book’ ‘my book’ ‘a book’ ‘the book’ ‘the book I’m reading’ ‘it’ ‘that one’ CS447: Natural Language Processing Some terminology Referring expressions (‘this book’, ‘it’) refer to some entity (e.g. a book), which is called the referent. Co-reference: two referring expressions that refer to the same entity co-refer (are co-referent). I saw a movie last night. I think you should see it too! The referent is evoked in its ﬁrst mention, and accessed in any subsequent mention. ""30 CS447: Natural Language Processing Indeﬁnite NPs -no determiner: I like walnuts. -the indeﬁnite determiner: She sent her a beautiful goose -numerals: I saw three geese. -indeﬁnite quantiﬁers: I ate some walnuts. -(indeﬁnite) this: I saw this beautiful Ford Falcon today Indeﬁnites usually introduce a new discourse entity. They can refer to a speciﬁc entity or not: I’m going to buy a computer today. ""31 CS447: Natural Language Processing Deﬁnite NPs -the deﬁnite article (the book), -demonstrative articles (this/that book, these/those books), -possessives (my/John’s book) Deﬁnite NPs can also consist of -personal pronouns (I, he) -demonstrative pronouns (this, that, these, those) -universal quantiﬁers (all, every) -(unmodiﬁed) proper nouns (John Smith, Mary, Urbana) Deﬁnite NPs refer to an identiﬁable entity (previously mentioned or not) ""32 CS447: Natural Language Processing Information status Every entity can be classiﬁed along two dimensions: Hearer-new vs. hearer-old Speaker assumes entity is (un)known to the hearer Hearer-old: I will call Sandra Thompson. Hearer-new: I will call a colleague in California (=Sandra Thompson) Special case of hearer-old: hearer-inferrable I went to the student union. The food court was really crowded. Discourse-new vs. discourse-old: Speaker introduces new entity into the discourse, or refers to an entity that has been previously introduced. Discourse-old: I will call her/Sandra now. Discourse-new: I will call my friend Sandra now. ""33 CS447: Natural Language Processing Coreference resolution Victoria Chen, Chief Financial Ofﬁcer of Megabucks Banking Corp since 2004, saw her pay jump 20%, to $1.3 million, as the 37-year-old also became the Denver-based ﬁnancial services company’s president. It has been ten years since she came to Megabucks from rival Lotsabucks. Coreference chains: 1. {Victoria Chen, Chief Financial Ofﬁcer...since 2004, her, the 37-year-old, the Denver-based ﬁnancial services company’s president} 2. {Megabucks Banking Corp, Denver-based ﬁnancial services company, Megabucks} 3. {her pay} 4. {rival Lotsabucks} ""34 CS447: Natural Language Processing Coref as binary classiﬁcation Represent each NP-NP pair (+context) as a feature vector. Training: Learn a binary classiﬁer to decide whether NPi is a possible antecedent of NPj Decoding (running the system on new text): -Pass through the text from beginning to end -For each NPi: Go through NPi-1...NP1 to ﬁnd best antecedent NPj. Corefer NPi with NPj. If the classiﬁer can’t identify an antecedent for NPi, it’s a new entity. ""35 CS447: Natural Language Processing Features for Coref resolution -Do the two NPs have the same head noun? (e.g. company) -Do they contain the same modiﬁer? (e.g. Denver-based)? -Does the gender and number of the NPs match? -Does one NP contain an alias (acronym) of the other? (United States = USA, Chief Executive Ofﬁce = CEO) -Is one NP a hypernym/synonym of the other? -Is one NP an appositive of the other? [Victoria Chen], [CFO of Megabucks] -Are both NPs named entities of the same type? [CEO] = PERSON, Victoria Chen = PERSON ""36 CS447: Natural Language Processing Evaluation: B-cubed F-score The test data consists of D documents d with N total mentions m (mention boundaries are given as input) -In the gold standard, each mention m belongs to a ‘true’ cluster of mentions (=connected component) of size tm -In the system output, each mention m belongs to a predicted cluster of mentions (=connected component) of size pm -For each mention m, the intersection of the gold standard and system output clusters deﬁnes a common cluster of mentions of size cm ""37 Precision P = 1 N ! d∈D ! m∈d cm pm Recall R = 1 N ! d∈D ! m∈d cm tm F-measure = 2PR P + R CS447: Natural Language Processing Special case: Pronoun resolution Task: Find the antecedent of an anaphoric pronoun in context 1. John saw a beautiful Ford Falcon at the dealership. 2. He showed it to Bob. 3. He bought it. he2, it2 = John, Ford Falcon, or dealership? he3, it2 = John, Ford Falcon, dealership, or Bob? ""38 CS447: Natural Language Processing Anaphoric pronouns Anaphoric pronouns refer back to some previously introduced entity/discourse referent: John showed Bob his car. He was impressed. John showed Bob his car. This took ﬁve minutes. The antecedent of an anaphor is the previous expression that refers to the same entity. There are number/gender/person agreement constraints: girls can’t be the antecedent of he Usually, we need some form of inference to identify the antecedents. ""39 CS447: Natural Language Processing Salience/Focus Only some recently mentioned entities can be referred to by pronouns: John went to Bob’s party and parked next to a classic Ford Falcon. He went inside and talked to Bob for more than an hour. Bob told him that he recently got engaged. He also said he bought it (??? )/ the Falcon yesterday. Key insight (also captured in Centering Theory) Capturing which entities are salient (in focus) reduces the amount of search (inference) necessary to interpret pronouns! ""40 CS447: Natural Language Processing Entity-based coherence Discourse 1: John went to his favorite music store to buy a piano. It was a store John had frequented for many years. He was excited that he could finally buy a piano. It was closing just as John arrived. Discourse 2: John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. ""41 CS447: Natural Language Processing Entity-based coherence Discourse 1: John went to his favorite music store to buy a piano. It was a store John had frequented for many years. He was excited that he could finally buy a piano. It was closing just as John arrived. Discourse 2: John went to his favorite music store to buy a piano. He had frequented the store for many years. He was excited that he could finally buy a piano. He arrived just as the store was closing for the day. How we refer to entities inﬂuences how coherent a discourse is (Centering theory) ""42 CS447: Natural Language Processing Centering Theory Grosz, Joshi, Weinstein (1986, 1995) A linguistic theory of entity-based coherence and salience It predicts which entities are salient at any point during a discourse. It also predicts whether a discourse is entity-coherent, based on its referring expressions. Centering is about local (=within a discourse segment) coherence and salience Centering theory itself is not a computational model or an algorithm: many of its assumptions are not precise enough to be implemented directly. (Poesio et al. 2004) But many algorithms have been developed based on speciﬁc instantiations of the assumptions that Centering theory makes. The textbook presents a centering-based pronoun-resolution algorithm ""43 CS447: Natural Language Processing Using Centering Theory for Summarization ""44 CS447: Natural Language Processing (J. Hockenmaier) Summarization “The process of distilling the most important information from a text to produce and abridged version for a particular task and user” -Abstract or extract? -Generic (no speciﬁc task/user) or query-focused? -Single-document or multi-document? Output: - Abstracts (of scientiﬁc papers) - Headlines (or newspaper articles) - Snippets (for webpages) - Answers to complex questions (from multiple sources) ""45 CS447: Natural Language Processing (J. Hockenmaier) Extracts from a single document Goal: Produce a paragraph that summarizes a document 1. Content selection: Find ‘important’ (key) sentences Extract key facts/phrases 2. Information ordering: What order should these key facts be presented in? 3. Sentence realization: Produce a coherent paragraph from the list of key facts ""46 CS447: Natural Language Processing (J. Hockenmaier) Centroid-based content selection Which sentences are most central in a document? Binary classiﬁcation task: sentence -> {include, don’t include} Method A: Central sentences = salient/informative sentences: - a sentence is salient if it contains many salient words. - a word is salient (=informative) in a document if it occurs signiﬁcantly more often than expected (if -2 log λ(w) > 10) Likelihood ratio λ(w): Pdoc(w)/PEnglish(w) Method B: Central sentences = most similar to other s’s in doc. - compute sentence-based TF/IDF for the words in a document (sentence=TF/IDF’s document, document= TF/IDF’s collection) - distance between sentences: cosine of TD/IDF vectors of all words - centrality of sentence i: average distance to all other sentences in document ""47 CS447: Natural Language Processing (J. Hockenmaier) RST-based summarization Use a discourse parser to identify rhetorical relations between sentences/clauses in a document. This gives a discourse tree with hierarchical nucleus-satellite relations between clauses This discourse tree deﬁnes a salience ranking: the highest nuclei in the tree are the most salient ""48 CS447: Natural Language Processing (J. Hockenmaier) Information ordering and Sentence Realization In which order should the key phrases be presented? Simplest case: order in which they appear in document Finding the optimal solution is NP-complete, but we can approximate Use centering theory to measure coherence Use coreference resolution and parsing to produce an ‘entity grid’ (which entities occur in which sentence, and in which role), then ﬁnd good sequences of transitions Sentence realization may require some rephrasing: Use longer descriptions to introduce entities, shorter ones to refer back “Bush met Putin today. George W. Bush said…” => “George W. Bush met Putin today. Bush said…” ""49 CS447: Natural Language Processing Happy fall break! ""50 "
278,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 25 Neural Approaches to NLP CS447: Natural Language Processing (J. Hockenmaier) Where we’re at Lecture 25: Word Embeddings and neural LMs Lecture 26: Recurrent networks Lecture 27: Sequence labeling and Seq2Seq Lecture 28: Review for the ﬁnal exam Lecture 29: In-class ﬁnal exam !2 CS447: Natural Language Processing (J. Hockenmaier) Motivation !3 CS447: Natural Language Processing (J. Hockenmaier) NLP research questions redux How do you represent (or predict) words? Do you treat words in the input as atomic categories, as continuous vectors, or as structured objects? How do you handle rare/unseen words, typos, spelling variants, morphological information? Lexical semantics: do you capture word meanings/senses? How do you represent (or predict) word sequences? Sequences = sentences, paragraphs, documents, dialogs,… As a vector, or as a structured object? How do you represent (or predict) structures? Structures = labeled sequences, trees, graphs, formal languages (e.g. DB records/queries, logical representations) How do you represent “meaning”? !4 CS447: Natural Language Processing (J. Hockenmaier) Two core problems for NLP Ambiguity: Natural language is highly ambiguous -Words have multiple senses and different POS -Sentences have a myriad of possible parses -etc. Coverage (compounded by Zipf’s Law) -Any (wide-coverage) NLP system will come across words or constructions that did not occur during training. -We need to be able to generalize from the seen events during training to unseen events that occur during testing (i.e. when we actually use the system). -We typically have very little labeled training data !5 CS447: Natural Language Processing (J. Hockenmaier) Statistical models for NLP NLP makes heavy use of statistical models as a way to handle both the ambiguity and the coverage issues. -Probabilistic models (e.g. HMMs, MEMMs, CRFs, PCFGs) -Other machine learning-based classiﬁers Basic approach: -Decide which output is desired (may depend on available labeled training data) -Decide what kind of model to use -Deﬁne features that could be useful (this may require further processing steps, i.e. a pipeline) -Train and evaluate the model. -Iterate: reﬁne/improve the model and/or the features, etc. !6 CS447: Natural Language Processing (J. Hockenmaier) Example: Language Modeling A language model deﬁnes a distribution P(w) over the strings w = w1w2..wi… in a language Typically we factor P(w) such that we compute the probability word by word: P(w) = P(w1) P(w2 | w1)… P(wi | w1…wi−1) Standard n-gram models make the Markov assumption that wi depends only on the preceding n−1 words: P(wi | w1…wi−1) := P(wi | wi−n+1…wi−1) We know that this independence assumption is invalid (there are many long-range dependencies), but it is computationally and statistically necessary (we can’t store or estimate larger models) !7 CS447: Natural Language Processing (J. Hockenmaier) Motivation for neural approaches to NLP: Markov assumptions Traditional sequence models (n-gram language models, HMMs, MEMMs, CRFs) make rigid Markov assumptions (bigram/trigram/n-gram). Recurrent neural nets (RNNs, LSTMs) can capture arbitrary-length histories without requiring more parameters. !8 CS447: Natural Language Processing (J. Hockenmaier) Features for NLP Many systems use explicit features: - Words (does the word “river” occur in this sentence?) - POS tags - Chunk information, NER labels - Parse trees or syntactic dependencies (e.g. for semantic role labeling, etc.) Feature design is usually a big component of building any particular NLP system. Which features are useful for a particular task and model typically requires experimentation, but there are a number of commonly used ones (words, POS tags, syntactic dependencies, NER labels, etc.) Features deﬁne equivalence classes of data points. Assumption: they provide useful abstractions & generalizations Features may be noisy (because they are compute by other NLP systems) !9 CS447: Natural Language Processing (J. Hockenmaier) Motivation for neural approaches to NLP: Features can be brittle Word-based features: How do we handle unseen/rare words? Many features are produced by other NLP systems (POS tags, dependencies, NER output, etc.) These systems are often trained on labeled data. Producing labeled data can be very expensive. We typically don’t have enough labeled data from the domain of interest. We might not get accurate features for our domain of interest. !10 CS447: Natural Language Processing (J. Hockenmaier) Features in neural approaches Many of the current successful neural approaches to NLP do not use traditional discrete features. Words in the input are often represented as dense vectors (aka. word embeddings, e.g. word2vec) Traditional approaches: each word in the vocabulary is a separate feature. No generalization across words that have similar meanings. Neural approaches: Words with similar meanings have similar vectors. Models generalize across words with similar meanings Other kinds of features (POS tags, dependencies, etc.) are often ignored. !11 CS447: Natural Language Processing (J. Hockenmaier) Neural approaches to NLP !12 CS447: Natural Language Processing (J. Hockenmaier) What is “deep learning”? Neural networks, typically with several hidden layers (depth = # of hidden layers) Single-layer neural nets are linear classiﬁers Multi-layer neural nets are more expressive Very impressive performance gains in computer vision (ImageNet) and speech recognition over the last several years. Neural nets have been around for decades. Why have they suddenly made a comeback? Fast computers (GPUs!) and (very) large datasets have made it possible to train these very complex models. !13 CS447: Natural Language Processing (J. Hockenmaier) What are neural nets? Simplest variant: single-layer feedforward net !14 Input layer: vector x Output unit: scalar y Input layer: vector x Output layer: vector y For binary classiﬁcation tasks: Single output unit Return 1 if y > 0.5 Return 0 otherwise For multiclass classiﬁcation tasks: K output units (a vector) Each output unit yi = class i Return argmaxi(yi) CS447: Natural Language Processing (J. Hockenmaier) Multiclass models: softmax(yi) Multiclass classiﬁcation = predict one of K classes. Return the class i with the highest score: argmaxi(yi) In neural networks, this is typically done by using the softmax function, which maps real-valued vectors in RN into a distribution over the N outputs For a vector z = (z0…zK): P(i) = softmax(zi) = exp(zi) ∕ ∑k=0..K exp(zk) (NB: This is just logistic regression) !15 CS447: Natural Language Processing (J. Hockenmaier) Single-layer feedforward networks Single-layer (linear) feedforward network y = wx + b (binary classiﬁcation) w is a weight vector, b is a bias term (a scalar) This is just a linear classiﬁer (aka Perceptron) (the output y is a linear function of the input x) Single-layer non-linear feedforward networks: Pass wx + b through a non-linear activation function, e.g. y = tanh(wx + b) !16 CS546 Machine Learning in NLP Nonlinear activation functions Sigmoid (logistic function): σ(x) = 1/(1 + e−x) Useful for output units (probabilities) [0,1] range Hyperbolic tangent: tanh(x) = (e2x −1)/(e2x+1) Useful for internal units: [-1,1] range Hard tanh (approximates tanh) htanh(x) = −1 for x < −1, 1 for x > 1, x otherwise Rectiﬁed Linear Unit: ReLU(x) = max(0, x) Useful for internal units !17   '&&%'038""3% /&63""- /&5803,4 UJNFT UP QSPEVDF FYDFMMFOU SFTVMUTŉ ɩF 3F-6 VOJU DMJQT FBDI WBMVF x < 0 BU  %FTQJUF JUT TJN QMJDJUZ JU QFSGPSNT XFMM GPS NBOZ UBTLT FTQFDJBMMZ XIFO DPNCJOFE XJUI UIF ESPQPVU SFHVMBSJ[BUJPO UFDIOJRVF TFF 4FDUJPO   3F-6.x/ D NBY.0; x/ D ( 0 x < 0 x PUIFSXJTF:  ""T B SVMF PG UIVNC CPUI 3F-6 BOE UBOI VOJUT XPSL XFMM BOE TJHOJmDBOUMZ PVUQFSGPSN UIF TJHNPJE :PV NBZ XBOU UP FYQFSJNFOU XJUI CPUI UBOI BOE 3F-6 BDUJWBUJPOT BT FBDI POF NBZ QFSGPSN CFUUFS JO EJĊFSFOU TFUUJOHT 'JHVSF  TIPXT UIF TIBQFT PG UIF EJĊFSFOU BDUJWBUJPOT GVODUJPOT UPHFUIFS XJUI UIF TIBQFT PG UIFJS EFSJWBUJWFT 1.0 0.5 0.0 -0.5 -1.0 -6 -4 -2 0 2 4 6 1.0 0.5 0.0 -0.5 1.0 0.5 0.0 -0.5 -1.0 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 1.0 0.5 0.0 -0.5 1.0 0.5 0.0 -0.5 -1.0 1.0 0.5 0.0 -0.5 -1.0 1.0 0.5 0.0 -0.5 1.0 0.5 0.0 -0.5 sigmoid(x) tanh(x) hardtanh(x) ReLU(x) !f !x !f !x !f !x !f !x CS447: Natural Language Processing (J. Hockenmaier) Input layer: vector x Hidden layer: vector h1 Multi-layer feedforward networks We can generalize this to multi-layer feedforward nets !18 Hidden layer: vector hn Output layer: vector y … … … … … … … … …. CS447: Natural Language Processing (J. Hockenmaier) Challenges in using NNs for NLP Our input and output variables are discrete: words, labels, structures. NNs work best with continuous vectors. We typically want to learn a mapping (embedding) from discrete words (input) to dense vectors. We can do this with (simple) neural nets and related methods. The input to a NN is (traditionally) a ﬁxed-length vector. How do you represent a variable-length sequence as a vector? Use recurrent neural nets: read in one word at the time to predict a vector, use that vector and the next word to predict a new vector, etc. !19 CS447: Natural Language Processing (J. Hockenmaier) How does NLP use NNs? Word embeddings (word2vec, Glove, etc.) Train a NN to predict a word from its context (or the context from a word). This gives a dense vector representation of each word Neural language models: Use recurrent neural networks (RNNs) to predict word sequences More advanced: use LSTMs (special case of RNNs) Sequence-to-sequence (seq2seq) models: From machine translation: use one RNN to encode source string, and another RNN to decode this into a target string. Also used for automatic image captioning, etc. Recursive neural networks: Used for parsing !20 CS447: Natural Language Processing (J. Hockenmaier) Neural Language Models LMs deﬁne a distribution over strings: P(w1….wk) LMs factor P(w1….wk) into the probability of each word: P(w1….wk) = P(w1)·P(w2|w1)·P(w3|w1w2)·…· P(wk | w1….wk−1) A neural LM needs to deﬁne a distribution over the V words in the vocabulary, conditioned on the preceding words. Output layer: V units (one per word in the vocabulary) with softmax to get a distribution Input: Represent each preceding word by its d-dimensional embedding. - Fixed-length history (n-gram): use preceding n−1 words - Variable-length history: use a recurrent neural net !21 CS447: Natural Language Processing (J. Hockenmaier) Recurrent neural networks (RNNs) Basic RNN: Modify the standard feedforward architecture (which predicts a string w0…wn one word at a time) such that the output of the current step (wi) is given as additional input to the next time step (when predicting the output for wi+1). “Output” — typically (the last) hidden layer. !22 input output hidden input output hidden Feedforward Net Recurrent Net CS447: Natural Language Processing (J. Hockenmaier) Word Embeddings (e.g. word2vec) Main idea: If you use a feedforward network to predict the probability of words that appear in the context of (near) an input word, the hidden layer of that network provides a dense vector representation of the input word. Words that appear in similar contexts (that have high distributional similarity) wils have very similar vector representations. These models can be trained on large amounts of raw text (and pretrained embeddings can be downloaded) !23 CS447: Natural Language Processing (J. Hockenmaier) Sequence-to-sequence (seq2seq) Task (e.g. machine translation): Given one variable length sequence as input, return another variable length sequence as output Main idea: Use one RNN to encode the input sequence (“encoder”) Feed the last hidden state as input to a second RNN (“decoder”) that then generates the output sequence. !24 CS546 Machine Learning in NLP Neural Language Models !25 CS546 Machine Learning in NLP What is a language model? Probability distribution over the strings in a language, typically factored into distributions P(wi | …) for each word: P(w) = P(w1…wn) = ∏i P(wi | w1…wi-1) N-gram models assume each word depends only preceding n−1 words: P(wi | w1…wi-1) =def P(wi | wi−n+1…wi−1) To handle variable length strings, we assume each string starts with n−1 start-of-sentence symbols (BOS), or〈S〉 and ends in a special end-of-sentence symbol (EOS) or〈\S〉 !26 CS546 Machine Learning in NLP Shortcomings of traditional n-gram models -Models get very large (and sparse) as n increases -We can’t generalize across similar contexts -Markov (independence) assumptions in n-gram models are too strict Solutions offered by neural models: -Do not represent context words as distinct, discrete symbols, but use a dense vector representation where similar words have similar vectors [today’s class] -Use recurrent nets that can encode variable-lengths contexts [next class] !27 CS546 Machine Learning in NLP Neural n-gram models Task: Represent P(w | w1…wk) with a neural net Assumptions: -We’ll assume each word wi ∈ V in the context is a dense vector v(w): v(w) ∈ Rdim(emb) -V is a ﬁnite vocabulary, containing UNK, BOS, EOS tokens. -We’ll use a feedforward net with one hidden layer h The input x = [v(w1),…,v(wk)] to the NN represents the context w1…wk Each wi ∈ V is a dense vector v(w) The output layer is a softmax: P(w | w1…wk) = softmax(hW2 + b2) !28 CS546 Machine Learning in NLP Neural n-gram models Architecture: Input Layer: x = [v(w1)….v(wk)] v(w) = E[w] Hidden Layer: h = g(xW1 + b1) Output Layer: P(w | w1…wk) = softmax(hW2 + b2) Parameters: Embedding matrix: E ∈ R|V|×dim(emb) Weight matrices and biases: ﬁrst layer: W1 ∈ Rk·dim(emb)×dim(h) b1 ∈ Rdim(h) second layer: W2 ∈ Rk·dim(h)×|V| b2 ∈ R|V| !29 CS546 Machine Learning in NLP Neural n-gram models Advantages over traditional n-gram models: -Increasing the order requires only a small linear increase in the number of parameters. W1 goes from Rk·dim(emb)×dim(h) to R(k+1)·dim(emb)×dim(h) -Increasing the number of words in the vocabulary also leads only to a linear increase in the vocabulary -Easy to incorporate more context: just add more input units -Easy to generalize across contexts (embeddings!) Computing softmax over large V is expensive: requires matrix-vector multiplication with W2, followed by |V| exponentiations Solution (during training): only sample a subset of the vocabulary (or use hierarchical softmax) !30 CS546 Machine Learning in NLP Output embeddings: Each column in W2 is a dim(h)- dimensional vector that is associated with a vocabulary item w ∈ V h is a dense (non-linear) representation of the context Words that are similar appear in similar contexts. Hence their columns in W2 should be similar. Input embeddings: each row in the embedding matrix is a representation of a word. Word representations as by-product of neural LMs !31 hidden layer h output layer CS546 Machine Learning in NLP Modiﬁcations to neural LM If we want good word representations (rather than good language models), we can modify this model: 1) We can also take the words that follow into account: compute P(w3 | w1w2_w4w5) instead of P(w5 | w1w2w3w4) Now, the input context c = w1w2_w4w5, not w1w2w3w4 2) We don’t need a distribution over the output word. We just want the correct output word to have a higher score s(w,c) than other words w’. We remove the softmax, deﬁne s(w,c) as the activation of the output unit for w with input context c and use a (margin-based) ranking loss: Loss for predicting a random word w’ instead of w: L(w, c, w’) = max(0, 1 − (s(w, c) − s(w’, c)) !32 CS546 Machine Learning in NLP Obtaining Word Embeddings !33 CS546 Machine Learning in NLP Word2Vec (Mikolov et al. 2013) Modiﬁcation of neural LM: -Two different context representations: CBOW or Skip-Gram -Two different optimization objectives: Negative sampling (NS) or hierarchical softmax Task: train a classiﬁer to predict a word from its context (or the context from a word) Idea: Use the dense vector representation that this classiﬁer uses as the embedding of the word. !34 CS546 Machine Learning in NLP CBOW vs Skip-Gram !35 w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2) CBOW Skip-gram Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. R words from the future of the current word as correct labels. This will require us to do R ⇥2 word classiﬁcations, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10. CS447: Natural Language Processing (J. Hockenmaier) Word2Vec: CBOW CBOW = Continuous Bag of Words Remove the hidden layer, and the order information of the context. Deﬁne context vector c as a sum of the embedding vectors of each context word ci, and score s(t,c) as tc c = ∑i=1…k ci s(t, c) = tc !36 P( + |t, c) = 1 1 + exp( −(t ⋅c1 + t ⋅c2 + … + t ⋅ck) CS546 Machine Learning in NLP Word2Vec: SkipGram Don’t predict the current word based on its context, but predict the context based on the current word. Predict surrounding C words (here, typically C = 10). Each context word is one training example !37 CS447: Natural Language Processing (J. Hockenmaier) Skip-gram algorithm 1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples 3. Use logistic regression to train a classiﬁer to distinguish those two cases 4. Use the weights as the embeddings 11/27/18 &38 CS546 Machine Learning in NLP Word2Vec: Negative Sampling Training objective: Maximize log-likelihood of training data D+ ∪ D-: !39 L (Q,D,D0) = Â (w,c)2D logP(D = 1|w,c) + Â (w,c)2D0 logP(D = 0|w,c) CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training Data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 target c3 c4 11/27/18 &40 Asssume context words are those in +/- 2 word window CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Goal Given a tuple (t,c) = target, context (apricot, jam) (apricot, aardvark) Return the probability that c is a real context word: P(D = + | t, c) P( D= − | t, c) = 1 − P(D = + | t, c) 11/27/18 &41 CS447: Natural Language Processing (J. Hockenmaier) How to compute p(+ | t, c)? Intuition: Words are likely to appear near similar words Model similarity with dot-product! Similarity(t,c) ∝ t · c Problem: Dot product is not a probability! (Neither is cosine) &42 CS447: Natural Language Processing (J. Hockenmaier) Turning the dot product into a probability The sigmoid lies between 0 and 1: &43 σ(x) = 1 1 + exp(−x) P( + |t, c) = 1 1 + exp(−t ⋅c) P( −|t, c) = 1 − 1 1 + exp(−t ⋅c) = exp(−t ⋅c) 1 + exp(−t ⋅c) CS546 Machine Learning in NLP Word2Vec: Negative Sampling Distinguish “good” (correct) word-context pairs (D=1), from “bad” ones (D=0) Probabilistic objective: P( D = 1 | t, c ) deﬁned by sigmoid: P( D = 0 | t, c ) = 1 — P( D = 0 | t, c ) P( D = 1 | t, c ) should be high when (t, c) ∈ D+, and low when (t,c) ∈ D- !44 P(D = 1|w,c) = 1 1+exp(−s(w,c)) CS447: Natural Language Processing (J. Hockenmaier) For all the context words Assume all context words c1:k are independent: !45 P( + |t, c1:k) = k ∏ i=1 1 1 + exp(−t ⋅ci) log P( + |t, c1:k) = k ∑ i=1 log 1 1 + exp(−t ⋅ci) CS546 Machine Learning in NLP Word2Vec: Negative Sampling Training data: D+ ∪ D- D+ = actual examples from training data Where do we get D- from? Lots of options. Word2Vec: for each good pair (w,c), sample k words and add each wi as a negative example (wi,c) to D’ (D’ is k times as large as D) Words can be sampled according to corpus frequency or according to smoothed variant where freq’(w) = freq(w)0.75 (This gives more weight to rare words) !46 CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 Training data: input/output pairs centering on apricot Assume a +/- 2 word window !47 CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 Training data: input/output pairs centering on apricot Assume a +/- 2 word window Positive examples: (apricot, tablespoon), (apricot, of), (apricot, jam), (apricot, a) For each positive example, create k negative examples, using noise words: (apricot, aardvark), (apricot, puddle)… !48 CS447: Natural Language Processing (J. Hockenmaier) Summary: How to learn word2vec (skip-gram) embeddings For a vocabulary of size V: Start with V random 300- dimensional vectors as initial embeddings Train a logistic regression classiﬁer to distinguish words that co-occur in corpus from those that don’t Pairs of words that co-occur are positive examples Pairs of words that don't co-occur are negative examples Train the classiﬁer to distinguish these by slowly adjusting all the embeddings to improve the classiﬁer performance Throw away the classiﬁer code and keep the embeddings. &49 CS447: Natural Language Processing (J. Hockenmaier) Evaluating embeddings Compare to human scores on word similarity-type tasks: WordSim-353 (Finkelstein et al., 2002) SimLex-999 (Hill et al., 2015) Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) TOEFL dataset: Levied is closest in meaning to: imposed, believed, requested, correlated &50 CS447: Natural Language Processing (J. Hockenmaier) Properties of embeddings Similarity depends on window size C C = ±2 The nearest words to Hogwarts: Sunnydale Evernight C = ±5 The nearest words to Hogwarts: Dumbledore Malfoy hal@lood &51 CS447: Natural Language Processing (J. Hockenmaier) Analogy: Embeddings capture relational meaning! vector(‘king’) - vector(‘man’) + vector(‘woman’) = vector(‘queen’) vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’) &52 CS546 Machine Learning in NLP Using Word Embeddings !53 CS546 Machine Learning in NLP Using pre-trained embeddings Assume you have pre-trained embeddings E. How do you use them in your model? -Option 1: Adapt E during training Disadvantage: only words in training data will be affected. -Option 2: Keep E ﬁxed, but add another hidden layer that is learned for your task -Option 3: Learn matrix T ∈ dim(emb)×dim(emb) and use rows of E’ = ET (adapts all embeddings, not speciﬁc words) -Option 4: Keep E ﬁxed, but learn matrix Δ ∈ R|V|×dim(emb) and use E’ = E + Δ or E’ = ET + Δ (this learns to adapt speciﬁc words) !54 CS546 Machine Learning in NLP More on embeddings Embeddings aren’t just for words! You can take any discrete input feature (with a ﬁxed number of K outcomes, e.g. POS tags, etc.) and learn an embedding matrix for that feature. Where do we get the input embeddings from? We can learn the embedding matrix during training. Initialization matters: use random weights, but in special range (e.g. [-1/(2d), +(1/2d)] for d-dimensional embeddings), or use Xavier initialization We can also use pre-trained embeddings LM-based embeddings are useful for many NLP task !55 CS447: Natural Language Processing (J. Hockenmaier) Dense embeddings you can download! Word2vec (Mikolov et al.) https://code.google.com/archive/p/word2vec/ Fasttext http://www.fasttext.cc/ Glove (Pennington, Socher, Manning) http://nlp.stanford.edu/projects/glove/ &56 "
279,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 26 Word Embeddings and Recurrent Nets CS447: Natural Language Processing (J. Hockenmaier) Where we’re at Lecture 25: Word Embeddings and neural LMs Lecture 26: Recurrent networks Lecture 27: Sequence labeling and Seq2Seq Lecture 28: Review for the ﬁnal exam Lecture 29: In-class ﬁnal exam !2 CS447: Natural Language Processing (J. Hockenmaier) Recap !3 CS447: Natural Language Processing (J. Hockenmaier) What are neural nets? Simplest variant: single-layer feedforward net !4 Input layer: vector x Output unit: scalar y Input layer: vector x Output layer: vector y For binary classiﬁcation tasks: Single output unit Return 1 if y > 0.5 Return 0 otherwise For multiclass classiﬁcation tasks: K output units (a vector) Each output unit yi = class i Return argmaxi(yi) CS447: Natural Language Processing (J. Hockenmaier) Input layer: vector x Hidden layer: vector h1 Multi-layer feedforward networks We can generalize this to multi-layer feedforward nets !5 Hidden layer: vector hn Output layer: vector y … … … … … … … … …. CS447: Natural Language Processing (J. Hockenmaier) Multiclass models: softmax(yi) Multiclass classiﬁcation = predict one of K classes. Return the class i with the highest score: argmaxi(yi) In neural networks, this is typically done by using the softmax function, which maps real-valued vectors in RN into a distribution over the N outputs For a vector z = (z0…zK): P(i) = softmax(zi) = exp(zi) ∕ ∑k=0..K exp(zk) (NB: This is just logistic regression) !6 CS546 Machine Learning in NLP Neural Language Models !7 CS447: Natural Language Processing (J. Hockenmaier) Neural Language Models LMs deﬁne a distribution over strings: P(w1….wk) LMs factor P(w1….wk) into the probability of each word: P(w1….wk) = P(w1)·P(w2|w1)·P(w3|w1w2)·…· P(wk | w1….wk−1) A neural LM needs to deﬁne a distribution over the V words in the vocabulary, conditioned on the preceding words. Output layer: V units (one per word in the vocabulary) with softmax to get a distribution Input: Represent each preceding word by its d-dimensional embedding. - Fixed-length history (n-gram): use preceding n−1 words - Variable-length history: use a recurrent neural net !8 CS546 Machine Learning in NLP Neural n-gram models Task: Represent P(w | w1…wk) with a neural net Assumptions: -We’ll assume each word wi ∈ V in the context is a dense vector v(w): v(w) ∈ Rdim(emb) -V is a ﬁnite vocabulary, containing UNK, BOS, EOS tokens. -We’ll use a feedforward net with one hidden layer h The input x = [v(w1),…,v(wk)] to the NN represents the context w1…wk Each wi ∈ V is a dense vector v(w) The output layer is a softmax: P(w | w1…wk) = softmax(hW2 + b2) !9 CS546 Machine Learning in NLP Neural n-gram models Architecture: Input Layer: x = [v(w1)….v(wk)] v(w) = E[w] Hidden Layer: h = g(xW1 + b1) Output Layer: P(w | w1…wk) = softmax(hW2 + b2) Parameters: Embedding matrix: E ∈ R|V|×dim(emb) Weight matrices and biases: ﬁrst layer: W1 ∈ Rk·dim(emb)×dim(h) b1 ∈ Rdim(h) second layer: W2 ∈ Rk·dim(h)×|V| b2 ∈ R|V| !10 CS546 Machine Learning in NLP Output embeddings: Each column in W2 is a dim(h)- dimensional vector that is associated with a vocabulary item w ∈ V h is a dense (non-linear) representation of the context Words that are similar appear in similar contexts. Hence their columns in W2 should be similar. Input embeddings: each row in the embedding matrix is a representation of a word. Word representations as by-product of neural LMs !11 hidden layer h output layer CS546 Machine Learning in NLP Obtaining Word Embeddings !12 CS447: Natural Language Processing (J. Hockenmaier) Word Embeddings (e.g. word2vec) Main idea: If you use a feedforward network to predict the probability of words that appear in the context of (near) an input word, the hidden layer of that network provides a dense vector representation of the input word. Words that appear in similar contexts (that have high distributional similarity) wils have very similar vector representations. These models can be trained on large amounts of raw text (and pretrained embeddings can be downloaded) !13 CS546 Machine Learning in NLP Word2Vec (Mikolov et al. 2013) Modiﬁcation of neural LM: -Two different context representations: CBOW or Skip-Gram -Two different optimization objectives: Negative sampling (NS) or hierarchical softmax Task: train a classiﬁer to predict a word from its context (or the context from a word) Idea: Use the dense vector representation that this classiﬁer uses as the embedding of the word. !14 CS546 Machine Learning in NLP CBOW vs Skip-Gram !15 w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2) CBOW Skip-gram Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. R words from the future of the current word as correct labels. This will require us to do R ⇥2 word classiﬁcations, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10. CS447: Natural Language Processing (J. Hockenmaier) Word2Vec: CBOW CBOW = Continuous Bag of Words Remove the hidden layer, and the order information of the context. Deﬁne context vector c as a sum of the embedding vectors of each context word ci, and score s(t,c) as tc c = ∑i=1…k ci s(t, c) = tc !16 P( + |t, c) = 1 1 + exp( −(t ⋅c1 + t ⋅c2 + … + t ⋅ck) CS546 Machine Learning in NLP Word2Vec: SkipGram Don’t predict the current word based on its context, but predict the context based on the current word. Predict surrounding C words (here, typically C = 10). Each context word is one training example !17 CS447: Natural Language Processing (J. Hockenmaier) Skip-gram algorithm 1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples 3. Use logistic regression to train a classiﬁer to distinguish those two cases 4. Use the weights as the embeddings 11/27/18 &18 CS546 Machine Learning in NLP Word2Vec: Negative Sampling Training objective: Maximize log-likelihood of training data D+ ∪ D-: !19 L (Q,D,D0) = Â (w,c)2D logP(D = 1|w,c) + Â (w,c)2D0 logP(D = 0|w,c) CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training Data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 target c3 c4 11/27/18 &20 Asssume context words are those in +/- 2 word window CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Goal Given a tuple (t,c) = target, context (apricot, jam) (apricot, aardvark) Return the probability that c is a real context word: P(D = + | t, c) P( D= − | t, c) = 1 − P(D = + | t, c) 11/27/18 &21 CS447: Natural Language Processing (J. Hockenmaier) How to compute p(+ | t, c)? Intuition: Words are likely to appear near similar words Model similarity with dot-product! Similarity(t,c) ∝ t · c Problem: Dot product is not a probability! (Neither is cosine) &22 CS447: Natural Language Processing (J. Hockenmaier) Turning the dot product into a probability The sigmoid lies between 0 and 1: &23 σ(x) = 1 1 + exp(−x) P( + |t, c) = 1 1 + exp(−t ⋅c) P( −|t, c) = 1 − 1 1 + exp(−t ⋅c) = exp(−t ⋅c) 1 + exp(−t ⋅c) CS546 Machine Learning in NLP Word2Vec: Negative Sampling Distinguish “good” (correct) word-context pairs (D=1), from “bad” ones (D=0) Probabilistic objective: P( D = 1 | t, c ) deﬁned by sigmoid: P( D = 0 | t, c ) = 1 — P( D = 0 | t, c ) P( D = 1 | t, c ) should be high when (t, c) ∈ D+, and low when (t,c) ∈ D- !24 P(D = 1|w,c) = 1 1+exp(−s(w,c)) CS447: Natural Language Processing (J. Hockenmaier) For all the context words Assume all context words c1:k are independent: !25 P( + |t, c1:k) = k ∏ i=1 1 1 + exp(−t ⋅ci) log P( + |t, c1:k) = k ∑ i=1 log 1 1 + exp(−t ⋅ci) CS546 Machine Learning in NLP Word2Vec: Negative Sampling Training data: D+ ∪ D- D+ = actual examples from training data Where do we get D- from? Lots of options. Word2Vec: for each good pair (w,c), sample k words and add each wi as a negative example (wi,c) to D’ (D’ is k times as large as D) Words can be sampled according to corpus frequency or according to smoothed variant where freq’(w) = freq(w)0.75 (This gives more weight to rare words) !26 CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 Training data: input/output pairs centering on apricot Assume a +/- 2 word window !27 CS447: Natural Language Processing (J. Hockenmaier) Skip-Gram Training data Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 Training data: input/output pairs centering on apricot Assume a +/- 2 word window Positive examples: (apricot, tablespoon), (apricot, of), (apricot, jam), (apricot, a) For each positive example, create k negative examples, using noise words: (apricot, aardvark), (apricot, puddle)… !28 CS447: Natural Language Processing (J. Hockenmaier) Summary: How to learn word2vec (skip-gram) embeddings For a vocabulary of size V: Start with V random 300- dimensional vectors as initial embeddings Train a logistic regression classiﬁer to distinguish words that co-occur in corpus from those that don’t Pairs of words that co-occur are positive examples Pairs of words that don't co-occur are negative examples Train the classiﬁer to distinguish these by slowly adjusting all the embeddings to improve the classiﬁer performance Throw away the classiﬁer code and keep the embeddings. &29 CS447: Natural Language Processing (J. Hockenmaier) Evaluating embeddings Compare to human scores on word similarity-type tasks: WordSim-353 (Finkelstein et al., 2002) SimLex-999 (Hill et al., 2015) Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) TOEFL dataset: Levied is closest in meaning to: imposed, believed, requested, correlated &30 CS447: Natural Language Processing (J. Hockenmaier) Properties of embeddings Similarity depends on window size C C = ±2 The nearest words to Hogwarts: Sunnydale Evernight C = ±5 The nearest words to Hogwarts: Dumbledore Malfoy hal@lood &31 CS447: Natural Language Processing (J. Hockenmaier) Analogy: Embeddings capture relational meaning! vector(‘king’) - vector(‘man’) + vector(‘woman’) = vector(‘queen’) vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’) &32 CS546 Machine Learning in NLP Using Word Embeddings !33 CS546 Machine Learning in NLP Using pre-trained embeddings Assume you have pre-trained embeddings E. How do you use them in your model? -Option 1: Adapt E during training Disadvantage: only words in training data will be affected. -Option 2: Keep E ﬁxed, but add another hidden layer that is learned for your task -Option 3: Learn matrix T ∈ dim(emb)×dim(emb) and use rows of E’ = ET (adapts all embeddings, not speciﬁc words) -Option 4: Keep E ﬁxed, but learn matrix Δ ∈ R|V|×dim(emb) and use E’ = E + Δ or E’ = ET + Δ (this learns to adapt speciﬁc words) !34 CS546 Machine Learning in NLP More on embeddings Embeddings aren’t just for words! You can take any discrete input feature (with a ﬁxed number of K outcomes, e.g. POS tags, etc.) and learn an embedding matrix for that feature. Where do we get the input embeddings from? We can learn the embedding matrix during training. Initialization matters: use random weights, but in special range (e.g. [-1/(2d), +(1/2d)] for d-dimensional embeddings), or use Xavier initialization We can also use pre-trained embeddings LM-based embeddings are useful for many NLP task !35 CS447: Natural Language Processing (J. Hockenmaier) Dense embeddings you can download! Word2vec (Mikolov et al.) https://code.google.com/archive/p/word2vec/ Fasttext http://www.fasttext.cc/ Glove (Pennington, Socher, Manning) http://nlp.stanford.edu/projects/glove/ &36 CS447: Natural Language Processing (J. Hockenmaier) Recurrent Neural Nets (RNNs) !37 CS447: Natural Language Processing (J. Hockenmaier) Recurrent Neural Nets (RNNs) The input to a feedforward net has a ﬁxed size. How do we handle variable length inputs? In particular, how do we handle variable length sequences? RNNs handle variable length sequences There are 3 main variants of RNNs, which differ in their internal structure: basic RNNs (Elman nets) LSTMs GRUs !38 CS447: Natural Language Processing (J. Hockenmaier) Recurrent neural networks (RNNs) Basic RNN: Modify the standard feedforward architecture (which predicts a string w0…wn one word at a time) such that the output of the current step (wi) is given as additional input to the next time step (when predicting the output for wi+1). “Output” — typically (the last) hidden layer. !39 input output hidden input output hidden Feedforward Net Recurrent Net CS447: Natural Language Processing (J. Hockenmaier) Basic RNNs Each time step corresponds to a feedforward net where the hidden layer gets its input not just from the layer below but also from the activations of the hidden layer at the previous time step !40 input output hidden CS447: Natural Language Processing (J. Hockenmaier) Basic RNNs Each time step corresponds to a feedforward net where the hidden layer gets its input not just from the layer below but also from the activations of the hidden layer at the previous time step !41 CS447: Natural Language Processing (J. Hockenmaier) A basic RNN unrolled in time !42 CS447: Natural Language Processing (J. Hockenmaier) RNNs for language modeling If our vocabulary consists of V words, the output layer (at each time step) has V units, one for each word. The softmax gives a distribution over the V words for the next word. To compute the probability of a string w0w1…wn wn+1 (where w0 = <s>, and wn+1 = <\s>), feed in wi as input at time step i and compute !43 ∏ i=1..n+1 P(wi|w0 . . . wi−1) CS447: Natural Language Processing (J. Hockenmaier) RNNs for language generation To generate a string w0w1…wn wn+1 (where w0 = <s>, and wn+1 = <\s>), give w0 as ﬁrst input, and then pick the next word according to the computed probability Feed this word in as input into the next layer. Greedy decoding: always pick the word with the highest probability (this only generates a single sentence — why?) Sampling: sample according to the given distribution !44 P(wi|w0 . . . wi−1) CS447: Natural Language Processing (J. Hockenmaier) RNNs for sequence labeling In sequence labeling, we want to assign a label or tag ti to each word wi Now the output layer gives a distribution over the T possible tags. The hidden layer contains information about the previous words and the previous tags. To compute the probability of a tag sequence t1…tn for a given string w1…wn feed in wi (and possibly ti-1) as input at time step i and compute P(ti | w1…wi-1, t1…ti-1) !45 CS447: Natural Language Processing (J. Hockenmaier) RNNs for sequence classiﬁcation If we just want to assign a label to the entire sequence, we don’t need to produce output at each time step, so we can use a simpler architecture. We can use the hidden state of the last word in the sequence as input to a feedforward net: !46 CS447: Natural Language Processing (J. Hockenmaier) Stacked RNNs We can create an RNN that has “vertical” depth (at each time step) by stacking: !47 CS447: Natural Language Processing (J. Hockenmaier) Bidirectional RNNs Unless we need to generate a sequence, we can run two RNNs over the input sequence — one in the forward direction, and one in the backward direction. Their hidden states will capture different context information. !48 CS447: Natural Language Processing (J. Hockenmaier) Further extensions Character and substring embeddings We can also learn embeddings for individual letters. This helps generalize better to rare words, typos, etc. These embeddings can be combined with word embeddings (or used instead of an UNK embedding) Context-dependent embeddings (ELMO, BERT, ….) Word2Vec etc. are static embeddings: they induce a type- based lexicon that doesn’t handle polysemy etc. Context-dependent embeddings produce token-speciﬁc embeddings that depend on the particular context in which a word appears. !49 "
28,"School of Computer Science Probabilistic Graphical Models Max-margin learning of GM Eric Xing Lecture 28, Apr 28, 2014 Reading: b r a c e 1 © Eric Xing @ CMU, 2005-2014 Classical Predictive Models • Input and output space: • Predictive function : • Examples: • Learning: where represents a convex loss, and is a regularizer preventing overfitting – Logistic Regression • Max-likelihood (or MAP) estimation • Corresponds to a Log loss with L2 R – Support Vector Machines (SVM) • Max-margin learning • Corresponds to a hinge loss with L2 R 2 © Eric Xing @ CMU, 2005-2014 Classical Predictive Models • Input and output space: • Learning: where represents a convex loss, and is a regularizer preventing overfitting – Logistic Regression • Max-likelihood (or MAP) estimation • Corresponds to a Log loss with L2 R – Support Vector Machines (SVM) • Max-margin learning • Corresponds to a hinge loss with L2 R Advantages: 1. Full probabilistic semantics 2. Straightforward Bayesian or direct regularization 3. Hidden structures or generative hierarchy Advantages: 1. Dual sparsity: few support vectors 2. Kernel tricks 3. Strong empirical results 3 © Eric Xing @ CMU, 2005-2014 Structured Prediction Problem “Do you want sugar in it?”  <verb pron verb noun prep pron> Unstructured prediction Structured prediction  Part of speech tagging  Image segmentation 4 © Eric Xing @ CMU, 2005-2014 OCR example brace Sequential structure x y a- z a- z a- z a- z a- z y x 5 © Eric Xing @ CMU, 2005-2014 Image Segmentation  Jointly segmenting/annotating images  Image-image matching, image- text matching  Problem:  Given structure (feature), learning  Learning sparse, interpretable, predictive structures/features         c c c c y x f x Z x y p ) , ( exp ) , ( 1 ) | (    6 © Eric Xing @ CMU, 2005-2014 Challenge: Structured outputs, and globally constrained to be a valid tree Dependency parsing of Sentences 7 © Eric Xing @ CMU, 2005-2014 Structured Prediction Graphical Models  Conditional Random Fields (CRFs) (Lafferty et al 2001) – Based on a Logistic Loss (LR) – Max-likelihood estimation (point- estimate)  Max-margin Markov Networks (M3Ns) (Taskar et al 2003) – Based on a Hinge Loss (SVM) – Max-margin learning (point-estimate) • Markov properties are encoded in the feature functions • Input and output space: L(D; w) , log X y0 exp(w>f(x; y0)) ¡w>f(x; y) L(D; w) , log max y0 w>f(x; y0) ¡w>f(x; y) + `(y0; y) 8 © Eric Xing @ CMU, 2005-2014 Structured Prediction Graphical Models Challenges: • SPARSE “Interpretable” prediction model • Prior information of structures • Latent structures/variables • Time series and non-stationarity • Scalable to large-scale problems (e.g., 104 input/output dimension) +R(w) Conditional Random Fields (CRFs) (Lafferty et al 2001) – Based on a Logistic Loss (LR) – Max-likelihood estimation (point- estimate)  Max-margin Markov Networks (M3Ns) (Taskar et al 2003) – Based on a Hinge Loss (SVM) – Max-margin learning (point-estimate) L(D; w) , log X y0 exp(w>f(x; y0)) ¡w>f(x; y) L(D; w) , log max y0 w>f(x; y0) ¡w>f(x; y) + `(y0; y) +R(w) 9 © Eric Xing @ CMU, 2005-2014 Comparing to unstructured predictive models • Input and output space: • Learning: where represents a convex loss, and is a regularizer preventing overfitting – Logistic Regression • Max-likelihood (or MAP) estimation • Corresponds to a Log loss with L2 R – Support Vector Machines (SVM) • Max-margin learning • Corresponds to a hinge loss with L2 R 10 © Eric Xing @ CMU, 2005-2014 Structured models space of feasible outputs scoring function Assumptions: linear combination of features sum of part scores: • index p represents a part in the structure 11 © Eric Xing @ CMU, 2005-2014 Large Margin Estimation © Eric Xing @ CMU, 2005-2014 Given training example (x, y*), we want: *Taskar et al. 03 Maximize margin Mistake weighted margin: # of mistakes in y Large Margin Estimation Recall from SVMs: Maximizing margin is equivalent to minimizing the square of the L2-norm of the weight vector w: New objective function: 13 © Eric Xing @ CMU, 2005-2014 OCR Example We want: argmaxword wT f( , word) = “brace” Equivalently: wT f( ,“brace”) > wT f( ,“aaaaa”) wT f( ,“brace”) > wT f( ,“aaaab”) … wT f( ,“brace”) > wT f( ,“zzzzz”) a lot! 14 © Eric Xing @ CMU, 2005-2014 Brute force enumeration of constraints:  The constraints are exponential in the size of the structure Alternative: min-max formulation  add only the most violated constraint  Handles more general loss functions  Only polynomial # of constraints needed Min-max Formulation 15 © Eric Xing @ CMU, 2005-2014 Min-max Formulation Key step: convert the maximization in the constraint from discrete to continuous  This enables us to plug it into a QP How to do this conversion?  Linear chain example in the next slides  discrete optim. continuous optim. 16 © Eric Xing @ CMU, 2005-2014 y z map for linear chain structures 0 1 . 0 0 0 . 0 . . . 0 0 0 0 0 1 0 : 0 0 1 : 0 1 0 : 0 0 1 : 0 0 1 : 0 A B : B 0 0 . 0 1 0 . 0 . . . 0 0 0 0 0 0 1 . 0 0 0 . 0 . . . 0 0 0 0 0 0 0 . 0 0 1 . 0 . . . 0 0 0 0 0 A B : B A B . B A B . B A B . B A B . B 17 © Eric Xing @ CMU, 2005-2014 y z map for linear chain structures 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 normalization agreement Rewriting the maximization function in terms of indicator variables: 18 © Eric Xing @ CMU, 2005-2014 Min-max formulation Original problem: Transformed problem:  Has integral solutions z for chains, trees  Can be fractional for untriangulated networks 19 © Eric Xing @ CMU, 2005-2014 Min-max formulation Using strong Lagrangian duality: (beyond the scope of this lecture) Use the result above to minimize jointly over w and : 20 © Eric Xing @ CMU, 2005-2014 Min-max formulation Formulation produces compact QP for  Low-treewidth Markov networks  Associative Markov networks  Context free grammars  Bipartite matchings  Any problem with compact LP inference © Eric Xing @ CMU, 2005-2014 Results: Handwriting Recognition © Eric Xing @ CMU, 2005-2014 Length: ~8 chars Letter: 16x8 pixels 10-fold Train/Test 5000/50000 letters 600/6000 words Models: Multiclass-SVMs* CRFs M3 nets Crammer & Singer 01 0 5 10 15 20 25 30 CRFs MC–SVMs M^3 nets Test error (average per-character) raw pixels quadratic kernel cubic kernel 45% error reduction over linear CRFs 33% error reduction over multiclass SVMs better Results: Hypertext Classification © Eric Xing @ CMU, 2005-2014 0 5 10 15 20 Test Error SVMs RMNS M^3Ns WebKB dataset  Four CS department websites: 1300 pages/3500 links  Classify each page: faculty, course, student, project, other  Train on three universities/test on fourth 53% error reduction over SVMs 38% error reduction over RMNs *Taskar et al 02 better MLE versus max-margin learning Likelihood-based estimation – Probabilistic (joint/conditional likelihood model) – Easy to perform Bayesian learning, and incorporate prior knowledge, latent structures, missing data – Bayesian or direct regularization – Hidden structures or generative hierarchy • Max-margin learning – Non-probabilistic (concentrate on input- output mapping) – Not obvious how to perform Bayesian learning or consider prior, and missing data – Support vector property, sound theoretical guarantee with limited samples – Kernel tricks • Maximum Entropy Discrimination (MED) (Jaakkola, et al., 1999) – Model averaging – The optimization problem (binary classification) 24 © Eric Xing @ CMU, 2005-2014 Structured MaxEnt Discrimination (SMED): Feasible subspace of weight distribution: Average from distribution of M3Ns Maximum Entropy Discrimination Markov Networks p 25 © Eric Xing @ CMU, 2005-2014 Solution to MaxEnDNet Theorem: – Posterior Distribution: – Dual Optimization Problem: 26 © Eric Xing @ CMU, 2005-2014 Gaussian MaxEnDNet (reduction to M3N) Theorem – Assume Posterior distribution: Dual optimization: Predictive rule: Thus, MaxEnDNet subsumes M3Ns and admits all the merits of max-margin learning Furthermore, MaxEnDNet has at least three advantages … M3N 27 © Eric Xing @ CMU, 2005-2014 Three Advantages An averaging Model: PAC-Bayesian prediction error guarantee (Theorem 3) Entropy regularization: Introducing useful biases Standard Normal prior => reduction to standard M3N (we’ve seen it) Laplace prior => Posterior shrinkage effects (sparse M3N) Integrating Generative and Discriminative principles (next class) Incorporate latent variables and structures (PoMEN) Semisupervised learning (with partially labeled data) 28 © Eric Xing @ CMU, 2005-2014 Laplace MaxEnDNet (primal sparse M3N) Laplace Prior: Corollary 4: Under a Laplace MaxEnDNet, the posterior mean of parameter vector w is: The Gaussian MaxEnDNet and the regular M3N has no such shrinkage there, we have (Zhu and Xing, ICML 2009) 29 © Eric Xing @ CMU, 2005-2014 LapMEDN vs. L2 and L1 regularization L1 and L2 norms KL norms Corollary 5: LapMEDN corresponding to solving the following primal optimization problem:  KL norm: 30 © Eric Xing @ CMU, 2005-2014 Recall Primal and Dual Problems of M3Ns Primal problem:  Algorithms – Cutting plane – Sub-gradient – … Dual problem:  Algorithms: – SMO – Exponentiated gradient – … • So, M3N is dual sparse! 31 © Eric Xing @ CMU, 2005-2014  Exact primal or dual function is hard to optimize  Use the hierarchical representation of Laplace prior, we get:  We optimize an upper bound:  Why is it easier? – Alternating minimization leads to nicer optimization problems Keep fixed Keep fixed - The effective prior is normal - Closed form solution of and its expectation Variational Learning of LapMEDN 32 © Eric Xing @ CMU, 2005-2014 Algorithmic issues of solving M3Ns Primal problem:  Algorithms – Cutting plane – Sub-gradient – … Dual problem:  Algorithms: – SMO – Exponentiated gradient – … Nonlinear Features with Kernels  Generative entropic kernels [Martins et al, JMLR 2009]  Nonparametric RKHS embedding of rich distributions [on going] Approximate decoders for global features  LP-relaxed Inference (polyhedral outer approx.) [Martins et al, ICML 09, ACL 09]  Balancing Accuracy and Runtime: Loss-augmented inference 33 © Eric Xing @ CMU, 2005-2014 Experimental results on OCR datasets brace Structured output x y a- z a- z a- z a- z a- z y x 34 © Eric Xing @ CMU, 2005-2014 We randomly construct OCR100, OCR150, OCR200, and OCR250 for 10 fold CV. Experimental results on OCR datasets 35 © Eric Xing @ CMU, 2005-2014 Feature Selection 36 © Eric Xing @ CMU, 2005-2014 Sensitivity to Regularization Constants • L1-CRFs are much sensitive to regularization constants; the others are more stable • LapM3N is the most stable one L1-CRF and L2-CRF: - 0.001, 0.01, 0.1, 1, 4, 9, 16 M3N and LapM3N: - 1, 4, 9, 16, 25, 36, 49, 64, 81 37 © Eric Xing @ CMU, 2005-2014 Summary: Margin-based Learning Paradigms Structured prediction Structured prediction Bayes learning Bayes learning 38 © Eric Xing @ CMU, 2005-2014 Open Problems Unsupervised CRF learning and MaxMargin Learning  Only X, but not Y (sometimes part of Y), is available  We want to recognize a pattern that is maximally different from the rest!  What does margin or conditional likelihood mean in these cases? Given only {Xn}, how can we define the cost function?  Algorithmic challenge  Stay tuned for lecture 29!         c c c c y x f x Z x y p ) , ( exp ) , ( ) | (    1   ) , ' ( ) , ( margin n n n n T x y F x y F w   39 © Eric Xing @ CMU, 2005-2014 Remember: Elements of Learning  Here are some important elements to consider before you start:  Task:  Embedding? Classification? Clustering? Topic extraction? …  Data and other info:  Input and output (e.g., continuous, binary, counts, …)  Supervised or unsupervised, of a blend of everything?  Prior knowledge? Bias?  Models and paradigms:  BN? MRF? Regression? SVM?  Bayesian/Frequents ? Parametric/Nonparametric?  Objective/Loss function:  MLE? MCLE? Max margin?  Log loss, hinge loss, square loss? …  Tractability and exactness trade off:  Exact inference? MCMC? Variational? Gradient? Greedy search?  Online? Batch? Distributed?  Evaluation:  Visualization? Human interpretability? Perperlexity? Predictive accuracy?  It is better to consider one element at a time! 40 © Eric Xing @ CMU, 2005-2014 "
280,"CS447: Natural Language Processing http://courses.engr.illinois.edu/cs447 Julia Hockenmaier juliahmr@illinois.edu 3324 Siebel Center Lecture 27 Seq2Seq, Attention; Generation and Dialog CS447: Natural Language Processing (J. Hockenmaier) Final exam Wednesday, Dec 12 in class Only materials after midterm Same format as midterm Review session this Friday! !2 CS447: Natural Language Processing (J. Hockenmaier) Where we’re at Lecture 25: Word Embeddings and neural LMs Lecture 26: Recurrent networks and Sequence Labeling Lecture 27: Seq2Seq, Attention, Generation and Dialog Lecture 28: Review for the ﬁnal exam Lecture 29: In-class ﬁnal exam !3 CS447: Natural Language Processing (J. Hockenmaier) Today’s lecture Traditional NLG and traditional dialogue systems very quick overview The workhorse behind current neural approaches: seq2seq models with attention !4 CS447: Natural Language Processing (J. Hockenmaier) Traditional NLG… !5 CS447: Natural Language Processing (J. Hockenmaier) What is Generation? Automatic production of natural language text, usually from underlying semantic representation -As “natural-language front ends” used to present information in databases etc.: weather forecasts, train systems, (personalized) museum/restaurant/shopping guides,… -In dialog systems -In summarization systems -In authoring aids to help people create routine documents: customer support, job ads, etc… !6 CS447: Natural Language Processing (J. Hockenmaier) Example: Rail travel information system -Domain knowledge: Train schedules -User Input: from a graphical user interface, or in natural language: “How can I get from Aberdeen to Glasgow?” -Desired output: There are 20 trains each day from Aberdeen to Glasgow. The next train is the Caledonian Express; it leaves Aberdeen at 10am. It is due to arrive in Glasgow at 1pm, but arrival may be slightly delayed because of snow on the track near Stirling. !7 CS447: Natural Language Processing (J. Hockenmaier) Some NLG systems !8 CS447: Natural Language Processing (J. Hockenmaier) Cogentex’s chart explainer http://www.cogentex.com/products/chartex/faq/bjs-sample.png !9 CS447: Natural Language Processing (J. Hockenmaier) Cogentex’s Camera system !10 CS447: Natural Language Processing (J. Hockenmaier) Edinburgh’s ILEX and M-PIRO ILEX: a web-based virtual museum gallery and a phone-based system for an actual gallery M-PIRO: adds an authoring tool for curators !11 What is that? This exhibit is a lekythos, created during the archaic period. It dates from circa 500 BC. It was painted by Amasis with the red ﬁgure technique and it originates from Attica. CS447: Natural Language Processing (J. Hockenmaier) The COMIC system Conversational Multimodal Interaction with Computers Dialog system for bathroom design applications !12 CS447: Natural Language Processing (J. Hockenmaier) NLG architecture !13 Text planner Sentence planner Linguistic realizer Goal Text plan Sentence plan Surface text CS447: Natural Language Processing (J. Hockenmaier) NLG architectures There are many dependencies between these tasks. The standard NLG system architecture consists of: Text planning: Content determination and discourse planning Sentence planning: Sentence aggregation, lexicalization and referring expression generation Linguistic realization: Syntactic, morphological and orthographic processing. !14 CS447: Natural Language Processing (J. Hockenmaier) 1. Content determination What information (what ‘messages’) should be communicated? 2. Discourse planning How should the messages be structured/ordered? 3. Sentence aggregation Which messages should be combined into individual sentences? 4. Lexicalization In which words/phrases should domain concepts/relations be expressed? 5. Referring expression generation How should entities be referred to? 6. Linguistic realization Generate a grammatical and orthographically well-formed text NLG tasks !15 Text planning Sentence planning CS447: Natural Language Processing (J. Hockenmaier) Content determination Input: user input and background knowledge (database) Output: a set of ‘messages’ to be communicated (here shown with gloss) !16 CS447: Natural Language Processing (J. Hockenmaier) Content determination Input: user input and background knowledge (database) Output: a set of ‘messages’ to be communicated User model: User’s task, user’s level of expertise, previous interactions with system (esp. in dialog) Need to ﬁlter, summarize and process input data Relies often on (system-speciﬁc) heuristics (looking at corpus helps!) !17 CS447: Natural Language Processing (J. Hockenmaier) Discourse planning How should the messages be ordered? What are the discourse relations that hold between them? Often represented as a tree: !18 CS447: Natural Language Processing (J. Hockenmaier) Sentence aggregation Which messages should be conveyed in a single sentence? The next train leaves at 10am. It is the Caledonian Express. The next train, which leaves at 10am, is the Caledonian Express. Linguistic means to combine messages (=clauses): - Relative clauses: The next train, which leaves at 10 am, is the Caledonian Express - Coordination: The Caledonian Express leaves at 10am, and is the next train - Subordination: The Caledonian Express is the next train, although it leaves only at 10am. - Lists: There are trains at 10am, at 11:30am and at 1:00pm. !19 CS447: Natural Language Processing (J. Hockenmaier) Lexicalization and referring expressions Lexicalization: Which words and phrases should be used to express domain concepts: - does the train ‘leave’ or ‘depart’? -a ‘statistical error’ is not the same as a ‘statistical mistake’ NLG systems need a domain lexicon Referring expression generation: When do you use a pronoun/a deﬁnite NP/an indeﬁnite NP to refer to an entity? Needs a discourse model !20 CS447: Natural Language Processing (J. Hockenmaier) Linguistic realization Generate a grammatically and orthographically correct English utterance: There are 20 trains each day from Glasgow to Edinburgh. !21 CS447: Natural Language Processing (J. Hockenmaier) NLG evaluation Many areas of NLP have shared task evaluations that allow comparisons of different algorithms/systems on the same data. But most NLG systems are very domain/application speciﬁc. -Every system starts from its own input representation -Not a single gold standard data set -Can we evaluate subtasks (e.g. referring expression generation)? -How can we compare system outputs against each other/ against human produced text? (metrics such as BLEU/ROUGE may not correlate highly enough with human judgments) !22 CS447: Natural Language Processing (J. Hockenmaier) Conversational Agents (Chapter 24) !23 CS447: Natural Language Processing (J. Hockenmaier) Conversational Agents Systems that are capable of performing a task-driven dialog with a human user. AKA: Spoken Language Systems Dialogue Systems Speech Dialogue Systems Applications: Travel arrangements (Amtrak, United airlines) Telephone call routing Tutoring Communicating with robots Anything with limited screen/keyboard !24 CS447: Natural Language Processing (J. Hockenmaier) A travel dialog: Communicator !25 CS447: Natural Language Processing (J. Hockenmaier) Call routing: ATT HMIHY !26 CS447: Natural Language Processing (J. Hockenmaier) A tutorial dialogue: ITSPOKE !27 CS447: Natural Language Processing (J. Hockenmaier) Dialogue System Architecture !28 CS447: Natural Language Processing (J. Hockenmaier) Dialogue Manager Controls the architecture and structure of dialogue -Takes input from ASR (speech recognizer) & NLU components -Maintains some sort of internal state -Interfaces with Task Manager -Passes output to Natural Language Generation/ Text-to-speech modules !29 CS447: Natural Language Processing (J. Hockenmaier) Four architectures for dialogue management Finite State Frame-based Information State Markov Decision Processes AI Planning !30 CS447: Natural Language Processing (J. Hockenmaier) Finite State Dialogue Manager !31 CS447: Natural Language Processing (J. Hockenmaier) Finite-state dialogue managers System completely controls the conversation with the user: -It asks the user a series of questions -It may ignore (or misinterpret) anything the user says that is not a direct answer to the system’s questions Systems that control conversation like this are system initiative or single initiative. “Initiative”: who has control of conversation In normal human-human dialogue, initiative shifts back and forth between participants. 12/1/15 !32 Speech and Language Processing -- Jurafsky and Martin CS447: Natural Language Processing (J. Hockenmaier) Task-driven dialog as slot ﬁlling If the purpose of the dialog is to complete a speciﬁc task (e.g. to book a plane ticket), that task can often be represented as a frame with a number of slots to ﬁll. The task is completed if all the necessary slots are ﬁlled. !33 CS447: Natural Language Processing (J. Hockenmaier) Frame-based dialog agents Based on a ""domain ontology"" A knowledge structure representing user intentions One or more frames Each a collection of slots Each slot having a value !34 CS447: Natural Language Processing (J. Hockenmaier) NLU with frame/slot semantics There are many ways to represent the meaning of sentences. For speech dialogue systems, most common is “Frame slot semantics”: Show me morning ﬂights from Boston to SF on Tuesday. SHOW: FLIGHTS: ORIGIN: CITY: Boston DATE: Tuesday TIME: morning DEST: CITY: San Francisco !35 CS447: Natural Language Processing (J. Hockenmaier) The Frame A set of slots, to be ﬁlled with information of a given type Each associated with a ques0on to the user Slot Type Ques0on ORIGIN city What city are you leaving from? DEST city Where are you going? DEP DATE date What day would you like to leave? DEP TIME time What time would you like to leave? !36 CS447: Natural Language Processing (J. Hockenmaier) Information-State and Dialogue Acts If we want a dialogue system to be more than just form-ﬁlling, it needs to be able to: Decide when the user has asked a question, made a proposal, rejected a suggestion Ground a user’s utterance, ask clariﬁcation questions, suggestion plans This suggests that: Conversational agent needs sophisticated models of interpretation and generation -In terms of speech acts and grounding -Needs more sophisticated representation of dialogue context than just a list of slots !37 The state of the art in 1977 !!!! CS447: Natural Language Processing (J. Hockenmaier) Back to Neural Nets… !39 CS447: Natural Language Processing (J. Hockenmaier) Basic RNNs Each time step corresponds to a feedforward net where the hidden layer gets its input not just from the layer below but also from the activations of the hidden layer at the previous time step !40 input output hidden CS447: Natural Language Processing (J. Hockenmaier) Basic RNNs Each time step corresponds to a feedforward net where the hidden layer gets its input not just from the layer below but also from the activations of the hidden layer at the previous time step !41 CS447: Natural Language Processing (J. Hockenmaier) A basic RNN unrolled in time !42 CS447: Natural Language Processing (J. Hockenmaier) RNNs for generation To generate a string w0w1…wn wn+1 (where w0 = <s>, and wn+1 = <\s>), give w0 as ﬁrst input, and then pick the next word according to the computed probability Feed this word in as input into the next layer. !43 P(wi|w0 . . . wi−1) input output hidden CS447: Natural Language Processing (J. Hockenmaier) RNNs for sequence classiﬁcation If we just want to assign a label to the entire sequence, we don’t need to produce output at each time step, so we can use a simpler architecture. We can use the hidden state of the last word in the sequence as input to a feedforward net: !44 CS447: Natural Language Processing (J. Hockenmaier) Stacked RNNs We can create an RNN that has “vertical” depth (at each time step) by stacking: !45 CS447: Natural Language Processing (J. Hockenmaier) Bidirectional RNNs Unless we need to generate a sequence, we can run two RNNs over the input sequence — one in the forward direction, and one in the backward direction. Their hidden states will capture different context information. !46 CS447: Natural Language Processing (J. Hockenmaier) Further extensions Character and substring embeddings We can also learn embeddings for individual letters. This helps generalize better to rare words, typos, etc. These embeddings can be combined with word embeddings (or used instead of an UNK embedding) Context-dependent embeddings (ELMO, BERT, ….) Word2Vec etc. are static embeddings: they induce a type- based lexicon that doesn’t handle polysemy etc. Context-dependent embeddings produce token-speciﬁc embeddings that depend on the particular context in which a word appears. !47 CS447: Natural Language Processing (J. Hockenmaier) Encoder-Decoder Models (seq2seq) !48 CS447: Natural Language Processing (J. Hockenmaier) Decoder Encoder Encoder-Decoder (seq2seq) model Task: Read an input sequence and return an output sequence -Machine translation: translate source into target language -Dialog system/chatbot: generate a response Reading the input sequence: RNN Encoder Generating the output sequence: RNN Decoder !49 input hidden output CS447: Natural Language Processing (J. Hockenmaier) Encoder-Decoder (seq2seq) model Encoder RNN: reads in the input sequence passes its last hidden state to the initial hidden state of the decoder Decoder RNN: generates the output sequence typically uses different parameters from the encoder may also use different input embeddings !50 CS447: Natural Language Processing (J. Hockenmaier) Attention mechanisms We want to condition the output generation of the decoder on a context-dependent representation of the input sequence. Attention computes a distribution over the encoder’s hidden states (for the input sequence) This distribution depends on the decoder’s hidden state (and is computed anew for each output symbol) The attention distribution is used to compute a weighted average of the encoder’s hidden state vectors. This context-dependent embedding of the input sequence is fed into the output of the decoder RNN. !51 CS447: Natural Language Processing (J. Hockenmaier) Attention mechanisms !52 https://colab.research.google.com/github/tensorﬂow/tensorﬂow/blob/master/tensorﬂow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb#scrollTo=TNfHIF71ulLu "
29,"School of Computer Science Probabilistic Graphical Models Posterior Regularization: an integrative paradigm for learning GMs Eric Xing (courtesy to Jun Zhu) Lecture 29, April 30, 2014 Reading: p 1 © Eric Xing @ CMU, 2005-2014 Max-margin learning Prior knowledge, bypass model selection, Data integration, scalable inference … generalization dual sparsity efficient solvers … nonlinear transformation rich forms of data … Regularized Bayesian Inference Learning GMs 2 © Eric Xing @ CMU, 2005-2014 Bayesian Inference A coherent framework of dealing with uncertainties Bayes’ rule offers a mathematically rigorous computational mechanism for combining prior knowledge with incoming evidence Thomas Bayes (1702 – 1761) • M: a model from some hypothesis space • x: observed data 3 © Eric Xing @ CMU, 2005-2014 Parametric Bayesian Inference A parametric likelihood: Prior on θ : Posterior distribution is represented as a finite set of parameters Examples: • Gaussian distribution prior + 2D Gaussian likelihood →Gaussian posterior distribution • Dirichilet distribution prior + 2D Multinomial likelihood → Dirichlet posterior distribution • Sparsity-inducing priors + some likelihood models → Sparse Bayesian inference 4 © Eric Xing @ CMU, 2005-2014 Nonparametric Bayesian Inference A nonparametric likelihood: Prior on : Posterior distribution Examples: →see next slide is a richer model, e.g., with an infinite set of parameters 5 © Eric Xing @ CMU, 2005-2014 probability measure binary matrix function Dirichlet Process Prior [Antoniak, 1974] + Multinomial/Gaussian/Softmax likelihood Indian Buffet Process Prior [Griffiths & Gharamani, 2005] + Gaussian/Sigmoid/Softmax likelihood Gaussian Process Prior [Doob, 1944; Rasmussen & Williams, 2006] + Gaussian/Sigmoid/Softmax likelihood Nonparametric Bayesian Inference 6 © Eric Xing @ CMU, 2005-2014 Why Bayesian Nonparametrics? Let the data speak for themselves Bypass the model selection problem  let data determine model complexity (e.g., the number of components in mixture models)  allow model complexity to grow as more data observed 7 © Eric Xing @ CMU, 2005-2014 It is desirable to further regularize the posterior distribution  An extra freedom to perform Bayesian inference  Arguably more direct to control the behavior of models  Can be easier and more natural in some examples likelihood model prior posterior Can we further control the posterior distributions? 8 © Eric Xing @ CMU, 2005-2014 Can we further control the posterior distributions? Directly control the posterior distributions?  Not obvious how … likelihood model prior posterior hard constraints (A single feasible space) soft constraints (many feasible subspaces with different complexities/penalties) 9 © Eric Xing @ CMU, 2005-2014 Bayes’ rule is equivalent to: A direct but trivial constraint on the posterior distribution [Zellner, Am. Stat. 1988] E.T. Jaynes (1988): “this fresh interpretation of Bayes’ theorem could make the use of Bayesian methods more attractive and widespread, and stimulate new developments in the general theory of inference” likelihood model prior posterior A reformulation of Bayesian inference 10 © Eric Xing @ CMU, 2005-2014 Regularized Bayesian Inference where, e.x., and Solving such constrained optimization problem needs convex duality theory So, where does the constraints come from? 11 © Eric Xing @ CMU, 2005-2014 Recall our evolution of the Max- Margin Learning Paradigms ? SVM SVM b r a c e M3N MED MED M3N MED-MN = SMED + “Bayesian” M3N 12 © Eric Xing @ CMU, 2005-2014 Structured MaxEnt Discrimination (SMED): Feasible subspace of weight distribution: Average from distribution of M3Ns Maximum Entropy Discrimination Markov Networks p 13 © Eric Xing @ CMU, 2005-2014 Can we use this scheme to learn models other than MN? 14 © Eric Xing @ CMU, 2005-2014 Recall the 3 advantages of MEDN An averaging Model: PAC-Bayesian prediction error guarantee (Theorem 3) Entropy regularization: Introducing useful biases Standard Normal prior => reduction to standard M3N (we’ve seen it) Laplace prior => Posterior shrinkage effects (sparse M3N) Integrating Generative and Discriminative principles (next class) Incorporate latent variables and structures (PoMEN) Semisupervised learning (with partially labeled data) 15 © Eric Xing @ CMU, 2005-2014 Latent Hierarchical MaxEnDNet Web data extraction  Goal: Name, Image, Price, Description, etc. Hierarchical labeling Advantages: o Computational efficiency o Long-range dependency o Joint extraction {image} {name, price} {name} {price} {name} {price} {image} {name, price} {desc} {Head} {Tail} {Info Block} {Repeat block} {Note} {Note} 16 © Eric Xing @ CMU, 2005-2014 Partially Observed MaxEnDNet (PoMEN)  Now we are given partially labeled data:  PoMEN: learning  Prediction: (Zhu et al, NIPS 2008) 17 © Eric Xing @ CMU, 2005-2014 Alternating Minimization Alg. Factorization assumption: Alternating minimization:  Step 1: keep fixed, optimize over  Step 2: keep fixed, optimize over o Normal prior • M3N problem (QP) o Laplace prior • Laplace M3N problem (VB) Equivalently reduced to an LP with a polynomial number of constraints 18 © Eric Xing @ CMU, 2005-2014 Experimental Results  Web data extraction:  Name, Image, Price, Description  Methods:  Hierarchical CRFs, Hierarchical M^3N  PoMEN, Partially observed HCRFs  Pages from 37 templates o Training: 185 (5/per template) pages, or 1585 data records o Testing: 370 (10/per template) pages, or 3391 data records  Record-level Evaluation o Leaf nodes are labeled  Page-level Evaluation o Supervision Level 1:  Leaf nodes and data record nodes are labeled o Supervision Level 2:  Level 1 + the nodes above data record nodes 19 © Eric Xing @ CMU, 2005-2014 Record-Level Evaluations  Overall performance:  Avg F1: o avg F1 over all attributes  Block instance accuracy: o % of records whose Name, Image, and Price are correct  Attribute performance: 20 © Eric Xing @ CMU, 2005-2014 Page-Level Evaluations  Supervision Level 1:  Leaf nodes and data record nodes are labeled  Supervision Level 2:  Level 1 + the nodes above data record nodes 4/29/2014 21 © Eric Xing @ CMU, 2005-2014 Structured MaxEnt Discrimination (SMED): Feasible subspace of weight distribution: Average from distribution of PoMENs We can use this for any p and p0 ! Key message from PoMEN p 22 © Eric Xing @ CMU, 2005-2014 Max-margin learning An all inclusive paradigm for learning general GM --- RegBayes 23 © Eric Xing @ CMU, 2005-2014 Predictive Latent Subspace Learning via a large-margin approach … where M is any subspace model and p is a parametric Bayesian prior 24 © Eric Xing @ CMU, 2005-2014 Finding latent subspace representations (an old topic)  Mapping a high-dimensional representation into a latent low-dimensional representation, where each dimension can have some interpretable meaning, e.g., a semantic topic Examples:  Topic models (aka LDA) [Blei et al 2003]  Total scene latent space models [Li et al 2009]  Multi-view latent Markov models [Xing et al 2005]  PCA, CCA, …    Athlete Horse Grass Trees Sky Saddle Unsupervised Latent Subspace Discovery 25 © Eric Xing @ CMU, 2005-2014  Unsupervised latent subspace representations are generic but can be sub- optimal for predictions  Many datasets are available with supervised side information  Can be noisy, but not random noise (Ames & Naaman, 2007)  labels & rating scores are usually assigned based on some intrinsic property of the data  helpful to suppress noise and capture the most useful aspects of the data  Goals:  Discover latent subspace representations that are both predictive and interpretable by exploring weak supervision information Tripadvisor Hotel Review (http://www.tripadvisor.com) LabelMe http://labelme.csail.mit.edu/ Many others Flickr (http://www.flickr.com/) Predictive Subspace Learning with Supervision 26 © Eric Xing @ CMU, 2005-2014 I. LDA: Latent Dirichlet Allocation  Joint Distribution:  Variational Inference with :  Minimize the variational bound to estimate parameters and infer the posterior distribution Generative Procedure: For each document d: Sample a topic proportion For each word: – Sample a topic – Sample a word (Blei et al., 2003) exact inference intractable! 27 © Eric Xing @ CMU, 2005-2014  Bayesian sLDA:  MED Estimation:  MedLDA Regression Model  MedLDA Classification Model predictive accuracy model fitting (Zhu et al, ICML 2009) Maximum Entropy Discrimination LDA (MedLDA) 28 © Eric Xing @ CMU, 2005-2014 Document Modeling  Data Set: 20 Newsgroups  110 topics + 2D embedding with t-SNE (var der Maaten & Hinton, 2008) MedLDA LDA 29 © Eric Xing @ CMU, 2005-2014 Classification  Data Set: 20Newsgroups – Binary classification: “alt.atheism” and “talk.religion.misc” (Simon et al., 2008) – Multiclass Classification: all the 20 categories  Models: DiscLDA, sLDA (Binary ONLY! Classification sLDA (Wang et al., 2009)), LDA+SVM (baseline), MedLDA, MedLDA+SVM  Measure: Relative Improvement Ratio 30 © Eric Xing @ CMU, 2005-2014 Regression  Data Set: Movie Review (Blei & McAuliffe, 2007)  Models: MedLDA(partial), MedLDA(full), sLDA, LDA+SVR  Measure: predictive R2 and per-word log-likelihood 31 © Eric Xing @ CMU, 2005-2014 Time Efficiency  Binary Classification  Multiclass: — MedLDA is comparable with LDA+SVM  Regression: — MedLDA is comparable with sLDA 32 © Eric Xing @ CMU, 2005-2014  The “Total Scene Understanding” Model (Li et al, CVPR 2009)  Using MLE to estimate model parameters Athlete Horse Grass Trees Sky Saddle class: Polo II. Upstream Scene Understanding Models 33 © Eric Xing @ CMU, 2005-2014 Scene Classification  8-category sports data set (Li & Fei-Fei, 2007):  Fei-Fei’s theme model: 0.65 (different image representation)  SVM: 0.673 •1574 images (50/50 split) •Pre-segment each image into regions •Region features: •color, texture, and location •patches with SIFT features •Global features: •Gist (Oliva & Torralba, 2001) •Sparse SIFT codes (Yang et al, 2009) 34 © Eric Xing @ CMU, 2005-2014 Classification results: $ROI+Gist(annotation) used human annotated interest regions. • 67-category MIT indoor scene (Quattoni & Torralba, 2009): • ~80 per-category for training; ~20 per-category for testing • Same feature representation as above • Gist global features MIT Indoor Scene 35 © Eric Xing @ CMU, 2005-2014 III. Supervised Multi-view RBMs  A probabilistic method with an additional view of response variables Y  Parameters can be learned with maximum likelihood estimation, e.g., special supervised Harmonium (Yang et al., 2007)  contrastive divergence is the commonly used approximation method in learning undirected latent variable models (Welling et al., 2004; Salakhutdinov & Murray, 2008). Y 1 Y L normalization factor 36 © Eric Xing @ CMU, 2005-2014  t-SNE (van der Maaten & Hinton, 2008) 2D embedding of the discovered latent space representation on the TRECVID 2003 data  Avg-KL: average pair-wise divergence MMH TWH Predictive Latent Representation 37 © Eric Xing @ CMU, 2005-2014 Predictive Latent Representation  Example latent topics discovered by a 60-topic MMH on Flickr Animal Data 38 © Eric Xing @ CMU, 2005-2014 Data Sets: – (Left) TRECVID 2003: (text + image features) – (Right) Flickr 13 Animal: (sift + image features) Models:  baseline(SVM),DWH+SVM, GM-Mixture+SVM, GM-LDA+SVM, TWH, MedLDA(sift only), MMH TRECVID Flickr Classification Results 39 © Eric Xing @ CMU, 2005-2014  Data Set: TRECVID 2003 – Each test sample is treated as a query, training samples are ranked based on the cosine similarity between a training sample and the given query – Similarity is computed based on the discovered latent topic representations  Models: DWH, GM-Mixture, GM-LDA, TWH, MMH  Measure: (Left) average precision on different topics and (Right) precision- recall curve Retrieval Results 40 © Eric Xing @ CMU, 2005-2014 Infinite SVM and infinite latent SVM: -- where SVMs meet NB for classification and feature selection … where M is any combinations of classifiers and p is a nonparametric Bayesian prior 41 © Eric Xing @ CMU, 2005-2014 Mixture of SVMs  Dirichlet process mixture of large-margin kernel machines  Learn flexible non-linear local classifiers; potentially lead to a better control on model complexity, e.g., few unnecessary components  The first attempt to integrate Bayesian nonparametrics, large-margin learning, and kernel methods SVM using RBF kernel Mixture of 2 linear SVM Mixture of 2 RBF-SVM 42 © Eric Xing @ CMU, 2005-2014 Infinite SVM RegBayes framework:  Model – latent class model  Prior – Dirichlet process  Likelihood – Gaussian likelihood  Posterior constraints – max-margin constraints direct and rich constraints on posterior distribution convex function 43 © Eric Xing @ CMU, 2005-2014 Infinite SVM  DP mixture of large-margin classifiers  Given a component classifier:  Overall discriminant function:  Prediction rule:  Learning problem: Graphical model with stick-breaking construction of DP process of determining which classifier to use: 44 © Eric Xing @ CMU, 2005-2014 Infinite SVM  Assumption and relaxation  Truncated variational distribution  Upper bound the KL-regularizer  Opt. with coordinate descent  For , we solve an SVM learning problem  For , we get the closed update rule  The last term regularizes the mixing proportions to favor prediction  For , the same update rules as in (Blei & Jordan, 2006) Graphical model with stick-breaking construction of DP 45 © Eric Xing @ CMU, 2005-2014 Experiments on high-dim real data  Classification results and test time:  Clusters:  simiar backgroud images group  a cluster has fewer categories For training, linear-iSVM is very efficient (~200s); RBF-iSVM is much slower, but can be significantly improved using efficient kernel methods (Rahimi & Recht, 2007; Fine & Scheinberg, 2001) 46 © Eric Xing @ CMU, 2005-2014 Learning Latent Features Infinite SVM is a Bayesian nonparametric latent class model  discover clustering structures  each data point is assigned to a single cluster/class Infinite Latent SVM is a Bayesian nonparametric latent feature/factor model  discover latent factors  each data point is mapped to a set (can be infinite) of latent factors  Latent factor analysis is a key technique in many fields; Popular models are FA, PCA, ICA, NMF, LSI, etc. 47 © Eric Xing @ CMU, 2005-2014 Infinite Latent SVM RegBayes framework:  Model – latent feature model  Prior – Indian Buffet process  Likelihood – Gaussian likelihood  Posterior constraints – max-margin constraints direct and rich constraints on posterior distribution convex function 48 © Eric Xing @ CMU, 2005-2014 Beta-Bernoulli Latent Feature Model A random finite binary latent feature models  is the relative probability of each feature being on, e.g.,  are binary vectors, giving the latent structure that’s used to generate the data, e.g., 49 © Eric Xing @ CMU, 2005-2014 Indian Buffet Process A stochastic process on infinite binary feature matrices Generative procedure:  Customer 1 chooses the first dishes:  Customer i chooses:  Each of the existing dishes with probability  additional dishes, where 50 © Eric Xing @ CMU, 2005-2014 Posterior Constraints – classification Suppose latent features z are given, we define latent discriminant function: Define effective discriminant function (reduce uncertainty):  Posterior constraints with max-margin principle 51 © Eric Xing @ CMU, 2005-2014 Experimental Results Classification  Accuracy and F1 scores on TRECVID2003 and Flickr image datasets 52 © Eric Xing @ CMU, 2005-2014 Large-margin learning Large-margin kernel machines Bayesian kernel machines; Infinite GPs Summary 53 © Eric Xing @ CMU, 2005-2014 Large-margin learning Linear Expectation Operator (resolve uncertainty) Summary 54 © Eric Xing @ CMU, 2005-2014 Summary • A general framework of MaxEnDNet for learning structured input/output models – Subsumes the standard M3Ns – Model averaging: PAC-Bayes theoretical error bound – Entropic regularization: sparse M3Ns – Generative + discriminative: latent variables, semi-supervised learning on partially labeled data, fast inference • PoMEN – Provides an elegant approach to incorporate latent variables and structures under max- margin framework – Enable Learning arbitrary graphical models discriminatively • Predictive Latent Subspace Learning – MedLDA for text topic learning – Med total scene model for image understanding – Med latent MNs for multi-view inference • Bayesian nonparametrics meets max-margin learning • Experimental results show the advantages of max-margin learning over likelihood methods in EVERY case. 55 © Eric Xing @ CMU, 2005-2014 Remember: Elements of Learning  Here are some important elements to consider before you start:  Task:  Embedding? Classification? Clustering? Topic extraction? …  Data and other info:  Input and output (e.g., continuous, binary, counts, …)  Supervised or unsupervised, of a blend of everything?  Prior knowledge? Bias?  Models and paradigms:  BN? MRF? Regression? SVM?  Bayesian/Frequents ? Parametric/Nonparametric?  Objective/Loss function:  MLE? MCLE? Max margin?  Log loss, hinge loss, square loss? …  Tractability and exactness trade off:  Exact inference? MCMC? Variational? Gradient? Greedy search?  Online? Batch? Distributed?  Evaluation:  Visualization? Human interpretability? Perperlexity? Predictive accuracy?  It is better to consider one element at a time! 56 © Eric Xing @ CMU, 2005-2014 "
3,"School of Computer Science Probabilistic Graphical Models Representation of undirected GM Eric Xing Lecture 3, February 22, 2014 Reading: KF-chap4 © Eric Xing @ CMU, 2005-2014 Directed edges give causality relationships (Bayesian Network or Directed Graphical Model): Undirected edges simply give correlations between variables (Markov Random Field or Undirected Graphical model): Two types of GMs Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 Receptor A Kinase C TF F Gene G Gene H Kinase E Kinase D Receptor B X1 X2 X3 X4 X5 X6 X7 X8 X1 X2 X3 X4 X5 X6 X7 X8 P(X1, X2, X3, X4, X5, X6, X7, X8) = P(X1) P(X2) P(X3| X1) P(X4| X2) P(X5| X2) P(X6| X3, X4) P(X7| X6) P(X8| X5, X6) P(X1, X2, X3, X4, X5, X6, X7, X8) = 1/Z exp{E(X1)+E(X2)+E(X3, X1)+E(X4, X2)+E(X5, X2) + E(X6, X3, X4)+E(X7, X6)+E(X8, X5, X6)} © Eric Xing @ CMU, 2005-2014 Review: independence properties of DAGs Defn: let Il(G) be the set of local independence properties encoded by DAG G, namely: Defn: A DAG G is an I-map (independence-map) of P if Il(G)I(P) A fully connected DAG G is an I-map for any distribution, since Il(G)I(P) for any P. Defn: A DAG G is a minimal I-map for P if it is an I-map for P, and if the removal of even a single edge from G renders it not an I-map. A distribution may have several minimal I-maps  Each corresponding to a specific node-ordering © Eric Xing @ CMU, 2005-2014   ) ; ( dsep : ) ( I Y Z X Y Z X G G   P-maps Defn: A DAG G is a perfect map (P-map) for a distribution P if I(P)I(G). Thm: not every distribution has a perfect map as DAG.  Pf by counterexample. Suppose we have a model where A C | {B,D}, and B D | {A,C}. This cannot be represented by any Bayes net.  e.g., BN1 wrongly says B D | A, BN2 wrongly says B D. A C D B C A D B BN1 BN2 A C D B MRF © Eric Xing @ CMU, 2005-2014 P-maps Defn: A DAG G is a perfect map (P-map) for a distribution P if I(P)I(G). Thm: not every distribution has a perfect map as DAG.  Pf by counterexample. Suppose we have a model where A C | {B,D}, and B D | {A,C}. This cannot be represented by any Bayes net.  e.g., BN1 wrongly says B D | A, BN2 wrongly says B D.  The fact that G is a minimal I-map for P is far from a guarantee that G captures the independence structure in P  The P-map of a distribution is unique up to I-equivalence between networks. That is, a distribution P can have many P-maps, but all of them are I-equivalent. © Eric Xing @ CMU, 2005-2014 Undirected graphical models (UGM) Pairwise (non-causal) relationships Can write down model, and score specific configurations of the graph, but no explicit way to generate samples Contingency constrains on node configurations X1 X4 X2 X3 X5 © Eric Xing @ CMU, 2005-2014 A Canonical Examples: understanding complex scene … ? air or water ? 7 © Eric Xing @ CMU, 2005-2014 Canonical example The grid model Naturally arises in image processing, lattice physics, etc. Each node may represent a single ""pixel"", or an atom  The states of adjacent or nearby nodes are ""coupled"" due to pattern continuity or electro-magnetic force, etc.  Most likely joint-configurations usually correspond to a ""low-energy"" state © Eric Xing @ CMU, 2005-2014 Social networks The New Testament Social Networks © Eric Xing @ CMU, 2005-2014 Protein interaction networks © Eric Xing @ CMU, 2005-2014 Modeling Go © Eric Xing @ CMU, 2005-2014 Information retrieval topic text image © Eric Xing @ CMU, 2005-2014 Representation Defn: an undirected graphical model represents a distribution P(X1 ,…,Xn) defined by an undirected graph H, and a set of positive potential functions yc associated with the cliques of H, s.t. where Z is known as the partition function: Also known as Markov Random Fields, Markov networks … The potential function can be understood as an contingency function of its arguments assigning ""pre-probabilistic"" score of their joint configuration.    C c c c n Z x x P ) ( ) , , ( x  1 1     n x x C c c c Z , , ) (  1 x  © Eric Xing @ CMU, 2005-2014 Global Markov Independencies Let H be an undirected graph: B separates A and C if every path from a node in A to a node in C passes through a node in B: A probability distribution satisfies the global Markov property if for any disjoint A, B, C, such that B separates A and C, A is independent of C given B: ) ; ( sep B C A H   ) ; ( sep : ) ( I B C A B C A H H   © Eric Xing @ CMU, 2005-2014 Local Markov independencies For each node Xi V, there is unique Markov blanket of Xi, denoted MBXi, which is the set of neighbors of Xi in the graph (those that share an edge with Xi) Defn: The local Markov independencies associated with H is: Iℓ(H): {Xi V – {Xi } – MBXi | MBXi : i), In other words, Xi is independent of the rest of the nodes in the graph given its immediate neighbors © Eric Xing @ CMU, 2005-2014 Structure: an undirected graph • Meaning: a node is conditionally independent of every other node in the network given its Directed neighbors • Local contingency functions (potentials) and the cliques in the graph completely determine the joint dist. • Give correlations between variables, but no explicit way to generate samples X Y1 Y2 Summary: Conditional Independence Semantics in an MRF © Eric Xing @ CMU, 2005-2014 I. Quantitative Specification: Cliques For G={V,E}, a complete subgraph (clique) is a subgraph G'={V'V,E'E} such that nodes in V' are fully interconnected  A (maximal) clique is a complete subgraph s.t. any superset V""V' is not complete.  A sub-clique is a not-necessarily-maximal clique. Example:  max-cliques = {A,B,D}, {B,C,D},  sub-cliques = {A,B}, {C,D}, …all edges and singletons A C D B © Eric Xing @ CMU, 2005-2014 Gibbs Distribution and Clique Potential Defn: an undirected graphical model represents a distribution P(X1 ,…,Xn) defined by an undirected graph H, and a set of positive potential functions c associated with cliques of H, s.t. where Z is known as the partition function: Also known as Markov Random Fields, Markov networks … The potential function can be understood as an contingency function of its arguments assigning ""pre-probabilistic"" score of their joint configuration.    C c c c n Z x x P ) ( ) , , ( x  1 1     n x x C c c c Z , , ) (  1 x  (A Gibbs distribution) © Eric Xing @ CMU, 2005-2014 Interpretation of Clique Potentials The model implies XZ|Y. This independence statement implies (by definition) that the joint must factorize as: We can write this as: , but  cannot have all potentials be marginals  cannot have all potentials be conditionals The positive clique potentials can only be thought of as general ""compatibility"", ""goodness"" or ""happiness"" functions over their variables, but not as probability distributions. ) | ( ) | ( ) ( ) , , ( y z p y x p y p z y x p  Y X Z ) , ( ) | ( ) , , ( ) | ( ) , ( ) , , ( y z p y x p z y x p y z p y x p z y x p   © Eric Xing @ CMU, 2005-2014 For discrete nodes, we can represent P(X1:4) as two 3D tables instead of one 4D table Example UGM – using max cliques A C D B ) ( ) ( ) , , , ( ' 234 124 4 3 2 1 1 x x c c Z x x x x P        4 3 2 1 234 124 x x x x c c Z , , , ) ( ) ( x x   A,B,D B,C,D ) ( 124 x c  ) ( 234 x c  © Eric Xing @ CMU, 2005-2014  We can represent P(X1:4) as 5 2D tables instead of one 4D table  Pair MRFs, a popular and simple special case  I(P') vs. I(P"") ? D(P') vs. D(P"") Example UGM – using subcliques A C D B ) ( ) ( ) ( ) ( ) ( ) ( ) , , , ( "" 34 34 24 24 23 23 14 14 12 12 4 3 2 1 1 1 x x x x x x       Z Z x x x x P ij ij ij      4 3 2 1 x x x x ij ij ij Z , , , ) (x  A,B A,D B,D C,D B,C © Eric Xing @ CMU, 2005-2014 Example UGM – canonical representation A C D B ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) , , , ( 4 4 3 3 2 2 1 1 34 34 24 24 23 23 14 14 12 12 234 124 4 3 2 1 1 x x x x Z x x x x P c c                x x x x x x x      4 3 2 1 4 4 3 3 2 2 1 1 34 34 24 24 23 23 14 14 12 12 234 124 x x x x c c x x x x Z , , , ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) (            x x x x x x x  Most general, subsume P' and P"" as special cases  I(P) vs. I(P') vs. I(P"") D(P) vs. D(P') vs. D(P"") © Eric Xing @ CMU, 2005-2014 Hammersley-Clifford Theorem  If arbitrary potentials are utilized in the following product formula for probabilities, then the family of probability distributions obtained is exactly that set which respects the qualitative specification (the conditional independence relations) described earlier  Thm : Let P be a positive distribution over V, and H a Markov network graph over V. If H is an I-map for P, then P is a Gibbs distribution over H.    C c c c n Z x x P ) ( ) , , ( x  1 1     n x x C c c c Z , , ) (  1 x  © Eric Xing @ CMU, 2005-2014 II: Independence properties: global independencies Let us return to the question of what kinds of distributions can be represented by undirected graphs (ignoring the details of the particular parameterization). Defn: the global Markov properties of a UG H are Is this definition sound and complete? Y Z X   ) ; ( sep : ) ) ( I Y Z X Y Z X H H   © Eric Xing @ CMU, 2005-2014 Soundness and completeness of global Markov property Defn: An UG H is an I-map for a distribution P if I(H) I(P), i.e., P entails I(H). Defn: P is a Gibbs distribution over H if it can be represented as Thm (soundness): If P is a Gibbs distribution over H, then H is an I-map of P. Thm (completeness): If sepH(X; Z |Y), then X P Z |Y in some P that factorizes over H.    C c c c n Z x x P ) ( ) , , ( x  1 1  © Eric Xing @ CMU, 2005-2014 Local and global Markov properties revisit For directed graphs, we defined I-maps in terms of local Markov properties, and derived global independence. For undirected graphs, we defined I-maps in terms of global Markov properties, and will now derive local independence. Defn: The pairwise Markov independencies associated with UG H = (V;E) are e.g.,   E Y X Y X V Y X H p    } , { : } , { \ ) ( I } , , { 4 3 2 5 1 X X X X X  1 2 3 4 5 © Eric Xing @ CMU, 2005-2014 Local Markov properties A distribution has the local Markov property w.r.t. a graph H=(V,E) if the conditional distribution of variable given its neighbors is independent of the remaining nodes Theorem (Hammersley-Clifford): If the distribution is strictly positive and satisfies the local Markov property, then it factorizes with respect to the graph. NH(X) is also called the Markov blanket of X.     V V     X X N X N X X H H H l : )) ( ) ( \ ) ( I © Eric Xing @ CMU, 2005-2014 Relationship between local and global Markov properties  Thm 5.5.5. If P |= Il(H) then P |= Ip(H).  Thm 5.5.6. If P = I(H) then P |= Il(H).  Thm 5.5.7. If P > 0 and P |= Ip(H), then P |= I(H).  Corollary (5.5.8): The following three statements are equivalent for a positive distribution P: P |= Il(H) P |= Ip(H) P |= I(H)  This equivalence relies on the positivity assumption.  We can design a distribution locally © Eric Xing @ CMU, 2005-2014 Perfect maps Defn: A Markov network H is a perfect map for P if for any X; Y;Z we have that Thm: not every distribution has a perfect map as UGM.  Pf by counterexample. No undirected network can capture all and only the independencies encoded in a v-structure X Z Y .   Y Z X P Y Z X H | ) ; ( sep     © Eric Xing @ CMU, 2005-2014 Exponential Form  Constraining clique potentials to be positive could be inconvenient (e.g., the interactions between a pair of atoms can be either attractive or repulsive). We represent a clique potential c(xc) in an unconstrained form using a real-value ""energy"" function c(xc): For convenience, we will call c(xc) a potential when no confusion arises from the context.  This gives the joint a nice additive strcuture where the sum in the exponent is called the ""free energy"": In physics, this is called the ""Boltzmann distribution"". In statistics, this is called a log-linear model.   ) ( exp ) ( c c c c x x       ) ( exp ) ( exp ) ( x x x H Z Z p C c c c            1 1     C c c c H ) ( ) ( x x  © Eric Xing @ CMU, 2005-2014 Example: Boltzmann machines A fully connected graph with pairwise (edge) potentials on binary-valued nodes (for ) is called a Boltzmann machine Hence the overall energy function has the form: 1 3 4 2     1 0 1 1 , or ,     i i x x                    C x x x Z x x Z x x x x P i i i ij j i ij ij j i ij    exp ) ( exp ) , , , ( , 1 1 4 3 2 1 ) ( ) ( ) ( ) ( ) (             x x x x x H T ij j ij i © Eric Xing @ CMU, 2005-2014 hidden units visible units { } ∑ ∑ ∑ , , , ) ( - ) , ( + ) ( + ) ( exp = ) | , ( j j i j i j i j i j j j i i i i A h x h x h x p θ        Restricted Boltzmann Machines Restricted Boltzmann Machines The Harmonium (Smolensky –’86) hidden units visible units History: Smolensky (’86), Proposed the architechture. Freund & Haussler (’92), The “Combination Machine” (binary), learning with projection pursuit. Hinton (’02), The “Restricted Boltzman Machine” (binary), learning with contrastive divergence. Marks & Movellan (’02), Diffusion Networks (Gaussian). Welling, Hinton, Osindero (’02), “Product of Student-T Distributions” (super-Gaussian) ) | ( ~ x h p h ) | ( ~ h x p x Properties of RBM  Factors are marginally dependent.  Factors are conditionally independent given observations on the visible nodes.  Iterative Gibbs sampling.  Learning with contrastive divergence ) | ( ∏ = ) | ( w w i i P P   how do we couple them?   ∏ ) ( exp ∝ ) ( ind i i i i x f p  x   ∏ ) ( exp ∝ ) ( ind j j j j h g p  h j h i x A Constructive Definition { } ∑ ∑ ∑ , , ) ( ) ( + ) ( + ) ( exp = ) | , ( j j i j j j i i T i j j j i i i i h g x f h g x f h x p       W    j h They map to the RBM random field: { } ∑ ∑ ∑ ∏ ) ( + = ) ( + = ˆ }) ˆ ({ + ) ( ˆ exp = ) | ( , ) | ( = ) | ( j j j j ia ia jb j jb jb ia ia ia ia i a i ia ia i i i h g W h g W A x f x p x p p        h h h x { } ∑ ∑ ∑ ∏ ) ( + = ) ( + = ˆ }) ˆ ({ + ) ( ˆ exp = ) | ( ) | ( = ) | ( i i i jb i jb ia i ia jb ia jb jb jb j b j jb jb j j j x f W x f W B h g h p h p p        x x x h { } ∑ ∑ ∑ , , ) ( ) ( + ) ( + ) ( exp = ) | , ( j j i j j j i i T i j j j i i i i h g x f h g x f h x p       W    vector of local sufficient statistics (features) coupling in the log-domain with shifted parameters i x A Constructive Definition An RBM for Text Modeling words counts topics [ ] ∏ ) ∑ + exp( + 1 ) ∑ + exp( , Bi = ) | ( i h W h W x j ij j j j ij j j i N p   h x xi = n: word i has count n hj = 3: topic j has strength 3 I ∈ i x , ∈R j h i j i i j x W h , ∑ = [ ] ∏ ∑ 1 , Normal = ) | ( j i i ij h x W p j   x h ( ) ( ) { } 2 , 2 1 ∑ ∑ + ) - ( log - ) ( log - ∑ exp ∝ ) ( ⇒ i j i i j i i i i i x W x N x x p    x Conditional Random Fields         c c c c y x f x Z x y p ) , ( exp ) , ( 1 ) | (    A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... Y1 Y2 Y5 … X1 … Xn  Discriminative  Doesn’t assume that features are independent  When labeling Xi future observations are taken into account 38 © Eric Xing @ CMU, 2005-2014 Conditional Models  Conditional probability P(label sequence y | observation sequence x) rather than joint probability P(y, x)  Specify the probability of possible label sequences given an observation sequence  Allow arbitrary, non-independent features on the observation sequence X  The probability of a transition between labels may depend on past and future observations  Relax strong independence assumptions in generative models © Eric Xing @ CMU, 2005-2014 Conditional Distribution  If the graph G = (V, E) of Y is a tree, the conditional distribution over the label sequence Y = y, given X = x, by the Hammersley Clifford theorem of random fields is: ─ x is a data sequence ─ y is a label sequence ─ v is a vertex from vertex set V = set of label random variables ─ e is an edge from edge set E over V ─ fk and gk are given and fixed. gk is a Boolean vertex feature; fk is a Boolean edge feature ─ k is the number of features ─ are parameters to be estimated ─ y|e is the set of components of y defined by edge e ─ y|v is the set of components of y defined by vertex v 1 2 1 2 ( , , , ; , , , ); and n n k k           (y | x) exp ( , y | ,x) ( , y | ,x)                k k e k k v e E,k v V ,k p f e g v Y1 Y2 Y5 … X1 … Xn © Eric Xing @ CMU, 2005-2014 (y| x) exp ( ,y| ,x) ( ,y| 1 (x) ,x)                k k e k k v e E,k v V ,k p f e g v Z Conditional Distribution (cont’d) CRFs use the observation-dependent normalization Z(x) for the conditional distributions:  Z(x) is a normalization over the data sequence x © Eric Xing @ CMU, 2005-2014 Conditional Random Fields  Allow arbitrary dependencies on input  Clique dependencies on labels  Use approximate inference for general graphs         c c c c y x f x Z x y p ) , ( exp ) , ( ) | (    1 42 © Eric Xing @ CMU, 2005-2014 Summary Undirected graphical models capture “relatedness”, “coupling”, “co-occurrence”, “synergism”, etc. between entities Local and global independence properties identifiable via graph separation criteria Defined on clique potentials Generally intractable to compute likelihood due to presence of “partition function”  Therefore not only inference, but also likelihood-based learning is difficult in general Can be used to define either joint or conditional distributions Important special cases:  Ising models  RBM  CRF © Eric Xing @ CMU, 2005-2014 "
30,"COMP 790.139 (Fall 2017) Natural Language Processing (with deep learning and connections to vision/robotics) Mohit Bansal Class Info/Logistics ! COMP 790.139 ‘Natural Language Processing’ ! **3 UNITS** ! Instructor: Mohit Bansal (SN258, http://www.cs.unc.edu/~mbansal/) ! Time: Wed 10.10am-12.40pm ! Room: FB008 ! Office Hours: Wed 12.40-1.40pm (by appointment), SN258 ! Course Webpage: http://www.cs.unc.edu/~mbansal/teaching/nlp-course-fall17.html ! **Course Email**: nlpcomp790unc@gmail.com ! Your email: check/fwd your connectcarolina xyz@email/live.unc.edu email for my welcome message and send me your preferred email id! About Me ! Asst. Professor, CS, UNC (joined Fall 2016) ! Research Asst. Professor, TTI-Chicago, 2013-2016 ! PhD, UC Berkeley, 2008-2013 ! Research Interests: ! Past: Syntactic parsing, coreference resolution, taxonomy induction, world knowledge and commonsense induction ! Current: Multimodal and embodied semantics (i.e., language with vision and speech, for robotics); human-like language generation and Q&A/dialogue; interpretable and structured deep learning ! Office SN258 ! Webpage: http://www.cs.unc.edu/~mbansal/, Email: mbansal@cs.unc.edu Your Introductions ! Please say your: ! Name ! Department/degree/major ! Research interests (Why NLP? Past ML/AI/NLP/CV experience? Coding experience?) ! Fun fact ☺ About the Course (and its Goals) ! Research-oriented graduate course! We will cover lots of interesting chapters+papers, brainstorm, & do fun projects! ! We’ll start with some basics of traditional NLP ! Then cover some specific, latest research topics, both based on traditional models and newer neural models (also some paper readings on certain topics, presented by students) ! Will also discuss connections of NLP with vision and robotics, and several deep learning for NLP models ! Brainstorm regularly and code + write up fun/novel projects! ! Some lecture(s) on academic/research quality paper writing Prerequisites ! Advanced, graduate-level, research-based class ! Some machine learning and coding experience is definitely expected and required! (please talk to me class if you haven’t yet discussed this with me over email) ! Homeworks, projects, summaries, and paper presentation will all require solid ML foundations/clarity and coding skills (e.g., linear algebra, diff eqns, logistic regression, supervised/unsup learning setups, classifiers, backprop and MLPs/NNs) ! Moreover, some basic NLP background is highly recommended Expectations/Grading (tentative) ! Project reports and presentations (40%; midterm+final) ! Homework assignments (30%) ! Paper/Chapter presentation (10%) ! Paper/Chapter written summaries (10%) ! Class participation, discussion and brainstorming (10%) Homeworks ! Coding based assignments ! On certain topics covered in class, e.g., parsing, Q&A, summarization/translation ! Code should be written from scratch (acknowledge any borrowed pieces) ! Preferably in tensorflow/python/pytorch Project ! Students will pick (early) their favorite topic among latest cutting-edge research topics covered in class ! And will try a novel idea (implementing+extending or original) -- I am happy to discuss details! ! Midterm and final report + presentation (and possibly some updates) ! Might be in pairs/groups depending on final class size ! Use ACL conference style files and aim for high-quality project write-ups ! Will have some lecture(s) on research-quality paper writing Paper Presentation ! Lead discussion for 1-2 chapters/papers on a topic some week (may be done in pairs/groups depending on class size) ! Read related chapters/papers and present background to audience ! Present task and ML details of given chapter/papers ! Present demo’s of related code, etc. ! Ask interesting questions to initiate brainstorming ! Mention some next steps, future work, extension ideas! Chapter/Paper Written Summaries ! 0.5-1 page (per paper) write-up for certain week’s chapters/papers ! Describe the task ! Summarize the methods/models ! Explain the novelty ! Discuss the next steps or potential improvements Class Participation and Brainstorming ! Audience students expected to take part in lively discussion in every class and after chapter/paper reading! ! Semi-regularly (i.e., after completing several chapters/papers in 2-3 weeks), we will have a brainstorming and ‘idea-generation’ session! ! Exact details to be announced soon but students expected to submit and discuss novel idea(s) on the whole general topic, e.g., new related task or dataset, new approach to existing task, combinations of tasks/approaches, etc. ! Don’t hesitate to propose fancy ideas ☺, but try to keep them grounded/feasible and think of how to approach them realistically (in terms of datasets, models, speed, memory, etc.) Reference Books ! SLP2: D. Jurafsky & James H. Martin. “Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition”. Prentice Hall, Second Edition, 2009. ! SLP3: Some draft chapters of the third edition are available online at https://web.stanford.edu/~jurafsky/slp3/ ! FSNLP: Chris Manning and Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press. Cambridge, MA: May 1999. http://nlp.stanford.edu/fsnlp/ ! ML Background: Andrew Ng’s Coursera Machine Learning course https://www.coursera.org/learn/machine-learning ! Stanford NLP + Deep Learning Class: http://web.stanford.edu/class/cs224n/ Course Syllabus/Topics (tentative) • Language Modeling • Part-of-speech Tagging • Syntactic Parsing: Constituent, Dependency, CCG, others • Coreference Resolution • Distributional Semantics: PMI, neural, CCA • Compositional Semantics: Logical-form, Semantic Parsing, Vector-form, neural (RNNs/ CNNs) • Question Answering: Factoid-based, Passage-based • Sentiment Analysis • Document Summarization • Machine Translation • Dialogue Models • Language and Vision: Image Captioning, Video Captioning, Visual Question Answering • Language and Robotics: Instructions for Navigation, Manipulation, Skill Learning; Human-Robot Interaction • Several interesting machine and deep learning models all along the way, e.g., deep +structured models, interpretable models, adversarial models, reward-based models (reinforcement learning) What is NLP? ! Question answering What is NLP? ! Question answering What is NLP? ! Question answering What is NLP? ! Machine Translation What is NLP? ! Sentiment Analysis What is NLP? ! Natural Language Generation: Summarization - Lohan charged with the0 of $2,500 necklace - Pleaded not guilty - Judge set bail at $40,000 - To reappear in court on Feb 23 What is NLP? ! Natural Language Generation: Conversation/Dialogue Mitchell2 Jian-Yun Nie1† Jianfeng Gao2 Bill Dolan2 Universit´ e de Montr´ eal, Montr´ eal, QC, Canada icrosoft Research, Redmond, WA, USA ebook AI Research, Menlo Park, CA, USA a Institute of Technology, Atlanta, GA, USA generation sys- o end on large witter conversa- tecture is used arise when in- on into classic system to take tterances. Our odels show con- t-sensitive and Translation and s. context because of your game ? message yeah i’m on my way now response ok good luck ! Figure 1: Example of three consecutive utterances occur- ring between two Twitter users A and B. However, an approach such as that presented in Rit- ter et al. (2011) does not address the challenge of generating responses that are sensitive to the context [Sordoni et al., 2015] What is NLP? ! Natural Language Generation: Image Captioning [UToronto] What is NLP? ! Natural Language Generation: Video Captioning [Pasunuru and Bansal, 2017] (a) (b) (c) Figure 5: Examples of generated video captions on the YouTube2Text dataset: (a) complex examples wher model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where m still correctly predicts one of the possible categories (c) complex examples where both models perform poorly Relevance Coherence the baseline; (b) ambiguous example Figure 3: Output examples where our CIDEnt-RL d l d b tt t il d ti th th Samuel R Bo and Christ tated corpu In EMNLP David L Che ing highly In Proceed Associatio Language Associatio Xinlei Chen ishna Ved C Lawren tions: Dat preprint ar Jianpeng Che Long sho reading. In Ido Dagan, O 2006. The challenge. ating pred tion, and r 190. Sprin Michael Den universal: for any tar What is NLP? ! Natural Language Generation: Visual Question Answering any open-ended answers contain only a few words or a closed set of answers that can provide a dataset containing ⇠0.25M images, ⇠0.76M questions, and ⇠10M answers on it provides. Numerous baselines and methods for VQA are provided and compared F multi-discipline ms. In particular, combines Com- sing (NLP), and R) has dramati- [10], [36], [24], om a belief that re a step towards art demonstrates an image paired erate reasonable ning may not be task? We believe AI algorithms, an wledge beyond a ll d ﬁ d Does it appear to be rainy? Does this person have 20/20 vision? Is this person expecting company? What is just under the tree? How many slices of pizza are there? Is this a vegetarian pizza? What color are her eyes? What is the mustache made of? Fig. 1: Examples of free-form, open-ended questions collected for images via Amazon Mechanical Turk Note that commonsense [Antol et al., 2015] What is NLP? ! Automatic Speech Recognition Some Exciting NLP Challenges Human-like Ambiguous Language You: I am under the weather today. Siri: Here is the weather today… 50 F ! Non-literal: Idioms, Metaphors Human-like Ambiguous Language Break a leg! ! Non-literal: Idioms, Metaphors Human-like Ambiguous Language Yeah, right! ! Humor, Sarcasm, Politeness/Rudeness I bet I can stop gambling! Please do not … Human-like Ambiguous Language Clean the dishes in the sink. ! Prepositional Attachment, Coreference Ambiguities Human-like Ambiguous Language ! Prepositional Attachment, Coreference Ambiguities Visually Grounded Language Get the mug on the table with black stripes. ! Text-Image Alignment: Most of our daily communication language points to several objects in the visual world Visually Grounded Language Is there milk in the refrigerator? ! Visual Question Answering: Humans asking machines about pictures/videos, e.g., for visually impaired, in remote/ dangerous scenarios, in household service settings Embodied Language (Robot Instructions) Turn right at the butterﬂy painting, then go to the end of the hall ! Task-based instructions, e.g., navigation, grasping, manipulation, skill learning Embodied Language (Robot Instructions) Cut some onions, and add to broth, stir it ! Task-based instructions, e.g., navigation, grasping, manipulation, skill learning Grounded Language Generation/Dialogue ! Both for answering human questions, and to ask questions back, and for casual chit-chat What food is in the refrigerator? Apples and oranges Grounded Language Generation/Dialogue ! Both for answering human questions, and to ask questions back, and for casual chit-chat Crack the window! You mean open it or break it? 10-min break? Language Modeling and Generation (some slides adapted/borrowed from courses by Dan Klein, Chris Manning, Richard Socher) Language Modeling ! A language model is a distribution over sequences of words (sentences) P(w) = P(w1 … wn) ! Purpose is to usually assign high weights to plausible sentences, e.g., in speech recognition or machine translation ! Also used for language generation now (predict next word given previous words), esp. w/ new RNN models Traditional N-gram LMs NͲGram Models  UsechainruletogeneratewordsleftͲtoͲright  Can’tconditionontheentireleftcontext  NͲgrammodelsmakeaMarkovassumption P(???|Turntopage134andlookatthepictureofthe) Traditional N-gram LMs Empirical NͲGrams  HowdoweknowP(w|history)?  Usestatisticsfromdata(examplesusingGoogleNͲGrams)  E.g.whatisP(door|the)?  Thisisthemaximumlikelihoodestimate 198015222 the first 194623024 the same 168504105 the following 158562063 the world 14112454 the door ----------------- 23135851162 the * Training Counts Sparsity Issue & Parameter Estimation ! New words all the time (antidisestablishmentarianism, kakorrhaphiophobia,, www.xyzabc156.com)….worse for new bigrams and trigrams! Parameter Estimation ! Maximum likelihood estimates won’t get us very far ! Need to smooth these estimates ! General method (procedurally) ! Take your empirical counts ! Modify them in various ways to improve estimates ! General method (mathematically) ! Often can give estimators a formal statistical interpretation ! … but not always ! Approaches that are mathematically obvious aren’t always what works 3516 wipe off the excess 1034 wipe off the dust 547 wipe off the sweat 518 wipe off the mouthpiece … 120 wipe off the grease 0 wipe off the sauce 0 wipe off the mice ----------------- 28048 wipe off the * Smoothing Techniques Smoothing ! We often want to make estimates from sparse statistics: ! Smoothing flattens spiky distributions so they generalize better ! Very important all over NLP, but easy to do badly! ! We’ll illustrate with bigrams today (h = previous word, could be anything). P(w | denied the) 3 allegations 2 reports 1 claims 1 request 7 total allegations charges motion benefits … allegations reports claims charges request motion benefits … allegations reports claims request P(w | denied the) 2.5 allegations 1.5 reports 0.5 claims 0.5 request 2 other 7 total Smoothing Techniques ! Classic Solution: add-one or add small priors to numer/denom ! Backing off to smaller n-grams ! Held-out Reweighting: Important to optimize/estimate how models generalize! So use held-out data to estimate the map of old count to new count ! Kneser-Ney Discounting: two successful ideas: ! Idea 1: observed n-grams occur more in training than they will later ! Idea 2: Type-based fertility (based on how common the word type is) ! Read Chen and Goodman, 1996 for various details and graphs! RNN Language Models ! Avoid huge number of n-grams; Memory requirement only scales with #words ! Can condition on all previous history (with forget gates) ! Loss function on identity of predicted word at each time step ! But harder/slower to train and reach optimum (and less interpretable)? Recurrent Neural Networks! 4/21/16 Richard Socher 9 • RNNs tie the weights at each time step • Condition the neural network on all previous words • RAM requirement only scales with number of words xt−1 xt xt+1 ht−1 ht ht+1 W W yt−1 yt yt+1 Distributional Semantics ! Words occurring in similar context have similar linguistic behavior (meaning) [Harris, 1954; Firth, 1957] ! Traditional approach: context-counting vectors ! Count left and right context in window ! Reweight with PMI or LLR ! Reduce dimensionality with SVD or NNMF [Pereira et al., 1993; Lund & Burgess, 1996; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Pado & Lapata, 2007; Turney and Pantel, 2010; Baroni and Lenci, 2010] ! More word representations: hierarchical clustering based on bigram LM LL [Brown et al., 1992] Ms. Haag plays Elianti . * obj p root nmod sbj Figure 1: An example of a labeled dependency tree. The tree contains a special token “*” which is always the root of the tree. Each arc is directed from head to modiﬁer and h l b l d ibi th f ti f th tt h t apple pear Apple IBM bought run of in 01 100 101 110 111 000 001 010 011 00 0 10 1 11 Figure 2: An example of a Brown word-cluster hierarchy. Each node in the tree is labeled with a bit-string indicat- ing the path from the root node to that node, where 0 i di t l ft b h d 1 i di t i ht b h food 0.6 -0.2 0.9 0.3 -0.4 0.5 Unsupervised Embeddings ! Vector space representations learned on unlabeled linear context (i.e., left/right words): distributional semantics (Harris, 1954; Firth, 1957) Distributional Semantics -- NNs ! Newer approach: context-predicting vectors (NNs) ! SENNA [Collobert and Weston, 2008; Collobert et al., 2011]: Multi-layer DNN w/ ranking-loss objective; BoW and sentence-level feature layers, followed by std. NN layers. Similar to [Bengio et al., 2003]. BENGIO, DUCHARME, VINCENT AND JAUVIN softmax tanh . . . . . . . . . . . . . . . . . . . . . across words most computation here index for index for index for shared parameters Matrix in look−up Table . . . C C wt−1 wt−2 C(wt−2) C(wt−1) C(wt−n+1) wt−n+1 i-th output = P(wt = i | context) Figure 1: Neural architecture: f(i w 1 ··· w +1) = g(i C(w 1) ··· C(w +1)) where g is the Distributional Semantics -- NNs ! CBOW, SKIP, word2vec [Mikolov et al., 2013]: Simple, super-fast NN w/ no hidden layer. Continuous BoW model predicts word given context, skip- gram model predicts surrounding context words given current word ! Other: [Mnih and Hinton, 2007; Turian et al., 2010] ! Demos: hDps://code.google.com/p/word2vec, hDp://metaopJmize.com/projects/wordreprs/, hDp://ml.nec-labs.com/senna/ w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2) CBOW Skip-gram Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. R words from the future of the current word as correct labels. This will require us to do R ⇥2 word classiﬁcations, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10. 4 Results To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much Distributional Semantics ! Other approaches: spectral methods, e.g., CCA ! Word-context correlation [Dhillon et al., 2011, 2012] ! Multilingual correlation [Faruqui and Dyer, 2014; Lu et al., 2015] ! Some later ideas: Train task-tailored embeddings to capture specific types of similarity/semantics, e.g., ! Dependency context [Bansal et al., 2014, Levy and Goldberg, 2014] ! Predicate-argument structures [Hashimoto et al., 2014; Madhyastha et al., 2014] ! Lexicon evidence (PPDB, WordNet, FrameNet) [Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wieting et al., 2015] ! Combining advantages of global matrix factorization and local context window methods [GloVe; Pennington et al., 2014] Compositional Semantics with NNs ! Composing, combining word vectors to representations for longer units: phrases, sentences, paragraphs, … ! Initial approaches: point-wise sum, multiplication [Mitchell and Lapata, 2010; Blacoe and Lapata, 2012] ! Vector-matrix compositionality [Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2011; Yessenalina and Cardie, 2011] ! Linguistic information added via say parses in RvNNs [Socher et al., 2011b, 2012, 2013a, 2013b, 2014; Hermann and Blunsom, 2013] ! Sequential RNNs (with GRU/LSTM gates) (Simple vector averaging w/ updating sometimes competitive) Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Softmax (= logistic regression) is not very powerful 4/7/16 Richard Socher 29 • Softmax only linear decision boundaries • à Lame when problem is complex • Wouldn’t it be cool to get these correct? NN and backprop slides from CS224d – Richard Socher Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Neural Nets for the Win! 4/7/16 Richard Socher 30 • Neural networks can learn much more complex functions and nonlinear decision boundaries! Compositional Semantics with NNs ! Feed-forward NNs with back-propagation A neuron is essentially a binary logistic regression unit hw,b(x) = f (wTx + b) f (z) = 1 1+e−z w, b are the parameters of this neuron i.e., this logistic regression model 33 b: We can have an “always on” feature, which gives a class prior, or separate it out, as a bias term Compositional Semantics with NNs ! Feed-forward NNs with back-propagation A neural network = running several logistic regressions at the same time Before we know it, we have a multilayer neural network…. 36 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Training with Backpropagation • Let’s consider the derivative of a single weight Wij • This only appears inside ai • For example: W23 is only used to compute a2 x1 x2 x3 +1 a1 a2 s U2 W23 19 b2 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Training with Backpropagation Derivative of weight Wij: 20 x1 x2 x3 +1 a1 a2 s U2 W23 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation where for logistic f Training with Backpropagation Derivative of single weight Wij : Local error signal Local input signal 21 x1 x2 x3 +1 a1 a2 s U2 W23 Recurrent NNs ! Recurrent NNs (RNNs) are non-tree, sequential versions of recursive RvNNs ! Weights tied together for each time step ! Loss function on identity of predicted word at each time step Recurrent Neural Networks! 4/21/16 Richard Socher 9 • RNNs tie the weights at each time step • Condition the neural network on all previous words • RAM requirement only scales with number of words xt−1 xt xt+1 ht−1 ht ht+1 W W yt−1 yt yt+1 LSTM RNNs c Memory cell Input gate Output gate Forget gate Figure 3: Long Short-term Memory (LSTM) unit. O d l (Fi 2) l LSTM h li f hj = annot Mult struct xj an based inclu perfo coder repre the or the m ! LSTM (Long short term memory) RNNs have gates for forgetting, allowing learning of longer-term connections by avoiding vanishing/ exploding gradients Character RNNs ! Can directly process each character as a unit! ! Helps learn prefixes, stems, suffixes (form vs. function, rare/ unseen words, etc.) http://karpathy.github.io/2015/05/21/rnn-effectiveness/ RNN Generations ! Automatically generate Shakespeare from RNNs! http://karpathy.github.io/2015/05/21/rnn-effectiveness/ RNN Generations ! Automatically generate Wikipedia-style text from RNNs! http://karpathy.github.io/2015/05/21/rnn-effectiveness/ RNN Generations ! Automatically generate source code from RNNs! http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Various Applications of such RNNs ! Language Modeling and Language Generation ! Classification: Sentiment Analysis ! Conditioned Generation: End-to-end MT, Summarization ! Others: Parsing, Captioning, Q&A, Dialogue (some will be covered in future weeks) "
31,"COMP 790.139 (Fall 2017) Natural Language Processing (with deep learning and connections to vision/robotics) Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, Richard Socher, Chris Manning, others) Announcements ! Chapter section summaries were due yesterday ! Make sure you regularly check your ConnectCarolina email id’s ! 1st Coding assignment to be out soon ! Start thinking of projects early! ! TA: Yixin Nie (yixin1@cs.unc.edu) -- will announce office hours soon! Recap of Distributional Semantics ! Words occurring in similar context have similar linguistic behavior (meaning) [Harris, 1954; Firth, 1957] ! Traditional approach: context-counting vectors ! Count left and right context in window ! Reweight with PMI or LLR ! Reduce dimensionality with SVD or NNMF [Pereira et al., 1993; Lund & Burgess, 1996; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Pado & Lapata, 2007; Turney and Pantel, 2010; Baroni and Lenci, 2010] ! More word representations: hierarchical clustering based on bigram LM LL [Brown et al., 1992] Ms. Haag plays Elianti . * obj p root nmod sbj Figure 1: An example of a labeled dependency tree. The tree contains a special token “*” which is always the root of the tree. Each arc is directed from head to modiﬁer and h l b l d ibi th f ti f th tt h t apple pear Apple IBM bought run of in 01 100 101 110 111 000 001 010 011 00 0 10 1 11 Figure 2: An example of a Brown word-cluster hierarchy. Each node in the tree is labeled with a bit-string indicat- ing the path from the root node to that node, where 0 i di t l ft b h d 1 i di t i ht b h food 0.6 -0.2 0.9 0.3 -0.4 0.5 Unsupervised Embeddings ! Vector space representations learned on unlabeled linear context (i.e., left/right words): distributional semantics (Harris, 1954; Firth, 1957) Distributional Semantics -- NNs ! Newer approach: context-predicting vectors (NNs) ! SENNA [Collobert and Weston, 2008; Collobert et al., 2011]: Multi-layer DNN w/ ranking-loss objective; BoW and sentence-level feature layers, followed by std. NN layers. Similar to [Bengio et al., 2003]. BENGIO, DUCHARME, VINCENT AND JAUVIN softmax tanh . . . . . . . . . . . . . . . . . . . . . across words most computation here index for index for index for shared parameters Matrix in look−up Table . . . C C wt−1 wt−2 C(wt−2) C(wt−1) C(wt−n+1) wt−n+1 i-th output = P(wt = i | context) Figure 1: Neural architecture: f(i w 1 ··· w +1) = g(i C(w 1) ··· C(w +1)) where g is the Distributional Semantics -- NNs ! CBOW, SKIP, word2vec [Mikolov et al., 2013]: Simple, super-fast NN w/ no hidden layer. Continuous BoW model predicts word given context, skip- gram model predicts surrounding context words given current word ! Other: [Mnih and Hinton, 2007; Turian et al., 2010] ! Demos: h#ps://code.google.com/p/word2vec, h#p://metaop7mize.com/projects/wordreprs/, h#p://ml.nec-labs.com/senna/ w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2) CBOW Skip-gram Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. R words from the future of the current word as correct labels. This will require us to do R ⇥2 word classiﬁcations, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10. 4 Results To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much Skipgram word2vec [Mikolov et al., 2013] Few mins. vs. days/weeks/months!! w(t) w(t-2) w(t-1) w(t+1) w(t+2) INPUT PROJECTION OUTPUT context window w Skip-gram word2vec Objective Function ! Objective of Skip-gram model is to max. the avg. log probability: p g aining objective of the Skip-gram model is to ﬁnd word representations that are us ting the surrounding words in a sentence or a document. More formally, given a sequ g words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the obability 1 T T ! t=1 ! −c≤j≤c,j̸=0 log p(wt+j|wt) c is the size of the training context (which can be a function of the center word wt) ts in more training examples and thus can lead to a higher accuracy, at the expens 2 ning time. The basic Skip-gram formulation deﬁnes p(wt+j|wt) using the softmax function: p(wO|wI) = exp ! v′ wO ⊤vwI "" #W w=1 exp ! v′ w ⊤vwI "" ere vw and v′ w are the “input” and “output” vector representations of w, and W is the nu of words in the vocabulary. This formulation is impractical because the cost of computi og p(wO|wI) is proportional to W, which is often large (105–107 terms). Hierarchical Softmax ! The above conditional probability is defined via the softmax function: where v and v′ are the “input” and “output” vector representations of w, and W is the number of words in the vocabulary [Mikolov et al., 2013] Efficient Skip-gram word2vec: ! Negative Sampling: ! I.e., to distinguish the target word wo from draws from the noise distribution Pn(w) using logistic regression, where there are k negative samples for each data sample. NCE posits that a good model should be able to differentiate data from noise by means of logistic regression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models by ranking the data above noise. While NCE can be shown to approximately maximize the log probability of the softmax, the Skip- gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality. We deﬁne Negative sampling (NEG) by the objective log σ(v′ wO ⊤vwI) + k % i=1 Ewi∼Pn(w) & log σ(−v′ wi ⊤vwI) ' (4) 3 [Mikolov et al., 2013] Efficient Skip-gram word2vec: ! Hierarchical Softmax: ! Instead of evaluating W output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only about log2(W) nodes. ! Uses a binary tree representation of the output layer with the W words as its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These define a random walk that assigns probabilities to words. ts leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These deﬁne a random walk that assigns probabilities to words. More precisely, each word w can be reached by an appropriate path from the root of the tree. Let n(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so n(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary ﬁxed child of n and let [ [x] ] be 1 if x is true and -1 otherwise. Then the hierarchical softmax deﬁnes p(wO|wI) as follows: p(w|wI) = L(w)−1 $ j=1 σ ! [ [n(w, j + 1) = ch(n(w, j))] ] · v′ n(w,j) ⊤vwI "" (3) where σ(x) = 1/(1 + exp(−x)). It can be veriﬁed that #W w=1 p(w|wI) = 1. This implies that the cost of computing log p(wO|wI) and ∇log p(wO|wI) is proportional to L(wO), which on average s no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which assigns two representations vw and v′ w to each word w, the hierarchical softmax formulation has one representation vw for each word w and one representation v′ n for every inner node n of the binary tree. The structure of the tree used by the hierarchical softmax has a considerable effect on the perfor- mance. Mnih and Hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy [10]. In our work we use a binary Huffman tree, as it assigns short codes to the frequent words which results in fast training. It has been observed before that grouping words together by their frequency works well as a very simple peedup technique for the neural network based language models [5, 8]. 2.2 Negative Sampling [Mikolov et al., 2013] Analogy Properties Learned [Mikolov et al., 2013] Figure 2: Left panel shows vector offsets for three word pairs illustrating the gender relation. Right panel shows a different projection, and the singular/plural relation for two words. In high-dimensional space, multiple relations can be embedded for a single word. id d W h l d l l d h Method LSA-80 LSA-320 LSA-640 RNN-80 RNN-320 RNN-640 RNN-1600 Table 2: Resu different word Method Analogy Properties Learned [Mikolov et al., 2013] -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Country and Capital Vectors Projected by PCA China Japan France Russia Germany Italy Spain Greece Turkey Beijing Paris Tokyo Poland Moscow Portugal Berlin Rome Athens Madrid Ankara Warsaw Lisbon Figure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their capital cities. The ﬁgure illustrates ability of the model to automatically organize concepts and learn implicitly the relationships between them, as during the training we did not provide any supervised information about what a capital city means. hi h i d l l P( | ) i h Ski bj i Th h k i Analogy Properties Learned [Mikolov et al., 2013] Newspapers New York New York Times Baltimore Baltimore Sun San Jose San Jose Mercury News Cincinnati Cincinnati Enquirer NHL Teams Boston Boston Bruins Montreal Montreal Canadiens Phoenix Phoenix Coyotes Nashville Nashville Predators NBA Teams Detroit Detroit Pistons Toronto Toronto Raptors Oakland Golden State Warriors Memphis Memphis Grizzlies Airlines Austria Austrian Airlines Spain Spainair Belgium Brussels Airlines Greece Aegean Airlines Company executives Steve Ballmer Microsoft Larry Page Google Samuel J. Palmisano IBM Werner Vogels Amazon Table 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples). The goal is to compute the fourth phrase using the ﬁrst three. Our best model achieved an accuracy of 72% on this dataset. This way, we can form many reasonable phrases without greatly increasing the size of the vocabu- lary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory intensive Many techniques have been previously developed to identify phrases in the text; however Analogy Properties Learned [Mikolov et al., 2013] NEG-15 with 10−5 subsampling HS with 10−5 subsampling Vasco de Gama Lingsugur Italian explorer Lake Baikal Great Rift Valley Aral Sea Alan Bean Rebbeca Naomi moonwalker Ionian Sea Ruegen Ionian Islands chess master chess grandmaster Garry Kasparov Table 4: Examples of the closest entities to the given short phrases, using two different models. Czech + currency Vietnam + capital German + airlines Russian + river French + actress koruna Hanoi airline Lufthansa Moscow Juliette Binoche Check crown Ho Chi Minh City carrier Lufthansa Volga River Vanessa Paradis Polish zolty Viet Nam ﬂag carrier Lufthansa upriver Charlotte Gainsbourg CTK Vietnamese Lufthansa Russia Cecile De Table 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two vectors are shown, using the best Skip-gram model. To maximize the accuracy on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy of 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B words, which suggests that the large amount of the training data is crucial. To gain further insight into how different the representations learned by different models are, we did inspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we show a sample of such comparison. Consistently with the previous results, it seems that the best representations of phrases are learned by a model with the hierarchical softmax and subsampling. Distributional Semantics ! Other approaches: spectral methods, e.g., CCA ! Word-context correlation [Dhillon et al., 2011, 2012] ! Multilingual correlation [Faruqui and Dyer, 2014; Lu et al., 2015] ! Multi-sense embeddings [Reisinger and Mooney, 2010; Neelakantan et al., 2014] ! Some later ideas: Train task-tailored embeddings to capture specific types of similarity/semantics, e.g., ! Dependency context [Bansal et al., 2014, Levy and Goldberg, 2014] ! Predicate-argument structures [Hashimoto et al., 2014; Madhyastha et al., 2014] ! Lexicon evidence (PPDB, WordNet, FrameNet) [Xu et al., 2014; Yu and Dredze, 2014; Faruqui et al., 2014; Wieting et al., 2015] ! Combining advantages of global matrix factorization and local context window methods [GloVe; Pennington et al., 2014] Multi-sense Embeddings ! Different vectors for each sense of a word Figure 1: Architecture of the Skip-gram model with window size Rt = 2. Context ct of word wt consists of wt−1, wt−2, wt+1, wt+2. and let each sense of word have its own embed- ding, and induce the senses by clustering the em- beddings of the context words around each token. Th i f h i h :RUG6HQVH 9HFWRUV Y ZW YJ ZW &RQWH[W 9HFWRUV YJ ZW YJ ZW YJ ZW $YHUDJH&RQWH[W 9HFWRU &RQWH[W&OXVWHU &HQWHUV Y ZW Y ZW 3UHGLFWHG 6HQVHVW ȝ ZW YFRQWH[W FW    ȝ ZW ȝ ZW  &RQWH[W 9HFWRUV YJ ZW YJ ZW YJ ZW YJ ZW Figure 2: Architecture of Multi-Sense Skip-gram (MSSG) model with window size Rt = 2 and K = 3. Context ct of word wt consists of wt−1, wt−2, wt+1, wt+2. The sense is predicted by ﬁnding the cluster center of the context that is clos- est to the average of the context vectors. of word wt when observed with context ct as words r, and tradi- uman lated esults roach based e iso- s the e pro- n the secki o his while s fol- pro- ntics, Sec- ction ... chose Zbigniew Brzezinski for the position of ... ... thus the symbol s position on his clothing was ... ... writes call options against the stock position ... ... offered a position with ... ... a position he would hold until his retirement in ... ... endanger their position as a cultural group... ... on the chart of the vessel s current position ... ... not in a position to help... (cluster#2) post appointme nt, role, job (cluster#4) lineman, tackle, role, scorer (cluster#1) location importance bombing (collect contexts) (cluster) (cluster#3) intensity, winds, hour, gust (similarity) single prototype Figure 1: Overview of the multi-prototype approach to near-synonym discovery for a single target word independent of context. Occurrences are clustered and cluster centroids are used as prototype vectors. Note the “hurricane” sense of position (cluster 3) is not typically considered appropriate in WSD. approach is to compute a single prototype vector for [Reisinger and Mooney, 2010] [Neelakantan et al., 2014] Syntactically Tailored Embeddings ! Context window size (SKIP) ! Smaller window ! syntactic/functional similarity ! Larger window ! topical similarity ! Similar effect in distributional representations The morning flight at the JFK airport was delayed context window (Lin and Wu, 2009) [Bansal et al., 2014] Cluster Examples ! SKIP, w = 10: [attendant, takeoff, airport, carry-on, airplane, flown, landings, flew, fly, cabins, …] [maternity, childbirth, clinic, physician, doctor, medical, health-care, day-care, …] [transactions, equity, investors, capital, financing, stock, fund, purchases, …] [Bansal et al., 2014] Cluster Examples ! SKIP, w = 1 [Mr., Mrs., Ms., Prof., III, Jr., Dr.] [Jeffrey, William, Dan, Robert, Stephen, Peter, John, Richard, ...] [Portugal, Iran, Cuba, Ecuador, Greece, Thailand, Indonesia, …] [truly, wildly, politically, financially, completely, potentially, ...] [his, your, her, its, their, my, our] [Your, Our, Its, My, His, Their, Her] [Bansal et al., 2014] Syntactically Tailored Embeddings ! Syntactic context (SKIPDEP) ! Condition on dependency context instead of linear ! First parse a large corpus with baseline parser: … said that the regulation of safety is … NMOD PMOD (child) (parent) (grandparent) (dep label) [Bansal et al., 2014] Syntactically Tailored Embeddings dep label dep label grandparent parent child [PMOD<L> regulation<G> of safety PMOD<L>] context windows ! Syntactic context (SKIPDEP) ! Condition on dependency context instead of linear ! Then convert each dependency to a tuple: ! Syntactic information in clustering, topic, semantic space models (Sagae and Gordon, 2009; Haffari et al., 2011; Grave et al., 2013; Boyd-Graber and Blei, 2008; Pado and Lapata, 2007) [Bansal et al., 2014] Intrinsic Evaluation Topical Syntactic/ Functional (Finkelstein et al., 2002) Representation SIM TAG BROWN – 89.3 SENNA 49.8 85.2 HUANG 62.6 78.1 SKIP, w = 10 44.6 71.5 SKIP, w = 5 44.4 81.1 SKIP, w = 1 37.8 86.6 SKIPDEP 34.6 88.3 System Test [Bansal et al., 2014] Parsing Experiments ! Main WSJ results: ( g) System Test Baseline 91.9 BROWN 92.7 SENNA 92.3 TURIAN 92.3 HUANG 92.4 SKIP 92.3 SKIPDEP 92.7 Ensemble Results ALL – BROWN 92.9 ALL 93.0 (faster) (complementary) [Bansal et al., 2014] Task-Trained Embeddings ! Can also directly train word embeddings on the task, via back-prop from the task supervision (XE errors), e.g., dependency parsing: [Chen and Manning, 2014; CS224n] Christopher Manning Model Architecture Input layer x lookup + concat Hidden layer h h = ReLU(Wx + b1) Output layer y y = softmax(Uh + b2) Softmax probabilities cross-entropy error will be back-propagated to the embeddings. Multilingual Embeddings via CCA ! Translational context (say, English ""! German) can help learn stronger embeddings, e.g., separate antonyms vs. synonyms ! CCA on translation pairs to map them to shared space [Faruqui and Dyer, 2014] transformations of each view via deep networks. 2.1 Canonical Correlation Analysis A popular method for multi-view representation learning is canonical correlation analysis (CCA; Hotelling, 1936). Its objective is to ﬁnd two vec- tors u ∈RDx and v ∈RDy such that projections of the two views onto these vectors are maximally (linearly) correlated: max u∈RDx,v∈RDy E ! (u⊤x)(v⊤y) "" # E [(u⊤x)2] # E [(v⊤y)2] = u⊤Σxyv # u⊤Σxxu # v⊤Σyyv (1) where Σxy and Σxx are the cross-view and within- view covariance matrices. (1) is extended to learn- ing multi-dimensional projections by optimizing the Wf where Wf the two ne covariance in the sam mation is CCA proj though DC like linear gradient-b gorithms or with a as we do proach, w samples ( the netwo Multi-view Embeddings via CCA [Faruqui and Dyer, 2014] Es En 512 67.2 71.6 64.5 70.5 78.2 53.6 44.2 44.5 Average – 56.6 64.5 51.0 62.0 65.5 60.8 44 44.7 Table 1: Spearman’s correlation (left) and accuracy (right) on different tasks. Before CCA After CCA Linear vs Deep CCA Embeddings WS-353 WS-SIM WS-REL SL-999 A Original 46.7 56.3 36.6 26.5 26 CCA-1 67.2 73.0 63.4 40.7 42 CCA-Ens 67.5 73.1 63.7 40.4 42 DCCA-1 (BestAvg) 69.6 73.9 65.6 38.9 35 DCCA-Ens (BestAvg) 70.8 75.2 67.3 41.7 42 DCCA-1 (MostBeat) 68.6 73.5 65.7 42.3 44 DCCA-Ens (MostBeat) 69.9 74.4 66.7 42.3 43 Table 1: Main results on word and bigram similarity tasks, tuned on the 7 dev tasks. S matches or improves the best linear CCA result; boldface indicates the best result in a g put embeddings are 640-dimensional and are trained via latent semantic analysis (LSA) on the WMT each pair, order the pai pute Spearman’s correlat ! Linear feature mapping not sufficiently powerful to capture hidden, non-linear relationships within data ! Use deep NNs to learn non-linear transformations of orig. embeddings to space where linear correlation maximized ! Linear CCA results: Deep-CCA word vector 2 English German word vector 1 View 1 View 2 u v f g foul foul awful ugly pretty charming cute gorgeous marvelous magniﬁcent elegant splendid hidous beastlygrotesque horrid schrecklichen h¨ assliche ziemlich bezaubernder clever blonden wunderbaren großartige elegante hervorragende abscheulichen gebot grotesk aufzukl¨ aren Figure 1: Illustration of deep CCA. [Lu, Wang, Bansal, Gimpel, Livescu, 2015] Deep-CCA ! 2 DNNs f, g extract features from the 2 input views x and y ! DNNs are trained to maximize output linear correlation of 2 views ! DNN weights and linear projections optimized together: ! Covariance matrices computed for , as in CCA ! Mini-batch SGD: Feed-forward a sample to estimate (u, v) and gradient and then update NN weights via back-propagation word vector 2 German foul schrecklichen h¨ assliche ziemlich bezaubernder clever blonden wunderbaren großartige elegante hervorragende abscheulichen gebot grotesk aufzukl¨ aren CCA. p networks. is representation nalysis (CCA; ﬁnd two vec- hat projections are maximally y) "" using deep neural networks, dubbed deep canonical correlation analysis (DCCA) and illustrated in Fig- ure 1. In this model, two (possibly deep) neural networks f and g are used to extract features from each view, and trained to maximize the correlations between outputs in the two views, measured by a linear CCA step with projection mappings (u, v). The neural network weights and the linear projec- tions are optimized together using the objective max Wf ,Wg,u,v u⊤Σfgv # u⊤Σffu # v⊤Σggv , (2) where Wf and Wg denote the weight parameters of the two networks, and where Σfg, Σff and Σgg are covariance matrices computed for {f(xi), g(yi)}N i=1 in the same way as CCA. The ﬁnal feature transfor- mation is the composition of the neural network and CCA projection, e.g., u⊤f(x) for the ﬁrst view. Al- though DCCA does not have a closed-form solution like linear CCA, the parameters can be learned via [Andrew et al., 2013] word vector 2 German schrecklichen che mlich bezaubernder clever blonden derbaren ßartige elegante rragende abscheulichen k aufzukl¨ aren works. sentation s (CCA; two vec- ojections maximally y)2] correlation analysis (DCCA) and illustrated in Fig- ure 1. In this model, two (possibly deep) neural networks f and g are used to extract features from each view, and trained to maximize the correlations between outputs in the two views, measured by a linear CCA step with projection mappings (u, v). The neural network weights and the linear projec- tions are optimized together using the objective max Wf ,Wg,u,v u⊤Σfgv # u⊤Σffu # v⊤Σggv , (2) where Wf and Wg denote the weight parameters of the two networks, and where Σfg, Σff and Σgg are covariance matrices computed for {f(xi), g(yi)}N i=1 in the same way as CCA. The ﬁnal feature transfor- mation is the composition of the neural network and CCA projection, e.g., u⊤f(x) for the ﬁrst view. Al- though DCCA does not have a closed-form solution like linear CCA, the parameters can be learned via gradient-based optimization, whether with batch al- Results Embeddings WS-353 WS-SIM WS-REL SL-999 A Original 46.7 56.3 36.6 26.5 26 CCA-1 67.2 73.0 63.4 40.7 42 CCA-Ens 67.5 73.1 63.7 40.4 42 DCCA-1 (BestAvg) 69.6 73.9 65.6 38.9 35 DCCA-Ens (BestAvg) 70.8 75.2 67.3 41.7 42 DCCA-1 (MostBeat) 68.6 73.5 65.7 42.3 44 DCCA-Ens (MostBeat) 69.9 74.4 66.7 42.3 43 Table 1: Main results on word and bigram similarity tasks, tuned on the 7 dev tasks. S matches or improves the best linear CCA result; boldface indicates the best result in a g put embeddings are 640-dimensional and are trained each pair, order the pai ! Word-similarity improvements ! Also gets improvements on bigram similarity datasets [Lu, Wang, Bansal, Gimpel, Livescu, 2015] Analysis .5 65.7 42.3 44.4 44.7 36.7 41.9 384 .4 66.7 42.3 43.7 47.4 38.8 43.3 384 m similarity tasks, tuned on 7 development tasks (see text for tches or improves the best linear CCA result; boldface indicates on 3.4 for discussion on NN results. Avg 42.6 45.9 dev sets. chell and rticularly at DCCA ver CCA NN task f annota- er for the better with DCCA worse with DCCA arrive come author creator locate ﬁnd leader manager way manner buddy companion recent new crowd bunch take obtain achieve succeed boundary border attention interest win accomplish join add contemplate think mood emotion Table 3: Highly-similar pairs in SimLex-999 that improved/degraded the most under DCCA. Pairs are sorted in decreasing order according to the amount of improvement/degradation. ! DCCA discards hypernymy, separates senses ! High-similarity word pairs that change most with DCCA [Lu, Wang, Bansal, Gimpel, Livescu, 2015] Analysis ! DCCA more cleanly separates synonym-antonym lists Original CCA-1 DCCA-1 (MostBeat) Figure 2: t-SNE visualization of synonyms (green) and antonyms (red, capitalized) of dangerous. δd(w) −(δc(w) + δo(w)). If ∆(w) < 0, then word pair w was closer to the human ranking using DCCA. Table 3 shows word pairs from SimLex-999 with high human similarity ratings (≥7 out of 10); column 1 shows pairs with smallest ∆values, and column 2 shows pairs with largest ∆values. vised LDA and LSA (Boyd-Graber and Blei, 2009; Zhao and Xing, 2006). There have been other recent deep learning ap- proaches to bilingual representations, e.g., based on a joint monolingual and bilingual objective (Zou et al., 2013). There has also been recent interest [Lu, Wang, Bansal, Gimpel, Livescu, 2015] Retrofitting Word Embeddings to Lexicons ! We want the inferred word vector to be close to the observed value qˆ and close to its neighbors qj, ∀j such that (i, j) ∈ E, where E is the set of relations in a dictionary/lexicon (e.g., WordNet, PPDB, etc.) [Faruqui et al., 2015] Figure 1: Word graph with edges between related words showing the observed (grey) and the inferred (white) word vector representations. Experimentally, we show that our method works well with different state-of-the-art word vector mod- tors to be retroﬁtted (and correspond to V⌦); shaded nodes are labeled with the corresponding vectors in ˆ Q, which are observed. The graph can be interpreted as a Markov random ﬁeld (Kindermann and Snell, 1980). The distance between a pair of vectors is deﬁned to be the Euclidean distance. Since we want the inferred word vector to be close to the observed value ˆ qi and close to its neighbors qj, 8j such that (i, j) 2 E, the objective to be minimized becomes: (Q) = n X i=1 2 4↵ikqi −ˆ qik2 + X (i,j)2E βijkqi −qjk2 3 5 where ↵and β values control the relative strengths of associations (more details in §6.1). I thi ﬁt t i th d t i d Figure 1: Word graph with edges between related words showing the observed (grey) and the inferred (white) word vector representations. Experimentally, we show that our method works well with different state-of-the-art word vector mod- els, using different kinds of semantic lexicons and gives substantial improvements on a variety of benchmarks, while beating the current state-of-the- art approaches for incorporating semantic informa- tion in vector training and trivially extends to mul- tiple languages. We show that retroﬁtting gives consistent improvement in performance on evalua- tors to be retroﬁtted (and correspond to V⌦); shaded nodes are labeled with the corresponding vectors in ˆ Q, which are observed. The graph can be interpreted as a Markov random ﬁeld (Kindermann and Snell, 1980). The distance between a pair of vectors is deﬁned to be the Euclidean distance. Since we want the inferred word vector to be close to the observed value ˆ qi and close to its neighbors qj, 8j such that (i, j) 2 E, the objective to be minimized becomes: (Q) = n X i=1 2 4↵ikqi −ˆ qik2 + X (i,j)2E βijkqi −qjk2 3 5 where ↵and β values control the relative strengths of associations (more details in §6.1). In this case, we ﬁrst train the word vectors inde- pendent of the information in the semantic lexicons and then retroﬁt them. is convex in Q and its so- lution can be found by solving a system of linear equations. To do so, we use an efﬁcient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011) Th i Q i i i li d b l Bias in Word Embeddings [Bolukbasi et al., 2016] Extreme she 1. homemaker 2. nurse 3. receptionist 4. librarian 5. socialite 6. hairdresser 7. nanny 8. bookkeeper 9. stylist 10. housekeeper Extreme he 1. maestro 2. skipper 3. protege 4. philosopher 5. captain 6. architect 7. ﬁnancier 8. warrior 9. broadcaster 10. magician Gender stereotype she-he analogies sewing-carpentry registered nurse-physician housewife-shopkeeper nurse-surgeon interior designer-architect softball-baseball blond-burly feminism-conservatism cosmetics-pharmaceuticals giggle-chuckle vocalist-guitarist petite-lanky sassy-snappy diva-superstar charming-affable volleyball-football cupcakes-pizzas lovely-brilliant Gender appropriate she-he analogies queen-king sister-brother mother-father waitress-waiter ovarian cancer-prostate cancer convent-monastery Figure 1: Left The most extreme occupations as projected on to the she−he gender direction on w2vNEWS. Occupations such as businesswoman, where gender is suggested by the orthography, were excluded. Right Automatically generated analogies for the pair she-he using the procedure described in text. Each automatically generated analogy is evaluated by 10 crowd-workers to whether or not it reﬂects gender stereotype. father is to a doctor as a mother is to a nurse. The primary embedding studied in this paper is the popular publicly-available word2vec [19, 20] 300 dimensional embedding trained on a corpus of Google News texts consisting of 3 million English words, which we refer to here as the w2vNEWS. One might have hoped that the Google News embedding would exhibit little gender bias because many of its authors are professional journalists. We also analyze other publicly available embeddings trained via other algorithms and ﬁnd similar biases (Appendix B). In this paper, we quantitatively demonstrate that word-embeddings contain biases in their geometry that reﬂect gender stereotypes present in broader society.1 Due to their wide-spread usage as basic features, word embeddings not only reﬂect such stereotypes but can also amplify them. This poses a i iﬁ i k d h ll f hi l i d i li i Th l i d f ! Debiasing word embeddings via identifying pairs (sets) of words to correct/neutralize, identify bias direction (subspace), and then debias via neutralize+equalize or soften algorithms. Compositional Semantics with NNs ! Composing, combining word vectors to representations for longer units: phrases, sentences, paragraphs, … ! Initial approaches: point-wise sum, multiplication [Mitchell and Lapata, 2010; Blacoe and Lapata, 2012] ! Vector-matrix compositionality [Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2011; Yessenalina and Cardie, 2011] ! Linguistic information added via say parses in RvNNs [Socher et al., 2011b, 2012, 2013a, 2013b, 2014; Hermann and Blunsom, 2013] ! Sequential RNNs (with GRU/LSTM gates) (Simple vector averaging w/ updating sometimes competitive) Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Softmax (= logistic regression) is not very powerful 4/7/16 Richard Socher 29 • Softmax only linear decision boundaries • à Lame when problem is complex • Wouldn’t it be cool to get these correct? NN and backprop slides from CS224d – Richard Socher Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Neural Nets for the Win! 4/7/16 Richard Socher 30 • Neural networks can learn much more complex functions and nonlinear decision boundaries! Compositional Semantics with NNs ! Feed-forward NNs with back-propagation A neuron is essentially a binary logistic regression unit hw,b(x) = f (wTx + b) f (z) = 1 1+e−z w, b are the parameters of this neuron i.e., this logistic regression model 33 b: We can have an “always on” feature, which gives a class prior, or separate it out, as a bias term Compositional Semantics with NNs ! Feed-forward NNs with back-propagation A neural network = running several logistic regressions at the same time Before we know it, we have a multilayer neural network…. 36 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Training with Backpropagation • Let’s consider the derivative of a single weight Wij • This only appears inside ai • For example: W23 is only used to compute a2 x1 x2 x3 +1 a1 a2 s U2 W23 19 b2 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation Training with Backpropagation Derivative of weight Wij: 20 x1 x2 x3 +1 a1 a2 s U2 W23 Compositional Semantics with NNs ! Feed-forward NNs with back-propagation where for logistic f Training with Backpropagation Derivative of single weight Wij : Local error signal Local input signal 21 x1 x2 x3 +1 a1 a2 s U2 W23 Syntactically Recursive NNs ! Socher et al., 2013a, 2014: RvNNs on constituent and dependency parse trees (A, a= ) (B, b= ) (C, c= ) P(1), p(1)= P(2), p(2)= = f W(B,C) b c = f W(A,P ) a p(1) (1) Figure 3: Example of a syntactically untied RNN in which the function to compute a parent vector depends on the syntactic categories of its children which we assume are given for now. PCFG. Hence, CVGs combine discrete, syntactic A man wearing a helmet jumps on his bike near a beach det nsubj partmod det dobj root prep poss pobj prep det pobj Figure 2: Example of a full dependency tree for a longer sentence. The DT-RNN will compute vector representations at every word that represents that word and an arbitrary number of child nodes. The ﬁnal representation is computed at the root node, here at the verb jumps. Note that more important activity and object words are higher up in this tree structure. supervised model of Huang et al. (2012) which can learn single word vector representations from both local and global contexts. The idea is to construct a neural network that outputs high scores for windows and documents that occur in a large unlabeled corpus and low scores for window-document pairs where one word is replaced by a random word. When such a network is optimized via gradient descent the derivatives backpropagate into a word embedding matrix A which stores word vectors as columns. In order to predict correct scores the vectors in the ma- trix capture co-occurrence statistics. We use d = 50 in all our experiments. The embedding matrix X is then used by ﬁnding the column index i of each word: [w] = i and retrieving the corresponding col- umn xw from X. Henceforth, we represent an input sentence s as an ordered list of (word,vector) pairs: (( ) ( )) Students bikes night ride at x1 x2 x3 x4 x5 h1 h2 h3 h4 h5 Figure 3: Example of a DT-RNN tree structure for com- puting a sentence representation in a bottom up fashion. dency tree for this sentence can be summarized by the following set of (child, parent) edges: d = {(1, 2), (2, 0), (3, 2), (4, 2), (5, 4)}. The DT-RNN model will compute parent vectors at each word that include all the dependent (chil- d ) d i b tt f hi i Recurrent NNs ! Recurrent NNs (RNNs) are non-tree, sequential versions of recursive RvNNs ! Weights tied together for each time step ! Loss function on identity of predicted word at each time step Recurrent Neural Networks! 4/21/16 Richard Socher 9 • RNNs tie the weights at each time step • Condition the neural network on all previous words • RAM requirement only scales with number of words xt−1 xt xt+1 ht−1 ht ht+1 W W yt−1 yt yt+1 LSTM RNNs c Memory cell Input gate Output gate Forget gate Figure 3: Long Short-term Memory (LSTM) unit. O d l (Fi 2) l LSTM h li f hj = annot Mult struct xj an based inclu perfo coder repre the or the m ! LSTM (Long short term memory) RNNs have gates for forgetting, allowing learning of longer-term connections by avoiding vanishing/ exploding gradients Character RNNs ! Can directly process each character as a unit! ! Helps learn prefixes, stems, suffixes (form vs. function, rare/ unseen words, etc.) http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Supervised Sentence Embedding Models ! Just like word embeddings were supervised using lexicons, dictionaries, taxonomies (WordNet) etc., sentence embeddings also benefit greatly from supervision! ! 2 examples: supervision based on bidirectional sentence similarity (paraphrases) or directed similarity (entailment vs contradiction vs neutral) Paraphrase-based Sentence Embeddings ! Phrases that mean the same, are replaceable in context main reason why ||| principal reason for informed about the outcome ||| notiﬁed of the results with particular emphasis ||| with speciﬁc focus we 'll have a good time ||| we 're gonna have fun 50 years ago ||| ﬁve decades ago that , according to ||| which , in accordance with program is aimed at ||| programme aims to are under the obligation ||| have a duty a critical component ||| an essential element Paraphrase-based Sentence Embeddings ! PPDB: Massive, useful resource (220M) automatically extracted from parallel bilingual corpora ! Idea summary: carefully extract a few (< 0.05%) +ve and -ve pairs from unannotated PPDB as weak supervision ! Train a parametric paraphrase model (2-view RNN with hinge loss) on these pairs, to be able to represent arbitrary phrases as embeddings ! This learns strong word/phrase embeddings that better predict paraphrases on new annotated PPDB subset and gets SoA on word/bigram similarity datasets [Wieting, Bansal, Gimpel, Livescu, Roth, 2015] [Ganitkevitch et al., 2013] Paraphrase Model ! 2 parse-based RvNNs with a hinge-based loss function 1 2 3 4 5 6 7 3 4 5 1 2cats catch mice The Cats eat mice Figure 1: An overview of our paraphrase model. The recursive auto larity in the space corresponds to the strengt paraphrase relationship between phrases. Our model is a recursive neural network similar to that used by Socher et al. (2014). use a constituent parser to obtain a binarized a phrase. For phrase x, we compute its vect through recursive computation on the parse. if phrase p is the yield of a parent node in tree, and phrases c1 and c2 are the yields of child nodes, we deﬁne g(p) recursively as fo g(p) = f(W[g(c1); g(c2)] + b) where f is an element-wise activation f (tanh), [g(c1); g(c2)] 2 R2n is the concat of the child vectors, W 2 Rn⇥2n is the co tion matrix, b 2 Rn is the offset, and n is mensionality of the word embeddings. If has no children (i.e., it is a single token), w g(p) = W (p) w , where Ww is the word emb matrix in which particular word vectors are Loss [Socher et al., 2011] Composition = Paraphrase Model ! Loss: +ve pairs closer than -ve pairs with margin δ Positive training pairs Negative training pairs Regularization terms min W,b,Ww 1 |X| X hx1,x2i2X max(0, δ −g(x1) · g(x2) + g(x1) · g(t1)) + max(0, δ −g(x1) · g(x2) + g(x2) · g(t2)) ◆ +λW (kWk2 + kbk2) + λWw kWwinitial −Wwk2 [Wieting, Bansal, Gimpel, Livescu, Roth, 2015] Entailment-based Embeddings ! SNLI and Multi-NLI corpora with sentence pairs of 3 relationships: entailment, contradiction, neutral/unrelated [Bowman et al., 2015; Williams et al., 2017] Premise Label Hypothesis Genre The Old One always comforted Ca'daan, except today. neutral Ca'daan knew the Old One very well. Fiction Your gift is appreciated by each and every student who will benefit from your generosity. neutral Hundreds of students will benefit from your generosity. Letters yes now you know if if everybody like in August when everybody's on vacation or something we can dress a little more casual or contradiction August is a black out month for vacations in the company. Telephone Speech At the other end of Pennsylvania Avenue, people began to line up for a White House tour. entailment People formed a line at the end of Pennsylvania Avenue. 9/11 Report A black race car starts up in front of a crowd of people. contradiction A man is driving down a lonely road. SNLI Entailment-based Embeddings [Conneau et al., 2017] Encoder Encoder MLP Prediction Premise Hypothesis Same Structure v u |v −u| v ⌦u [v, u, v ⌦u, |v −u|] Encoding Matching Entailment-based Embeddings [Conneau et al., 2017] Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57 ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0 90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7 .65/.64 fastText BOW† 76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62 GloVe BOW† 78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9 .51/.54 BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48 Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35 SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45 Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70 NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65 BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67 Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best Entailment-based Embeddings [Nie and Bansal, 2017] ! Encoder details: Starting point is 1-layer biLSTM with Max-pooling w1 w2 biLSTM biLSTM biLSTM Row max pooling Final Vector Representation Word Embedding Source Sentence Fine-tunning wn biLSTM w1 w2 biLSTM biLSTM biLSTM biLSTM biLSTM biLSTM biLSTM biLSTM Row max pooling Final Vector Representation Word Embedding Source Sentence Fine-tunning wn Entailment-based Embeddings [Nie and Bansal, 2017] ! Improved Encoders: e.g., via shortcut-stacked RNNs (to help learn higher-level semantic features and to help sparse gradients from max-pooling to flow to lower layers Entailment-based Embeddings [Nangia et al., 2017] ! RepEval 2017 Shared Task Results MultiNLI Overall 392,702 20,000 20,000 22.3 91% 98% 88.7% 67.4% Table 2: Key statistics for the corpus broken down by genre, presented alongside ﬁgures from SNLI for comparison. The ﬁrst ﬁve genres represent the matched section of the development and test sets, and the remaining ﬁve represent the mismatched section. The ﬁrst three statistics shown are the number of examples in each genre. #Wds. Prem. is the mean token count among premise sentences. ‘S’ parses is the percentage of premises or hypotheses which the Stanford Parser labeled as full sentences rather than fragments. Agrmt. is the percent of individual annotator labels that match the assigned gold label used in evaluation. BiLSTM Acc. gives the test accuracy on the full test set for the BiLSTM baseline model trained on MultiNLI and SNLI. Team Name Authors Matched Mismatched Model Details alpha (ensemble) Chen et al. 74.9% 74.9% STACK, CHAR, ATTN., POOL, PRODDIFF YixinNie-UNC-NLP Nie and Bansal 74.5% 73.5% STACK, POOL, PRODDIFF, SNLI alpha Chen et al. 73.5% 73.6% STACK, CHAR, ATTN, POOL, PRODDIFF Rivercorners (ensemble) Balazs et al. 72.2% 72.8% ATTN, POOL, PRODDIFF, SNLI Rivercorners Balazs et al. 72.1% 72.1% ATTN, POOL, PRODDIFF, SNLI LCT-MALTA Vu et al. 70.7% 70.8% CHAR, ENHEMB, PRODDIFF, POOL TALP-UPC Yang et al. 67.9% 68.2% CHAR, ATTN, SNLI BiLSTM baseline Williams et al. 67.0% 67.6% POOL, PRODDIFF, SNLI Table 3: RepEval 2017 shared task competition results. The Model Details column lists some of the key strategies used in each system, using keywords: STACK: use of multilayer bidirectional RNNs, CHAR: character-level embeddings, ENHEMB: embeddings enhanced with auxiliary features, POOL: max or mean pooling over RNN states, ATTN: intra-sentence attention, PRODDIFF: elementwise sentence prod- uct and difference features in the ﬁnal entailment classiﬁer, SNLI: use of the SNLI training set. veys the key differences between systems, and the Embeddings Systems vary reasonably widely Entailment-based Embeddings [Nie and Bansal, 2017] ! Shortcut-stacked RNNs also achieved encoding-based SotA on SNLI corpus Model Accuracy SNLI Multi-NLI Matched Multi-NLI Mismatched CBOW (Williams et al., 2017) 80.6 65.2 64.6 biLSTM Encoder (Williams et al., 2017) 81.5 67.5 67.1 300D Tree-CNN Encoder (Mou et al., 2015) 82.1 – – 300D SPINN-PI Encoder (Bowman et al., 2016) 83.2 – – 300D NSE Encoder (Munkhdalai and Yu, 2016) 84.6 – – biLSTM-Max Encoder (Conneau et al., 2017) 84.5 – – Our biLSTM-Max Encoder 85.2 71.7 71.2 Our Shortcut-Stacked Encoder 86.1 74.6 73.6 Table 5: Final Test Results on SNLI and Multi-NLI datasets. ation with 32 batch size. The starting learning is 0.0002 with half decay every two epochs. number of hidden units for MLP in classiﬁer 600. Dropout layer is also applied on the out- of each layer of MLP, with dropout rate set to to handle a complex task like Multi-NLI and i signiﬁcantly better to have the higher layer c nected to both the output and the original inpu all the previous layers (note that Table 1 results based on multi-layered models with shortcut c Classification Tasks: Sentiment Analysis Sentiment Analysis ! Earlier methods used bag of words, e.g., lexicons of positive and negative words and phrases ! Cannot distinguish tricky cases like: + white blood cells destroying an infection − an infection destroying white blood cells + There are slow and repetitive parts but it has just enough spice to keep it interesting. − Stealing Harvard doesn’t care about cleverness, wit or any other kind of intelligent humor. Sentiment Analysis ! Even simpler issues like negation hard to understand ! Socher et al., 2013b present new compositional training data and new composition model a Sentiment Treebank x Perelygin, Jean Y. Wu, Jason Chuang, ing, Andrew Y. Ng and Christopher Potts versity, Stanford, CA 94305, USA aperelyg,jcchuang,ang}@cs.stanford.edu anning,cgpotts}@stanford.edu use- onger ogress ty in quires on re- com- uce a ained n the – 0 0 This 0 ﬁlm – – – 0 does 0 n’t 0 + care + 0 about + + + + + cleverness 0 , 0 wit 0 or + 0 0 any 0 0 other + kind + 0 of + + intelligent + + humor 0 . Sentiment Analysis ! Even simpler issues like negation hard to understand ! Socher et al., 2013b present new compositional training data and new composition model 1. New Sentiment Treebank Sentiment Analysis ! Sentiment Compositionality: 1. New Sentiment Treebank • Parse trees of 11,855 sentences • 215,154 phrases with labels • Allows training and evaluating with compositional information Sentiment Analysis ! Better Models: Recursive Neural Tensor Network (RNTN) Recursive Neural Tensor Network Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank Socher et al. 2013 Sentiment Analysis ! Better Models: Tree-based LSTM-RNNs Tree LSTMs • We can use those ideas in grammatical tree structures! • Paper: Tai et al. 2015: Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks • Idea: Sum the child vectors in a tree structure • Each child has its own forget gate • Same softmax on h 5/5/16 Richard Socher Lecture 1, Slide 40 Sentiment Compositionality ! Demos: h#p://nlp.stanford.edu:8080/sen7ment/rntnDemo.html [Yessenalina and Cardie, 2011; Socher et al., 2013b] Results on Stanford Sentiment Treebank Method Fine-grained Binary RAE (Socher et al., 2013) 43.2 82.4 MV-RNN (Socher et al., 2013) 44.4 82.9 RNTN (Socher et al., 2013) 45.7 85.4 DCNN (Blunsom et al., 2014) 48.5 86.8 Paragraph-Vec (Le and Mikolov, 2014) 48.7 87.8 CNN-non-static (Kim, 2014) 48.0 87.2 CNN-multichannel (Kim, 2014) 47.4 88.1 DRNN (Irsoy and Cardie, 2014) 49.8 86.6 LSTM 45.8 86.7 Bidirectional LSTM 49.1 86.8 2-layer LSTM 47.5 85.5 2-layer Bidirectional LSTM 46.2 84.8 Constituency Tree LSTM (no tuning) 46.7 86.6 Constituency Tree LSTM 50.6 86.9 Table 2: Test set accuracies on the Stanford Senti- ment Treebank. Fine-grained: 5-class sentiment classiﬁcation. Binary: positive/negative senti- Method Mean vectors DT-RNN (Socher et al., 201 SDT-RNN (Socher et al., 20 Illinois-LH (Lai and Hocken UNAL-NLP (Jimenez et al., Meaning Factory (Bjerva et ECNU (Zhao et al., 2014) LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM Constituency Tree LSTM Dependency Tree LSTM Table 3: Test set re relatedness subtask. Pearson’s r, Spearm ror. Results are grou 5/5/16 Richard Socher Lecture 1, Slide 41 of word vectors Other Classification Tasks ! Sentence similarity ! Entailment classification ! Spam detection ! Document topic classification ! Others: humor, rumor, sarcasm detection, etc. SemEval has great new tasks every year with novel datasets in many cases! Some recent years: http://alt.qcri.org/semeval2017/index.php?id=tasks http://alt.qcri.org/semeval2016/index.php?id=tasks http://alt.qcri.org/semeval2015/index.php?id=tasks "
319,"? Option 1: Understand the problem, design a solution Option 2: Set it up as a machine learning problem data supervised learning Deep Reinforcement Learning, Decision Making, and Control CS 285 Instructor: Sergey Levine UC Berkeley data reinforcement learning What is reinforcement learning? What is reinforcement learning? Mathematical formalism for learning-based decision making Approach for learning decision making and control from experience How is this different from other machine learning topics? Standard (supervised) machine learning: Usually assumes: • i.i.d. data • known ground truth outputs in training Reinforcement learning: • Data is not i.i.d.: previous outputs influence future inputs! • Ground truth answer is not known, only know if we succeeded or failed • more generally, we know the reward decisions (actions) consequences observations rewards Actions: muscle contractions Observations: sight, smell Rewards: food Actions: motor current or torque Observations: camera images Rewards: task success measure (e.g., running speed) Actions: what to purchase Observations: inventory levels Rewards: profit (states) Complex physical tasks… Rajeswaran, et al. 2018 Unexpected solutions… Mnih, et al. 2015 In the real world… Kalashnikov et al. ‘18 In the real world… Kalashnikov et al. ‘18 Not just games and robots! Cathy Wu Why should we care about deep reinforcement learning? How do we build intelligent machines? Intelligent machines must be able to adapt Deep learning helps us handle unstructured environments Reinforcement learning provides a formalism for behavior decisions (actions) consequences observations rewards Mnih et al. ‘13 Schulman et al. ’14 & ‘15 Levine*, Finn*, et al. ‘16 What is deep RL, and why should we care? standard computer vision features (e.g. HOG) mid-level features (e.g. DPM) classifier (e.g. SVM) deep learning Felzenszwalb ‘08 end-to-end training standard reinforcement learning features more features linear policy or value func. deep reinforcement learning end-to-end training ? ? action action What does end-to-end learning mean for sequential decision making? Action (run away) perception action Action (run away) sensorimotor loop Example: robotics robotic control pipeline observations state estimation (e.g. vision) modeling & prediction planning low-level control controls sensorimotor loop tiny, highly specialized “visual cortex” tiny, highly specialized “motor cortex” The reinforcement learning problem is the AI problem! decisions (actions) consequences observations rewards Actions: muscle contractions Observations: sight, smell Rewards: food Actions: motor current or torque Observations: camera images Rewards: task success measure (e.g., running speed) Actions: what to purchase Observations: inventory levels Rewards: profit Deep models are what allow reinforcement learning algorithms to solve complex problems end to end! Why should we study this now? 1. Advances in deep learning 2. Advances in reinforcement learning 3. Advances in computational capability Why should we study this now? L.-J. Lin, “Reinforcement learning for robots using neural networks.” 1993 Tesauro, 1995 Why should we study this now? Atari games: Q-learning: V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, et al. “Playing Atari with Deep Reinforcement Learning”. (2013). Policy gradients: J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. “Trust Region Policy Optimization”. (2015). V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, et al. “Asynchronous methods for deep reinforcement learning”. (2016). Real-world robots: Guided policy search: S. Levine*, C. Finn*, T. Darrell, P. Abbeel. “End-to-end training of deep visuomotor policies”. (2015). Q-learning: D. Kalashnikov et al. “QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation”. (2018). Beating Go champions: Supervised learning + policy gradients + value functions + Monte Carlo tree search: D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, et al. “Mastering the game of Go with deep neural networks and tree search”. Nature (2016). What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward • Basic reinforcement learning deals with maximizing rewards • This is not the only problem that matters for sequential decision making! • We will cover more advanced topics • Learning reward functions from example (inverse reinforcement learning) • Transferring knowledge between domains (transfer learning, meta-learning) • Learning to predict and using prediction to act Where do rewards come from? Are there other forms of supervision? • Learning from demonstrations • Directly copying observed behavior • Inferring rewards from observed behavior (inverse reinforcement learning) • Learning from observing the world • Learning to predict • Unsupervised learning • Learning from other tasks • Transfer learning • Meta-learning: learning to learn Imitation learning Bojarski et al. 2016 More than imitation: inferring intentions Warneken & Tomasello Inverse RL examples Finn et al. 2016 Prediction Ebert et al. 2017 Prediction for real-world control Xie et al. 2019 Using tools with predictive models Playing games with predictive models Kaiser et al. 2019 real predicted But sometimes there are issues… How do we build intelligent machines? How do we build intelligent machines? • Imagine you have to build an intelligent machine, where do you start? Learning as the basis of intelligence • Some things we can all do (e.g. walking) • Some things we can only learn (e.g. driving a car) • We can learn a huge variety of things, including very difficult things • Therefore our learning mechanism(s) are likely powerful enough to do everything we associate with intelligence • But it may still be very convenient to “hard-code” a few really important bits A single algorithm? [BrainPort; Martinez et al; Roe et al.] Seeing with your tongue Auditory Cortex adapted from A. Ng • An algorithm for each “module”? • Or a single flexible algorithm? What must that single algorithm do? • Interpret rich sensory inputs • Choose complex actions Why deep reinforcement learning? • Deep = can process complex sensory input …and also compute really complex functions • Reinforcement learning = can choose complex actions Some evidence in favor of deep learning Some evidence for reinforcement learning • Percepts that anticipate reward become associated with similar firing patterns as the reward itself • Basal ganglia appears to be related to reward system • Model-free RL-like adaptation is often a good fit for experimental data of animal adaptation • But not always… What can deep learning & RL do well now? • Acquire high degree of proficiency in domains governed by simple, known rules • Learn simple skills with raw sensory inputs, given enough experience • Learn from imitating enough human- provided expert behavior What has proven challenging so far? • Humans can learn incredibly quickly • Deep RL methods are usually slow • Humans can reuse past knowledge • Transfer learning in deep RL is an open problem • Not clear what the reward function should be • Not clear what the role of prediction should be Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. - Alan Turing general learning algorithm environment observations actions "
32,"COMP 790.139 (Fall 2017) Natural Language Processing (with deep learning and connections to vision/robotics) Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, Richard Socher, Chris Manning, JurafskyMartin-SLP3, others) Lecture 3: POS-Tagging, NER, Seq Labeling, Coreference Part-of-Speech Tagging Part-of-Speech Tagging ! Basic form of linguistic structure: ‘syntactic word classes’ ! Tag sequence of words w/ syntactic categories (noun, verb, prep, etc.) PartsͲofͲSpeech(English)  Onebasickindoflinguisticstructure:syntacticwordclasses Open class (lexical) words Closed class (functional) Nouns Verbs Proper Common Auxiliary Main Adjectives Adverbs Prepositions Particles Determiners Conjunctions Pronouns more more IBM Italy cat / cats snow see registered can had yellow slowly to with off up the some and or he its Numbers 122,312 one Penn Treebank Tagset CC conjunction, coordinating and both but either or CD numeral, cardinal mid-1890 nine-thirty 0.5 one DT determiner a all an every no that the EX existential there there FW foreign word gemeinschaft hund ich jeux IN preposition or conjunction, subordinating among whether out on by if JJ adjective or numeral, ordinal third ill-mannered regrettable JJR adjective, comparative braver cheaper taller JJS adjective, superlative bravest cheapest tallest MD modal auxiliary can may might will would NN noun, common, singular or mass cabbage thermostat investment subhumanity NNP noun, proper, singular Motown Cougar Yvette Liverpool NNPS noun, proper, plural Americans Materials States NNS noun, common, plural undergraduates bric-a-brac averages POS genitive marker ' 's PRP pronoun, personal hers himself it we them PRP$ pronoun, possessive her his mine my our ours their thy your RB adverb occasionally maddeningly adventurously RBR adverb, comparative further gloomier heavier less-perfectly RBS adverb, superlative best biggest nearest worst RP particle aboard away back by on open through TO ""to"" as preposition or infinitive marker to UH interjection huh howdy uh whammo shucks heck VB verb, base form ask bring fire see take VBD verb, past tense pleaded swiped registered saw VBG verb, present participle or gerund stirring focusing approaching erasing VBN verb, past participle dilapidated imitated reunifed unsettled VBP verb, present tense, not 3rd person singular twist appear comprise mold postpone VBZ verb, present tense, 3rd person singular bases reconstructs marks uses WDT WH-determiner that what whatever which whichever WP WH-pronoun that what whatever which who whom WP$ WH-pronoun, possessive whose WRB Wh-adverb however whenever where why Part-of-Speech Ambiguities ! A word can have multiple parts of speech ! Disambiguating features: lexical identity (word), context, morphology (suffixes, prefixes), capitalization, gazetteers (dictionaries), … PartͲofͲSpeechAmbiguity  Wordscanhavemultiplepartsofspeech  Twobasicsourcesofconstraint:  Grammaticalenvironment  Identityofthecurrentword  Manymorepossiblefeatures:  Suffixes,capitalization,namedatabases(gazetteers),etc… Fed raises interest rates 0.5 percent NNP NNS NN NNS CD NN VBN VBZ VBP VBZ VBD VB Uses of Part-of-Speech Tagging ! Useful in itself: ! Text-to-speech: read, lead, record ! Lemmatization: saw[v] → see, saw[n] → saw ! Shallow Chunking: grep {JJ | NN}* {NN | NNS} ! Useful for downstream tasks (e.g., in parsing, and as features in various word/text classification tasks) ! Preprocessing step in parsing: allows fewer parse options if less tag ambiguity (but some cases still decided by parser) ! Demos: http://nlp.stanford.edu:8080/corenlp/ Classic Solution: HMMs 8 ClassicSolution:HMMs  Wewantamodelofsequencessandobservationsw  Assumptions:  StatesaretagnͲgrams  Usuallyadedicatedstartandendstate/word  Tag/statesequenceisgeneratedbyamarkov model  Wordsarechosenindependently,conditionedonlyonthetag/state  Thesearetotallybrokenassumptions:why? s1 s2 sn w1 w2 wn s0 ! Generative mode with state sequence and emissions at every time step: ! Several strong independence assumptions! ! States = POS tag n-grams ! Next tag only depends on k previous tags ! Word generated only depends on current tag state States ! Markov order defines how many states in the history are being conditioned on, e.g., 1 = bigrams, 2 = trigrams States encodewhatisrelevantaboutthepast tionsP(s|s’)encodewellͲformedtagsequences bigramtagger,states=tags trigramtagger,states=tagpairs <i,i> s1 s2 sn w1 w2 wn s0 < i, t1> < t1, t2> < tn-1, tn> <i> s1 s2 sn w1 w2 wn s0 < t1> < t2> < tn>  Statesencodewhatisrelevantaboutthepast  TransitionsP(s|s’)encodewellͲformedtagsequences  Inabigramtagger,states=tags  Inatrigramtagger,states=tagpairs <i,i> s1 s2 sn w1 w2 wn s0 < i, t1> < t1, t2> < tn-1, tn> <i> s1 s2 sn w1 w2 wn s0 < t1> < t2> < tn> Estimating Transitions EstimatingTransitions  Usestandardsmoothingmethodstoestimatetransitions:  Cangetalotfancier(e.g.KNsmoothing)orusehigherorders,butinthis caseitdoesn’tbuymuch  Oneoption:encodemoreintothestate,e.g.whetherthepreviousword wascapitalized(Brants 00)  BIGIDEA:ThebasicapproachofstateͲsplitting/refinementturnsoutto ) ( ˆ ) 1 ( ) | ( ˆ ) , | ( ˆ ) , | ( 2 1 1 1 2 1 2 2 1 i i i i i i i i i t P t t P t t t P t t t P O O O O          ! For higher order Markov chains, harder to estimate transition probabilities ! Therefore, can use standard language modeling style smoothing techniques like back-off or Kneser-Ney or Good-Turing ! More effective to have richer info encoded in the states themselves, i.e., state splitting/refinement Estimating Emissions EstimatingEmissions  Emissionsaretrickier:  Wordswe’veneverseenbefore  Wordswhichoccurwithtagswe’veneverseenthemwith  Oneoption:breakoutthefancysmoothing(e.g.KN,GoodͲTuring)  Issue:unknownwordsaren’tblackboxes:  Basicsolution:unknownwordsclasses(affixesorshapes)  Commonapproach:EstimateP(t|w)andinvert  [Brants 00]usedasuffixtrie asits(inverted)emissionmodel 343,127.23 11-year Minteria reintroducibly D+,D+.D+ D+-x+ Xx+ x+-“ly” ! Unknown and rare words (also unseen word-state pairs) big problem is estimating emission probabilities! ! Can use word shapes to get unknown word classes, e.g., 45,698.00 ! D+, D+. D+ 30-year ! D+-x+ ! Another trick: estimate P(t|w) instead and then invert! Inference (Viterbi) Disambiguation(Inference)  Problem:findthemostlikely(Viterbi)sequenceunderthemodel  Givenmodelparameters,wecanscoreanytagsequence  Inprinciple,we’redone– listallpossibletagsequences,scoreeachone, pick the best one (the Viterbi state sequence) Fed raises interest rates 0.5 percent . NNP VBZ NN NNS CD NN . P(NNP|<i,i>) P(Fed|NNP) P(VBZ|<NNP,i>) P(raises|VBZ) P(NN|VBZ,NNP) .. <i,i> <i,NNP> <NNP, VBZ> <VBZ, NN> <NN, NNS> <NNS, CD> <CD, NN> <STOP> ! After estimating all transition and emission probabilities, next step is to infer or decode the most-probable sequence of states (e.g., POS tags) given the sequence of observations (e.g., words) Inference (Viterbi) ! Viterbi algo: Recursive dynamic program ! vt(j) cell of trellis represents prob of HMM in state j after first t observations & passing through most-prob state sequence q0 q1 q2… qt-1 Note that we represent the most probable path by taking the maximum over ssible previous state sequences max q0,q1,...,qt−1. Like other dynamic programming rithms, Viterbi ﬁlls each cell recursively. Given that we had already computed bability of being in every state at time t −1, we compute the Viterbi probabi taking the most probable of the extensions of the paths that lead to the curr l. For a given state qj at time t, the value vt(j) is computed as vt(j) = N max i=1 vt−1(i) aij bj(ot) (9 The three factors that are multiplied in Eq. 9.19 for extending the previous pa compute the Viterbi probability at time t are vt−1(i) the previous Viterbi path probability from the previous time st aij the transition probability from previous state qi to current state bj(ot) the state observation likelihood of the observation symbol ot gi the current state j APTER 9 • HIDDEN MARKOV MODELS Note that we represent the most probable path by taking the maximum over all possible previous state sequences max q0,q1,...,qt−1. Like other dynamic programming al- gorithms, Viterbi ﬁlls each cell recursively. Given that we had already computed the probability of being in every state at time t −1, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state qj at time t, the value vt(j) is computed as vt(j) = N max i=1 vt−1(i) aij bj(ot) (9.19) The three factors that are multiplied in Eq. 9.19 for extending the previous paths to compute the Viterbi probability at time t are vt−1(i) the previous Viterbi path probability from the previous time step aij the transition probability from previous state qi to current state qj bj(ot) the state observation likelihood of the observation symbol ot given the current state j function VITERBI(observations of len T, state-graph of len N) returns best-path create a path probability matrix viterbi[N+2 T] [JurafskyMartin-SLP3] Inference (Viterbi) aij the transition probability from previous state qi to current state qj bj(ot) the state observation likelihood of the observation symbol ot given the current state j function VITERBI(observations of len T, state-graph of len N) returns best-path create a path probability matrix viterbi[N+2,T] for each state s from 1 to N do ; initialization step viterbi[s,1] a0,s ⇤bs(o1) backpointer[s,1] 0 for each time step t from 2 to T do ; recursion step for each state s from 1 to N do viterbi[s,t] N max s0=1 viterbi[s0,t −1] ⇤as0,s ⇤bs(ot) backpointer[s,t] N argmax s0=1 viterbi[s0,t −1] ⇤as0,s viterbi[qF,T] N max s=1 viterbi[s,T] ⇤as,qF ; termination step backpointer[qF,T] N argmax s=1 viterbi[s,T] ⇤as,qF ; termination step return the backtrace path by following backpointers to states back in time from backpointer[qF,T] Figure 9.11 Viterbi algorithm for ﬁnding optimal sequence of hidden states. Given an observation sequence and an HMM l = (A,B), the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence Note that states 0 [JurafskyMartin-SLP3] State Lattice Traversal h#ps://en.wikipedia.org/wiki/Viterbi_algorithm Instead, the most common decoding algorithms for HMMs is the Viterbi algo rithm. Like the forward algorithm, Viterbi is a kind of dynamic programming Viterbi algorithm that makes uses of a dynamic programming trellis. Viterbi also strongly resembles another dynamic programming variant, the minimum edit distance algorithm of Chapter 3. start H C H C H C end P(C|start) * P(3|C) .2 * .1 P(H|H) * P(1|H) .6 * .2 P(C|C) * P(1|C) .5 * .5 P(C|H) * P(1|C) .3 * .5 P(H|C) * P(1|H) .4 * .2 P(H|start)*P(3|H) .8 * .4 v1(2)=.32 v1(1) = .02 v2(2)= max(.32*.12, .02*.08) = .038 v2(1) = max(.32*.15, .02*.25) = .048 start start start t C H end end end qF q2 q1 q0 o1 o2 o3 3 1 3 Figure 9.10 The Viterbi trellis for computing the best path through the hidden state space for the ice-cream eating events 3 1 3. Hidden states are in circles, observations in squares. White (unﬁlled) circles indicate illegal transitions. The ﬁgure shows the computation of vt(j) for two states at two time steps. The computation in each [JurafskyMartin-SLP3] Forward-Backward EM Algo for HMM Training [JurafskyMartin-SLP3] P(O|l) = aT(qF) = b1(q0) = N X j=1 a0j bj(o1) b1(j) (9.30) Figure 9.13 illustrates the backward induction step. ot+1 ot ai1 ai2 aiN ai3 b1(ot+1) βt(i)= Σj βt+1(j) aij bj(ot+1) q1 q2 q3 qN q1 qi q2 q1 q2 ot-1 q3 qN βt+1(N) βt+1(3) βt+1(2) βt+1(1) b2(ot+1) b3(ot+1) bN(ot+1) Figure 9.13 The computation of bt(i) by summing all the successive values bt+1(j) weighted by their transition probabilities aij and their observation probabilities bj(ot+1). Start and end states not shown. We are now ready to understand how the forward and backward probabilities can help us compute the transition probability aij and observation probability bi(ot) from an observation sequence, even though the actual path taken through the machine is hidden. Let’s begin by seeing how to estimate ˆ aij by a variant of Eq. 9.26: ˆ aij = expected number of transitions from state i to state j expected number of transitions from state i (9.31) How do we compute the numerator? Here’s the intuition. Assume we had some estimate of the probability that a given transition i ! j was taken at a particular point 16 CHAPTER 9 • HIDDEN MARKOV MODELS xt(i, j) = P(qt = i,qt+1 = j|O,l) (9.32) To compute xt, we ﬁrst compute a probability which is similar to xt, but differs in including the probability of the observation; note the different conditioning of O from Eq. 9.32: not-quite-xt(i, j) = P(qt = i,qt+1 = j,O|l) (9.33) ot+2 ot+1 αt(i) ot-1 ot aijbj(ot+1) si sj βt+1(j) Figure 9.14 Computation of the joint probability of being in state i at time t and state j at Overview of Accuracies Overview:Accuracies  Roadmapof(known/unknown)accuracies:  Mostfreq tag: ~90%/~50%  TrigramHMM: ~95%/~55%  TnT (HMM++): 96.2%/86.0%  Maxent P(t|w): 93.7%/82.6%  MEMMtagger: 96.9%/86.9%  StateͲofͲtheͲart: 97+%/89+%  Upperbound: ~98% Most errors on unknown words ! Known/Unknown POS-tag accuracy history: Better Discriminative Features? ! Need richer features (both inside the word and around it)! ! Word-based feature examples: ! Suffixes (e.g., -ly, -ing, -ed) ! Prefixes (e.g., un-, im-, dis-) ! Capital vs lower-cased ! Just a simple maxent tag-given-word P(t|w) feature-based model itself gets 93.7%/82.6% known/unknown POS- tagging accuracy! Better Discriminative Features? WhyLinearContextisUseful Lotsofrichlocalinformation!  Wecouldfixthiswithafeaturethatlookedatthenextword  Wecouldfixthisbylinkingcapitalizedwordstotheirlowercaseversions Solution:discriminativesequencemodels(MEMMs,CRFs) PRP VBD IN RB IN PRP VBD . They left as soon as he arrived . NNP NNS VBD VBN . Intrinsic flaws remained undetected . RB JJ ! Similarly, we also need linear context features, e.g., words to the right of the currently-predicted tag ! Solution: Discriminative sequence models such as CRFs and MEMMs that can incorporate such full- sentence features! MaxEnt Markov Model (MEMM) Tagger ! Sequence model adaptation of MaxEnt (multinomial logistic regression) classifier ! MEMM = discriminative, HMM = generative ! Left-to-right local decisions, but can condition of both previous tags as well as entire input [Ratnaparkhi, 1996] MEMMTaggers Idea:leftͲtoͲrightlocaldecisions,conditiononprevioustag andalsoentireinput  TrainupP(ti|w,tiͲ1,tiͲ2)asanormalmaxentmodel,thenusetoscor sequences  ThisisreferredtoasanMEMMtagger[Ratnaparkhi96] MaxEnt Markov Model (MEMM) Tagger ! Difference between HMM and MEMM: q y tive sequence model. generative Let the sequence of words be W = wn 1 and the sequence of ta HMM to compute the best tag sequence that maximizes P(T|W) w rule and the likelihood P(W|T): ˆ T = argmax T P(T|W) = argmax T P(W|T)P(T) = argmax T Y i P(wordi|tagi) Y i P(tagi|tagi−1) In an MEMM, by contrast, we compute the posterior P(T|W) it to discriminate among the possible tag sequences: ˆ T = argmax T P(T|W) = argmax T Y i P(ti|wi,ti−1) We could do this by training a logistic regression classiﬁer to co probability P(ti|wi,ti−1). Fig. 10.11 shows the intuition of the d direction of the arrows; HMMs compute likelihood (observation w on tags) but MEMMs compute posterior (tags conditioned on obse mbining all these features, a state-of-the-art trigram HMM like that of Brants has a tagging accuracy of 96.7% on the Penn Treebank. mum Entropy Markov Models rn now to a second sequence model, the maximum entropy Markov model EMM. The MEMM is a sequence model adaptation of the MaxEnt (multino- ogistic regression) classiﬁer. Because it is based on logistic regression, the M is a discriminative sequence model. By contrast, the HMM is a genera- quence model. t the sequence of words be W = wn 1 and the sequence of tags T = tn 1. In an to compute the best tag sequence that maximizes P(T|W) we rely on Bayes’ nd the likelihood P(W|T): ˆ T = argmax T P(T|W) = argmax T P(W|T)P(T) = argmax T Y i P(wordi|tagi) Y i P(tagi|tagi−1) (10.26) an MEMM, by contrast, we compute the posterior P(T|W) directly, training scriminate among the possible tag sequences: ˆ T = argmaxP(T|W) 10.5 • MAXIMUM ENTROPY MARKOV MODELS 17 will MD VB DT NN Janet back the bill NNP will MD VB DT NN Janet back the bill NNP 10.11 A schematic view of the HMM (top) and MEMM (bottom) representation of ability computation for the correct sequence of tags for the back sentence. The HMM es the likelihood of the observation given the hidden state, while the MEMM computes erior of each state, conditioned on the previous state and current observation. Features in a MEMM We lied in Eq. 10.27. We actually don’t build MEMMs that condition just on ti 1 In fact an MEMM conditioned on just these two features (the observed 10.5 • MAXIMUM ENTROPY MARKOV MOD will MD VB DT NN Janet back the bill NNP will MD VB DT NN Janet back the bill NNP Figure 10.11 A schematic view of the HMM (top) and MEMM (bottom) repre the probability computation for the correct sequence of tags for the back sentence. computes the likelihood of the observation given the hidden state, while the MEMM the posterior of each state, conditioned on the previous state and current observati 10.5.1 Features in a MEMM Oops. We lied in Eq. 10.27. We actually don’t build MEMMs that condit wi and ti−1. In fact, an MEMM conditioned on just these two features (th word and the previous tag), as shown in Fig. 10.11 and Eq. 10.27 is no mor than the generative HMM model and in fact may be less accurate. The reason to use a discriminative sequence model is that discriminati make it easier to incorporate a much wider variety of features. Because all computation is based on the two probabilities P(tag|tag) and P(word| HMM MEMM [JurafskyMartin-SLP3] MEMM Features ! MEMM can condition on several richer features, e.g., from words in entire input sentence ! Word shapes, tag-word n-gram templates, etc. [JurafskyMartin-SLP3] g gg g p a way to encode the knowledge into one of these two probabilities. We saw in the previous section that it was possible to model capitalization or word endings by cleverly ﬁtting in probabilities like P(capitalization|tag), P(sufﬁx|tag), and so on into an HMM-style model. But each time we add a feature we have to do a lot of complicated conditioning which gets harder and harder as we have more and more such features and, as we’ll see, there are lots more features we can add. Figure 10.12 shows a graphical intuition of some of these additional features. will MD VB Janet back the bill NNP <s> wi wi+1 wi-1 ti-1 ti-2 wi-1 Figure 10.12 An MEMM for part-of-speech tagging showing the ability to condition on more features. A basic MEMM part-of-speech tagger conditions on the observation word it- self, neighboring words, and previous tags, and various combinations, using feature templates like the following: es hti wi 2i hti wi 1i hti wii hti wi+1i hti wi+2i Perceptron Tagger PerceptronTaggers  Linearmodels:  …thatdecomposealongthesequence  …allowustopredictwiththeViterbialgorithm  …whichmeanswecantrainwiththeperceptronalgorithm (orrelatedupdates,likeMIRA) [Collins PerceptronTaggers models: decomposealongthesequence wustopredictwiththeViterbialgorithm chmeanswecantrainwiththeperceptronalgorithm atedupdates,likeMIRA) [Collins 01] ! For log-linear models, score of tags-given-words has the formulation of: ! This can be decomposed into sum of features: ! Hence, we can use perceptron or MIRA style algorithms to train these models and learn the feature weights! Perceptron Training Algorithm [Collins 2001] Conditional Random Field (CRF) Tagger ConditionalRandomFields  Makeamaxentmodeloverentiretaggings  MEMM  CRF CRF Training CRFs  Likeanymaxent model,derivativeis:  Soallweneedistobeabletocomputetheexpectationofeachfeature (forexamplethenumberoftimesthelabelpairDTͲNNoccurs,orthe numberoftimesNNͲinterestoccurs)underthemodeldistribution  Criticalquantity:countsofposteriormarginals: CRFs  Likeanymaxent model,derivativeis:  Soallweneedistobeabletocomputetheexpectationofeachfeature (forexamplethenumberoftimesthelabelpairDTͲNNoccurs,orthe numberoftimesNNͲinterestoccurs)underthemodeldistribution  Criticalquantity:countsofposteriormarginals: ! Derivatives needed have the form of “feature counts minus expected feature counts”: ! These expected feature counts (under model distribution) in turn need posterior marginals: Posterior Marginals ComputingPosteriorMarginals  Howmany(expected)timesiswordwtaggedwiths?  Howtocomputethatmarginal? ^ N V J D $ ^ N V J D $ ^ N V J D $ ^ N V J D $ ^ N V J D $ ^ N V J D $ START Fed raises interest rates END ! And these posterior marginals in turn need the state trellis traversal similar to forward-backward discussed for HMM training: POS Tagging: Other Models ! Universal POS tagset for multilingual and cross-lingual tagging and parsing [Petrov et al., 2012] 12 tags: NOUN, VERB, ADJ, ADV, PRON, DET, ADP, NUM, CONJ, PRT, ., X ! Unsupervised tagging also works reasonably well! [Yarowsky et al., 2001; Xi and Hwa, 2005; Berg-Kirkpatrick et al., 2010; Christodoulopoulos et al., 2010; Das and Petrov, 2011] RNN-based POS-Tagger ! Context captured by bidirectional LSTM; softmax on tag labels [Ling et al., 2015 (and others)] under e left right .This better wn to- ity to uman r new ing model, y is a projecting the combined state li. cats ﬁsh eat ........ ........ ........ NNS VBP NN Bi-LSTM Word Lookup or Lexical Composition Model Softmax over Labels embedings for words embedings for words in context Fi 3 Ill i f l k f Char-RNN-based POS-Tagger ! Use character-based RNNs to compose word embeddings (to learn function) [Ling et al., 2015 (and others)] phding hire mixing modelling styling rg blaming ire christening ular words under words on the left hose on the right once words1.This gniﬁcantly better use unknown to- re, this ability to to that of human meanings for new eech Tagging lity of our model, morphology is a l The size of the forward sf and backward states sb and the combined state l are hyperparameters of the model, denoted as df WS, db WS and dWS, re- spectively. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state li. cats ﬁsh eat ........ ........ ........ NNS VBP NN Bi-LSTM Word Lookup or Lexical Composition Model Softmax over Labels embedings for words embedings for words in context Figure 3: Illustration of our neural network for Char-RNN-based POS-Tagger ! Use character-based RNNs to compose word embeddings (to learn function) [Ling et al., 2015 (and others)] m have relatively reg- th lexical similarities uch as, the word pairs e ( ) course. er to word (C2W) onal LSTMs (Graves which are able to endencies in sequence own in Figure 1. The ustrated on bottom) is we wish to obtain is to represent w. This and output of a word p), allowing it to eas- rk. lphabet of characters ary would contain an d lowercase letter as ation. The input word quence of characters ength of w. Each ci r 1ci, with one on the We deﬁne a projec cats cat job .... ........ cats c a t s a c t .... .... s Character Lookup Table ........ Table Bi-LSTM embeddings for word ""cats"" embeddings for word ""cats"" Other Sequence Labeling Tasks ! Named Entity Recognition ! Spelling Correction ! Word Alignment ! Noun Phrase Chunking ! Supersense Tagging ! Multiword Expressions Named Entity Recognition ! Label proper nouns as person, location, organization, other ! Also prefers rich contextual features ! CRF models perform strongly for this ! Neural+CRF versions even stronger ! [Bikel et al., 1999] Feature Rich Sequence Models  Problem:HMMsmakeithardtoworkwitharbitraryfeatures ofasentence  Example:nameentityrecognition(NER) Prev Cur Next State Other ??? ??? Word at Grace Road Tag IN NNP NNP Sig x Xx Xx Local Context Tim Boon has signed a contract extension with Leicestershire which will keep him at Grace Road . PER PER O O O O O O ORG O O O O O LOC LOC O [Lample et al., 2016] where A is a matrix of transition scores such that Ai,j represents the score of a transition from the tag i to tag j. y0 and yn are the start and end tags of a sentence, that we add to the set of possi- ble tags. A is therefore a square matrix of size k+2. A softmax over all possible tag sequences yields a probability for the sequence y: p(y|X) = es(X,y) P e y2YX es(X,e y) . During training, we maximize the log-probability of the correct tag sequence: l ( ( |X)) (X ) l 0 X s(X e y) 1 Figure 1: Main architecture of the network. Word embeddings are given to a bidirectional LSTM. li represents the word i and its left context, ri represents the word i and its right context. Fine-Grained NER [Gillick et al., 2014] PERSON artist actor author director music education student teacher athlete business coach doctor legal military political ﬁgure religious leader title LOCATION structure airport government hospital hotel restaurant sports facility theatre geography body of water island mountain transit bridge railway road celestial city country park ORGANIZATION company broadcast news education government military music political party sports league sports team stock exchange transit OTHER art broadcast ﬁlm music stage writing event accident election holiday natural disaster protest sports event violent conﬂict health malady treatment award body part currency language programming language living thing animal product camera car computer mobile phone software weapon food heritage internet legal religion scientiﬁc sports & leisure supernatural Figure 1: Our type taxonomy includes types at three levels, e.g. PERSON (level 1), artist (level 2), actor (level 3). Each assigned type (such as artist) also implies the more general ancestor types (such as PERSON). The top level types were chosen to align with the most common type set used in traditional entity tagging systems. Fine-Grained NER [Ling and Weld, 2012] l language baselines, accuracy n entities, d tags can on extrac- d its data se and ex- ystem, in- ly-labeled nt our ex- ude with a on e problem Fi 2 112 t d i FIGER Th b ld f d t i Coreference Resolution Coreference Resolution ! Mentions to entity/event clusters ! Demos: h#p://nlp.stanford.edu:8080/corenlp/process President Barack Obama received the Serve America Act after congress’ vote. He signed the bill last Thursday. The president said it would greatly increase service opportunities for the American people. President Barack Obama received the Serve America Act after congress’ vote . He signed the bill … Mention-pair Models m a3 (a1, m) Pair-wise classifier coref(a1, m) Features f [Soon et al. 2001, Ng and Cardie 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010] wTf a2 a1 A(m) Pair-wise classification approach: Mention-pair Model For each mention m, m [Soon et al. 2001, Ng and Cardie 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010] Standard features Type Feature Description LEXICAL SOON_STR Do the strings match after removing determiners ? GRAMMATICAL NUMBER Do NPi and NPj agree in number ? GENDER Do NPi and NPj agree in gender ? APPOSITIVE Are the NPs in an appositive relationship ? SEMANTIC WORDNET_CLASS Do NPi and NPj have the same WordNet class ? ALIAS Is one NP an alias of the other ? POSITIONAL SENTNUM Distance between the NPs in terms of # of sentences NPi NPj ! Weaknesses: All pairs, Transitivity/Independence errors (He – Obama – She), Insufficient information [Soon et al. 2001, Ng and Cardie 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010] Entity-centric Models ! Each coreference decision is globally informed by previously clustered mentions and their shared attributes Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precis Figure 1 The architecture of our coreference system. [Haghighi and Klein, 2009; Lee et al., 2013; Durrett et al., 2013] ! Lee et al., 2013’s deterministic (rule-based) system: multiple, cautious sieves from high to low precision ! Durrett et al., 2013’s entity-level model is discriminative, probabilistic using factor graphs and BP Mention-Ranking Models (Learned) ! Log-linear model to select at most 1 antecedent for each mention or determine that it begins a new cluster ! Recent work (Wiseman et al., 2016, Clark & Manning, 2016) has used NNs for non-linear and vector-space coreference features to achieve SoA! [Denis and Baldridge, 2008; Durrett and Klein, 2013] 1 2 New 1 New Men9on\Ranking%Architecture [Voters]1%agree%when%[they]1%are%given%[a%chance]2%to%decide%if%[they]1%...% 1 2 New New 3 Denis%and%Baldridge%(2008),%Durre4%et%al.%(2013) [1STWORD=a] [LENGTH=2] ... [Voters;they] [NOM\PRONOUN] ... A1 A2 A3 A4 Pr(Ai = a|x) / exp(w>f(i, a, x)) Adding Knowledge to Coref ! External corpora: Web, Wikipedia, YAGO, FrameNet, Gender/ Number/Person lists/classifiers, 3D Images, Videos ! Methods: ! Self-training, Bootstrapping ! Co-occurrence, Distributional, and Pattern-based Features ! Entity Linking ! Visual Cues from 3D Images and Videos ! Daumé III and Marcu, 2005; Markert and Nissim, 2005; Bergsma and Lin, 2006; Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Kobdani et al., 2011; Rahman and Ng, 2011; Bansal and Klein, 2012; Durrett and Klein, 2014; Kong et al., 2014; Ramanathan et al., 2014 Web Features for Coreference When Obama met Jobs , the president discussed the … count(Obama * president) vs count(Jobs * president) [Bansal and Klein, 2012] Web Features for Coreference [Bansal and Klein, 2012] When Obama met Jobs , the … He signed bills that … count(Obama signed bills) vs count(Jobs signed bills) Results 69.5 69.8 70.0 70.4 70.7 71.3 68 69 70 71 72 MUC F1 Baseline! +Coocc! +Hearst! +Entity! +Cluster! +Pronoun! Setup: Standard train/dev/test splits on ACE 2004, 2005 [Bansal and Klein, ACL 2012] Results 67.0 69.1 64 68 72 Haghighi & Klein, 2010 Us MUC F1 Setup: Standard train/dev/test splits on ACE 2004, 2005 [Bansal and Klein, ACL 2012] Results 77.0 80.0 74 78 82 Haghighi & Klein, 2010 Us B3 F1 Setup: Standard train/dev/test splits on ACE 2004, 2005 [Bansal and Klein, ACL 2012] Visual Cues for Coreference [Kong, Lin, Bansal, Urtasun, and Fidler, 2014] What are you talking about? Text-to-Image Coreference 1 Dahua Lin3 Mohit Bansal3 Raquel Urtasun2,3 Sanja Fidler2,3 1Tsinghua University, 2University of Toronto, 3TTI Chicago tsinghua.edu.cn, {dhlin,mbansal}@ttic.edu,{fidler,urtasun}@cs.toronto.edu Abstract xploit natural sentential descriptions rder to improve 3D semantic parsing. so, we reason about which particular noun is referring to in the image. This isual information in order to disam- coreference resolution problem that rds this goal, we propose a structure exploits potentials computed from text o reason about the class of the 3D ob- as well as to align the nouns/pronouns al objects. We demonstrate the effec- ach on the challenging NYU-RGBD v2 rich with natural lingual descriptions. proach signiﬁcantly improves 3D de- ssiﬁcation accuracy, and is able to re- xt-to-image alignment. Furthermore, visual information, we are also able to h coreference in text, improving upon anford coreference system [15]. o where you wake up late on a Satur- you want is for your personal robot to oody mary. You could say “It is in the Figure 1. Our model uses lingual descriptions (a string of depen- dent sentences) to improve visual scene parsing as well as to de- termine which visual objects the text is referring to. We also deal with coreference within text (e.g., pronouns like “it” or “them”). system is key for the deployment of such systems. To date, however, attempts to utilize more complex natural descrip- tions are rare. This is due to the inherent difﬁculties of both t max sent min words max words 10 6 144 s # scene mentioned scene correct 0.48 83% tics per description. 0.0 0.0 .02 0.0 0.0 0.0 .04 0.0 .12 0.0 0.0 .15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 .05 0.0 0.0 .01 .02 0.0 0.0 .02 0.0 0.0 .01 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 .95 0.0 .05 0.0 0.0 0.0 0.0 0.0 .06 0.0 0.0 .12 0.0 0.0 0.0 0.0 .33 0.0 0.0 0.0 0.0 0.0 0.0 0.0 .38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 precision recall F-measure object class 94.7% 94.2% 94.4% scene 85.7% 85.7% 85.7% color 64.2% 93.0% 75.9% size 55.8% 96.0% 70.6% Table 3. Parser accuracy (based on Stanford’s parser [31]) MUC B3 Method precision recall F1 precision recall F1 Stanford [15] 61.56 62.59 62.07 75.05 76.15 75.59 Ours 83.69 51.08 63.44 88.42 70.02 78.15 Table 4. Co-reference accuracy of [15] and our model. tributes for the linked pronouns as well Our annotation ! Joint coreference and 3D image recognition Neural Models for Coreference ! Mention-pair model as simple feed-forward network: [Clark and Manning, 2016; Wiseman et al., 2015] Neural Mention-Pair Model Standard feed-forward neural network • From (Clark and Manning, 2016); similar to Wiseman et al. (2015) • Input layer: word embeddings and a few categorical features Candidate Antecedent Embeddings Candidate Antecedent Features Mention Features Mention Embeddings Hidden Layer h2 Input Layer h0 Hidden Layer h1 ReLU(W1h0 + b1) ReLU(W2h1 + b2) ReLU(W3h2 + b3) Additional Features Hidden Layer h3 Score s W4h3 + b4 "
320,"Supervised Learning of Behaviors CS 285 Instructor: Sergey Levine UC Berkeley 1. run away 2. ignore 3. pet Terminology & notation 1. run away 2. ignore 3. pet Terminology & notation Aside: notation Richard Bellman Lev Pontryagin управление Images: Bojarski et al. ‘16, NVIDIA training data supervised learning Imitation Learning behavioral cloning The original deep imitation learning system ALVINN: Autonomous Land Vehicle In a Neural Network 1989 Does it work? No!timestate (s) Does it work? Yes! Video: Bojarski et al. ‘16, NVIDIA Why did that work? Bojarski et al. ‘16, NVIDIA Can we make it work more often?timestate (x) cost stability (more on this later) Can we make it work more often? Can we make it work more often? DAgger: Dataset Aggregation Ross et al. ‘11 DAgger Example Ross et al. ‘11 What’s the problem? Ross et al. ‘11 Deep imitation learning in practice Can we make it work without more data? • DAgger addresses the problem of distributional “drift” • What if our model is so good that it doesn’t drift? • Need to mimic expert behavior very accurately • But don’t overfit! Why might we fail to fit the expert? 1. Non-Markovian behavior 2. Multimodal behavior behavior depends only on current observation If we see the same thing twice, we do the same thing twice, regardless of what happened before Often very unnatural for human demonstrators behavior depends on all past observations How can we use the whole history? variable number of frames, too many weights How can we use the whole history? RNN state RNN state RNN state shared weights Typically, LSTM cells work better here Aside: why might this work poorly? “causal confusion” see: de Haan et al., “Causal Confusion in Imitation Learning” Question 1: Does including history mitigate causal confusion? Question 2: Can DAgger mitigate causal confusion? Why might we fail to fit the expert? 1. Non-Markovian behavior 2. Multimodal behavior 1. Output mixture of Gaussians 2. Latent variable models 3. Autoregressive discretization Why might we fail to fit the expert? 1. Output mixture of Gaussians 2. Latent variable models 3. Autoregressive discretization Why might we fail to fit the expert? 1. Output mixture of Gaussians 2. Latent variable models 3. Autoregressive discretization Look up some of these: • Conditional variational autoencoder • Normalizing flow/realNVP • Stein variational gradient descent Why might we fail to fit the expert? 1. Output mixture of Gaussians 2. Latent variable models 3. Autoregressive discretization (discretized) distribution over dimension 1 only discrete sampling discrete sampling dim 1 value dim 2 value Imitation learning: recap • Often (but not always) insufficient by itself • Distribution mismatch problem • Sometimes works well • Hacks (e.g. left/right images) • Samples from a stable trajectory distribution • Add more on-policy data, e.g. using Dagger • Better models that fit more accurately training data supervised learning A case study: trail following from human demonstration data Case study 1: trail following as classification Cost functions, reward functions, and a bit of theory Imitation learning: what’s the problem? • Humans need to provide data, which is typically finite • Deep learning works best when data is plentiful • Humans are not good at providing some kinds of actions • Humans can learn autonomously; can our machines do the same? • Unlimited data from own experience • Continuous self-improvement 1. run away 2. ignore 3. pet Terminology & notation Aside: notation Richard Bellman Lev Pontryagin Cost functions, reward functions, and a bit of theory A cost function for imitation? training data supervised learning Ross et al. ‘11 Some analysis More general analysis For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning” More general analysis For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning” Another way to imitate Another imitation idea Goal-conditioned behavioral cloning 1. Collect data 2. Train goal conditioned policy 3. Reach goals Going beyond just imitation? Start with a random policy Collect data with random goals Treat this data as “demonstrations” for the goals that were reached Use this to improve the policy Repeat "
321,"PyTorch and Neural Nets CS285 Deep RL Instructor: Marwa Abdulhai [Adapted from Kevin Li’s CS285 Fa21 Slides] Goal of this course Train an agent to perform useful tasks train the model data agent collect data 𝜋!(𝑎|𝑠) 𝑄!(𝑠, 𝑎) ) ∆""#$= 𝑓 !(𝑠"", 𝑎"") Goal of this course Train an agent to perform useful tasks train the model data agent collect data 𝜋!(𝑎|𝑠) 𝑄!(𝑠, 𝑎) ) ∆""#$= 𝑓 !(𝑠"", 𝑎"") 𝜋!(𝑎|𝑠) 𝑄!(𝑠, 𝑎) ) ∆""#$= 𝑓 !(𝑠"", 𝑎"") train the model focus for today’s lecture! How do train a model? 𝜃∗= arg min"" ∑#,% ∈' ℒ( 𝑓 "" 𝑥, 𝑦) dataset neural network loss gradient descent PyTorch does all of these! What is PyTorch? Python library for: • Defining neural networks • Automating computing gradients • And more! (datasets, optimizers, GPUs, etc.) How does PyTorch work? [picture from Stanford’s CS231n] You define: ℎ! = 𝜎𝑊 !𝑥 ℎ"" = 𝜎𝑊 ""ℎ! 𝑦= 𝜎𝑊 #ℎ"" PyTorch computes: $% $& ! = $% $'"" $'"" $'! $'! $& ! $% $& "" = $% $'"" $'"" $& ! 𝜕𝑦 𝜕𝑊 # PyTorch Tutorial (Colab) https://colab.research.google.com/drive/1XQu1mUbGtvkQY-D7_YCOZlRzSnjp4u9f?usp=sharing https://bit.ly/3CM6lcf "
322,"Introduction to Reinforcement Learning CS 285 Instructor: Sergey Levine UC Berkeley Definitions 1. run away 2. ignore 3. pet Terminology & notation Images: Bojarski et al. ‘16, NVIDIA training data supervised learning Imitation Learning Reward functions Definitions Andrey Markov Definitions Andrey Markov Richard Bellman Definitions Richard Bellman Definitions The goal of reinforcement learning we’ll come back to partially observed later The goal of reinforcement learning The goal of reinforcement learning Finite horizon case: state-action marginal state-action marginal Infinite horizon case: stationary distribution stationary distribution stationary = the same before and after transition Infinite horizon case: stationary distribution stationary distribution stationary = the same before and after transition Expectations and stochastic systems infinite horizon case finite horizon case In RL, we almost always care about expectations +1 -1 Algorithms The anatomy of a reinforcement learning algorithm generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy A simple example generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Another example: RL by backprop generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Which parts are expensive? generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy real robot/car/power grid/whatever: 1x real time, until we invent time travel MuJoCo simulator: up to 10000x real time trivial, fast expensive Value Functions How do we deal with all these expectations? what if we knew this part? Definition: Q-function Definition: value function Using Q-functions and value functions The anatomy of a reinforcement learning algorithm generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy this often uses Q- functions or value functions Types of Algorithms Types of RL algorithms • Policy gradients: directly differentiate the above objective • Value-based: estimate value function or Q-function of the optimal policy (no explicit policy) • Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy • Model-based RL: estimate the transition model, and then… • Use it for planning (no explicit policy) • Use it to improve a policy • Something else Model-based RL algorithms generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Model-based RL algorithms improve the policy 1. Just use the model to plan (no policy) • Trajectory optimization/optimal control (primarily in continuous spaces) – essentially backpropagation to optimize over actions • Discrete planning in discrete action spaces – e.g., Monte Carlo tree search 2. Backpropagate gradients into the policy • Requires some tricks to make it work 3. Use the model to learn a value function • Dynamic programming • Generate simulated experience for model-free learner Value function based algorithms generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Direct policy gradients generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Actor-critic: value functions + policy gradients generate samples (i.e. run the policy) fit a model/ estimate the return improve the policy Tradeoffs Between Algorithms Why so many RL algorithms? • Different tradeoffs • Sample efficiency • Stability & ease of use • Different assumptions • Stochastic or deterministic? • Continuous or discrete? • Episodic or infinite horizon? • Different things are easy or hard in different settings • Easier to represent the policy? • Easier to represent the model? generate samples (i.e. run the policy) fit a model/ estimate return improve the policy Comparison: sample efficiency • Sample efficiency = how many samples do we need to get a good policy? • Most important question: is the algorithm off policy? • Off policy: able to improve the policy without generating new samples from that policy • On policy: each time the policy is changed, even a little bit, we need to generate new samples generate samples (i.e. run the policy) fit a model/ estimate return improve the policy just one gradient step Comparison: sample efficiency More efficient (fewer samples) Less efficient (more samples) on-policy off-policy Why would we use a less efficient algorithm? Wall clock time is not the same as efficiency! evolutionary or gradient-free algorithms on-policy policy gradient algorithms actor-critic style methods off-policy Q-function learning model-based deep RL model-based shallow RL Comparison: stability and ease of use Why is any of this even a question??? • Does it converge? • And if it converges, to what? • And does it converge every time? • Supervised learning: almost always gradient descent • Reinforcement learning: often not gradient descent • Q-learning: fixed point iteration • Model-based RL: model is not optimized for expected reward • Policy gradient: is gradient descent, but also often the least efficient! Comparison: stability and ease of use • Value function fitting • At best, minimizes error of fit (“Bellman error”) • Not the same as expected reward • At worst, doesn’t optimize anything • Many popular deep RL value fitting algorithms are not guaranteed to converge to anything in the nonlinear case • Model-based RL • Model minimizes error of fit • This will converge • No guarantee that better model = better policy • Policy gradient • The only one that actually performs gradient descent (ascent) on the true objective Comparison: assumptions • Common assumption #1: full observability • Generally assumed by value function fitting methods • Can be mitigated by adding recurrence • Common assumption #2: episodic learning • Often assumed by pure policy gradient methods • Assumed by some model-based RL methods • Common assumption #3: continuity or smoothness • Assumed by some continuous value function learning methods • Often assumed by some model-based RL methods Examples of Algorithms Examples of specific algorithms • Value function fitting methods • Q-learning, DQN • Temporal difference learning • Fitted value iteration • Policy gradient methods • REINFORCE • Natural policy gradient • Trust region policy optimization • Actor-critic algorithms • Asynchronous advantage actor-critic (A3C) • Soft actor-critic (SAC) • Model-based RL algorithms • Dyna • Guided policy search We’ll learn about most of these in the next few weeks! Example 1: Atari games with Q-functions • Playing Atari with deep reinforcement learning, Mnih et al. ‘13 • Q-learning with convolutional neural networks Example 2: robots and model-based RL • End-to-end training of deep visuomotor policies, L.* , Finn* ’16 • Guided policy search (model-based RL) for image-based robotic manipulation Example 3: walking with policy gradients • High-dimensional continuous control with generalized advantage estimation, Schulman et al. ‘16 • Trust region policy optimization with value function approximation Example 4: robotic grasping with Q-functions • QT-Opt, Kalashnikov et al. ‘18 • Q-learning from images for real-world robotic grasping "
323,"Policy Gradients CS 285 Instructor: Sergey Levine UC Berkeley The goal of reinforcement learning we’ll come back to partially observed later The goal of reinforcement learning infinite horizon case finite horizon case Evaluating the objective Direct policy differentiation a convenient identity Direct policy differentiation Evaluating the policy gradient generate samples (i.e. run the policy) fit a model to estimate return improve the policy Understanding Policy Gradients Evaluating the policy gradient Comparison to maximum likelihood training data supervised learning Example: Gaussian policies What did we just do? good stuff is made more likely bad stuff is made less likely simply formalizes the notion of “trial and error”! Partial observability What is wrong with the policy gradient? high variance generate samples (i.e. run the policy) fit a model to estimate return improve the policy Review • Evaluating the RL objective • Generate samples • Evaluating the policy gradient • Log-gradient trick • Generate samples • Understanding the policy gradient • Formalization of trial-and-error • Partial observability • Works just fine • What is wrong with policy gradient? Reducing Variance Reducing variance “reward to go” Baselines but… are we allowed to do that?? subtracting a baseline is unbiased in expectation! average reward is not the best baseline, but it’s pretty good! a convenient identity Analyzing variance This is just expected reward, but weighted by gradient magnitudes! generate samples (i.e. run the policy) fit a model to estimate return improve the policy Review • The high variance of policy gradient • Exploiting causality • Future doesn’t affect the past • Baselines • Unbiased! • Analyzing variance • Can derive optimal baselines Off-Policy Policy Gradients Policy gradient is on-policy • Neural networks change only a little bit with each gradient step • On-policy learning can be extremely inefficient! Off-policy learning & importance sampling importance sampling Deriving the policy gradient with IS a convenient identity The off-policy policy gradient if we ignore this, we get a policy iteration algorithm (more on this in a later lecture) A first-order approximation for IS (preview) We’ll see why this is reasonable later in the course! Implementing Policy Gradients Policy gradient with automatic differentiation Policy gradient with automatic differentiation Pseudocode example (with discrete actions): Maximum likelihood: # Given: # actions - (N*T) x Da tensor of actions # states - (N*T) x Ds tensor of states # Build the graph: logits = policy.predictions(states) # This should return (N*T) x Da tensor of action logits negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits=logits) loss = tf.reduce_mean(negative_likelihoods) gradients = loss.gradients(loss, variables) Policy gradient with automatic differentiation Pseudocode example (with discrete actions): Policy gradient: # Given: # actions - (N*T) x Da tensor of actions # states - (N*T) x Ds tensor of states # q_values – (N*T) x 1 tensor of estimated state-action values # Build the graph: logits = policy.predictions(states) # This should return (N*T) x Da tensor of action logits negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits=logits) weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values) loss = tf.reduce_mean(weighted_negative_likelihoods) gradients = loss.gradients(loss, variables) q_values Policy gradient in practice • Remember that the gradient has high variance • This isn’t the same as supervised learning! • Gradients will be really noisy! • Consider using much larger batches • Tweaking learning rates is very hard • Adaptive step size rules like ADAM can be OK-ish • We’ll learn about policy gradient-specific learning rate adjustment methods later! generate samples (i.e. run the policy) fit a model to estimate return improve the policy Review • Policy gradient is on-policy • Can derive off-policy variant • Use importance sampling • Exponential scaling in T • Can ignore state portion (approximation) • Can implement with automatic differentiation – need to know what to backpropagate • Practical considerations: batch size, learning rates, optimizers Advanced Policy Gradients What else is wrong with the policy gradient? (image from Peters & Schaal 2008) Essentially the same problem as this: Covariant/natural policy gradient Covariant/natural policy gradient see Schulman, L., Moritz, Jordan, Abbeel (2015) Trust region policy optimization (figure from Peters & Schaal 2008) Advanced policy gradient topics • What more is there? • Next time: introduce value functions and Q-functions • Later in the class: more on natural gradient and automatic step size adjustment Example: policy gradient with importance sampling Levine, Koltun ‘13 • Incorporate example demonstrations using importance sampling • Neural network policies Example: trust region policy optimization Schulman, Levine, Moritz, Jordan, Abbeel. ‘15 • Natural gradient with automatic step adjustment • Discrete and continuous actions • Code available (see Duan et al. ‘16) Policy gradients suggested readings • Classic papers • Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm • Baxter & Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later) • Peters & Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient • Deep reinforcement learning policy gradient papers • Levine & Koltun (2013). Guided policy search: deep RL with importance sampled policy gradient (unrelated to later discussion of guided policy search) • Schulman, L., Moritz, Jordan, Abbeel (2015). Trust region policy optimization: deep RL with natural policy gradient and adaptive step size • Schulman, Wolski, Dhariwal, Radford, Klimov (2017). Proximal policy optimization algorithms: deep RL with importance sampled policy gradient "
324,"Actor-Critic Algorithms CS 285 Instructor: Sergey Levine UC Berkeley Recap: policy gradients generate samples (i.e. run the policy) fit a model to estimate return improve the policy “reward to go” Improving the policy gradient “reward to go”sum together these rewards What about the baseline?sum together these rewards State & state-action value functions the better this estimate, the lower the variance unbiased, but high variance single-sample estimate generate samples (i.e. run the policy) fit a model to estimate return improve the policy Value function fitting generate samples (i.e. run the policy) fit a model to estimate return improve the policy Policy evaluation generate samples (i.e. run the policy) fit a model to estimate return improve the policysum together these rewards Monte Carlo evaluation with function approximationsum together these rewards the same function should fit multiple samples! Can we do better? Policy evaluation examples TD-Gammon, Gerald Tesauro 1992 AlphaGo, Silver et al. 2016 From Evaluation to Actor Critic An actor-critic algorithm generate samples (i.e. run the policy) fit a model to estimate return improve the policy Aside: discount factors episodic tasks continuous/cyclical tasks Aside: discount factors for policy gradients Which version is the right one? Further reading: Philip Thomas, Bias in natural actor-critic algorithms. ICML 2014 Actor-critic algorithms (with discount) Actor-Critic Design Decisions Architecture design two network design + simple & stable - no shared features between actor & critic shared network design Online actor-critic in practice works best with a batch (e.g., parallel workers) synchronized parallel actor-critic asynchronous parallel actor-critic Can we remove the on-policy assumption entirely? off-policy actor-critic replay buffer transitions that we saw in prior time steps form a batch by using old previously seen transitions Let’s see what that looks like replay buffer batch size This algorithm is broken! Can you spot the problems? Fixing the value function Fixing the policy update What else is left? Is there any remaining problem? Some implementation details could also use reparameterization trick to better estimate the integral lots of fancier ways to fit Q-functions (more on this in next two lectures) Example practical algorithm: Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. Soft Actor-Critic: Off- Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. 2018. We’ll also learn about algorithms that do this with deterministic policies later! Critics as Baselines Critics as state-dependent baselines + no bias - higher variance (because single-sample estimate) + lower variance (due to critic) - not unbiased (if the critic is not perfect) + no bias + lower variance (baseline is closer to rewards) Control variates: action-dependent baselines + no bias - higher variance (because single-sample estimate) + goes to zero in expectation if critic is correct! - not correct use a critic without the bias (still unbiased), provided second term can be evaluated Gu et al. 2016 (Q-Prop) Eligibility traces & n-step returns + lower variance - higher bias if value is wrong (it always is) + no bias - higher variance (because single-sample estimate) Can we combine these two, to control bias/variance tradeoff? Generalized advantage estimation Schulman, Moritz, Levine, Jordan, Abbeel ‘16 Do we have to choose just one n? Cut everywhere all at once! weighted combination of n-step returns How to weight? Mostly prefer cutting earlier (less variance) exponential falloff similar effect as discount! remember this? discount = variance reduction! Review, Examples, and Additional Readings Review • Actor-critic algorithms: • Actor: the policy • Critic: value function • Reduce variance of policy gradient • Policy evaluation • Fitting value function to policy • Discount factors • Carpe diem Mr. Robot • …but also a variance reduction trick • Actor-critic algorithm design • One network (with two heads) or two networks • Batch-mode, or online (+ parallel) • State-dependent baselines • Another way to use the critic • Can combine: n-step returns or GAE generate samples (i.e. run the policy) fit a model to estimate return improve the policy Actor-critic examples • High dimensional continuous control with generalized advantage estimation (Schulman, Moritz, L., Jordan, Abbeel ‘16) • Batch-mode actor-critic • Blends Monte Carlo and function approximator estimators (GAE) Actor-critic examples • Asynchronous methods for deep reinforcement learning (Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu ‘16) • Online actor-critic, parallelized batch • N-step returns with N = 4 • Single network for actor and critic Actor-critic suggested readings • Classic papers • Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for reinforcement learning with function approximation: actor-critic algorithms with value function approximation • Deep reinforcement learning actor-critic papers • Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016). Asynchronous methods for deep reinforcement learning: A3C -- parallel online actor-critic • Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuous control using generalized advantage estimation: batch-mode actor-critic with blended Monte Carlo and function approximator returns • Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy- gradient with an off-policy critic: policy gradient with Q-function control variate "
325,"Value Function Methods CS 285 Instructor: Sergey Levine UC Berkeley Recap: actor-critic generate samples (i.e. run the policy) fit a model to estimate return improve the policy Can we omit policy gradient completely? forget policies, let’s just do this! generate samples (i.e. run the policy) fit a model to estimate return improve the policy Policy iteration High level idea: generate samples (i.e. run the policy) fit a model to estimate return improve the policy how to do this? Dynamic programming 0.2 0.3 0.4 0.5 0.6 0.7 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.5 0.5 0.5 just use the current estimate here Policy iteration with dynamic programming generate samples (i.e. run the policy) fit a model to estimate return improve the policy 0.2 0.3 0.4 0.5 0.6 0.7 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.5 0.5 0.5 Even simpler dynamic programming approximates the new value! generate samples (i.e. run the policy) fit a model to estimate return improve the policy Fitted Value Iteration & Q-Iteration Fitted value iteration curse of dimensionality generate samples (i.e. run the policy) fit a model to estimate return improve the policy What if we don’t know the transition dynamics? need to know outcomes for different actions! Back to policy iteration… can fit this using samples Can we do the “max” trick again? doesn’t require simulation of actions! + works even for off-policy samples (unlike actor-critic) + only one network, no high-variance policy gradient - no convergence guarantees for non-linear function approximation (more on this later) forget policy, compute value directly can we do this with Q-values also, without knowing the transitions? Fitted Q-iteration Review generate samples (i.e. run the policy) fit a model to estimate return improve the policy • Value-based methods • Don’t learn a policy explicitly • Just learn value or Q-function • If we have value function, we have a policy • Fitted Q-iteration From Q-Iteration to Q-Learning Why is this algorithm off-policy? dataset of transitions Fitted Q-iteration What is fitted Q-iteration optimizing? most guarantees are lost when we leave the tabular case (e.g., use neural networks) Online Q-learning algorithms generate samples (i.e. run the policy) fit a model to estimate return improve the policy off policy, so many choices here! Exploration with Q-learning We’ll discuss exploration in detail in a later lecture! “epsilon-greedy” final policy: why is this a bad idea for step 1? “Boltzmann exploration” Review generate samples (i.e. run the policy) fit a model to estimate return improve the policy • Value-based methods • Don’t learn a policy explicitly • Just learn value or Q-function • If we have value function, we have a policy • Fitted Q-iteration • Batch mode, off-policy method • Q-learning • Online analogue of fitted Q- iteration Value Functions in Theory Value function learning theory 0.2 0.3 0.4 0.5 0.6 0.7 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.5 0.5 0.5 Value function learning theory 0.2 0.3 0.4 0.5 0.6 0.7 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.5 0.5 0.5 Non-tabular value function learning Non-tabular value function learning Conclusions: value iteration converges (tabular case) fitted value iteration does not converge not in general often not in practice What about fitted Q-iteration? Applies also to online Q-learning But… it’s just regression! Q-learning is not gradient descent! no gradient through target value A sad corollary An aside regarding terminology Review generate samples (i.e. run the policy) fit a model to estimate return improve the policy • Value iteration theory • Operator for backup • Operator for projection • Backup is contraction • Value iteration converges • Convergence with function approximation • Projection is also a contraction • Projection + backup is not a contraction • Fitted value iteration does not in general converge • Implications for Q-learning • Q-learning, fitted Q-iteration, etc. does not converge with function approximation • But we can make it work in practice! • Sometimes – tune in next time "
326,"Deep RL with Q-Functions CS 285 Instructor: Sergey Levine UC Berkeley Recap: Q-learning generate samples (i.e. run the policy) fit a model to estimate return improve the policy What’s wrong? Q-learning is not gradient descent! no gradient through target value Correlated samples in online Q-learning - sequential states are strongly correlated - target value is always changing synchronized parallel Q-learning asynchronous parallel Q-learning Another solution: replay buffers special case with K = 1, and one gradient step any policy will work! (with broad support) just load data from a buffer here dataset of transitions Fitted Q-iteration still use one gradient step Another solution: replay buffers dataset of transitions (“replay buffer”) off-policy Q-learning + samples are no longer correlated + multiple samples in the batch (low-variance gradient) but where does the data come from? need to periodically feed the replay buffer… Putting it together K = 1 is common, though larger K more efficient dataset of transitions (“replay buffer”) off-policy Q-learning Target Networks What’s wrong? Q-learning is not gradient descent! no gradient through target value use replay buffer This is still a problem! Q-Learning and Regression one gradient step, moving target perfectly well-defined, stable regression Q-Learning with target networks targets don’t change in inner loop! supervised regression “Classic” deep Q-learning algorithm (DQN) Mnih et al. ‘13 You’ll implement this in HW3! Alternative target network Intuition: get target from here no lag here maximal lag Feels weirdly uneven, can we always have the same lag? Popular alternative (similar to Polyak averaging): A General View of Q-Learning Fitted Q-iteration and Q-learning just SGD A more general view dataset of transitions (“replay buffer”) target parameters current parameters A more general view dataset of transitions (“replay buffer”) target parameters current parameters • Online Q-learning (last lecture): evict immediately, process 1, process 2, and process 3 all run at the same speed • DQN: process 1 and process 3 run at the same speed, process 2 is slow • Fitted Q-iteration: process 3 in the inner loop of process 2, which is in the inner loop of process 1 Improving Q-Learning Are the Q-values accurate? As predicted Q increases, so does the return Are the Q-values accurate? Overestimation in Q-learning Double Q-learning Double Q-learning in practice Multi-step returns Q-learning with N-step returns + less biased target values when Q-values are inaccurate + typically faster learning, especially early on - only actually correct when learning on-policy • ignore the problem • often works very well • cut the trace – dynamically choose N to get only on-policy data • works well when data mostly on-policy, and action space is small • importance sampling For more details, see: “Safe and efficient off-policy reinforcement learning.” Munos et al. ‘16 Q-Learning with Continuous Actions Q-learning with continuous actions What’s the problem with continuous actions? this max this max How do we perform the max? particularly problematic (inner loop of training) Option 1: optimization • gradient based optimization (e.g., SGD) a bit slow in the inner loop • action space typically low-dimensional – what about stochastic optimization? Q-learning with stochastic optimization Simple solution: + dead simple + efficiently parallelizable - not very accurate but… do we care? How good does the target need to be anyway? More accurate solution: • cross-entropy method (CEM) • simple iterative stochastic optimization • CMA-ES • substantially less simple iterative stochastic optimization works OK, for up to about 40 dimensions Easily maximizable Q-functions Option 2: use function class that is easy to optimize Gu, Lillicrap, Sutskever, L., ICML 2016 NAF: Normalized Advantage Functions + no change to algorithm + just as efficient as Q-learning - loses representational power Q-learning with continuous actions Option 3: learn an approximate maximizer DDPG (Lillicrap et al., ICLR 2016) “deterministic” actor-critic (really approximate Q-learning) Q-learning with continuous actions Option 3: learn an approximate maximizer Implementation Tips and Examples Simple practical tips for Q-learning • Q-learning takes some care to stabilize • Test on easy, reliable tasks first, make sure your implementation is correct • Large replay buffers help improve stability • Looks more like fitted Q-iteration • It takes time, be patient – might be no better than random for a while • Start with high exploration (epsilon) and gradually reduce Slide partly borrowed from J. Schulman Advanced tips for Q-learning • Bellman error gradients can be big; clip gradients or use Huber loss • Double Q-learning helps a lot in practice, simple and no downsides • N-step returns also help a lot, but have some downsides • Schedule exploration (high to low) and learning rates (high to low), Adam optimizer can help too • Run multiple random seeds, it’s very inconsistent between runs Slide partly borrowed from J. Schulman Fitted Q-iteration in a latent space • “Autonomous reinforcement learning from raw visual data,” Lange & Riedmiller ‘12 • Q-learning on top of latent space learned with autoencoder • Uses fitted Q-iteration • Extra random trees for function approximation (but neural net for embedding) Q-learning with convolutional networks • “Human-level control through deep reinforcement learning,” Mnih et al. ‘13 • Q-learning with convolutional networks • Uses replay buffer and target network • One-step backup • One gradient step • Can be improved a lot with double Q-learning (and other tricks) Q-learning with continuous actions • “Continuous control with deep reinforcement learning,” Lillicrap et al. ‘15 • Continuous actions with maximizer network • Uses replay buffer and target network (with Polyak averaging) • One-step backup • One gradient step per simulator step Q-learning on a real robot • “Robotic manipulation with deep reinforcement learning and …,” Gu*, Holly*, et al. ‘17 • Continuous actions with NAF (quadratic in actions) • Uses replay buffer and target network • One-step backup • Four gradient steps per simulator step for efficiency • Parallelized across multiple robots Large-scale Q-learning with continuous actions (QT-Opt) live data collection stored data from all past experiments training buffers Bellman updaters training threads Kalashnikov, Irpan, Pastor, Ibarz, Herzong, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, Levine. QT-Opt: Scalable Deep Reinforcement Learning of Vision- Based Robotic Manipulation Skills Q-learning suggested readings • Classic papers • Watkins. (1989). Learning from delayed rewards: introduces Q-learning • Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neural networks • Deep reinforcement learning Q-learning papers • Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcement learning: early image-based Q-learning method using autoencoders to construct embeddings • Mnih et al. (2013). Human-level control through deep reinforcement learning: Q- learning with convolutional networks for playing Atari. • Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning. • Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization. • Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-based acceleration: continuous Q-learning with action-quadratic value functions. • Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling network architectures for deep reinforcement learning: separates value and advantage estimation in Q-function. Review generate samples (i.e. run the policy) fit a model to estimate return improve the policy • Q-learning in practice • Replay buffers • Target networks • Generalized fitted Q-iteration • Double Q-learning • Multi-step Q-learning • Q-learning with continuous actions • Random sampling • Analytic optimization • Second “actor” network "
327,"Advanced Policy Gradients CS 285 Instructor: Sergey Levine UC Berkeley Recap: policy gradients generate samples (i.e. run the policy) fit a model to estimate return improve the policy “reward to go” can also use function approximation here Why does policy gradient work? generate samples (i.e. run the policy) fit a model to estimate return improve the policy look familiar? Policy gradient as policy iteration Policy gradient as policy iteration importance sampling Ignoring distribution mismatch? ? why do we want this to be true? is it true? and when? Bounding the Distribution Change Ignoring distribution mismatch? ? why do we want this to be true? is it true? and when? Bounding the distribution change seem familiar? not a great bound, but a bound! Bounding the distribution change Proof based on: Schulman, Levine, Moritz, Jordan, Abbeel. “Trust Region Policy Optimization.” Bounding the objective value Where are we at so far? Policy Gradients with Constraints A more convenient bound KL divergence has some very convenient properties that make it much easier to approximate! How do we optimize the objective? How do we enforce the constraint? can do this incompletely (for a few grad steps) Natural Gradient How (else) do we optimize the objective? Use first order Taylor approximation for objective (a.k.a., linearization) How do we optimize the objective? (see policy gradient lecture for derivation) exactly the normal policy gradient! Can we just use the gradient then? Can we just use the gradient then? not the same! second order Taylor expansion Can we just use the gradient then? natural gradient Is this even a problem in practice? (image from Peters & Schaal 2008) Essentially the same problem as this: (figure from Peters & Schaal 2008) Practical methods and notes • Natural policy gradient • Generally a good choice to stabilize policy gradient training • See this paper for details: • Peters, Schaal. Reinforcement learning of motor skills with policy gradients. • Practical implementation: requires efficient Fisher-vector products, a bit non-trivial to do without computing the full matrix • See: Schulman et al. Trust region policy optimization • Trust region policy optimization • Just use the IS objective directly • Use regularization to stay close to old policy • See: Proximal policy optimization generate samples (i.e. run the policy) fit a model to estimate return improve the policy Review • Policy gradient = policy iteration • Optimize advantage under new policy state distribution • Using old policy state distribution optimizes a bound, if the policies are close enough • Results in constrained optimization problem • First order approximation to objective = gradient ascent • Regular gradient ascent has the wrong constraint, use natural gradient • Practical algorithms • Natural policy gradient • Trust region policy optimization "
328,"Optimal Control and Planning CS 285 Instructor: Sergey Levine UC Berkeley Today’s Lecture 1. Introduction to model-based reinforcement learning 2. What if we know the dynamics? How can we make decisions? 3. Stochastic optimization methods 4. Monte Carlo tree search (MCTS) 5. Trajectory optimization • Goals: • Understand how we can perform planning with known dynamics models in discrete and continuous spaces Recap: the reinforcement learning objective Recap: model-free reinforcement learning assume this is unknown don’t even attempt to learn it What if we knew the transition dynamics? • Often we do know the dynamics 1. Games (e.g., Atari games, chess, Go) 2. Easily modeled systems (e.g., navigating a car) 3. Simulated environments (e.g., simulated robots, video games) • Often we can learn the dynamics 1. System identification – fit unknown parameters of a known model 2. Learning – fit a general-purpose model to observed transition data Does knowing the dynamics make things easier? Often, yes! 1. Model-based reinforcement learning: learn the transition dynamics, then figure out how to choose actions 2. Today: how can we make decisions if we know the dynamics? a. How can we choose actions under perfect knowledge of the system dynamics? b. Optimal control, trajectory optimization, planning 3. Next week: how can we learn unknown dynamics? 4. How can we then also learn policies? (e.g. by imitating optimal control) Model-based reinforcement learning policy system dynamics The objective 1. run away 2. ignore 3. pet The deterministic case The stochastic open-loop case why is this suboptimal? Aside: terminology what is this “loop”? closed-loop open-loop only sent at t = 1, then it’s one-way! The stochastic closed-loop casegloballocal (more on this later) Open-Loop Planning But for now, open-loop planning Stochastic optimization simplest method: guess & check “random shooting method” Cross-entropy method (CEM) can we do better? typically use Gaussian distribution see also: CMA-ES (sort of like CEM with momentum) What’s the problem? 1. Very harsh dimensionality limit 2. Only open-loop planning What’s the upside? 1. Very fast if parallelized 2. Extremely simple Discrete case: Monte Carlo tree search (MCTS) Discrete case: Monte Carlo tree search (MCTS) e.g., random policy Discrete case: Monte Carlo tree search (MCTS) +10 +15 Discrete case: Monte Carlo tree search (MCTS) Q = 10 N = 1 Q = 12 N = 1 Q = 16 N = 1 Q = 22 N = 2 Q = 38 N = 3 Q = 10 N = 1 Q = 12 N = 1 Q = 22 N = 2 Q = 30 N = 3 Q = 8 N = 1 Additional reading 1. Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, Colton. (2012). A Survey of Monte Carlo Tree Search Methods. • Survey of MCTS methods and basic summary. Trajectory Optimization with Derivatives Can we use derivatives? Shooting methods vs collocation shooting method: optimize over actions only Shooting methods vs collocation collocation method: optimize over actions and states, with constraints Linear case: LQR linear quadratic Linear case: LQR Linear case: LQR Linear case: LQR linear linear quadratic Linear case: LQR linear linear quadratic timestate (x) Linear case: LQR Linear case: LQR LQR for Stochastic and Nonlinear Systems Stochastic dynamics The stochastic closed-loop case Nonlinear case: DDP/iterative LQR Nonlinear case: DDP/iterative LQR Nonlinear case: DDP/iterative LQR Nonlinear case: DDP/iterative LQR Nonlinear case: DDP/iterative LQR Nonlinear case: DDP/iterative LQR Case Study and Additional Readings Case study: nonlinear model-predictive control Additional reading 1. Mayne, Jacobson. (1970). Differential dynamic programming. • Original differential dynamic programming algorithm. 2. Tassa, Erez, Todorov. (2012). Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization. • Practical guide for implementing non-linear iterative LQR. 3. Levine, Abbeel. (2014). Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics. • Probabilistic formulation and trust region alternative to deterministic line search. What’s wrong with known dynamics? Next time: learning the dynamics model "
329,"Model-Based Reinforcement Learning CS 285 Instructor: Sergey Levine UC Berkeley 1. Basics of model-based RL: learn a model, use model for control • Why does naïve approach not work? • The effect of distributional shift in model-based RL 2. Uncertainty in model-based RL 3. Model-based RL with complex observations 4. Next time: policy learning with model-based RL • Goals: • Understand how to build model-based RL algorithms • Understand the important considerations for model-based RL • Understand the tradeoffs between different model class choices Today’s Lecture Why learn the model? Does it work? Yes! • Essentially how system identification works in classical robotics • Some care should be taken to design a good base policy • Particularly effective if we can hand-engineer a dynamics representation using our knowledge of physics, and fit just a few parameters Does it work? No! • Distribution mismatch problem becomes exacerbated as we use more expressive model classes go right to get higher! Can we do better? What if we make a mistake? Can we do better? every N stepsREPLANNING HELPS WITH MODEL ERRORS This will be on HW4! How to replan? every N steps • The more you replan, the less perfect each individual plan needs to be • Can use shorter horizons • Even random sampling can often work well here! Uncertainty in Model-Based RL A performance gap in model-based RL Nagabandi, Kahn, Fearing, L. ICRA 2018 pure model-based (about 10 minutes real time) model-free training (about 10 days…) Why the performance gap? need to not overfit here… …but still have high capacity over here Why the performance gap? every N steps very tempting to go here… How can uncertainty estimation help? expected reward under high-variance prediction is very low, even though mean is the same! Intuition behind uncertainty-aware RL every N steps only take actions for which we think we’ll get high reward in expectation (w.r.t. uncertain dynamics) This avoids “exploiting” the model The model will then adapt and get better There are a few caveats… Need to explore to get better Expected value is not the same as pessimistic value Expected value is not the same as optimistic value …but expected value is often a good start Uncertainty-Aware Neural Net Models How can we have uncertainty-aware models? why is this not enough? Idea 1: use output entropy what is the variance here? Two types of uncertainty: aleatoric or statistical uncertainty epistemic or model uncertainty “the model is certain about the data, but we are not certain about the model” How can we have uncertainty-aware models? Idea 2: estimate model uncertainty “the model is certain about the data, but we are not certain about the model” the entropy of this tells us the model uncertainty! Quick overview of Bayesian neural networks expected weight uncertainty about the weight For more, see: Blundell et al., Weight Uncertainty in Neural Networks Gal et al., Concrete Dropout We’ll learn more about variational inference later! Bootstrap ensembles Train multiple models and see if they agree! How to train? Main idea: need to generate “independent” datasets to get “independent” models Bootstrap ensembles in deep learning This basically works Very crude approximation, because the number of models is usually small (< 10) Resampling with replacement is usually unnecessary, because SGD and random initialization usually makes the models sufficiently independent Planning with Uncertainty, Examples How to plan with uncertainty distribution over deterministic models Other options: moment matching, more complex posterior estimation with BNNs, etc. Example: model-based RL with ensembles exceeds performance of model-free after 40k steps (about 10 minutes of real time) before after More recent example: PDDM Deep Dynamics Models for Learning Dexterous Manipulation. Nagabandi et al. 2019 Further readings • Deisenroth et al. PILCO: A Model-Based and Data-Efficient Approach to Policy Search. Recent papers: • Nagabandi et al. Neural Network Dynamics for Model- Based Deep Reinforcement Learning with Model-Free Fine-Tuning. • Chua et al. Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models. • Feinberg et al. Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning. • Buckman et al. Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion. Model-Based RL with Images What about complex observations? What is hard about this? • High dimensionality • Redundancy • Partial observability high-dimensional but not dynamic low-dimension but dynamic State space (latent space) models observation model dynamics model reward model How to train? standard (fully observed) model: latent space model: Model-based RL with latent space models “encoder” full smoothing posterior single-step encoder + most accurate - most complicated + simplest - least accurate We will discuss variational inference in more detail next week! we’ll talk about this one for now Model-based RL with latent space models deterministic encoder Everything is differentiable, can train with backprop Model-based RL with latent space models latent space dynamics image reconstruction reward model Many practical methods use a stochastic encoder to model uncertainty Model-based RL with latent space models every N steps Learn directly in observation space Finn, L. Deep Visual Foresight for Planning Robot Motion. ICRA 2017. Ebert, Finn, Lee, L. Self-Supervised Visual Planning with Temporal Skip Connections. CoRL 2017. Use predictions to complete tasks Designated Pixel Goal Pixel Task execution "
33,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, Richard Socher, Chris Manning, JurafskyMartin-SLP3, others) Lecture 4: Syntactic Parsing (Constituent, Dependency, CCG, etc.) Announcements ! Chapter section summary due Sunday Sep24 midnight ! Coding-HW1 (on word vector training+evaluation_ +visualization) will be release in 1-2 days – TA Yixin will give overview of the homework today! ! TA Yixin Nie’s office hours: 2.30-3.30pm Wednesdays (SN-372; might move to 2nd floor reading room) Coding HW1 (TA Yixin Nie’s presentation) Syntactic Parsing Constituent Parsing Syntactic Parsing -- Constituent ! Phrase-structure parsing or Bracketing ! Demos: http://tomato.banatao.berkeley.edu:8080/parser/parser.html VBD VP met NP S NP her PRP John NNP Probabilistic Context-free Grammars ! A context-free grammar is a tuple <N, T, S, R> N : the set of non-terminals Phrasal categories: S, NP, VP, ADJP, etc. Parts-of-speech (pre-terminals): NN, JJ, DT, VB T : the set of terminals (the words) S : the start symbol Often written as ROOT or TOP Not usually the sentence non-terminal S R : the set of rules Of the form X → Y1 Y2 … Yk, with X, Yi ∈ N Examples: S → NP VP, VP → VP CC VP Also called rewrites, productions, or local trees Probabilistic Context-free Grammars ! A PCFG: ! Adds a top-down production probability per rule P(Y1 Y2 … Yk | X) ! Allows us to find the ‘most probable parse’ for a sentence ! The probability of a parse is just the product of the probabilities of the individual rules  ROOT → S 1 S → NP VP . 1 NP → PRP 1 VP → VBD ADJP 1 ….. Treebank PCFG Model F1 Baseline 72.0 [Charniak, 1996] 3 TreebankPCFGs  UsePCFGsforbroadcoverageparsing  Cantakeagrammarrightoffthetrees(doesn’tworkwell): ROOT o S 1 S o NP VP . 1 NP o PRP 1 VP o VBD ADJP 1 ….. Model F1 Baseline 72.0 [Charniak 96] ! Can just count the frequency of each rule and normalize (but not very effective) Real Treebank Examples TreebankSentences ! Long, complex sentences with several clauses, nested prepositions, etc. Grammar Refinement ! Conditional independence assumptions often too strong! Not every NP expansion can fill every NP slot ! Better results by enriching the grammar e.g., ! Lexicalization [Collins, 1999; Charniak, 2000] ConditionalIndependence?  NoteveryNPexpansioncanfilleveryNPslot  Agrammarwithsymbolslike“NP”won’tbecontextͲfree  Statistically,conditionalindependencetoostrong -noise -She Grammar Refinement ! Conditional independence assumptions often too strong! Not every NP expansion can fill every NP slot ! Better results by enriching the grammar e.g., ! Lexicalization [Collins, 1999; Charniak, 2000] ! Markovization, Manual Tag-splitting [Johnson, 1998; Klein & Manning, 2003] ConditionalIndependence?  NoteveryNPexpansioncanfilleveryNPslot  Agrammarwithsymbolslike“NP”won’tbecontextͲfree  Statistically,conditionalindependencetoostrong ^VP ^S Grammar Refinement ! Conditional independence assumptions often too strong! Not every NP expansion can fill every NP slot ! Better results by enriching the grammar e.g., ! Lexicalization [Collins, 1999; Charniak, 2000] ! Markovization, Manual Tag-splitting [Johnson, 1998; Klein & Manning, 2003] ! Latent Tag-splitting [Matsuzaki et al., 2005; Petrov et al., 2006] ConditionalIndependence?  NoteveryNPexpansioncanfilleveryNPslot  Agrammarwithsymbolslike“NP”won’tbecontextͲfree  Statistically,conditionalindependencetoostrong -7 -3  bestScore(s) for (i : [0,n-1]) for (X : tags[s[i]]) score[X][i][i+1] = tagScore(X,s[i]) for (diff : [2,n]) for (i : [0,n-diff]) j = i + diff for (X->YZ : rule) for (k : [i+1, j-1]) score[X][i][j] = max{score[X][i][j], score(X->YZ) *score[Y][i][k] *score[Z][k][j]} Y Z X i k j CKY (or CYK) Parsing Algorithm (Bottom-up) [Cocke, 1970; Kasami, 1965; Younger, 1967] for k i+1 to j −1 do for all {A | A ! BC 2 grammar and B 2 table[i,k] and C 2 table[k, j]} table[i,j] table[i,j] [ A Figure 12.5 The CKY algorithm. ... ... [0,n] [i,i+1] [i,i+2] [i,j-2] [i,j-1] [i+1,j] [i+2,j] [j-1,j] [j-2,j] [i,j] ... [0,1] [n-1, n] Fi 12 6 All th t ﬁll th [i j]th ll i th CKY t bl CKY Parsing Algorithm (Bottom-up) [Jurafsky-Martin-SLP3] 12.2 • CKY PARSING: A DYNAMIC PROGRAMMING APPROACH 7 function CKY-PARSE(words,grammar) returns table for j from 1 to LENGTH(words) do for all {A | A ! words[j] 2 grammar} table[j −1, j] table[j −1, j] [ A for i from j −2 downto 0 do for k i+1 to j −1 do for all {A | A ! BC 2 grammar and B 2 table[i,k] and C 2 table[k, j]} table[i,j] table[i,j] [ A Figure 12.5 The CKY algorithm. Latent Variable Grammars [Petrov et al., 2006] 39 LatentVariableGrammars Parse Tree Sentence Parameters ... Derivations Learning Latent Splits (Inside-Outside) [Petrov et al., 2006] Forward LearningLatentAnnotations EMalgorithm: X1 X2 X7 X4 X5 X6 X3 He was right .  Brackets are known  Base categories are known  Only induce subcategories JustlikeForwardͲBackwardforHMMs. Backward ! Forward-backward (last week) but for trees DT Tag Splits Example [Petrov et al., 2006] RefinementoftheDTtag DT DT-1 DT-2 DT-3 DT-4 Other Learned Splits [Petrov et al., 2006] LearnedSplits  Proper Nouns (NNP):  Personal pronouns (PRP): NNP-14 Oct. Nov. Sept. NNP-12 John Robert James NNP-2 J. E. L. NNP-1 Bush Noriega Peters NNP-15 New San Wall NNP-3 York Francisco Street PRP-0 It He I PRP-1 it he they PRP-2 it them him Other Learned Splits [Petrov et al., 2006]  Relativeadverbs(RBR):  CardinalNumbers(CD): RBR-0 further lower higher RBR-1 more less More RBR-2 earlier Earlier later CD-7 one two Three CD-4 1989 1990 1988 CD-11 million billion trillion CD-0 1 50 100 CD-3 1 30 31 CD-9 78 58 34 LearnedSplits Latent PCFG Results [Petrov et al., 2006] FinalResults(Accuracy)  40 words F1 all F1 ENG Charniak&Johnson ‘05 (generative) 90.1 89.6 Split / Merge 90.6 90.1 GER Dubey ‘05 76.3 - Split / Merge 80.8 80.1 CHN Chiang et al. ‘02 80.0 76.6 Split / Merge 86.3 83.4 Still higher numbers from reranking / self-training methods Evaluating Constituent Parsers a hypothesis parse Ch of a sentence s is labeled “correct” if there is a constituent in the reference parse Cr with the same starting point, ending point, and non-terminal symbol. We can then measure the precision and recall just as we did for chunking in the previous chapter. labeled recall: = # of correct constituents in hypothesis parse of s # of correct constituents in reference parse of s labeled precision: = # of correct constituents in hypothesis parse of s # of total constituents in hypothesis parse of s As with other uses of precision and recall, instead of reporting them separately we often report a single number, the F-measure (van Rijsbergen, 1975): The F- measure measure is deﬁned as Fb = (b 2 +1)PR b 2P+R The b parameter differentially weights the importance of recall and precision based perhaps on the needs of an application. Values of b > 1 favor recall and values of b < 1 favor precision. When b = 1, precision and recall are equally balanced this is sometimes called Fb=1 or just F1: F1 = 2PR P+R (13.42) [Jurafsky-Martin-SLP3] pter. l: = # of correct constituents in hypothesis parse of s # of correct constituents in reference parse of s sion: = # of correct constituents in hypothesis parse of s # of total constituents in hypothesis parse of s ther uses of precision and recall, instead of reporting them separately, ort a single number, the F-measure (van Rijsbergen, 1975): The F- ﬁned as Fb = (b 2 +1)PR b 2P+R rameter differentially weights the importance of recall and precision, s on the needs of an application. Values of b > 1 favor recall and values or precision. When b = 1, precision and recall are equally balanced; mes called Fb=1 or just F1: F1 = 2PR P+R (13.42) asure derives from a weighted harmonic mean of precision and recall we often report a single number, the F-measure (van Rijsbergen, measure measure is deﬁned as Fb = (b 2 +1)PR b 2P+R The b parameter differentially weights the importance of recal based perhaps on the needs of an application. Values of b > 1 favor r of b < 1 favor precision. When b = 1, precision and recall are eq this is sometimes called Fb=1 or just F1: F1 = 2PR P+R The F-measure derives from a weighted harmonic mean of prec Remember that the harmonic mean of a set of numbers is the recipro metic mean of the reciprocals: HarmonicMean(a1,a2,a3,a4,...,an) = n 1 a1 + 1 a2 + 1 a3 +...+ a and hence the F-measure is 2 b p b , p q y ; this is sometimes called Fb=1 or just F1: F1 = 2PR P+R (13.42) The F-measure derives from a weighted harmonic mean of precision and recall. Remember that the harmonic mean of a set of numbers is the reciprocal of the arith- metic mean of the reciprocals: HarmonicMean(a1,a2,a3,a4,...,an) = n 1 a1 + 1 a2 + 1 a3 +...+ 1 an (13.43) and hence the F-measure is F = 1 a 1 P +(1−a) 1 R or ✓ with b 2 = 1−a a ◆ F = (b 2 +1)PR b 2P+R (13.44) We additionally use a new metric, crossing brackets, for each sentence s: cross-brackets: the number of constituents for which the reference parse has a bracketing such as ((A B) C) but the hypothesis parse has a bracketing such as (A (B C)). Other Results ! Collins, 1999 ! 88.6 F1 (generative lexical) ! Charniak and Johnson, 2005 ! 89.7 / 91.3 F1 (generative lexical / reranking) ! Petrov et al., 2006 ! 90.7 F1 (generative unlexical) ! McClosky et al., 2006 – 92.1 F1 (generative + reranking + self‐training) Syntactic Ambiguities ! I saw the old man with a telescope ! I shot an elephant in my pajamas ! I cleaned the dishes in my pajamas ! I cleaned the dishes in the sink Real-Data PP Attachment Ambiguities Ambiguities:PPAttachment Attachment Ambiguity Types SyntacticAmbiguitiesI  Prepositionalphrases: Theycookedthebeansinthepotonthestovewithhandles.  Particlevs.preposition: Thepuppytoreupthestaircase.  Complementstructures Thetouristsobjectedtotheguidethattheycouldn’thear. Sheknowsyoulikethebackofherhand.  Gerundvs.participialadjective Visitingrelativescanbeboring. Changingschedulesfrequentlyconfusedpassengers. Attachment Ambiguity Types SyntacticAmbiguitiesII  ModifierscopewithinNPs impracticaldesignrequirements plasticcupholder  Multiplegapconstructions Thechickenisreadytoeat. Thecontractorsarerichenoughtosue.  Coordinationscope: Smallratsandmicecansqueezeintoholesorcracksinthe wall. World Knowledge is Important Clean the dishes in the sink. Web Features for Syntactic Parsing They considered running the ad during the Super Bowl. VP VBD considered S VP VBG running NP the ad PP IN during NP the Super Bowl VP VBD considered S VP VBG running NP the ad PP IN during NP the Super Bowl Dependency: Constituent: [Nakov and Hearst 2005; Pitler et al., 2010; Bansal and Klein, 2011] Web Features for Syntactic Parsing count(running it during) > count(considered it during) Web Ngrams [Bansal and Klein, 2011] They considered running the ad during the Super Bowl. 90.5 91.5 92.5 McDonald & Pereira 2006 Us UAS ! 7-10% relative error reduction over 90-92% parsers Visual Recognition Cues ! Joint parsing and image recognition the mug on the table with a crack Visual Recognition Cues ! Joint parsing and image recognition the mug on the table with a crack red chair and table light green table Visual Recognition Cues [Christie et al., 2016] h,santol,ygoyal,kbk,dbatra}@vt.edu eously per- epositional captioned guage can- ously rea- If we con- hant in my e (and not if it is the e pajamas a diverse h semantic ase attach- y reranked We show d preposi- n modules PASCAL Sentence Dataset Consistent NLP: Sentence Parsing Ambiguity: (woman on couch) vs (dog on couch) Output: Parse Tree “A dog is standing next to a woman on a couch” Vision: Semantic Segmentation Labels: Chairs, desks, etc. Person Couch Couch Person Dog Solution #1 Solution #M Ambiguity:* (dog*next*to*woman)*on*couch vs*dog*next*to*(woman*on*couch) Ambiguity:* (dog*next*to*woman)*on*couch vs*dog*next*to*(woman*on*couch) Figure 1: Overview of our approach. We propose a model for simultaneous 2D semantic segmentation and preposi- Dependency Parsing Dependency Parsing ! Predicting directed head-modifier relationship pairs ! Demos: http://nlp.stanford.edu:8080/corenlp/ raising $ 30 million from debt dobj pobj prep num num Dependency Parsing DependencyParsing  Lexicalizedparserscanbeseenasproducingdependencytrees  Eachlocalbinarytreecorrespondstoanattachmentinthedependency graph questioned lawyer witness the the Constituent Parse (with head words) Dependency Parse ! Can convert (lexicalized) constituent tree to dependency tree (each local binary tree gives us a dependency attachment from head to modifier) Dependency Parsing 14.1 • DEPENDENCY RELATIONS 3 Clausal Argument Relations Description NSUBJ Nominal subject DOBJ Direct object IOBJ Indirect object CCOMP Clausal complement XCOMP Open clausal complement Nominal Modiﬁer Relations Description NMOD Nominal modiﬁer AMOD Adjectival modiﬁer NUMMOD Numeric modiﬁer APPOS Appositional modiﬁer DET Determiner CASE Prepositions, postpositions and other case markers Other Notable Relations Description CONJ Conjunct CC Coordinating conjunction Figure 14.2 Selected dependency relations from the Universal Dependency set. (de Marn- effe et al., 2014) in terms of the role that the dependent plays with respect to its head Familiar notions [Jurafsky-Martin-SLP3] Dependency Parsing [Jurafsky-Martin-SLP3] HAPTER 14 • DEPENDENCY PARSING Relation Examples with head and dependent NSUBJ United canceled the ﬂight. DOBJ United diverted the ﬂight to Reno. We booked her the ﬁrst ﬂight to Miami. IOBJ We booked her the ﬂight to Miami. NMOD We took the morning ﬂight. AMOD Book the cheapest ﬂight. NUMMOD Before the storm JetBlue canceled 1000 ﬂights. APPOS United, a unit of UAL, matched the fares. DET The ﬂight was canceled. Which ﬂight was delayed? CONJ We ﬂew to Denver and drove to Steamboat. CC We ﬂew to Denver and drove to Steamboat. CASE Book the ﬂight through Houston. Figure 14.3 Examples of core Universal Dependency relations. 2 Dependency Formalisms In their most general form the dependency structures we’re discussing are simply Dependency Parsing ! Pure (projective, 1st order) dependency parsing is only cubic [Eisner, 1996] ! Non-projective dependency parsing useful for Czech & other languages – MST algorithms [McDonald et al., 2005] Transition-based Dependency Parsing 8 CHAPTER 14 • DEPENDENCY PARSING Dependency Relations wn w1 w2 s2 ... s1 sn Parser Input buffer Stack Oracle Figure 14.5 Basic transition-based parser. The parser examines the top two elements of the stack and selects an action based on consulting an oracle that examines the current conﬁgura- tion. ROOT node, the word list is initialized with the set of the words or lemmatized tokens in the sentence, and an empty set of relations is created to represent the parse. In the ﬁnal goal state, the stack and the word list should be empty, and the set of relations will represent the ﬁnal parse. In the standard approach to transition-based parsing, the operators used to pro- duce new conﬁgurations are surprisingly simple and correspond to the intuitive ac- tions one might take in creating a dependency tree by examining the words in a single pass over the input from left to right (Covington, 2001): 10 CHAPTER 14 • DEPENDENCY PARSING Step Stack Word List Action Relation Added 0 [root] [book, me, the, morning, ﬂight] SHIFT 1 [root, book] [me, the, morning, ﬂight] SHIFT 2 [root, book, me] [the, morning, ﬂight] RIGHTARC (book ! me) 3 [root, book] [the, morning, ﬂight] SHIFT 4 [root, book, the] [morning, ﬂight] SHIFT 5 [root, book, the, morning] [ﬂight] SHIFT 6 [root, book, the, morning, ﬂight] [] LEFTARC (morning ﬂight) 7 [root, book, the, ﬂight] [] LEFTARC (the ﬂight) 8 [root, book, ﬂight] [] RIGHTARC (book ! ﬂight) 9 [root, book] [] RIGHTARC (root ! book) 10 [root] [] Done Figure 14.7 Trace of a transition-based parse. Parsing: Other Models and Methods ! Combinatory Categorial Grammar [Steedman, 1996, 2000; Clark and Curran, 2004] ! Transition-based Dependency Parsing [Yamada and Matsumoto, 2003; Nivre, 2003] ! Tree-Insertion Grammar, DOP [Schabes and Waters, 1995; Hwa, 1998; Scha, 1990; Bod, 1993; Goodman, 1996; Bansal and Klein, 2010] ! Tree-Adjoining Grammar [Resnik, 1992; Joshi and Schabes, 1998; Chiang, 2000] ! Shift-Reduce Parser [Nivre and Scholz, 2004; Sagae and Lavie, 2005] ! Other: Reranking, A*, K-Best, Self-training, Co-training, System Combination, Cross-lingual Transfer [Sarkar, 2001; Steedman et al., 2003; Charniak and Johnson, 2005; Hwa et al., 2005; Huang and Chiang, 2005; McClosky et al., 2006; Fossum and Knight, 2009; Pauls and Klein, 2009; McDonald et al., 2011] ! Other Demos: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Demo, http://4.easy-ccg.appspot.com/ CCG Parsing CCGParsing  Combinatory CategorialGrammar  Fully(monoͲ) lexicalizedgrammar  Categoriesencode argumentsequences  Verycloselyrelated tothelambda calculus(morelater)  Canhavespurious ambiguities(why?) ! Combinatory Categorial Grammars: ! Each category encodes an argument sequence (fwd/bwd slashes specify argument order/direction) ! Closely related to lambda calculus ! Captures both syntactic and semantic info ! Naturally allows meaning representation and semantic parsing (next week!) Parser Reranking ParseReranking  Assumethenumberofparsesisverysmall  WecanrepresenteachparseTasanarbitraryfeaturevectorM(T)  Typically,alllocalrulesarefeatures  AlsononͲlocalfeatures,likehowrightͲbranchingtheoveralltreeis  [CharniakandJohnson05]givesarichsetoffeatures ! Can first get the k-best list of parses based on parser probability ! Then we can fire features on full tree (as opposed to local features in the parser’s dynamic program) ! Can fire non-local, global features like tree depth, width, right- branching vs left-branching, etc. ! See [Charniak and Johnson, 2005] for feature list. Data Oriented Parsing (TIGs) DataͲorientedparsing:  Rewritelarge(possiblylexicalized)subtreesinasinglestep  Formally,atreeͲinsertiongrammar  Derivationalambiguitywhethersubtreesweregeneratedatomically orcompositionally ! DOP is formally a Tree-Insertion Grammar, i.e., we can rewrite a large subtree in a single step ! Hence, this brings in derivational ambiguity Data Oriented Parsing (TIGs) TIG:Insertion Neural Models for Parsing Word Embeddings for Parsing ! Discrete or continuous, trained on large amounts of context ! BROWN (Brown et al., 1992): ! SKIPGRAM (Mikolov et al., 2013): anti . bj p ncy tree. The ways the root modiﬁer and achment. luster-based ental results, ction 6 con- 2006; Nivre ncy parsing. ic informa- apple pear Apple IBM bought run of in 01 100 101 110 111 000 001 010 011 00 0 10 1 11 Figure 2: An example of a Brown word-cluster hierarchy. Each node in the tree is labeled with a bit-string indicat- ing the path from the root node to that node, where 0 indicates a left branch and 1 indicates a right branch. lowing maximization: PARSE(x; w) = argmax y∈Y(x) X r∈y w · f(x, r) Above, we have assumed that each part is scored by a linear model with parameters w and feature- mapping f(·). For many different part factoriza- tions and structure domains Y(·), it is possible to solve the above maximization efﬁciently, and several recent efforts have concentrated on designing new maximization algorithms with increased context- iti it (Ei 2000 M D ld t l 2005b SKIP Few mins. vs. days/weeks/months!! w(t) w(t-2) w(t-1) w(t+1) w(t+2) INPUT PROJECTION OUTPUT context window w Mikolov et al., 2013! apple ! 000 pear ! 001 Apple ! 010 apple ! [0.65 0.15 -0.21 0.15 0.70 -0.90] pear ! [0.51 0.05 -0.32 0.20 0.80 -0.95] Apple ! [0.11 0.33 0.51 -0.05 -0.41 0.50] [Koo et al., 2008; Bansal et al., 2014] Word Embeddings for Parsing [Mr., Mrs., Ms., Prof., III, Jr., Dr.] [Jeffrey, William, Dan, Robert, Stephen, Peter, John, Richard, ...] [Portugal, Iran, Cuba, Ecuador, Greece, Thailand, Indonesia, …] [truly, wildly, politically, financially, completely, potentially, ...] [his, your, her, its, their, my, our] [Your, Our, Its, My, His, Their, Her] dep label dep label grandparent parent child [PMOD<L> regulation<G> of safety PMOD<L>] ! Condition on dependency context instead of linear, then convert each dependency to a tuple: [Bansal et al., 2014] 90.5 91.5 92.5 McDonald & Pereira 2006 Us UAS ! 10% rel. error reduction over 90-92% parsers Neural Dependency Parser [Chen and Manning, 2014; CS224n] Christopher Manning Model Architecture Input layer x lookup + concat Hidden layer h h = ReLU(Wx + b1) Output layer y y = softmax(Uh + b2) Softmax probabilities cross-entropy error will be back-propagated to the embeddings. Neural Dependency Parser [CS224n] Further developments in transition-based neural dependency parsing This work was further developed and improved by others, including in particular at Google • Bigger, deeper networks with better tuned hyperparameters • Beam search • Global, conditional random field (CRF)-style inference over the decision sequence Leading to SyntaxNet and the Parsey McParseFace model https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html Method UAS LAS (PTB WSJ SD 3.3 Chen & Manning 2014 92.0 89.7 Weiss et al. 2015 93.99 92.05 Andor et al. 2016 94.61 92.79 Neural Constituent Parser [Socher et al., 2013; CS224n] ! Compositional Vector Grammar (CVG) (A, a= ) (B, b= ) (C, c= ) P(1), p(1)= P(2), p(2)= Syntactically Untied Recursive Neural Network = f W(B,C) b c = f W(A,P ) a p(1) (1) p(1) = f ✓ W (B,C) b c #◆ 459 provided by a PCFG since the latter sum to 1 all children. Assuming that node p1 has syntactic catego P1, we compute the second parent vector via: p(2) = f ✓ W (A,P1) a p(1) #◆ . The score of the last parent in this trigram is co puted via: s ⇣ p(2)⌘ = ' v(A,P1)(T p(2) + log P(P2 ! A P 3.4 Parsing with CVGs The above scores (Eq. 4) are used in the search the correct tree for a sentence. The goodness o tree is measured in terms of its score and the CV score of a complete tree is the sum of the scores each node: s(CVG(✓, x, ˆ y)) = X d2N(ˆ y) s ⇣ pd⌘ . The main objective function in Eq 3 include s ⇣ p(1)⌘ = # v(B,C)$T p(1) + log P(P1 ! B C), (4) 459 where P(P1 ! B C) comes from the PCFG. This can be interpreted as the log probability of a discrete-continuous rule application with the fol- lowing factorization: P((P1, p1) ! (B, b)(C, c)) (5) = P(p1 ! b c|P1 ! B C)P(P1 ! B C), Note, however, that due to the continuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children. Assuming that node p1 has syntactic category P1, we compute the second parent vector via: p(2) = f ✓ W (A,P1) a p(1) #◆ . The score of the last parent in this trigram is com details). Since each probability look-up is cheap but computing SU-RNN scores requires a matrix product, we would like to reduce the number of SU-RNN score computations to only those trees that require semantic information. We note that labeled F1 of the Stanford PCFG parser on the test set is 86.17%. However, if one used an oracle to select the best tree from the top 200 trees that it produces, one could get an F1 of 95.46%. We use this knowledge to speed up inference via two bottom-up passes through the parsing chart. During the ﬁrst one, we use only the base PCFG to run CKY dynamic programming through the tree. The k = 200-best parses at the top cell of the chart are calculated using the efﬁcient algorithm of (Huang and Chiang, 2005). Then, the second pass is a beam search with the full CVG model (in- cluding the more expensive matrix multiplications f h SU RNN) Thi b h l id where P(P1 ! B C) comes from the PCFG. This can be interpreted as the log probability of a discrete-continuous rule application with the fol- lowing factorization: P((P1, p1) ! (B, b)(C, c)) (5) = P(p1 ! b c|P1 ! B C)P(P1 ! B C), Note, however, that due to the continuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children. Assuming that node p1 has syntactic category P1, we compute the second parent vector via: p(2) = f ✓ W (A,P1) a p(1) #◆ . The score of the last parent in this trigram is com- puted via: s ⇣ p(2)⌘ = ' v(A,P1)(T p(2) + log P(P2 ! A P1). 3.4 Parsing with CVGs The above scores (Eq 4) are used in the search for details). Since each probability look up is ch but computing SU-RNN scores requires a ma product, we would like to reduce the numbe SU-RNN score computations to only those t that require semantic information. We note labeled F1 of the Stanford PCFG parser on the set is 86.17%. However, if one used an oracl select the best tree from the top 200 trees th produces, one could get an F1 of 95.46%. We use this knowledge to speed up inference two bottom-up passes through the parsing ch During the ﬁrst one, we use only the base PCFG run CKY dynamic programming through the t The k = 200-best parses at the top cell of chart are calculated using the efﬁcient algori of (Huang and Chiang, 2005). Then, the sec pass is a beam search with the full CVG model cluding the more expensive matrix multiplicat of the SU-RNN). This beam search only con ers phrases that appear in the top 200 parses. T is similar to a re-ranking setup but with one m difference: the SU-RNN rule score computatio each node still only has access to its child vect not the whole tree or other global features. T allows the second pass to be very fast. We use This can be interpreted as the log probability of a discrete-continuous rule application with the fol- lowing factorization: P((P1, p1) ! (B, b)(C, c)) (5) = P(p1 ! b c|P1 ! B C)P(P1 ! B C), Note, however, that due to the continuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children. Assuming that node p1 has syntactic category P1, we compute the second parent vector via: p(2) = f ✓ W (A,P1) a p(1) #◆ . The score of the last parent in this trigram is com- puted via: s ⇣ p(2)⌘ = ' v(A,P1)(T p(2) + log P(P2 ! A P1). 3.4 Parsing with CVGs The above scores (Eq. 4) are used in the search for the correct tree for a sentence. The goodness of a tree is measured in terms of its score and the CVG score of a complete tree is the sum of the scores at but computing SU-RNN scores requires a matrix product, we would like to reduce the number of SU-RNN score computations to only those trees that require semantic information. We note that labeled F1 of the Stanford PCFG parser on the test set is 86.17%. However, if one used an oracle to select the best tree from the top 200 trees that it produces, one could get an F1 of 95.46%. We use this knowledge to speed up inference via two bottom-up passes through the parsing chart. During the ﬁrst one, we use only the base PCFG to run CKY dynamic programming through the tree. The k = 200-best parses at the top cell of the chart are calculated using the efﬁcient algorithm of (Huang and Chiang, 2005). Then, the second pass is a beam search with the full CVG model (in- cluding the more expensive matrix multiplications of the SU-RNN). This beam search only consid- ers phrases that appear in the top 200 parses. This is similar to a re-ranking setup but with one main difference: the SU-RNN rule score computation at each node still only has access to its child vectors, not the whole tree or other global features. This allows the second pass to be very fast. We use this setup in our experiments below. Goodness of a tree is measured in terms of its score and the CVG score of a complete tree is the sum of the scores at each node ! "
330,"Model-Based Policy Learning CS 285 Instructor: Sergey Levine UC Berkeley Last time: model-based RL with MPC every N steps The stochastic open-loop case why is this suboptimal? The stochastic closed-loop case Backpropagate directly into the policy? backprop backprop backprop easy for deterministic policies, but also possible for stochastic policy What’s the problem with backprop into policy? big gradients here small gradients here backprop backprop backprop What’s the problem with backprop into policy? backprop backprop backprop What’s the problem with backprop into policy? • Similar parameter sensitivity problems as shooting methods • But no longer have convenient second order LQR-like method, because policy parameters couple all the time steps, so no dynamic programming • Similar problems to training long RNNs with BPTT • Vanishing and exploding gradients • Unlike LSTM, we can’t just “choose” a simple dynamics, dynamics are chosen by nature backprop backprop backprop What’s the solution? • Use derivative-free (“model-free”) RL algorithms, with the model used to generate synthetic samples • Seems weirdly backwards • Actually works very well • Essentially “model-based acceleration” for model-free RL Model-Free Learning With a Model Model-free optimization with a model • Policy gradient might be more stable (if enough samples are used) because it does not require multiplying many Jacobians • See a recent analysis here: • Parmas et al. ‘18: PIPP: Flexible Model-Based Policy Search Robust to the Curse of Chaos Policy gradient: Backprop (pathwise) gradient: Model-based RL via policy gradient What’s a potential problem with this approach? The curse of long model-based rollouts How quickly does error accumulate? How to get away with short rollouts? - huge accumulating error - never see later time steps + much lower error + much lower error + see all time steps - wrong state distribution Model-based RL with short rollouts Dyna-Style Algorithms Model-based RL with short rollouts Model-free optimization with a model Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. Dyna online Q-learning algorithm that performs model-free RL with a model General “Dyna-style” model-based RL recipe + only requires short (as few as one step) rollouts from model + still sees diverse states Model-accelerated off-policy RL dataset of transitions (“replay buffer”) target parameters current parameters buffer of model-based transitions Model-Based Acceleration (MBA) Model-Based Value Expansion (MVE) Model-Based Policy Optimization (MBPO) Gu et al. Continuous deep Q-learning with model-based acceleration. ‘16 Feinberg et al. Model-based value expansion. ’18 Janner et al. When to trust your model: model-based policy optimization. ‘19 + why is this a good idea? - why is this a bad idea? Multi-Step Models & Successor Representations What kind of model do we need to evaluate a policy? The job of the model is to evaluate the policy generate samples (i.e. run the policy) fit a model to estimate return improve the policy (if you can evaluate it, you can make it better) let’s keep it simple (easy to re-derive for action-dependent rewards) What kind of model do we need to evaluate a policy? generate samples (i.e. run the policy) fit a model to estimate return improve the policy (if you can evaluate it, you can make it better) just to ensure it sums to 1 What kind of model do we need to evaluate a policy? generate samples (i.e. run the policy) fit a model to estimate return improve the policy (if you can evaluate it, you can make it better) This is called a successor representation Dayan. Improving Generalisation for Temporal Difference Learning: The Successor Representation. 1993. Successor representations A few issues… ➢Not clear if learning successor representation is easier than model-free RL ➢How to scale to large state spaces? ➢How to extend to continuous state spaces? Successor features so what? If the number of features is much less than the number of states, learning them is much easier! Successor features Using successor features Idea 1: recover a Q-function very quickly Is this the optimal Q-function? Equivalent to one step of policy iteration Better than nothing, but not optimal Using successor features Idea 2: recover many Q-functions Finds the highest reward policy in each state Barreto et al. Successor Features for Transfer in Reinforcement Learning. 2016. Continuous successor representations always zero for any sampled state if states are continuous Continuous successor representations The C-Learning algorithm This is an on policy algorithm Could also derive an off policy algorithm Eysenbach, Salakhutdinov, Levine. C-Learning: Learning to Achieve Goals via Recursive Classification. 2020. "
331,"Exploration (Part 1) CS 285 Instructor: Sergey Levine UC Berkeley What’s the problem? this is easy (mostly) this is impossible Why? Montezuma’s revenge • Getting key = reward • Opening door = reward • Getting killed by skull = nothing (is it good? bad?) • Finishing the game only weakly correlates with rewarding events • We know what to do because we understand what these sprites mean! Put yourself in the algorithm’s shoes • “the only rule you may be told is this one” • Incur a penalty when you break a rule • Can only discover rules through trial and error • Rules don’t always make sense to you • Temporally extended tasks like Montezuma’s revenge become increasingly difficult based on • How extended the task is • How little you know about the rules • Imagine if your goal in life was to win 50 games of Mao… • (and you didn’t know this in advance) Mao Another example Exploration and exploitation • Two potential definitions of exploration problem • How can an agent discover high-reward strategies that require a temporally extended sequence of complex behaviors that, individually, are not rewarding? • How can an agent decide whether to attempt new behaviors (to discover ones with higher reward) or continue to do the best thing it knows so far? • Actually the same problem: • Exploitation: doing what you know will yield highest reward • Exploration: doing things you haven’t done before, in the hopes of getting even higher reward Exploration and exploitation examples • Restaurant selection • Exploitation: go to your favorite restaurant • Exploration: try a new restaurant • Online ad placement • Exploitation: show the most successful advertisement • Exploration: show a different random advertisement • Oil drilling • Exploitation: drill at the best known location • Exploration: drill at a new location Examples from D. Silver lecture notes: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf Exploration is hard Can we derive an optimal exploration strategy? what does optimal even mean? regret vs. Bayes-optimal strategy? more on this later… theoretically tractable theoretically intractable multi-armed bandits (1-step stateless RL problems) contextual bandits (1-step RL problems) small, finite MDPs (e.g., tractable planning, model-based RL setting) large, infinite MDPs, continuous spaces What makes an exploration problem tractable? multi-arm bandits can formalize exploration as POMDP identification contextual bandits policy learning is trivial even with POMDP small, finite MDPs can frame as Bayesian model identification, reason explicitly about value of information large or infinite MDPs optimal methods don’t work …but can take inspiration from optimal methods in smaller settings use hacks Bandits What’s a bandit anyway? the drosophila of exploration problems How can we define the bandit? • solving the POMDP yields the optimal exploration strategy • but that’s overkill: belief state is huge! • we can do very well with much simpler strategies expected reward of best action (the best we can hope for in expectation) actual reward of action actually taken Three Classes of Exploration Methods How can we beat the bandit? • Variety of relatively simple strategies • Often can provide theoretical guarantees on regret • Variety of optimal algorithms (up to a constant factor) • But empirical performance may vary… • Exploration strategies for more complex MDP domains will be inspired by these strategies expected reward of best action (the best we can hope for in expectation) actual reward of action actually taken Optimistic exploration some sort of variance estimate intuition: try each arm until you are sure it’s not great number of times we picked this action Probability matching/posterior sampling this is a model of our bandit • This is called posterior sampling or Thompson sampling • Harder to analyze theoretically • Can work very well empirically See: Chapelle & Li, “An Empirical Evaluation of Thompson Sampling.” Information gain Bayesian experimental design: Information gain example Example bandit algorithm: Russo & Van Roy “Learning to Optimize via Information-Directed Sampling” don’t bother taking actions if you won’t learn anything don’t take actions that you’re sure are suboptimal General themes • Most exploration strategies require some kind of uncertainty estimation (even if it’s naïve) • Usually assumes some value to new information • Assume unknown = good (optimism) • Assume sample = truth • Assume information gain = good UCB: Thompson sampling: Info gain: Why should we care? • Bandits are easier to analyze and understand • Can derive foundations for exploration methods • Then apply these methods to more complex MDPs • Not covered here: • Contextual bandits (bandits with state, essentially 1-step MDPs) • Optimal exploration in small MDPs • Bayesian model-based reinforcement learning (similar to information gain) • Probably approximately correct (PAC) exploration Exploration in Deep RL Recap: classes of exploration methods in deep RL • Optimistic exploration: • new state = good state • requires estimating state visitation frequencies or novelty • typically realized by means of exploration bonuses • Thompson sampling style algorithms: • learn distribution over Q-functions or policies • sample and act according to sample • Information gain style algorithms • reason about information gain from visiting new states Optimistic exploration in RL UCB: “exploration bonus” can we use this idea with MDPs? + simple addition to any RL algorithm - need to tune bonus weight The trouble with counts But wait… what’s a count? Uh oh… we never see the same thing twice! But some states are more similar than others Fitting generative models Exploring with pseudo-counts Bellemare et al. “Unifying Count-Based Exploration…” What kind of bonus to use? UCB: Lots of functions in the literature, inspired by optimal methods for bandits or small MDPs MBIE-EB (Strehl & Littman, 2008): BEB (Kolter & Ng, 2009): this is the one used by Bellemare et al. ‘16 Does it work? Bellemare et al. “Unifying Count-Based Exploration…” What kind of model to use? need to be able to output densities, but doesn’t necessarily need to produce great samples opposite considerations from many popular generative models in the literature (e.g., GANs) Bellemare et al.: “CTS” model: condition each pixel on its top- left neighborhood Other models: stochastic neural networks, compression length, EX2 More Novelty-Seeking Exploration Counting with hashes What if we still count states, but in a different space? Tang et al. “#Exploration: A Study of Count-Based Exploration” Implicit density modeling with exemplar models need to be able to output densities, but doesn’t necessarily need to produce great samples Fu et al. “EX2: Exploration with Exemplar Models…” Can we explicitly compare the new state to past states? Intuition: the state is novel if it is easy to distinguish from all previous seen states by a classifier Implicit density modeling with exemplar models Fu et al. “EX2: Exploration with Exemplar Models…” Heuristic estimation of counts via errors need to be able to output densities, but doesn’t necessarily need to produce great samples …and doesn’t even need to output great densities …just need to tell if state is novel or not! low novelty high novelty Heuristic estimation of counts via errors low novelty high novelty - also related to information gain, which we’ll discuss next time! this will be in HW5! Burda et al. Exploration by random network distillation. 2018. Posterior Sampling in Deep RL Posterior sampling in deep RL Thompson sampling: Osband et al. “Deep Exploration via Bootstrapped DQN” What do we sample? How do we represent the distribution? since Q-learning is off-policy, we don’t care which Q-function was used to collect data Bootstrap Osband et al. “Deep Exploration via Bootstrapped DQN” Why does this work? Osband et al. “Deep Exploration via Bootstrapped DQN” Exploring with random actions (e.g., epsilon-greedy): oscillate back and forth, might not go to a coherent or interesting place Exploring with random Q-functions: commit to a randomized but internally consistent strategy for an entire episode + no change to original reward function - very good bonuses often do better Information Gain in Deep RL Reasoning about information gain (approximately) Info gain: Generally intractable to use exactly, regardless of what is being estimated! Reasoning about information gain (approximately) Generally intractable to use exactly, regardless of what is being estimated A few approximations: (Schmidhuber ‘91, Bellemare ‘16) intuition: if density changed a lot, the state was novel (Houthooft et al. “VIME”) Reasoning about information gain (approximately) VIME implementation: Houthooft et al. “VIME” Reasoning about information gain (approximately) VIME implementation: Houthooft et al. “VIME” + appealing mathematical formalism - models are more complex, generally harder to use effectively Approximate IG: Exploration with model errors Stadie et al. 2015: • encode image observations using auto-encoder • build predictive model on auto-encoder latent states • use model error as exploration bonus Schmidhuber et al. (see, e.g. “Formal Theory of Creativity, Fun, and Intrinsic Motivation): • exploration bonus for model error • exploration bonus for model gradient • many other variations Many others! low novelty high novelty Recap: classes of exploration methods in deep RL • Optimistic exploration: • Exploration with counts and pseudo-counts • Different models for estimating densities • Thompson sampling style algorithms: • Maintain a distribution over models via bootstrapping • Distribution over Q-functions • Information gain style algorithms • Generally intractable • Can use variational approximation to information gain Suggested readings Schmidhuber. (1992). A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers. Stadie, Levine, Abbeel (2015). Incentivizing Exploration in Reinforcement Learning with Deep Predictive Models. Osband, Blundell, Pritzel, Van Roy. (2016). Deep Exploration via Bootstrapped DQN. Houthooft, Chen, Duan, Schulman, De Turck, Abbeel. (2016). VIME: Variational Information Maximizing Exploration. Bellemare, Srinivasan, Ostroviski, Schaul, Saxton, Munos. (2016). Unifying Count- Based Exploration and Intrinsic Motivation. Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman, De Turck, Abbeel. (2016). #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. Fu, Co-Reyes, Levine. (2017). EX2: Exploration with Exemplar Models for Deep Reinforcement Learning. "
332,"Exploration (Part 2) CS 285 Instructor: Sergey Levine UC Berkeley Recap: what’s the problem? this is easy (mostly) this is impossible Why? Unsupervised learning of diverse behaviors What if we want to recover diverse behavior without any reward function at all? Why? Learn skills without supervision, then use them to accomplish goals Learn sub-skills to use with hierarchical reinforcement learning Explore the space of possible behaviors An Example Scenario training time: unsupervised How can you prepare for an unknown future goal? In this lecture… Definitions & concepts from information theory Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills In this lecture… Definitions & concepts from information theory Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills Some useful identities Some useful identities Information theoretic quantities in RL quantifies coverage can be viewed as quantifying “control authority” in an information-theoretic way In this lecture… Definitions & concepts from information theory Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills An Example Scenario training time: unsupervised How can you prepare for an unknown future goal? Learn without any rewards at all (but there are many other choices) Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 12 Learn without any rewards at all Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 13 Learn without any rewards at all Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 14 How do we get diverse goals? Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 15 How do we get diverse goals? Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 16 How do we get diverse goals? Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 17 goals get higher entropy due to Skew-Fit goal final state How do we get diverse goals? Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 18 Reinforcement learning with imagined goals Nair*, Pong*, Bahl, Dalal, Lin, L. Visual Reinforcement Learning with Imagined Goals. ’18 Dalal*, Pong*, Lin*, Nair, Bahl, Levine. Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. ‘19 imagined goal RL episode 19 In this lecture… Definitions & concepts from information theory Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills Aside: exploration with intrinsic motivation Can we use this for state marginal matching? Lee*, Eysenbach*, Parisotto*, Xing, Levine, Salakhutdinov. Efficient Exploration via State Marginal Matching See also: Hazan, Kakade, Singh, Van Soest. Provably Efficient Maximum Entropy Exploration MaxEnt on actions variants of SMM State marginal matching for exploration much better coverage! Lee*, Eysenbach*, Parisotto*, Xing, Levine, Salakhutdinov. Efficient Exploration via State Marginal Matching See also: Hazan, Kakade, Singh, Van Soest. Provably Efficient Maximum Entropy Exploration In this lecture… Definitions & concepts from information theory Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills Is state entropy really a good objective? 25 more or less the same thing Gupta, Eysenbach, Finn, Levine. Unsupervised Meta-Learning for Reinforcement Learning See also: Hazan, Kakade, Singh, Van Soest. Provably Efficient Maximum Entropy Exploration In this lecture… Definitions & concepts from information theory A distribution-matching formulation of reinforcement learning Learning without a reward function by reaching goals A state distribution-matching formulation of reinforcement learning Is coverage of valid states a good exploration objective? Beyond state covering: covering the space of skills Learning diverse skills task index Intuition: different skills should visit different state-space regions Eysenbach, Gupta, Ibarz, Levine. Diversity is All You Need. Reaching diverse goals is not the same as performing diverse tasks not all behaviors can be captured by goal-reaching Diversity-promoting reward function Policy(Agent) Discriminator(D) Skill (z) Environment Action State Predict Skill Eysenbach, Gupta, Ibarz, Levine. Diversity is All You Need. Eysenbach, Gupta, Ibarz, Levine. Diversity is All You Need. Cheetah Ant Examples of learned tasks Mountain car A connection to mutual information Eysenbach, Gupta, Ibarz, Levine. Diversity is All You Need. See also: Gregor et al. Variational Intrinsic Control. 2016 "
333,"Offline Reinforcement Learning CS 285 Instructor: Sergey Levine UC Berkeley 2 The generalization gap this is done many times Mnih et al. ‘13 Schulman et al. ’14 & ‘15 Levine*, Finn*, et al. ‘16 enormous gulf 3 What makes modern machine learning work? 4 Can we develop data-driven RL methods? on-policy RL off-policy RL offline reinforcement learning Levine, Kumar, Tucker, Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. ‘20 big datasets from past interaction train for many epochs occasionally get more data 5 What does offline RL mean? on-policy RL off-policy RL offline reinforcement learning generally not known 6 Types of offline RL problems not necessarily obvious what this means 7 How is this even possible? 1. Find the “good stuff” in a dataset full of good and bad behaviors 2. Generalization: good behavior in one place may suggest good behavior in another place 3. “Stitching”: parts of good behaviors can be recombined What do we expect offline RL methods to do? Bad intuition: it’s like imitation learning Though it can be shown to be provably better than imitation learning even with optimal data, under some structural assumptions! See: Kumar, Hong, Singh, Levine. Should I Run Offline Reinforcement Learning or Behavioral Cloning? Better intuition: get order from chaos “Macro-scale” stitching But this is just the clearest example! “Micro-scale” stitching: If we have algorithms that properly perform dynamic programming, we can take this idea much further and get near-optimal policies from highly suboptimal data 8 A vivid example 9 Singh, Yu, Yang, Zhang, Kumar, Levine. COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning. ‘20 blocked by object blocked by drawer closed drawer training task 10 Why should we care? this is done many times Does it work? Kalashnikov, Irpan, Pastor, Ibarz, Herzong, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, Levine. QT-Opt: Scalable Deep Reinforcement Learning of Vision-Based Robotic Manipulation Skills live data collection stored data from all past experiments training buffers Bellman updaters training threads Does it work? Kalashnikov, Irpan, Pastor, Ibarz, Herzong, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, Levine. QT-Opt: Scalable Deep Reinforcement Learning of Vision-Based Robotic Manipulation Skills Method Offline QT-Opt Finetuned QT-Opt Dataset 580k offline 580k offline + 28k online Success 87% 96% Failure 13% 4% Why is offline RL hard? Kumar, Fu, Tucker, Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. NeurIPS ‘19 amount of data log scale (massive overestimation) how well it does how well it thinks it does (Q-values) 13 14 Why is offline RL hard? Levine, Kumar, Tucker, Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. ‘20 Fundamental problem: counterfactual queries Training data What the policy wants to do Is this good? Bad? How do we know if we didn’t see it in the data? Online RL algorithms don’t have to handle this, because they can simply try this action and see what happens Offline RL methods must somehow account for these unseen (“out-of-distribution”) actions, ideally in a safe way …while still making use of generalization to come up with behaviors that are better than the best thing seen in the data! 15 Distribution shift in a nutshell Kumar, Fu, Tucker, Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. NeurIPS ‘19 Example empirical risk minimization (ERM) problem: usually we are not worried – neural nets generalize well! 16 Where do we suffer from distribution shift? Kumar, Fu, Tucker, Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. NeurIPS ‘19 target value behavior policy how well it does how well it thinks it does (Q-values) 17 Issues with generalization are not corrected online RL setting offline RL setting Existing challenges with sampling error and function approximation error in standard RL become much more severe in offline RL Batch RL via Importance Sampling Offline RL with policy gradients Offline RL with policy gradients Estimating the returns To avoid exponentially exploding importance weights, we must use value function estimation! We’ll see how to do this shortly, but first let’s conclude our discussion of importance sampling The doubly robust estimator this is exponential! Jiang, N. and Li, L. (2015). Doubly robust off-policy value evaluation for reinforcement learning. doubly robust estimation (bandit case) model or function approximator model or function approximator Marginalized importance sampling Additional readings: importance sampling Classic work on importance sampled policy gradients and return estimation: Precup, D. (2000). Eligibility traces for off-policy policy evaluation. Peshkin, L. and Shelton, C. R. (2002). Learning from scarce experience. Doubly robust estimators and other improved importance-sampling estimators: Jiang, N. and Li, L. (2015). Doubly robust off-policy value evaluation for reinforcement learning. Thomas, P. and Brunskill, E. (2016). Data-efficient off-policy policy evaluation for reinforcement learning. Analysis and theory: Thomas, P. S., Theocharous, G., and Ghavamzadeh, M. (2015). High-confidence off-policy evaluation. Marginalized importance sampling: Hallak, A. and Mannor, S. (2017). Consistent on-line off-policy evaluation. Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2019). Off-policy policy gradient with state distribution correction. Additional readings in our offline RL survey: Section 3.1, 3.2, 3.3, 3.4: https://arxiv.org/abs/2005.01643 Batch RL via Linear Fitted Value Functions Offline value function estimation How have people thought about it before? Extend existing ideas for approximate dynamic programming and Q-learning to offline setting Derive tractable solutions with simple (e.g., linear) function approximators How are people thinking about it now? Derive approximate solutions with highly expressive function approximators (e.g., deep nets) The primary challenge turns out to be distributional shift generally not concerned with distributional shift before (maybe it was not such a big problem with linear models) We’ll discuss some older offline/batch RL methods next for completeness Warmup: linear models material adapted from Ron Parr total # of features total # of states Can we do (offline) model-based RL in feature space? 1. Estimate the reward 2. Estimate the transitions 3. Recover the value function 4. Improve the policy vector of rewards for all state-action tuples but we’ll talk about sample-based setting soon! estimated feature-space transition matrix real transition matrix (on states) Recovering the value function material adapted from Ron Parr total # of features total # of states but wait – do we even need the model? substitute after a bit of algebra… this is called least-squares temporal difference (LSTD) Doing it all with samples material adapted from Ron Parr total # of features total # of states total # of sample Everything else works exactly the same way, only now we have some sampling error Improving the policy material adapted from Ron Parr 1. Estimate the reward 2. Estimate the transitions 3. Recover the value function 4. Improve the policy or just do these together with LSTD! That’s not going to work for offline RL! Least-squares policy iteration (LSPI) material adapted from Ron Parr Main idea: replace LSTD with LSTDQ – LSTD but for Q-functions total # of features total # of states total # of features typically replicated for each action total # of states-action tuples LSPI: What’s the issue? how well it does how well it thinks it does (Q-values) target value behavior policy In general, all approximate dynamic programming (e.g., fitted value/Q iteration) methods will suffer from action distributional shift, and we must fix it! "
334,"Reinforcement Learning Theory Basics CS 285 Instructor: Sergey Levine UC Berkeley 2 What questions do we ask in RL theory? Lots of different questions! But here are a few common ones: But there are many others! We’ll focus on these types of questions today 3 What kinds of assumptions do we make? Effective analysis is very hard in RL without strong assumptions The trick is to make assumptions that admit interesting conclusions without divorcing us (too much) from reality Exploration: Performance of RL methods is greatly complicated by exploration – how likely are we to find (potentially sparse) rewards? Theoretical guarantees typically address worst case performance, and worst case exploration is extremely hard Learning: If we somehow “abstract away” exploration, how many samples do we need to effectively learn a model or value function that results in good performance? What’s the point? 1. Prove that our RL algorithms will work perfectly every time Usually not possible with current deep RL methods, which are often not even guaranteed to converge 2. Understand how errors are affected by problem parameters Do larger discounts work better than smaller ones? If we want half the error, do we need 2x the samples? 4x? something else? Usually we use precise theory to get imprecise qualitative conclusions about how various factors influence the performance of RL algorithms under strong assumptions, and try to make the assumptions reasonable enough that these conclusions are likely to apply to real problems (but they are not guaranteed to apply to real problems) Don’t take someone seriously if they say their RL algorithm has “provable guarantees” – the assumptions are always unrealistic, and theory is at best a rough guide to what might happen 5 Some basic sample complexity analysis Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar 6 Concentration inequalities Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar 7 Concentration inequalities Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar 8 A few useful lemmas Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar 9 A few useful lemmas Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar difference in probabilities true value evaluation operator 10 A few useful lemmas Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar triangle inequality 11 Putting them together… Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar technically need to use the union bound here to account for probabilities 12 What does this mean? Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar more samples = lower error error grows quadratically in the horizon each backup “accumulates” error 13 Some simple implications… Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar same policy What About Model-Free RL? 15 Analyzing fitted Q-iteration Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar Bellman operator approximate Bellman operator Note: these are not models, this is the effect of averaging together transitions in the data! which norm? “sampling error” “approximation error” 16 Let’s analyze sampling error Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar estimation error of continuous random variable just use Hoeffding’s inequality directly! 17 Let’s analyze sampling error Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar using union bound 18 Let’s analyze approximation error Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar This is a strong assumption! we’ll analyze the exact backup operator for now, but we’ll come back to approximate backups later! 19 Let’s analyze approximation error Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar approximation error scales with “horizon” 20 Putting it together Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar “sampling error” “approximation error” sampling approximation 21 What does it all mean? Based on RL Theory Textbook. Agarwal, Jiang, Kakade, Sun. https://rltheorybook.github.io & slides by Aviral Kumar “sampling error” “approximation error” error “compounds” with horizon, over iterations and due to sampling so far we needed strong (infinity norm) assumptions more advanced results can be derived with p-norms under some distribution: "
335,"Offline Reinforcement Learning Part 2 CS 285 Instructor: Sergey Levine UC Berkeley 2 Offline Reinforcement Learning on-policy RL off-policy RL offline reinforcement learning generally not known 3 Where do we suffer from distribution shift? Kumar, Fu, Tucker, Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. NeurIPS ‘19 target value behavior policy how well it does how well it thinks it does (Q-values) 4 How do prior methods address this? Levine, Kumar, Tucker, Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. ‘20 This solves distribution shift, right? No more erroneous values? “policy constraint” method very old idea (but it had no single name?) Todorov et al. [passive dynamics in linearly- solvable MDPs] Kappen et al. [KL-divergence control, etc.] trust regions, covariant policy gradients, natural policy gradients, etc. used in some form in recent papers: Jaques et al. ‘19 (“Way Off Policy…”) Fujimoto et al. ‘18 (“Off Policy…”) Fox et al. ‘15 (“Taming the Noise…”) Wu et al. ‘19 (“Behavior Regularized…”) Kumar et al. ‘19 (“Stabilizing…”) Issue 1: we usually don’t know the behavior policy • human-provided data • data from hand-designed controller • data from many past RL runs • all of the above Issue 2: this is both too pessimistic and not pessimistic enough 5 Explicit policy constraint methods What kinds of constraints can we use? + easy to implement (more on this later) - not necessarily what we want unreliable OOD values reliable values best policy for KL constraint best in-support policy - significantly more complex to implement + much closer to what we really want For more information, see: Levine, Kumar, Tucker, Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. ‘20 Kumar, Fu, Tucker, Levine. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. ‘19 Wu, Tucker, Nachum. Behavior Regularized Offline Reinforcement Learning. `19 6 Explicit policy constraint methods How do we implement constraints? 1. Modify the actor objective easy to compute and differentiate for Gaussian or categorical policies Lagrange multiplier 2. Modify the reward function simple modification to directly penalize divergence also accounts for future divergence See: Wu, Tucker, Nachum. Behavior Regularized Offline Reinforcement Learning. `19 generally, the best modern offline RL methods do not do either of these things 7 Implicit policy constraint methods straightforward to show via duality approximate via weighted max likelihood! samples from dataset critic can be used to give us this Peng*, Kumar*, Levine. Advantage-Weighted Regression. ‘19 See also: Peters et al. (REPS) Rawlik et al. (“psi-learning”) …many follow-ups Nair, Dalal, Gupta, Levine. Accelerating Online Reinforcement Learning with Offline Datasets. ‘20 8 Implicit policy constraint methods Peng*, Kumar*, Levine. Advantage-Weighted Regression. ‘19 Nair, Dalal, Gupta, Levine. Accelerating Online Reinforcement Learning with Offline Datasets. ‘20 9 Can we also avoid all OOD actions in the Q update? Kostrikov, Nair, Levine. Offline Reinforcement Learning with Implicit Q-Learning. ‘21 just another neural network distribution is induced by actions only value of best policy supported by data could another loss give us this? 10 Implicit Q-learning (IQL) Kostrikov, Nair, Levine. Offline Reinforcement Learning with Implicit Q-Learning. ‘21 Q-learning with implicit policy improvement Now we can do value function updates without ever risking out-of-distribution actions! We’ll see results soon, but first let’s talk about Option 2… Conservative Q-Learning 12 how well it does how well it thinks it does (Q-values) regular objective term to push down big Q-values true Q-function Conservative Q-learning (CQL) always pushes Q-values down push up on (s, a) samples in data Kumar, Zhou, Tucker, Levine. Conservative Q-Learning for Offline Reinforcement Learning. ‘20 Conservative Q-learning (CQL) Kumar, Zhou, Tucker, Levine. Conservative Q-Learning for Offline Reinforcement Learning. ‘20 Conservative Q-learning (CQL) Kumar, Zhou, Tucker, Levine. Conservative Q-Learning for Offline Reinforcement Learning. ‘20 Conservative Q-learning (CQL) regularization maximum entropy regularization Model-Based Offline RL How does model-based RL work? the model answers “what if” questions what goes wrong when we can’t collect more data? these states are OOD …so the model’s predictions are invalid MOPO: Model-Based Offline Policy Optimization uncertainty penalty Yu*, Thomas*, Yu, Ermon, Zou, Levine, Finn, Ma. MOPO: Model-Based Offline Policy Optimization. ‘20 See also: Kidambi et al., MOReL : Model-Based Offline Reinforcement Learning. ’20 (concurrent) MOPO: Theoretical Analysis Yu*, Thomas*, Yu, Ermon, Zou, Levine, Finn, Ma. MOPO: Model-Based Offline Policy Optimization. ‘20 some implications: improves over behavior policy quantifies “optimality gap” in terms of model error COMBO: Conservative Model-Based RL Yu, Kumar, Rafailov, Rajeswaran, Levine, Finn. COMBO: Conservative Offline Model-Based Policy Optimization. 2021. Basic idea: just like CQL minimizes Q-value of policy actions, we can minimize Q-value of model state-action tuples state-action tuples from the model Intuition: if the model produces something that looks clearly different from real data, it’s easy for the Q-function to make it look bad Trajectory Transformer Janner, Li, Levine. Reinforcement Learning as One Big Sequence Modeling Problem. 2021. Trajectory Transformer making accurate predictions to hundreds of steps Basic ideas: Why does this work? generating high-probability trajectories avoids out-of-distribution states & actions using a really big model works well in offline mode (lots of compute, captures complex behavior policies) The model: How to do control: Summary, Applications, Open Questions 23 Which offline RL algorithm do I use? If you want to only train offline… Conservative Q-learning Implicit Q-learning + just one hyperparameter + well understood and widely tested + more flexible (offline + online) - more hyperparameters If you want to only train offline and finetune online Advantage-weighted actor-critic (AWAC) + widely used and well tested Implicit Q-learning + seems to perform much better! If you have a good way to train models in your domain COMBO + similar properties as CQL, but benefits from models - not always easy to train a good model in your domain! Trajectory transformer + very powerful and effective models - extremely computationally expensive to train and evaluate 24 The power of offline RL standard real-world RL process offline RL process 1. instrument the task so that we can run RL safety mechanisms autonomous collection rewards, resets, etc. 2. wait a long time for online RL to run 3. change the algorithm in some small way 4. throw it all in the garbage and start over for the next task 1. collect initial dataset human-provided scripted controller baseline policy all of the above 2. Train a policy offline 3. change the algorithm in some small way 4. collect more data, add to growing dataset 5. keep the dataset and use it again for the next project! 25 Offline RL in robotic manipulation: MT-Opt, AMs 12 different tasks Thousands of objects Months of data collection Kalashnikov, Irpan, Pastor, Ibarz, Herzong, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, Levine. QT-Opt: Scalable Deep Reinforcement Learning of Vision-Based Robotic Manipulation Skills Kalashnikov, Varley, Chebotar, Swanson, Jonschkowski, Finn, Levine, Hausman. MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale. 2021. New hypothesis: could we learn these tasks without rewards using goal-conditioned RL? reuse the same exact data Actionable Models: Offline RL with Goals Chebotar, Hausman, Lu, Xiao, Kalashnikov, Varley, Irpan, Eysenbach, Julian, Finn, Levine. Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills. 2021. 26 No reward function at all, task is defined entirely using a goal image! Uses a conservative offline RL method designed for goal-reaching, based on CQL Works very well as an unsupervised pretraining objective! 1. Train goal-conditioned Q- function with offline RL 2. Finetune with a task reward and limited data 27 More examples Kahn, Abbeel, Levine. BADGR: An Autonomous Self-Supervised Learning- Based Navigation System. 2020. Early 2020: Greg Kahn collects 40 hours of robot navigation data Shah, Eysenbach, Kahn, Rhinehart, Levine. ViNG: Learning Open-World Navigation with Visual Goals. 2020. Late 2020: Dhruv Shah uses it to build goal-conditioned navigation system Early 2021: Dhruv Shah uses the same dataset to train an exploration system Shah, Eysenbach, Rhinehart, Levine. RECON: Rapid Exploration for Open-World Navigation with Latent Goal Models. 2020. Takeaways, conclusions, future directions 28 1. Collect a dataset using any policy or mixture of policies 2. Run offline RL on this dataset to learn a policy 3. Deploy the policy in the real world current offline RL algorithms “the dream” “the gap” • An offline RL workflow • Supervised learning workflow: train/test split • Offline RL workflow: ??? • Statistical guarantees • Biggest challenge: distributional shift/counterfactuals • Can we make any guarantees? • Scalable methods, large-scale applications • Dialogue systems • Data-driven navigation and driving A starting point: Kumar, Singh, Tian, Finn, Levine. A Workflow for Offline Model-Free Robotic Reinforcement Learning. CoRL 2021 "
336,"Variational Inference and Generative Models CS 285 Instructor: Sergey Levine UC Berkeley 1. Probabilistic latent variable models 2. Variational inference 3. Amortized variational inference 4. Generative models: variational autoencoders • Goals • Understand latent variable models in deep learning • Understand how to use (amortized) variational inference Today’s Lecture Probabilistic models Latent variable models mixture element Latent variable models in general “easy” distribution (e.g., Gaussian) “easy” distribution (e.g., Gaussian) “easy” distribution (e.g., conditional Gaussian) Latent variable models in RL conditional latent variable models for multi-modal policies latent variable models for model-based RL Other places we’ll see latent variable models Mombaur et al. ‘09 Muybridge (c. 1870) Ziebart ‘08 Li & Todorov ‘06 Using RL/control + variational inference to model human behavior Using generative models and variational inference for exploration How do we train latent variable models? Estimating the log-likelihood Variational Inference The variational approximation The variational approximation Jensen’s inequality A brief aside… Entropy: Intuition 1: how random is the random variable? Intuition 2: how large is the log probability in expectation under itself high low this maximizes the first part this also maximizes the second part (makes it as wide as possible) A brief aside… KL-Divergence: Intuition 1: how different are two distributions? Intuition 2: how small is the expected log probability of one distribution under another, minus entropy? why entropy? this maximizes the first part this also maximizes the second part (makes it as wide as possible) The variational approximation The variational approximation How do we use this? how? What’s the problem? Amortized Variational Inference What’s the problem? Amortized variational inference how do we calculate this? Amortized variational inference look up formula for entropy of a Gaussian can just use policy gradient! What’s wrong with this gradient? The reparameterization trick Is there a better way? most autodiff software (e.g., TensorFlow) will compute this for you! Another way to look at it… this often has a convenient analytical form (e.g., KL-divergence for Gaussians) Reparameterization trick vs. policy gradient • Policy gradient • Can handle both discrete and continuous latent variables • High variance, requires multiple samples & small learning rates • Reparameterization trick • Only continuous latent variables • Very simple to implement • Low variance Example Models The variational autoencoder Using the variational autoencoder Conditional models Examples 1. collect data 2. learn embedding of image & dynamics model (jointly) 3. run iLQG to learn to reach image of goal a type of variational autoencoder with temporally decomposed latent state! Local models with images Local models with images variational autoencoder with stochastic dynamics We’ll see more of this for… Mombaur et al. ‘09 Muybridge (c. 1870) Ziebart ‘08 Li & Todorov ‘06 Using RL/control + variational inference to model human behavior Using generative models and variational inference for exploration "
337,"Reframing Control as an Inference Problem CS 285 Instructor: Sergey Levine UC Berkeley Today’s Lecture 1. Does reinforcement learning and optimal control provide a reasonable model of human behavior? 2. Is there a better explanation? 3. Can we derive optimal control, reinforcement learning, and planning as probabilistic inference? 4. How does this change our RL algorithms? 5. (next lecture) We’ll see this is crucial for inverse reinforcement learning • Goals: • Understand the connection between inference and control • Understand how specific RL algorithms can be instantiated in this framework • Understand why this might be a good idea Optimal Control as a Model of Human Behavior Mombaur et al. ‘09 Muybridge (c. 1870) Ziebart ‘08 Li & Todorov ‘06 optimize this to explain the data What if the data is not optimal? some mistakes matter more than others! behavior is stochastic but good behavior is still the most likely A probabilistic graphical model of decision making no assumption of optimal behavior! Why is this interesting? • Can model suboptimal behavior (important for inverse RL) • Can apply inference algorithms to solve control and planning problems • Provides an explanation for why stochastic behavior might be preferred (useful for exploration and transfer learning) Inference = planning how to do inference? Control as Inference Inference = planning how to do inference? Backward messages which actions are likely a priori (assume uniform for now) A closer look at the backward pass “optimistic” transition (not a good idea!) Backward pass summary The action prior remember this? what if the action prior is not uniform? (“soft max”) can always fold the action prior into the reward! uniform action prior can be assumed without loss of generality Policy computation Policy computation with value functions Policy computation summary • Natural interpretation: better actions are more probable • Random tie-breaking • Analogous to Boltzmann exploration • Approaches greedy policy as temperature decreases Forward messages Forward/backward message intersection states with high probability of reaching goal states with high probability of being reached from initial state (with high reward) state marginals Forward/backward message intersection states with high probability of reaching goal states with high probability of being reached from initial state (with high reward) state marginals Li & Todorov, 2006 Summary 1. Probabilistic graphical model for optimal control 2. Control = inference (similar to HMM, EKF, etc.) 3. Very similar to dynamic programming, value iteration, etc. (but “soft”) Control as Variational Inference The optimism problem “optimistic” transition (not a good idea!) Addressing the optimism problem we want this but not this! Control via variational inference The variational lower bound Optimizing the variational lower bound Optimizing the variational lower bound Backward pass summary - variational Summary variants: For more details, see: Levine. (2018). Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. Algorithms for RL as Inference Q-learning with soft optimality Policy gradient with soft optimality Ziebart et al. ‘10 “Modeling Interaction via the Principle of Maximum Causal Entropy” policy entropy intuition: often referred to as “entropy regularized” policy gradient combats premature entropy collapse turns out to be closely related to soft Q-learning: see Haarnoja et al. ‘17 and Schulman et al. ‘17 Policy gradient vs Q-learning can ignore (baseline) off-policy correction descent (vs ascent) Benefits of soft optimality • Improve exploration and prevent entropy collapse • Easier to specialize (finetune) policies for more specific tasks • Principled approach to break ties • Better robustness (due to wider coverage of states) • Can reduce to hard optimality as reward magnitude increases • Good model for modeling human behavior (more on this later) Review • Reinforcement learning can be viewed as inference in a graphical model • Value function is a backward message • Maximize reward and entropy (the bigger the rewards, the less entropy matters) • Variational inference to remove optimism • Soft Q-learning • Entropy-regularized policy gradient generate samples (i.e. run the policy) fit a model to estimate return improve the policy Example Methods Stochastic models for learning control • How can we track both hypotheses? Stochastic energy-based policies Haarnoja*, Tang*, Abbeel, L., Reinforcement Learning with Deep Energy-Based Policies. ICML 2017 Stochastic energy-based policies provide pretraining 1.Q-function update Update Q-function to evaluate current policy: 2. Update policy Update the policy with gradient of information projection: This converges to . In practice, only take one gradient step on this objective 3. Interact with the world, collect more data Soft actor-critic update messages fit variational distribution Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, L. Soft Actor-Critic Algorithms and Applications. ‘18 0 min 12 min 30 min 2 hours Training time sites.google.com/view/composing-real-world-policies/ Haarnoja, Pong, Zhou, Dalal, Abbeel, L. Composable Deep Reinforcement Learning for Robotic Manipulation. ‘18 After 2 hours of training sites.google.com/view/composing-real-world-policies/ Haarnoja, Pong, Zhou, Dalal, Abbeel, L. Composable Deep Reinforcement Learning for Robotic Manipulation. ‘18 Haarnoja, Zhou, Ha, Tan, Tucker, L. Learning to Walk via Deep Reinforcement Learning. ‘19 Haarnoja, Zhou, Ha, Tan, Tucker, L. Learning to Walk via Deep Reinforcement Learning. ‘19 Soft optimality suggested readings • Todorov. (2006). Linearly solvable Markov decision problems: one framework for reasoning about soft optimality. • Todorov. (2008). General duality between optimal control and estimation: primer on the equivalence between inference and control. • Kappen. (2009). Optimal control as a graphical model inference problem: frames control as an inference problem in a graphical model. • Ziebart. (2010). Modeling interaction via the principle of maximal causal entropy: connection between soft optimality and maximum entropy modeling. • Rawlik, Toussaint, Vijaykumar. (2013). On stochastic optimal control and reinforcement learning by approximate inference: temporal difference style algorithm with soft optimality. • Haarnoja*, Tang*, Abbeel, L. (2017). Reinforcement learning with deep energy based models: soft Q-learning algorithm, deep RL with continuous actions and soft optimality • Nachum, Norouzi, Xu, Schuurmans. (2017). Bridging the gap between value and policy based reinforcement learning. • Schulman, Abbeel, Chen. (2017). Equivalence between policy gradients and soft Q-learning. • Haarnoja, Zhou, Abbeel, L. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. • Levine. (2018). Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review "
338,"Inverse Reinforcement Learning CS 285 Instructor: Sergey Levine UC Berkeley Today’s Lecture 1. So far: manually design reward function to define a task 2. What if we want to learn the reward function from observing an expert, and then use reinforcement learning? 3. Apply approximate optimality model from last time, but now learn the reward! • Goals: • Understand the inverse reinforcement learning problem definition • Understand how probabilistic models of behavior can be used to derive inverse reinforcement learning algorithms • Understand a few practical inverse reinforcement learning algorithms we can use Optimal Control as a Model of Human Behavior Mombaur et al. ‘09 Muybridge (c. 1870) Ziebart ‘08 Li & Todorov ‘06 optimize this to explain the data Why should we worry about learning rewards? The imitation learning perspective Standard imitation learning: • copy the actions performed by the expert • no reasoning about outcomes of actions Human imitation learning: • copy the intent of the expert • might take very different actions! Why should we worry about learning rewards? The reinforcement learning perspective what is the reward? Inverse reinforcement learning Infer reward functions from demonstrations by itself, this is an underspecified problem many reward functions can explain the same behavior A bit more formally “forward” reinforcement learning inverse reinforcement learning reward parameters Feature matching IRL still ambiguous! Feature matching IRL & maximum margin Issues: • Maximizing the margin is a bit arbitrary • No clear model of expert suboptimality (can add slack variables…) • Messy constrained optimization problem – not great for deep learning! Further reading: • Abbeel & Ng: Apprenticeship learning via inverse reinforcement learning • Ratliff et al: Maximum margin planning Optimal Control as a Model of Human Behavior Mombaur et al. ‘09 Muybridge (c. 1870) Ziebart ‘08 Li & Todorov ‘06 A probabilistic graphical model of decision making no assumption of optimal behavior! Learning the Reward Function Learning the optimality variable reward parameters The IRL partition function Estimating the expectation Estimating the expectation The MaxEnt IRL algorithm Why MaxEnt? Ziebart et al. 2008: Maximum Entropy Inverse Reinforcement Learning Approximations in High Dimensions • MaxEnt IRL so far requires… • Solving for (soft) optimal policy in the inner loop • Enumerating all state-action tuples for visitation frequency and gradient • To apply this in practical problem settings, we need to handle… • Large and continuous state and action spaces • States obtained via sampling only • Unknown dynamics What’s missing so far? Unknown dynamics & large state/action spaces Assume we don’t know the dynamics, but we can sample, like in standard RL More efficient sample-based updates Importance sampling Update reward using samples & demos generate policy samples from π update π w.r.t. reward policy π reward r guided cost learning algorithm policy π (Finn et al. ICML ’16) slides adapted from C. Finn IRL and GANs It looks a bit like a game… policy π Generative Adversarial Networks Goodfellow et al. ‘14 Isola et al. ‘17 Arjovsky et al. ‘17 Zhu et al. ‘17 Inverse RL as a GAN Finn*, Christiano* et al. “A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models.” Inverse RL as a GAN Finn*, Christiano* et al. “A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models.” Generalization via inverse RL demonstration reproduce behavior under different conditions what can we learn from the demonstration to enable better transfer? need to decouple the goal from the dynamics! policy = reward + dynamics Fu et al. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning Can we just use a regular discriminator? Ho & Ermon. Generative adversarial imitation learning. Pros & cons: + often simpler to set up optimization, fewer moving parts - discriminator knows nothing at convergence - generally cannot reoptimize the “reward” IRL as adversarial optimization Generative Adversarial Imitation Learning Guided Cost Learning robot attempt classifier Ho & Ermon, NIPS 2016 Hausman, Chebotar, Schaal, Sukhatme, Lim Peng, Kanazawa, Toyer, Abbeel, Levine Finn et al., ICML 2016 robot attempt reward function actually the same thing! Suggested Reading on Inverse RL Classic Papers: Abbeel & Ng ICML ’04. Apprenticeship Learning via Inverse Reinforcement Learning. Good introduction to inverse reinforcement learning Ziebart et al. AAAI ’08. Maximum Entropy Inverse Reinforcement Learning. Introduction to probabilistic method for inverse reinforcement learning Modern Papers: Finn et al. ICML ’16. Guided Cost Learning. Sampling based method for MaxEnt IRL that handles unknown dynamics and deep reward functions Wulfmeier et al. arXiv ’16. Deep Maximum Entropy Inverse Reinforcement Learning. MaxEnt inverse RL using deep reward functions Ho & Ermon NIPS ’16. Generative Adversarial Imitation Learning. Inverse RL method using generative adversarial networks Fu, Luo, Levine ICLR ‘18. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning "
339,"Transfer and Multi-Task Learning CS 285 Instructor: Sergey Levine UC Berkeley What’s the problem? this is easy (mostly) this is impossible Why? Montezuma’s revenge • Getting key = reward • Opening door = reward • Getting killed by skull = bad Montezuma’s revenge • We know what to do because we understand what these sprites mean! • Key: we know it opens doors! • Ladders: we know we can climb them! • Skull: we don’t know what it does, but we know it can’t be good! • Prior understanding of problem structure can help us solve complex tasks quickly! Can RL use the same prior knowledge as us? • If we’ve solved prior tasks, we might acquire useful knowledge for solving a new task • How is the knowledge stored? • Q-function: tells us which actions or states are good • Policy: tells us which actions are potentially useful • some actions are never useful! • Models: what are the laws of physics that govern the world? • Features/hidden states: provide us with a good representation • Don’t underestimate this! Aside: the representation bottleneck slide adapted from E. Schelhamer, “Loss is its own reward” Transfer learning terminology transfer learning: using experience from one set of tasks for faster learning and better performance on a new task in RL, task = MDP! source domain target domain “shot”: number of attempts in the target domain 0-shot: just run a policy trained in the source domain 1-shot: try the task once few shot: try the task a few times How can we frame transfer learning problems? 1. Forward transfer: train on one task, transfer to a new task a) Transferring visual representations & domain adaptation b) Domain adaptation in reinforcement learning c) Randomization 2. Multi-task transfer: train on many tasks, transfer to a new task a) Sharing representations and layers across tasks in multi-task learning b) Contextual policies c) Optimization challenges for multi-task learning d) Algorithms 3. Transferring models and value functions a) Model-based RL as a mechanism for transfer b) Successor features & representations No single solution! Survey of various recent research papers Forward Transfer Pretraining + Finetuning The most popular transfer learning method in (supervised) deep learning! What issues are we likely to face? Domain shift: representations learned in the source domain might not work well in the target domain Difference in the MDP: some things that are possible to do in the source domain are not possible to do in the target domain Finetuning issues: if pretraining & finetuning, the finetuning process may still need to explore, but optimal policy during finetuning may be deterministic! Domain adaptation in computer vision train here do well here (same network) correct answer incorrect answer Invariance assumption: everything that is different between domains is irrelevant Is this true? can we force this layer to be invariant to domain? domain classifier: guess domain from z reversed gradient How do we apply this idea in RL? adversarial loss causes internal CNN features to be indistinguishable for sim and real simulated images real images Tzeng*, Devin*, et al., “Adapting Visuomotor Representations with Weak Pairwise Constraints” Domain adaptation in RL for dynamics? Why is invariance not enough when the dynamics don’t match? When might this not work? Eysenbach et al., “Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers” What if we can also finetune? 1. RL tasks are generally much less diverse • Features are less general • Policies & value functions become overly specialized 2. Optimal policies in fully observed MDPs are deterministic • Loss of exploration at convergence • Low-entropy policies adapt very slowly to new settings Finetuning with maximum-entropy policies How can we increase diversity and entropy? policy entropy Act as randomly as possible while collecting high rewards! Example: pre-training for robustness Learning to solve a task in all possible ways provides for more robust transfer! Example: pre-training for diversity Haarnoja*, Tang*, et al. “Reinforcement Learning with Deep Energy-Based Policies” Domain adaptation: suggested readings Tzeng, Hoffman, Zhang, Saenko, Darrell. Deep Domain Confusion: Maximizing for Domain Invariance. 2014. Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, Lempitsky. Domain- Adversarial Training of Neural Networks. 2015. Tzeng*, Devin*, et al., Adapting Visuomotor Representations with Weak Pairwise Constraints. 2016. Eysenbach et al., Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers. 2020. …and many many others! Finetuning: suggested readings Finetuning via MaxEnt RL: Haarnoja*, Tang*, et al. (2017). Reinforcement Learning with Deep Energy-Based Policies. Andreas et al. Modular multitask reinforcement learning with policy sketches. 2017. Florensa et al. Stochastic neural networks for hierarchical reinforcement learning. 2017. Kumar et al. One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. 2020 …and many many others! Forward Transfer with Randomization What if we can manipulate the source domain? • So far: source domain (e.g., empty room) and target domain (e.g., corridor) are fixed • What if we can design the source domain, and we have a difficult target domain? • Often the case for simulation to real world transfer EPOpt: randomizing physical parameters train test adapt training on single torso mass training on model ensemble unmodeled effects ensemble adaptation Rajeswaran et al., “EPOpt: Learning robust neural network policies…” Preparing for the unknown: explicit system ID Yu et al., “Preparing for the Unknown: Learning a Universal Policy with Online System Identification” model parameters (e.g., mass) system identification RNN policy Another example Xue Bin Peng et al., “Sim-to-Real Transfer of Robotic Control with Dynamics Randomization” CAD2RL: randomization for real-world control Sadeghi et al., “CAD2RL: Real Single-Image Flight without a Single Real Image” also called domain randomization Sadeghi et al., “CAD2RL: Real Single-Image Flight without a Single Real Image” CAD2RL: randomization for real-world control Sadeghi et al., “CAD2RL: Real Single-Image Flight without a Single Real Image” Randomization for manipulation Tobin, Fong, Ray, Schneider, Zaremba, Abbeel James, Davison, Johns Source domain randomization and domain adaptation suggested readings Rajeswaran, et al. (2017). EPOpt: Learning Robust Neural Network Policies Using Model Ensembles. Yu et al. (2017). Preparing for the Unknown: Learning a Universal Policy with Online System Identification. Sadeghi & Levine. (2017). CAD2RL: Real Single Image Flight without a Single Real Image. Tobin et al. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. James et al. (2017). Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task. Methods that also incorporate domain adaptation together with randomization: Bousmalis et al. (2017). Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. Rao et al. (2017). RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real. … and many many others! Multi-Task Transfer Can we learn faster by learning multiple tasks? learn learn learn learn learn learn Multi-task learning can: - Accelerate learning of all tasks that are learned together - Provide better pre-training for down-stream tasks Can we solve multiple tasks at once? Multi-task RL corresponds to single-task RL in a joint MDP etc. sample etc.sample etc.sample MDP 0 MDP 1 MDP 2 pick MDP randomly in first state What is difficult about this? • Gradient interference: becoming better on one task can make you worse on another • Winner-take-all problem: imagine one task starts getting good – algorithm is likely to prioritize that task (to increase average expected reward) at the expensive of others In practice, this kind of multi-task RL is very challening Actor-mimic and policy distillation Distillation for Multi-Task Transfer Parisotto et al. “Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning” some other details (e.g., feature regression objective) – see paper (just supervised learning/distillation) analogous to guided policy search, but for transfer learning -> see model-based RL slides Combining weak policies into a strong policy supervised learning trajectory-centric RL local neural net policies For details, see: “Divide and Conquer Reinforcement Learning” Distillation Transfer Results Parisotto et al. “Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning” How does the model know what to do? • So far: what to do is apparent from the input (e.g., which game is being played) • What if the policy can do multiple things in the same environment? Contextual policies e.g., do dishes or laundry images: Peng, van de Panne, Peters Contextual policies e.g., do dishes or laundry images: Peng, van de Panne, Peters will discuss more in the context of meta-learning! Transferring Models and Value Functions The problem setting Common setting: • Autonomous car learns how to drive to a few destinations, and then has to navigate to a new one • A kitchen robot learns to cook many different recipes, and then has to cook a new one in the same kitchen What is the best object to transfer? Model: very simple to transfer, since the model is already (in principle) independent of the reward Value function: not straightforward to transfer by itself, since the value function entangles the dynamics and reward, but possible with a decomposition - what kind of “dynamics relevant” information does a value function contain? Policy: possible to do with contextual policies, but otherwise tricky, because the policy contains the least dynamics information Transferring models source domain target domain why might zero-shot transfer not always work? Transferring value functions Not so fast! Value functions couple dynamics, rewards, and policies! Is this really such a good idea? Yes, because of linearity Key observation: the value function is linear in the reward function Successor representations & successor features this is no longer linear! Successor representations & successor features Aside: successor representations Dayan. Improving generalization for temporal difference learning: The successor representation. 1993. Transfer with successor features For more details, see: Barreto et al., Successor Features for Transfer in Reinforcement Learning Recap 1. Forward transfer: train on one task, transfer to a new task a) Transferring visual representations & domain adaptation b) Domain adaptation in reinforcement learning c) Randomization 2. Multi-task transfer: train on many tasks, transfer to a new task a) Sharing representations and layers across tasks in multi-task learning b) Contextual policies c) Optimization challenges for multi-task learning d) Algorithms 3. Transferring models and value functions a) Model-based RL as a mechanism for transfer b) Successor features & representations No single solution! Survey of various recent research papers "
34,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, Artzi-FitzGerald-Zettlemoyer CCG tutorial, JurafskyMartin-SLP3, others) Lecture 5: Semantic Parsing (Semantic Role Labeling, Lambda-Calculus, CCG, DCS, etc.) Announcements ! Next week: 1st half of class is Colloquium Speaker: Claire Cardie (Cornell) – 10-11am (please reach FB141 by 9.55am) -- then we will continue class from 11am-12.30pm in our FB008 room ! Chapter section summary were due Sunday Sep24 midnight ! Coding-HW1 (on word vector training+evaluation_ +visualization) has been released (and details emailed) last week – due Oct5 midnight (2 weeks total)! ! TA Yixin Nie’s office hours: 2.30-3.30pm Wednesdays (moved to 2nd floor reading room) Semantic Role Labeling Semantic Role Labeling (SRL) Semantic Role Labeling (SRL) ! Characterize clauses as relations with roles: ! Want to more than which NP is the subject (but not much more): ! Relations like subject are syntactic, relations like agent or message are semantic ! Typical pipeline: ! Parse, then label roles ! Almost all errors locked in by parser ! Really, SRL is quite a lot easier than parsing ! Role-based relations for the different clauses in the sentence: ! More semantic relations (e.g., agent, reason, message) than just subject/object style syntactic roles ! Typical traditional pipelines involves POS-tagging and parsing, and then features extracted on those (plus NER, etc.)…but then several errors caused by wrong parse! Semantic Role Labeling (SRL) SRL Example PropBank vs. FrameNet PropBank / FrameNet ! FrameNet: roles shared between verbs ! PropBank: each verb has it’s own roles ! PropBank has each verb get its own roles, whereas FrameNet shares roles between verbs (e.g., argue and banter in figure below) ! PropBank more convenient w.r.t. being layered over Treebank parses (and hence more coverage) PropBank Roles ! Based on Dowty, 1991: roles are verb-sense specific in PropBank (role definitions depend on specific verb and relation to other roles) ! Each verb sense has numbered arguments e.g., ARG-0, ARG-1, etc. ! ARG-0 is usually PROTO-AGENT ! ARG-1 is usually PROTO-PATIENT ! ARG-2 is usually benefactive, instrument, attribute ! ARG-3 is usually start point, benefactive, instrument, attribute ! ARG-4 is usually end point (e.g., for move or push style verbs) (ARG-2,3,4 onwards not very consistent and highly depend on specific verb and its sense in the sentence, hence labeling of PropBank is tricky) PropBank Example 1 PropBank Example PropBank Example 2 PropBank Example PropBank Example 3 PropBank Example Shared Arguments Shared Arguments Simple SRL Algo A)simple)modern)algorithm given these features, where N is the number of potential semantic roles p extra NONE role for non-role constituents. Most standard classiﬁcation algo have been used (logistic regression, SVM, etc). Finally, for each test sentenc labeled, the classiﬁer is run on each relevant constituent. We give more de the algorithm after we discuss features. function SEMANTICROLELABEL(words) returns labeled tree parse PARSE(words) for each predicate in parse do for each node in parse do featurevector EXTRACTFEATURES(node, predicate, parse) CLASSIFYNODE(node, featurevector, parse) Figure 22.4 A generic semantic-role-labeling algorithm. CLASSIFYNODE is a 1-of siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labe such as FrameNet or PropBank. Features for Semantic Role Labeling 42 SRL Features Features Headword6of6constituent Examiner Headword6POS NNP Voice6of6the6clause Active Subcategorization of6pred VP6K>6VBD6NP6PP 45 10 CHAPTER 22 • SEMANTIC ROLE LABELING S NP-SBJ = ARG0 VP DT NNP NNP NNP The San Francisco Examiner VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP issued DT JJ NN IN NP a special edition around NN NP-TMP noon yesterday Figure 22.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line shows the path feature NP""S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner. • The headword of the constituent, Examiner. The headword of a constituent can be computed with standard head rules, such as those given in Chapter 11 in Fig. ??. Certain headwords (e.g., pronouns) place strong constraints on the possible semantic roles they are likely to ﬁll. • The headword part of speech of the constituent, NNP. • The path in the parse tree from the constituent to the predicate. This path is marked by the dotted line in Fig. 22.5. Following Gildea and Jurafsky (2000), we can use a simple linear representation of the path, NP""S#VP#VBD. "" and # represent upward and downward movement in the tree, respectively. The path is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate. • The voice of the clause in which the constituent appears, in this case, active (as contrasted with passive). Passive sentences tend to have strongly different linkings of semantic roles to surface form than do active ones. Named6Entity6type6of6constit ORGANIZATION First6and6last6words6of6constit The,6Examiner Linear6position,clause re:6predicate before Path-based Features for SRL Path Features Some SRL Results Results Features: ! Path from target to filler ! Filler’s syntactic type, headword, case ! Target’s identity ! Sentence voice, etc. ! Lots of other second-order features Gold vs parsed source trees ! SRL is fairly easy on gold trees ! Harder on automatic parses ! So major feature categories in traditional feature-based SRL models were: ! Headword, syntactic type, case, etc. of candidate node/ constituent ! Linear and tree path from predicate target to node ! Active vs. passive voice ! Second order and higher order features ! Accuracy for such feature-based SRL models then highly depends on accuracy of underlying parse tree! ! So quite high SRL results when using ground-truth parses ! Much lower results with automatically-predicted parses! Schematic of Frame Semantics (FrameNet) Schematic)of)Frame)Semantics in which the evoked frame is selected for each predicate; and argument identiﬁcation (Section 6), in which arguments to each frame are identiﬁed and labeled with a role from that frame. Experiments demonstrating favorable performance to the previous state of the art on SemEval 2007 and FrameNet data sets are described in each section. Some novel aspects of our approach include a latent-variable model (Section 5.2) and a semi- supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of words not in the FrameNet lexicon; a uniﬁed model for ﬁnding and labeling arguments Figure 1 An example sentence from the annotations released as part of FrameNet 1.5 with three targets marked in bold. Note that this annotation is partial because not all potential targets have been annotated with predicate-argument structures. Each target has its evoked semantic frame marked above it, enclosed in a distinct shape or border style. For each frame, its semantic roles are shown enclosed within the same shape or border style, and the spans fulﬁlling the roles are connected to the latter using dotted lines. For example, manner evokes the CONDUCT frame, and has the AGENT and MANNER roles fulﬁlled by Austria and most un-Viennese, respectively. 35 Figure6from6Das6et6al6(2014) [Das et al., 2014] PropBank vs. FrameNet Representations ameNet and)PropBank representatio Computational Linguistics Volume 40, Number 1 (a) (b) Compositional Semantics Compositional Semantics I: Logic form ! Logic-form based (lambda calculus), Semantic Parsing ! Useful for Q&A, IE, grounding, comprehension tasks (summarization, reading tasks) ! A lot of focus has been on KB-based Question Answering in this direction (where input-output training data is question-answer pairs, and latent intermediate representation is the question’s semantic parse, which is ‘executed’ against the KB to get the answer) Question Answering ! Initial approaches to Q&A: pattern matching, pattern learning, query rewriting, information extraction ! Next came a large-scale, open-domain IE system like IBM Watson Articles [Ferrucci et al., 2010] Deep Q&A: Semantic Parsing ! Complex, free-form, multi-clause questions Deep Q&A: Semantic Parsing ! Complex, free-form, multi-clause questions Semantic Parsing: Logic forms ! Parsing with logic (booleans, individuals, functions) and lambda forms Sentence loves(john,mary) Noun Phrase john Verb Phrase λx.loves(x,mary) Name john Verb λy.λx.loves(x,y) Noun Phrase Name mary “John” john “loves” λy.λx.loves(x,y) “Mary” mary Parse tree with associated semantics [Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Poon and Domingos, 2009; Artzi and Zettlemoyer, 2011, 2013; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Poon 2013; Berant and Liang, 2014; Iyyer et al., 2014] Truth-Conditional Semantics Truth-Conditional Semantics ! Linguistic expressions: ! “Bob sings” ! Logical translations: ! sings(bob) ! Could be p_1218(e_397) ! Denotation: ! [[bob]] = some specific person (in some context) ! [[sings(bob)]] = ??? ! Types on translations: ! bob : e (for entity) ! sings(bob) : t (for truth-value) S NP Bob bob VP sings λy.sings(y) sings(bob) ! Examples like “Bob sings” ! Logical translation of this will be something like: sings(bob) ! Types on these translations are entities (e) and truth-values (t), e.g.: bob: e sings(bob): t Truth-Conditional Semantics Truth-Conditional Semantics ! Linguistic expressions: ! “Bob sings” ! Logical translations: ! sings(bob) ! Could be p_1218(e_397) ! Denotation: ! [[bob]] = some specific person (in some context) ! [[sings(bob)]] = ??? ! Types on translations: ! bob : e (for entity) ! sings(bob) : t (for truth-value) S NP Bob bob VP sings λy.sings(y) sings(bob) ! For verbs (and verb phrases), sings combines with bob to produce sings(bob) ! In general, we use lambda-calculus or λ-calculus, i.e., a notation for functions whose arguments have not yet been filled/resolved/satisfied ! λx.sings(x) ! This is a ‘predicate’, i.e., a function which take an entity (type e) and produces a truth value (type t), denoted as e ! t Compositional Semantics Compositional Semantics ! So now we have meanings for the words ! How do we know how to combine words? ! Associate a combination rule with each grammar rule: ! S : β(α) →NP : α VP : β (function application) ! VP : λx . α(x) ∧β(x) →VP : α and : ∅ VP : β (intersection) ! Example: S NP VP Bob VP and sings VP dances bob λy.sings(y) λz.dances(z) λx.sings(x) ∧dances(x) [λx.sings(x) ∧dances(x)](bob) sings(bob) ∧dances(bob) ! Now after we have these meanings for words, we want to combine them into meaning for phrases and sentences ! For this, we associate a combination rule with each grammar rule of the parse tree, e.g.: S: β(α) ! NP: α VP: β (function application) VP: λx . α(x) Λβ(x) ! VP: α and:  VP: β (intersection) Transitive Verbs & Quantifiers ! Transitive verbs example is ‘like’ predicate: ! λx.λy.likes(y,x) ! These are two-place predicates hence e!(e!t) ! Whereas ‘likes Amy’ = λy.likes(y,amy) is just a one-place predicate because x has been satisfied/resolved Transitive Verbs & Quantifiers Other Cases ansitive verbs: likes : λx.λy.likes(y,x) Two-place predicates of type e→(e→t). likes Amy : λy.likes(y,Amy) is just like a one-place predicate. uantifiers: What does “Everyone” mean here? Everyone : λf.∀x.f(x) Mostly works, but some problems ! Have to change our NP/VP rule. ! Won’t work for “Amy likes everyone.” “Everyone likes someone.” This gets tricky quickly! S NP VP Everyone VBP NP Amy likes λx.λy.likes(y,x) λy.likes(y,amy) amy λf.∀x.f(x) [λf.∀x.f(x)](λy.likes(y,amy)) ∀x.likes(x,amy) ! What about the ‘everyone’ quantifier, e.g., “Everyone likes Amy”? ! Everyone = λf.x.f(x) ! See example figure on how this works ! ! Gets tricky for examples like: “Amy likes everyone” and “Everyone likes someone” Indefinites Can’t be right! ! ∃x : waffle(x) ∧ate(bob,x) ! What does the translation of “a” have to be? ! What about “the”? ! What about “every”? S NP VP Bob VBD NP a waffle ate ! If we say “Bob ate a waffle” and “Amy ate a waffle”, then using: ate(bob, waffle) ate(amy, waffle) ! Doesn’t seem correct for ‘a waffle’ ! More correct seems to use ‘there exists’ operator: ! x: waffle(x) Λate(bob, x) ! And what about ‘the’ and ‘every’? Tense and Events ! We need event variables because just verbs don’t get us far! ! Example: “Bob sang” ! sang(bob)? ! e: singing(e) Λagent(e, bob) Λ(time(e) < now) ! Hence, these event variable e help us represent complex tense and aspect structures: ! Example: “Bob had been singing when Mary coughed” ! e, e’: singing(e) Λagent(e, bob) Λ coughing(e’) Λagent(e’, mary) Λ (start(e) < start(e’) Λend(e) = end(e’)) Λ (time(e’) < now) Adverbs Adverbs ! What about adverbs? ! “Bob sings terribly” ! terribly(sings(bob))? ! (terribly(sings))(bob)? ! ∃e present(e) ∧ type(e, singing) ∧ agent(e,bob) ∧ manner(e, terrible) ? ! It’s really not this simple.. S NP VP Bob VBP ADVP terribly sings ! Example: “Bob sings terribly” ! terribly(sings(bob))? ! (terribly(sings))(bob)? ! e: present(e) Λtype(e, singing) Λ agent(e, bob) Λmanner(e, terrible)? ! Gets tricky pretty quickly… CCG Parsing CCGParsing  Combinatory CategorialGrammar  Fully(monoͲ) lexicalizedgrammar  Categoriesencode argumentsequences  Verycloselyrelated tothelambda calculus(morelater)  Canhavespurious ambiguities(why?) ! Combinatory Categorial Grammars: ! Each category encodes an argument sequence (fwd/bwd slashes specify argument order/direction) ! Closely related to lambda calculus ! Captures both syntactic and semantic info ! Naturally allows meaning representation and semantic parsing CCG Parsing Mapping to LF: Zettlemoyer & Collins 05/07 Given training examples like: Input: List one way flights to Prague. Output: λx.flight(x)∧one_way(x)∧to(x,PRG) Challenging Learning Problem: • Derivations (or parses) are not annotated • Approach: [Zettlemoyer & Collins 2005] • Learn a lexicon and parameters for a weighted Combinatory Categorial Grammar (CCG) [Slides from Luke Zettlemoye ! Given training examples with paired sentences/questions and their logical-form lambda calculus, ! This is a tricky learning problem because the derivations are not annotated, so we learn lexicon and parameters for a weighted CCG (e.g., based on [Zettlemoyer and Collins, 2005]) CCG Lexicon CCG Lexicon Words Category flights N : λx.flight(x) to (N\N)/NP : λx.λf.λy.f(x) ∧to(y,x) Prague NP : PRG New York city NP : NYC … … Combinator Rules Parsing Rules (Combinators) Application • X/Y : f Y : a => X : f(a) • Y : a X\Y : f => X : f(a) Composition • X/Y : f Y/Z : g => X/Z : λx.f(g(x)) • Z\Y : f X\Y : g => X\Z : λx.f(g(x)) Additional rules: • Type Raising • Crossed Composition Y \ Z ! Application Unary Rules: ! Composition Rules: ! Type Raising ! Crossed Composition Parsing Rules (Combinators) Application • X/Y : f Y : a => X : f(a) • Y : a X\Y : f => X : f(a) Composition • X/Y : f Y/Z : g => X/Z : λx.f(g(x)) • Z\Y : f X\Y : g => X\Z : λx.f(g(x)) Additional rules: • Type Raising • Crossed Composition CCG Parsing Example CCG Parsing to Prague flights N\N λ λ λ λf.λ λ λ λx.f(x)∧ ∧ ∧ ∧to(x,PRG) N λ λ λ λx.flight(x)∧ ∧ ∧ ∧to(x,PRG) Show me N λ λ λ λx.flight(x) (N\N)/NP λ λ λ λy.λ λ λ λf.λ λ λ λx.f(y)∧ ∧ ∧ ∧to(x,y) NP PRG S/N λ λ λ λf.f S λ λ λ λx.flight(x)∧ ∧ ∧ ∧to(x,PRG) Weighted CCG Weighted CCG Given a log-linear model with a CCG lexicon Λ, a feature vector f, and weights w. ! The best parse is: Where we consider all possible parses y for the sentence x given the lexicon Λ. y* = argmax y w ⋅f (x,y) ! Given a log-linear model with a CCG lexicon L, a feature vector f , and weights w , the best parse is ! Where y is the set of all parses for sentence x based on lexicon L Lexicon Problem and Factored Lexicons ! Lexicon is key component of CCG ! But same word often paired with many different categories ! Difficult to learn with limited sentence-logicform data ! Factored Lexicons is one solution: lexical entries share info; decomposition leads to more compact lexicons [Artzi-FitzGerald-Zettlemoyer CCG tutorial] Weak Supervision ! Instead of relying on sentence-logicform pairs as training data, we can learn from query-answer pairs ! Logical forms are latent, and we can check which one gets the correct answer on being executed against a knowledge base (KB) [Clarke et al. 2010; Liang et al. 2011] Weak Supervision Learning from Query Answers What is the largest state that borders Texas? New Mexico argmax(λx.state(x) ^ border(x, TX), λy.size(y)) argmax(λx.river(x) ^ in(x, TX), λy.size(y)) New Mexico Rio Grande [Clarke et al. 2010; Liang et al. 2011] Weak Supervision [Chen and Mooney 2011; Kim and Mooney 2012;Artzi and Zettlemoyer 2013b] ! Learning from Instruction-Demonstration Pairs Weak Supervision [Artzi and Zettlemoyer 2011] ! Learning from Conversation Logs Weak Supervision Learning from Conversation Logs SYSTEM how can I help you ? (OPEN_TASK) USER i ‘ d like to ﬂy to new york SYSTEM ﬂying to new york . (CONFIRM: from(ﬂ, ATL)) leaving what city ? (ASK: λx.from(fl,x)) USER from boston on june seven with american airlines SYSTEM ﬂying to new york . (CONFIRM: to(ﬂ, NYC)) what date would you like to depart boston ? (ASK: λx.date(fl,x)∧to(fl, BOS)) USER june seventh [CONVERSATION CONTINUES] Other Semantic Parsing Ideas ! Various recent ideas/extensions: ! Dependency-based compositional semantics (DCS) ! Bootstrapping w/ conversations ! On-the-fly ontology matching ! Question answering on Freebase ! Paraphrasing ! Unsupervised SP (clustering lambda forms) ! Grounded USP (via databases) [Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Poon and Domingos, 2009; Artzi and Zettlemoyer, 2011, 2013; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Poon 2013; Berant and Liang, 2014; Iyyer et al., 2014; Yao and Van Durne, 2014] Dependency-based Compositional Semantics (DCS) [Liang et al., 2013] UC Berkeley jordan@cs.berkeley.edu UC Berkeley klein@cs.berkeley.edu ring begins by rms, but train- m this mapping notation of the aper, we learn via latent log- automatically n tackling this we introduce a hich highlights yntax and efﬁ- s. On two stan- arks (GEO and e highest pub- iring no anno- (parameters) (world) ✓ w x z y (question) (logical form) (answer) state with the largest area x1 x1 1 1 c argmax area state ⇤⇤ Alaska z ⇠p✓(z | x) y = JzKw Semantic Parsing Evaluation Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Dependency-based Compositional Semantics (DCS) [Liang et al., 2013] ut lack the full ex- . n of this work is dependency-based which is both sim- e logical forms in desirable for two dependency trees, ing; and (ii) eval- s computationally EM-like algorithm GEO and JOBS orms all existing d logical forms. Relations R j j0 (join) E (extract) ⌃ (aggregate) Q (quantify) Xi (execute) C (compare) Table 1: Possible relations appearing on the edges of a DCS tree. Here, j, j0 2 {1, 2, . . . } and i 2 {1, 2, . . . }⇤. z.p 2 P and (ii) a sequence of edges z.e1, . . . , z.em, each edge e consisting of a relation e.r 2 R (see Table 1) and a child tree e.c 2 Z. We write a DCS tree z as hp; r1 : c1; . . . ; rm : cmi. Figure 2(a) shows an example of a DCS tree. Al- though a DCS tree is a logical form, note that it looks lik i d d i h di i Dependency-based Compositional Semantics (DCS) [Liang et al., 2013] Example: major city in California z = hcity; 1 1:hmajori ; 1 1:hloc; 2 1:hCAiii 1 1 1 1 major 2 1 CA loc city λc 9m 9` 9s . city(c) ^ major(m)^ loc(`) ^ CA(s)^ c1 = m1 ^ c1 = `1 ^ `2 = s1 (a) DCS tree (b) Lambda calculus formula (c) Denotation: JzKw = {SF, LA, . . . } Figure 2: (a) An example of a DCS tree (written in both the mathematical and graphical notation). Each node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree z with only join relations en- codes a constraint satisfaction problem. (c) The denota- tion of z is the set of consistent values for the root node. for each child i, the ji-th component of v must equal number of major cities 1 2 1 1 ⌃ 1 1 major city ⇤⇤ count ⇤⇤ (a) Counting Figure 3: Example relation (⌃) to (a) c take the average ov where s is all maj Dependency-based Compositional Semantics (DCS) [Liang et al., 2013] California borders which states? x1 x1 2 1 1 1 CA e ⇤⇤ state border ⇤⇤ Alaska borders no states. x1 x1 2 1 1 1 AK q no state border ⇤⇤ Some river traverses every city. x12 x12 2 1 1 1 q some river q every city traverse ⇤⇤ x21 x21 2 1 1 1 q some river q every city traverse ⇤⇤ (narrow) (wide) city traversed by no rivers x12 x12 1 2 e ⇤⇤ 1 1 q no river traverse city ⇤⇤ (a) Extraction (e) (b) Quantiﬁcation (q) (c) Quantiﬁer ambiguity (q, q) (d) Quantiﬁcation (q, e) state bordering the most states x12 x12 1 1 e ⇤⇤ 2 1 c argmax state border state ⇤⇤ state bordering more states than Texas x12 x12 1 1 e ⇤⇤ 2 1 c 3 1 TX more state border state ⇤⇤ state bordering the largest state 1 1 2 1 x12 x12 1 1 e ⇤⇤ c argmax size state ⇤⇤ border state x12 x12 1 1 e ⇤⇤ 2 1 1 1 c argmax size state border state ⇤⇤ (absolute) (relative) Every state’s largest city is major. x1 x1 x2 x2 1 1 1 1 2 1 q every state loc c argmax size city major ⇤⇤ (e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantiﬁcation+Superlative (q, c) Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge These trees reﬂect the Semantic Parsing on Freebase Mapping questions to answers via latent logical forms. To narrow down the logical predicate space, they use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. [Berant et al., 2013] that on ally earn hal- the for m in ping dge we onal ates. pite Occidental College, Columbia University Execute on Database Type.University u Education.BarackObama Type.University Education BarackObama Which college did Obama go to ? alignment alignment bridging Figure 1: Our task is to map questions to answers via la- tent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Free- Semantic Parsing via Paraphrasing For each candidate logical form (red), they generate canonical utterances (purple). The model is trained to paraphrase the input utterance (green) into the canonical utterances associated with the correct denotation (blue). [Berant and Liang, 2014] What party did Clay establish? paraphrase model What political party founded by Henry Clay? ... What event involved the people Henry Clay? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utter- ances (in purple). The model is trained to paraphrase the in- put utterance (in green) into the canonical utterances associ- ated with the correct denotation (in blue). Semantic Parsing via Ontology Matching The main challenge in semantic parsing is the mismatch between language and the knowledge base. (a) Traditional: map utterances directly to logical forms, (b) Kwiatkowski et al. (2013): map utterance to intermediate, underspecified logical form, then perform ontology matching to handle the mismatch, (c) Berant and Liang (2014): generate intermediate, canonical text utterances for logical forms, then use paraphrase models. [Kwiatkowski et al., 2013; Berant and Liang, 2014] utterance underspeciﬁed logical form canonical utterance logical form ontology matching paraphrase direct (traditional) (Kwiatkowski et al. 2013) (this work) Figure 2: The main challenge in semantic parsing is cop- ing with the mismatch between language and the KB. (a) Traditionally, semantic parsing maps utterances directly to logical forms. (b) Kwiatkowski et al. (2013) map the utter- ance to an underspeciﬁed logical form, and perform ontology matching to handle the mismatch. (c) We approach the prob- lem in the other direction generating canonical utterances for FREE917 917 ques tors. On improvem state-of-th current b code of o at ht softwar 2 Setup Our task base K, an pairs {(xi maps new (Berant and Liang, 2014) "
340,"Meta-Learning & Transfer Learning CS 285 Instructor: Sergey Levine UC Berkeley What’s the problem? this is easy (mostly) this is impossible Why? Montezuma’s revenge • Getting key = reward • Opening door = reward • Getting killed by skull = bad Montezuma’s revenge • We know what to do because we understand what these sprites mean! • Key: we know it opens doors! • Ladders: we know we can climb them! • Skull: we don’t know what it does, but we know it can’t be good! • Prior understanding of problem structure can help us solve complex tasks quickly! Can RL use the same prior knowledge as us? • If we’ve solved prior tasks, we might acquire useful knowledge for solving a new task • How is the knowledge stored? • Q-function: tells us which actions or states are good • Policy: tells us which actions are potentially useful • some actions are never useful! • Models: what are the laws of physics that govern the world? • Features/hidden states: provide us with a good representation • Don’t underestimate this! Transfer learning terminology transfer learning: using experience from one set of tasks for faster learning and better performance on a new task in RL, task = MDP! source domain target domain “shot”: number of attempts in the target domain 0-shot: just run a policy trained in the source domain 1-shot: try the task once few shot: try the task a few times How can we frame transfer learning problems? 1. Forward transfer: learn policies that transfer effectively a) Train on source task, then run on target task (or finetune) b) Relies on the tasks being quite similar! 2. Multi-task transfer: train on many tasks, transfer to a new task a) Sharing representations and layers across tasks in multi-task learning b) New task needs to be similar to the distribution of training tasks 3. Meta-learning: learn to learn on many tasks a) Accounts for the fact that we’ll be adapting to a new task during training! Pretraining + Finetuning The most popular transfer learning method in (supervised) deep learning! What issues are we likely to face? Domain shift: representations learned in the source domain might not work well in the target domain Difference in the MDP: some things that are possible to do in the source domain are not possible to do in the target domain Finetuning issues: if pretraining & finetuning, the finetuning process may still need to explore, but optimal policy during finetuning may be deterministic! Domain adaptation in computer vision train here do well here (same network) correct answer incorrect answer Invariance assumption: everything that is different between domains is irrelevant Is this true? can we force this layer to be invariant to domain? domain classifier: guess domain from z reversed gradient Domain adaptation in RL for dynamics? Why is invariance not enough when the dynamics don’t match? When might this not work? Eysenbach et al., “Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers” What if we can also finetune? 1. RL tasks are generally much less diverse • Features are less general • Policies & value functions become overly specialized 2. Optimal policies in fully observed MDPs are deterministic • Loss of exploration at convergence • Low-entropy policies adapt very slowly to new settings See “exploration 2” lecture on unsupervised skill discovery and “control as inference” lecture on MaxEnt RL methods! How to maximize forward transfer? Basic intuition: the more varied the training domain is, the more likely we are to generalize in zero shot to a slightly different domain. “Randomization” (dynamics/appearance/etc.): widely used for simulation to real world transfer (e.g., in robotics) EPOpt: randomizing physical parameters train test adapt training on single torso mass training on model ensemble unmodeled effects ensemble adaptation Rajeswaran et al., “EPOpt: Learning robust neural network policies…” More randomization! Sadeghi et al., “CAD2RL: Real Single-Image Flight without a Single Real Image.” 2016 Xue Bin Peng et al., “Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.” 2018 Lee et al., “Learning Quadrupedal Locomotion over Challenging Terrain.” 2020 Some suggested readings Domain adaptation: Tzeng, Hoffman, Zhang, Saenko, Darrell. Deep Domain Confusion: Maximizing for Domain Invariance. 2014. Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, Lempitsky. Domain-Adversarial Training of Neural Networks. 2015. Tzeng*, Devin*, et al., Adapting Visuomotor Representations with Weak Pairwise Constraints. 2016. Eysenbach et al., Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers. 2020. Finetuning: Finetuning via MaxEnt RL: Haarnoja*, Tang*, et al. (2017). Reinforcement Learning with Deep Energy-Based Policies. Andreas et al. Modular multitask reinforcement learning with policy sketches. 2017. Florensa et al. Stochastic neural networks for hierarchical reinforcement learning. 2017. Kumar et al. One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. 2020 Simulation to real world transfer: Rajeswaran, et al. (2017). EPOpt: Learning Robust Neural Network Policies Using Model Ensembles. Yu et al. (2017). Preparing for the Unknown: Learning a Universal Policy with Online System Identification. Sadeghi & Levine. (2017). CAD2RL: Real Single Image Flight without a Single Real Image. Tobin et al. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. Tan et al. (2018). Sim-to-Real: Learning Agile Locomotion For Quadruped Robots. …and many many others! How can we frame transfer learning problems? 1. Forward transfer: learn policies that transfer effectively a) Train on source task, then run on target task (or finetune) b) Relies on the tasks being quite similar! 2. Multi-task transfer: train on many tasks, transfer to a new task a) Sharing representations and layers across tasks in multi-task learning b) New task needs to be similar to the distribution of training tasks 3. Meta-learning: learn to learn on many tasks a) Accounts for the fact that we’ll be adapting to a new task during training! Can we learn faster by learning multiple tasks? learn learn learn learn learn learn Multi-task learning can: - Accelerate learning of all tasks that are learned together - Provide better pre-training for down-stream tasks Can we solve multiple tasks at once? Multi-task RL corresponds to single-task RL in a joint MDP etc. sample etc.sample etc.sample MDP 0 MDP 1 MDP 2 pick MDP randomly in first state How does the model know what to do? • What if the policy can do multiple things in the same environment? Contextual policies e.g., do dishes or laundry images: Peng, van de Panne, Peters Goal-conditioned policies another state Convenient: no need to manually define rewards for each task Can transfer in zero shot to a new task if it’s another goal! Often hard to train in practice (see references) Not all tasks are goal reaching tasks! A few relevant papers: • Kaelbling. Learning to achieve goals. • Schaul et al. Universal value function approximators. • Andrychowicz et al. Hindsight experience replay. • Eysenbach et al. C-learning: Learning to achieve goals via recursive classification. Meta-Learning What is meta-learning? • If you’ve learned 100 tasks already, can you figure out how to learn more efficiently? • Now having multiple tasks is a huge advantage! • Meta-learning = learning to learn • In practice, very closely related to multi-task learning • Many formulations • Learning an optimizer • Learning an RNN that ingests experience • Learning a representation image credit: Ke Li Why is meta-learning a good idea? • Deep reinforcement learning, especially model-free, requires a huge number of samples • If we can meta-learn a faster reinforcement learner, we can learn new tasks efficiently! • What can a meta-learned learner do differently? • Explore more intelligently • Avoid trying actions that are know to be useless • Acquire the right features more quickly Meta-learning with supervised learning image credit: Ravi & Larochelle ‘17 Meta-learning with supervised learning (few shot) training set input (e.g., image) output (e.g., label) training set • How to read in training set? • Many options, RNNs can work • More on this later test input test label What is being “learned”? (few shot) training set test input test label What is being “learned”? meta-learned weights RNN hidden state Meta Reinforcement Learning The meta reinforcement learning problem The meta reinforcement learning problem 0.5 m/s 0.7 m/s -0.2 m/s -0.7 m/s Contextual policies and meta-learning “context” Meta-RL with recurrent policies meta-learned weights RNN hidden state Meta-RL with recurrent policies +0 +0 +0 +1 +0 +1 crucially, RNN hidden state is not reset between episodes! Why recurrent policies learn to explore episode meta-episode optimizing total reward over the entire meta-episode with RNN policy automatically learns to explore! Meta-RL with recurrent policies Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos, Blundell, Kumaran, Botvinick. Learning to Reinforcement Learning. 2016. Duan, Schulman, Chen, Bartlett, Sutskever, Abbeel. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. 2016. Heess, Hunt, Lillicrap, Silver. Memory-based control with recurrent neural networks. 2015. Architectures for meta-RL Duan, Schulman, Chen, Bartlett, Sutskever, Abbeel. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. 2016. standard RNN (LSTM) architecture attention + temporal convolution Mishra, Rohaninejad, Chen, Abbeel. A Simple Neural Attentive Meta-Learner. Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy Meta- Reinforcement learning via Probabilistic Context Variables. parallel permutation-invariant context encoder Gradient-Based Meta-Learning Back to representations… is pretraining a type of meta-learning? better features = faster learning of new task! Meta-RL as an optimization problem Finn, Abbeel, Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. MAML for RL in pictures What did we just do?? Just another computation graph… Can implement with any autodiff package (e.g., TensorFlow) But has favorable inductive bias… MAML for RL in videos after MAML training after 1 gradient step (forward reward) after 1 gradient step (backward reward) More on MAML/gradient-based meta-learning for RL MAML meta-policy gradient estimators: • Finn, Abbeel, Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. • Foerster, Farquhar, Al-Shedivat, Rocktaschel, Xing, Whiteson. DiCE: The Infinitely Differentiable Monte Carlo Estimator. • Rothfuss, Lee, Clavera, Asfour, Abbeel. ProMP: Proximal Meta-Policy Search. Improving exploration: • Gupta, Mendonca, Liu, Abbeel, Levine. Meta-Reinforcement Learning of Structured Exploration Strategies. • Stadie*, Yang*, Houthooft, Chen, Duan, Wu, Abbeel, Sutskever. Some Considerations on Learning to Explore via Meta-Reinforcement Learning. Hybrid algorithms (not necessarily gradient-based): • Houthooft, Chen, Isola, Stadie, Wolski, Ho, Abbeel. Evolved Policy Gradients. • Fernando, Sygnowski, Osindero, Wang, Schaul, Teplyashin, Sprechmann, Pirtzel, Rusu. Meta- Learning by the Baldwin Effect. Meta-RL as a POMDP Meta-RL as… partially observed RL? Meta-RL as… partially observed RL? encapsulates information policy needs to solve current task Meta-RL as… partially observed RL? encapsulates information policy needs to solve current task some approximate posterior (e.g., variational) act as though z was correct! this is not optimal! why? but it’s pretty good, both in theory and in practice! See, e.g. Russo, Roy. Learning to Optimize via Posterior Sampling. Variational inference for meta-RL Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy Meta-Reinforcement learning via Probabilistic Context Variables. ICML 2019. maximize post-update reward (same as standard meta-RL) stay close to prior Specific instantiation: PEARL Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy Meta-Reinforcement learning via Probabilistic Context Variables. ICML 2019. perform maximization using soft actor-critic (SAC), state-of-the-art off-policy RL algorithm • Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy Meta-Reinforcement learning via Probabilistic Context Variables. ICML 2019. • Zintgraf, Igl, Shiarlis, Mahajan, Hofmann, Whiteson. Variational Task Embeddings for Fast Adaptation in Deep Reinforcement Learning. • Humplik, Galashov, Hasenclever, Ortega, Teh, Heess. Meta reinforcement learning as task inference. References on meta-RL, inference, and POMDPs The three perspectives on meta-RL everything needed to solve task The three perspectives on meta-RL everything needed to solve task + conceptually simple + relatively easy to apply - vulnerable to meta-overfitting - challenging to optimize in practice + good extrapolation (“consistent”) + conceptually elegant - complex, requires many samples + simple, effective exploration via posterior sampling + elegant reduction to solving a special POMDP - vulnerable to meta-overfitting - challenging to optimize in practice But they’re not that different! everything needed to solve task just perspective 1, but with stochastic hidden variables! just a particular architecture choice for these Meta-RL and emergent phenomena Humans and animals seemingly learn behaviors in a variety of ways: Highly efficient but (apparently) model-free RL Episodic recall Model-based RL Causal inference etc. Perhaps each of these is a separate “algorithm” in the brain But maybe these are all emergent phenomena resulting from meta-RL? meta-RL gives rise to episodic learning model-free meta-RL gives rise to model-based adaptation meta-RL gives rise to causal reasoning (!) Dasgupta, Wang, Chiappa, Mitrovic, Ortega, Raposo, Hughes, Battaglia, Botvinick, Kurth-Nelson. Causal Reasoning from Meta-Reinforcement Learning. Wang, Kurth-Nelson, Kumaran, Tirumala, Soyer, Leibo, Hassabis, Botvinick. Prefrontal Cortex as a Meta- Reinforcement Learning System. Ritter, Wang, Kurth-Nelson, Jayakumar, Blundell, Pascanu, Botvinick. Been There, Done That: Meta-Learning with Episodic Recall. "
343,"Introduction Lecture 1 Language is long considered a core problem of AI Turing test (Turing 1950) Q: Please write me a sonnet on the subject of the Forth Bridge. A: Count me out on this one. I never could write poetry. Q: Add 34957 to 70764. A: (Pause about 30 seconds and then give as answer) 105621. Key problem of AI With many applications Translation Machine reading https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/ Conversational interfaces Conversational interfaces https://youtu.be/yiQX-_Y0gms [MacCartney and Potts cs224u] What is NLP? Hirschberg and Manning, 2015 Deep learning progress Speech recognition Significant process. Increasingly deployed technology. Computer vision ImageNet error rate from dsiac.org Hopes high for self-driving cars and other innovations Progress on various language pairs https://research.googleblog.com/2016/09/a-neural-network-for-machine.html Michael I. Jordan (UC Berkeley) AMA: “If you got a billion dollars to spend on a huge research project that you get to lead, what would you like to do?” michaelijordan: I'd use the billion dollars to build a NASA-size program focusing on natural language processing (NLP), in all of its glory (semantics, pragmatics, etc). Intellectually I think that NLP is fascinating, allowing us to focus on highly-structured inference problems, on issues that go to the core of ""what is thought"" but remain eminently practical, and on a technology that surely would make the world a better place. Although current deep learning research tends to claim to encompass NLP, I'm (1) much less convinced about the strength of the results, compared to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision, the way to go is to couple huge amounts of data with black-box learning architectures. http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan Yann LeCun (NYU and Facebook) Across the industry, it's already reinventing image and speech recognition. But like Google, LeCun and FAIR are pushing for more. The next big frontier, he says, is natural language processing, which seeks to give machines the power to understand not just individual words but entire sentences and paragraphs. https://www.wired.com/2014/12/fb/ Geoff Hinton (U Toronto and Google) I think that the most exciting areas over the next five years will be really understanding text and videos. I will be disappointed if in five years’ time we do not have something that can watch a YouTube video and tell a story about what happened. In a few years time we will put [Deep Learning] on a chip that fits into someone’s ear and have an English-decoding chip that’s just like a real Babel fish. https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/ (3 years ago) Themes of deep learning ● Use very general models and algorithms ● Distributed representations ● Large design space of potential models ○ Engineering model architectures rather than features ● Leverage available compute and data ○ Learn as much as possible from data rather than design for domains http://karpathy.github.io/2015/05/21/rnn-effectiveness/ The hammer: deep learning http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Themes of deep learning ● Use very general models and algorithms ● Distributed representations ● Large design space of potential models ○ Engineering model architectures rather than features ● Leverage available compute and data ○ Learn as much as possible from data rather than design for domains http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Representations Themes of deep learning ● Use very general models and algorithms ● Distributed representations ● Large design space of potential models ○ Engineering model architectures rather than features ● Leverage available compute and data ○ Learn as much as possible from data rather than design for domains http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Models http://colah.github.io/posts/2015-08-Understanding-LSTMs/ LSTM cell http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Seq2seq with attention https://research.googleblog.com/2016/09/a-neural-network-for-machine.html Transformer The nails in NLP [MacCartney and Potts cs224u] Reality [MacCartney and Potts cs224u] Conversational interfaces Conversational interfaces more or less solved easily tolerable great amount of progress Definitely the limiting factor More conversational search Tested by Sida on 02/05/2018 which states border new jersey? Conversational interfaces Works great for a number of useful queries ● which states borders New Jersey? ● who is Donald Trump married to? ● what is 15% of $120? Fails to work (tested on Google assistant 02/05/2018) ● how many legs does the goat have? ● how many eyes does the chicken have? ● is boiling water hotter than iced water? ● is the day brighter than the night? Why? Realities of NLIs, sometimes Ambiguity Ambiguity The Pope’s baby steps on gays Boy paralyzed after tumor fights back to gain black belt Enraged cow injures farmer with axe Juvenile Court to Try Shooting Defendant World knowledge Arbitrary world knowledge required for full understanding The city councilmen refused the demonstrators a permit because they [feared/advocated] violence. What does the pronoun ""they"" refer to in each case? Common sense ● how many eyes does the chicken have? ● is boiling water hotter than iced water? ● is the day brighter than the night? Often missing from the data Pragmatics When a diplomat says yes, he means ‘perhaps’; When he says perhaps, he means ‘no’; When he says no, he is not a diplomat.—Voltaire (Quoted, in Spanish, in Escandell 1993.) Goals of this class ● Learn the fundamentals ○ Machine learning / deep learning focused ○ Broadly applicable methods and abstractions ○ Identify important problems ● Gain practical experience ○ Complete assignments with significant implementation components ○ To support you in completing a project that is worthy of a top NLP conference ● Learn the language of the field ○ Read research papers, and gain the ability to learn more on your own Diagnosis ● In this class: ○ Methods and approaches of applied machine learning ○ Build NLP systems and learn from empirical feedback ○ Understanding NLP problems and issues ○ Critically read NLP papers ● Not in this class ○ details of deep learning CoreNLP, NLTK, deep learning software ■ but the ability to learn such things is expected ○ background in calculus, linear algebra, etc. ○ theory of machine learning / linguistics Class plan ● First half ○ the machine learning tools ○ distributed representations: word vectors etc. ○ deep learning models: RNN, seq2seq, attention ○ test barebone models, and compare against baselines ● Midterm ● Second half (not too sure yet) ○ study/implementation of representative/important/cutting-edge papers ○ more careful study of semantic parsing and natural language interfaces ● Project Warm up with least squares Logistics ● Class website ● Piazza ● Schedule office hours ○ Misha: 10am on Monday ● Waitlist and enrollment problems ● Sections next week ○ review of python, numpy ○ assignment setup ○ review of probability, optimization etc.? "
347,"COS 495 Precept 2 Machine Learning in Practice Misha Precept Objectives • Review how to train and evaluate machine learning algorithms in practice. • Make sure everyone knows the basic jargon. • Develop basic tools that you will use when implementing and evaluating your ﬁnal projects. Terminology Review Supervised Learning: • Given a set of (example, label) pairs, learning how to predict the label of a given example. • Examples: classiﬁcation, regression. Unsupervised Learning: • Given a set of examples, learning useful properties of the distribution of these examples. • Examples: word embeddings, text generation. Other (e.g. Reinforcement, Online) Learning: • Often involves an adaptive setting with a changing environment. Gaining some interest in NLP. Example Problem: Document Classiﬁcation Given 50K (movie review, rating) pairs split into a training set (25K) and test set (25K), learn a function For simplicity, represent each review as a Bag-of- Words (BoW) vector and each label as +1 or -1: f : reviews 7! {positive, negative} Xtrain: 25K V -dimensional vectors x1, . . . , x25K. Ytrain: 25K numbers y1, . . . , y25K 2 {±1}. Approach: Linear SVM • We will use a linear classiﬁer: • We will target a low hinge loss on the test set: f(x) = sign ! wT x "" , w 2 RV X (x,y)2(X,Y )test max "" 0, 1 −y · wT x Regularization • If the vocabulary size is larger than the number of training samples then there is an inﬁnite number of linear classiﬁer that will perfectly separate the data. This makes the problem ill-posed. • We want to pick one that generalizes well, so we use regularization to encourage a ‘less-complex’ classiﬁcation function: wT w + C 25K X i=1 max "" 0, 1 −yi · wT xi , C 2 R+ Regularization Cross-Validation Validation: • To determine C, we hold out some (say 5K examples) of our training data in order to use it as a temporary test set (also called ‘dev set’) to test different values of C. Cross-Validation: • Split data into k dev sets (‘folds’) and determine C by holding out each of them one a time and averaging the result. Parameters are often picked from powers of 10 (e.g. pick the best-performing C out of 10-2, … , 102) Evaluation Metrics: Accuracy • Although we target a low convex loss, in the end we care about correct labeling alone. Thus for results we report the average accuracy: 1 25K X (x,y)2(Xtest,Ytest) 1{f(x)=y} where f(x) = sign "" wT x # Evaluation Metrics: Precision/Recall/F1 Sometimes, average accuracy is a poor measure of performance. For example, say we want to detect sarcastic comments, which do not occur very often, and learn a system that marks them as positive. precision = # True Positives # True Positives + # False Positives recall = # True Positives # True Positives + # False Negatives F1 = 2 · precision · recall precision + recall Precision v.s. Recall Example Problem: Document Similarity Given a set of (sentence-1, sentence-2, score) triples split into a training set (5K) and a test set (1K), learn a function: f : sentences ⇥sentences 7! R Approach: Regression • Represent each pair of documents as a dense vector and minimizes the mean-squared-error between the function output and the score: • Tricky part is determining the function: linear, quadratic, neural network? 1 10K 10K X i=1 kyi −f(xi)k2 2 Under-ﬁtting • Under-ﬁtting occurs when you cannot get sufﬁciently low error on your training set. • Usually means the true function generating the data is more complex than your model. Over-ﬁtting • Overﬁtting occurs when the gap between the training error and the test error (i.e. ‘generalization error’) is large. • Can occur if you have too many learned parameters (as we saw in the BoW example). Finding a Good Model • Regularization: encourages simpler models and can incorporate prior information. • Cross-validation: determine optimal model capacity by testing on held out data. • Information criteria (Akaike, Bayesian) What Changes When We Switch to Deep Learning? More hyperparameters: • Learning rate, number of layers, number of hidden units, type of nonlinearity, … • Sometimes cross-validated, oftentimes not. Higher model capacity: • Deep nets can ﬁt any function. • Various regularization methods (dropout, early stopping, weight-tying, …) Mini-batch Learning Useful Tips in NLP: Sparse Matrices • Often we deal with sparse features such as Bag-of- Words vectors. Storing dense arrays of size 25K x V is impractical. • Sparse matrices (e.g. in scipy.sparse) allow usual matrix operations to be done efﬁciently without massive memory overhead. Useful Tips in NLP: Feature Hashing/Sampling • In some settings we have too many different features to handle (e.g. spam ﬁltering, large corpus vocab). • Can deal with this by min counting, but this discards data and is hard to use in an online setting. • Different approaches: • Feature hashing: randomly map features to one of a ﬁxed number of bins (used in spam ﬁltering). • Sampling: only consider a small number of features when training (used for training word embeddings). "
348,"Thinking about your Final Projects Misha Why Start Now? • Mentor check-ins start early in the second half of the semester • Opportunity to: • review literature • have ideas in your head • ﬁnd partners • 30% of your ﬁnal grade Types of Projects • Model: • propose a new model, analyze it, and test it on several tasks for validation • Task: • use available tools to try and do as well as possible on a certain task • Analysis: • analyze existing models or tasks to understand what they tell us Example of Model Project • Yang, Lu, & Zheng. A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings. EMNLP 2017. • Motivate and describe their model: • Learn small-corpus embeddings via transfer learning • Use existing large-corpus embeddings as regularization • Evaluate the model’s usefulness on several tasks: • Entity recognition • Sentiment analysis Model Project Details • Models should be well-founded: major design decisions should have some linguistic or mathematical motivation. • Models will most-likely build upon existing work, and so must be placed in that context. • Empirical veriﬁcation should be conducted on model assumptions, not just downstream tasks. • Models can help improve performance on downstream tasks by augmenting existing methods. Example of Task Project • Herbelot & Baroni. High-risk learning: Acquiring new word vectors from tiny data. EMNLP 2017. • Motivate and describe the task: • Learn a word vector using only the word’s deﬁnition and vectors of other words. • Simulates one-shot learning of word embeddings when we don’t have enough data about a word but still want to represent it. • Devise a method to do well on this task: • Modify word2vec algorithm to learn quickly from one example. • Compare method to baseline/previous methods. Task Project Details • If introducing a task, need to show that previous tasks are insufﬁcient and come up with baseline methods. • If approach performs better/worse than existing methods, give some reasons as to why. • Be careful not to overﬁt - i.e. devise a method that only works well on your model because you have been evaluating using the test set. Example of Analysis Project • Chen, Bolton, & Manning. A Thorough Examination of the CNN/ Daily Mail Reading Comprehension Task. ACL 2016. • Describe what they analyze and demonstrate its importance: • Reading comprehension is an important goal in NLP • Efforts often evaluated on the CNN/Daily Mail task, appealing due to its size and simplicity • Perform the analysis and gain new insights: • Design a very simple system that does well on the task, indicating that it is perhaps too easy • Doing better than the state-of-the-art may be impossible due to annotation/co-reference errors in the original task Analysis Project Details • Choose an existing model/task for which you believe present understanding is insufﬁcient. • Sometimes goal is to ‘break’ the model or show that the task may not be that useful by demonstrating points of failure. • If the outcome is negative for the subject of consideration, try to introduce alternatives. Closing Thoughts • Project categories have a lot of overlap and good reports will often have components of each. • Negative results are okay, but hopefully they lead to better understanding. • Attend the many NLP colloquia this spring (see course schedule for times). "
35,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, JurafskyMartin-SLP3, others) Lecture 6: Semantic Parsing 2; Question Answering Announcements ! Coding-HW1 (on word vector training+evaluation_ +visualization) was due Oct5 midnight! ! Midterm project presentation next week (look for details in email). ! Coding-HW2 will be announced soon! SRL and Semantic Parsing 2 (AMR, Neural Models, etc.) SRL Features Features Headword6of6constituent Examiner Headword6POS NNP Voice6of6the6clause Active Subcategorization of6pred VP6K>6VBD6NP6PP 45 10 CHAPTER 22 • SEMANTIC ROLE LABELING S NP-SBJ = ARG0 VP DT NNP NNP NNP The San Francisco Examiner VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP issued DT JJ NN IN NP a special edition around NN NP-TMP noon yesterday Figure 22.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line shows the path feature NP""S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner. • The headword of the constituent, Examiner. The headword of a constituent can be computed with standard head rules, such as those given in Chapter 11 in Fig. ??. Certain headwords (e.g., pronouns) place strong constraints on the possible semantic roles they are likely to ﬁll. • The headword part of speech of the constituent, NNP. • The path in the parse tree from the constituent to the predicate. This path is marked by the dotted line in Fig. 22.5. Following Gildea and Jurafsky (2000), we can use a simple linear representation of the path, NP""S#VP#VBD. "" and # represent upward and downward movement in the tree, respectively. The path is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate. • The voice of the clause in which the constituent appears, in this case, active (as contrasted with passive). Passive sentences tend to have strongly different linkings of semantic roles to surface form than do active ones. Named6Entity6type6of6constit ORGANIZATION First6and6last6words6of6constit The,6Examiner Linear6position,clause re:6predicate before Path-based Features for SRL Path Features Some SRL Results Results Features: ! Path from target to filler ! Filler’s syntactic type, headword, case ! Target’s identity ! Sentence voice, etc. ! Lots of other second-order features Gold vs parsed source trees ! SRL is fairly easy on gold trees ! Harder on automatic parses ! So major feature categories in traditional feature-based SRL models were: ! Headword, syntactic type, case, etc. of candidate node/ constituent ! Linear and tree path from predicate target to node ! Active vs. passive voice ! Second order and higher order features ! Accuracy for such feature-based SRL models then highly depends on accuracy of underlying parse tree! ! So quite high SRL results when using ground-truth parses ! Much lower results with automatically-predicted parses! Schematic of Frame Semantics (FrameNet) Schematic)of)Frame)Semantics in which the evoked frame is selected for each predicate; and argument identiﬁcation (Section 6), in which arguments to each frame are identiﬁed and labeled with a role from that frame. Experiments demonstrating favorable performance to the previous state of the art on SemEval 2007 and FrameNet data sets are described in each section. Some novel aspects of our approach include a latent-variable model (Section 5.2) and a semi- supervised extension of the predicate lexicon (Section 5.5) to facilitate disambiguation of words not in the FrameNet lexicon; a uniﬁed model for ﬁnding and labeling arguments Figure 1 An example sentence from the annotations released as part of FrameNet 1.5 with three targets marked in bold. Note that this annotation is partial because not all potential targets have been annotated with predicate-argument structures. Each target has its evoked semantic frame marked above it, enclosed in a distinct shape or border style. For each frame, its semantic roles are shown enclosed within the same shape or border style, and the spans fulﬁlling the roles are connected to the latter using dotted lines. For example, manner evokes the CONDUCT frame, and has the AGENT and MANNER roles fulﬁlled by Austria and most un-Viennese, respectively. 35 Figure6from6Das6et6al6(2014) [Das et al., 2014] PropBank vs. FrameNet Representations ameNet and)PropBank representatio Computational Linguistics Volume 40, Number 1 (a) (b) Neural SRL cess of our t advances ks such as 2015) and 016),2 and Lewis and orce struc- out adding . equence y as input. f BIO tags the tag O, + + + The 0 P(BARG0) + + + cats 0 P(IARG0) + + + love 1 P(BV) + + + hats 0 P(BARG1) Softmax Transform Gates LSTM Word & Predicate Figure 1: Highway LSTM with four layers. The curved connections represent highway connec- i d h l b l f [He et al., 2017] Neural SRL [He et al., 2017] Development WSJ Test Brown Test Combined Method P R F1 Comp. P R F1 Comp. P R F1 Comp. F1 Ours (PoE) 83.1 82.4 82.7 64.1 85.0 84.3 84.6 66.5 74.9 72.4 73.6 46.5 83.2 Ours 81.6 81.6 81.6 62.3 83.1 83.0 83.1 64.3 72.9 71.4 72.1 44.8 81.6 Zhou 79.7 79.4 79.6 - 82.9 82.8 82.8 - 70.7 68.2 69.4 - 81.1 FitzGerald (Struct.,PoE) 81.2 76.7 78.9 55.1 82.5 78.2 80.3 57.3 74.5 70.0 72.2 41.3 - T¨ ackstr¨ om (Struct.) 81.2 76.2 78.6 54.4 82.3 77.6 79.9 56.0 74.3 68.6 71.3 39.8 - Toutanova (Ensemble) - - 78.6 58.7 81.9 78.8 80.3 60.1 - - 68.8 40.8 - Punyakanok (Ensemble) 80.1 74.8 77.4 50.7 82.3 76.8 79.4 53.8 73.4 62.9 67.8 32.3 77.9 Table 1: Experimental results on CoNLL 2005, in terms of precision (P), recall (R), F1 and percentage of completely correct predicates (Comp.). We report results of our best single and ensemble (PoE) model. The comparison models are Zhou and Xu (2015), FitzGerald et al. (2015), T¨ ackstr¨ om et al. (2015), Toutanova et al. (2008) and Punyakanok et al. (2008). Development Test Method P R F1 Comp. P R F1 Comp. Ours (PoE) 83.5 83.2 83.4 67.5 83.5 83.3 83.4 68.5 Ours 81.8 81.4 81.5 64.6 81.7 81.6 81.7 66.0 Zhou - - 81.1 - - - 81.3 - FitzGerald (Struct.,PoE) 81.0 78.5 79.7 60.9 81.2 79.0 80.1 62.6 Development WSJ Test Brown Test Combined Method P R F1 Comp. P R F1 Comp. P R F1 Comp. F1 Ours (PoE) 83.1 82.4 82.7 64.1 85.0 84.3 84.6 66.5 74.9 72.4 73.6 46.5 83.2 Ours 81.6 81.6 81.6 62.3 83.1 83.0 83.1 64.3 72.9 71.4 72.1 44.8 81.6 Zhou 79.7 79.4 79.6 - 82.9 82.8 82.8 - 70.7 68.2 69.4 - 81.1 FitzGerald (Struct.,PoE) 81.2 76.7 78.9 55.1 82.5 78.2 80.3 57.3 74.5 70.0 72.2 41.3 - T¨ ackstr¨ om (Struct.) 81.2 76.2 78.6 54.4 82.3 77.6 79.9 56.0 74.3 68.6 71.3 39.8 - Toutanova (Ensemble) - - 78.6 58.7 81.9 78.8 80.3 60.1 - - 68.8 40.8 - Punyakanok (Ensemble) 80.1 74.8 77.4 50.7 82.3 76.8 79.4 53.8 73.4 62.9 67.8 32.3 77.9 Table 1: Experimental results on CoNLL 2005, in terms of precision (P), recall (R), F1 and percentage of completely correct predicates (Comp.). We report results of our best single and ensemble (PoE) model. The comparison models are Zhou and Xu (2015), FitzGerald et al. (2015), T¨ ackstr¨ om et al. (2015), Toutanova et al. (2008) and Punyakanok et al. (2008). Development Test Method P R F1 Comp. P R F1 Comp. Ours (PoE) 83.5 83.2 83.4 67.5 83.5 83.3 83.4 68.5 Ours 81.8 81.4 81.5 64.6 81.7 81.6 81.7 66.0 Zhou - - 81.1 - - - 81.3 - FitzGerald (Struct.,PoE) 81.0 78.5 79.7 60.9 81.2 79.0 80.1 62.6 T¨ ackstr¨ om (Struct.) 80.5 77.8 79.1 60.1 80.6 78.2 79.4 61.8 Pradhan (revised) - - - - 78.5 76.6 77.5 55.8 Table 2: Experimental results on CoNLL 2012 in the same metrics as above. We compare our best i l d bl (P E) d l i t Zh d X (2015) Fit G ld t l (2015) T¨ k t ¨ ! CoNLL 2005 dataset: ! CoNLL 2012 dataset: Neural SRL [Marcheggiani et al., 2017] Anton Frolov , Ivan Titov rsity of Amsterdam nce Department, Yandex matics, University of Edinburgh giani@uva.nl yandex-team.ru inf.ed.ac.uk Sequa makes and repairs jet engines. 01 01 01 A0 A1 A0 A1 A1 Figure 1: A semantic dependency graph. et al., 1998). In contrast, CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajic et al., 2009) popularized dependency-based se- mantic role labeling where the goal is to iden- tify syntactic heads of arguments rather than entire constituents. Figure 1 shows an example of such a dependency-based representation: node labels are senses of predicates (e.g., “01” indicates that the ﬁrst sense from the PropBank sense repository is used for predicate makes in this sentence) and edge labels are semantic roles (e g A0 is a proto-agent A1 Classiﬁer k layers BiLSTM Mary eats an apple ◦ Figure 2: Predicting an argument and its label with an LSTM encoder. been previously believed to require syntactic infor- mation. For the predicate disambiguation subtask we use models from previous work. In order to identify and classify arguments, we x = con 2.2 One que man for netw F LS and can tenc wor its l two for spe war and LS stac Neural Semantic Parsing [Jia and Liang, 2016] u y pliang@cs.stanford.edu - t c . - - . - - l d a k d - g - - - w - h what are the major cities in utah ? what states border maine ? Original Examples Train Model Sequence-to-sequence RNN Sample New Examples Synchronous CFG Induce Grammar what are the major cities in [states border [maine]] ? what are the major cities in [states border [utah]] ? what states border [states border [maine]] ? what states border [states border [utah]] ? Recombinant Examples Figure 1: An overview of our system. Given a dataset, we induce a high-precision synchronous context-free grammar. We then sample from this grammar to generate new “recombinant” exam- ples, which we use to train a sequence-to-sequence RNN. GEO x: “what is the population of iowa ?” y: _answer ( NV , ( _population ( NV , V1 ) , _const ( V0 , _stateid ( iowa ) ) ) ) ATIS x: “can you list all ﬂights from chicago to milwaukee” y: ( _lambda $0 e ( _and ( _flight $0 ) ( _from $0 chicago : _ci ) ( _to $0 milwaukee : _ci ) ) ) Overnight x: “when is the weekly standup” y: ( call listValue ( call getProperty meeting.weekly_standup ( string start_time ) ) ) Figure 2: One example from each of our domains. We tokenize logical forms as shown, thereby cast- ing semantic parsing as a sequence-to-sequence task. ing prior knowledge into a domain-general struc- tured prediction model. In data recombination, prior knowledge about a task is used to build a high-precision generative model that expands the (201 tree W man inpu • • • Neural Semantic Parsing [Jia and Liang, 2016] Examples (“what states border texas ?”, answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas))))) (“what is the highest mountain in ohio ?”, answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio)))))) Rules created by ABSENTITIES ROOT ! h “what states border STATEID ?”, answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(STATEID ))))i STATEID ! h “texas”, texas i ROOT ! h “what is the highest mountain in STATEID ?”, answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(STATEID )))))i STATEID ! h“ohio”, ohioi Rules created by ABSWHOLEPHRASES ROOT ! h “what states border STATE ?”, answer(NV, (state(V0), next_to(V0, NV), STATE ))i STATE ! h “states border texas”, state(V0), next_to(V0, NV), const(V0, stateid(texas))i ROOT ! h “what is the highest mountain in STATE ?”, answer(NV, highest(V0, (mountain(V0), loc(V0, NV), STATE )))i Rules created by CONCAT-2 ROOT ! hSENT1 </s> SENT2, SENT1 </s> SENT2i SENT ! h “what states border texas ?”, answer(NV, (state(V0), next_to(V0, NV), const(V0, stateid(texas)))) i SENT ! h “what is the highest mountain in ohio ?”, answer(NV, highest(V0, (mountain(V0), loc(V0, NV), const(V0, stateid(ohio))))) i Figure 3: Various grammar induction strategies illustrated on GEO. Each strategy converts the rules of an input grammar into rules of an output grammar. This ﬁgure shows the base case where the input grammar has rules ROOT ! hx, yi for each (x, y) pair in the training dataset. Neural Semantic Parsing [Krishnamurthy et al., 2017] WRGHFRGHU WRGHFRGHU 3UHGLFWHG *UDPPDU 5XOHV R RQ RQ (QFRGHU (QWLW\ /LQNLQJ :KLFKDWKOHWHZDVIURP 6RXWK.RUHDDIWHUWKH\HDU"" ȺȺƌĕǁĕƌƖĕëƢķŒĕƢĕȻȺëŝďȺŝëƢļŦŝƖŦƪƢķȱŏŦƌĕëȻ ȺǈĕëƌȺŝƪśǡĈĕŒŒȺɴɲǡǟǠǟȻȻȻȻȻ J ZKLFKDWKOHWH /670 7DEOH.QRZOHGJH*UDSK QDWLRQ IURPHQFRGHU JUHDW EULWDLQ VRXWK NRUHD /670 (QWLW\ /LQNLQJ  (QWLW\ /LQNLQJ /670 $WWHQWLRQ Ĉ J /670 $WWHQWLRQ ĈʌɵƌȤĈɴ  4XHVWLRQ /RJLFDO )RUP 'HFRGHU .QRZOHGJHJUDSK (PEHGGLQJ /670 DWKOHWH NDUO VFKDIHU NLP \XQD \HDU   Figure 1: Overview of our semantic parsing model. The encoder performs entity embedding and linking before encoding the question with a bidirectional LSTM. The decoder predicts a sequence of grammar rules that generate a well-typed logical form. 3.1 Preliminaries We follow (Pasupat and Liang, 2015) in using the same table structure representation and λ-DCS language for expressing logical forms. In this representation, tables are expressed as knowledge from cells c to rows r. Other operations have more complex functional types, e.g., reverse has type hhc, ri, hr, cii, which enables us to write (reverse country).1 The parser assigns ev- ery λ-DCS constant a type, then applies standard i l t i f l ith Neural Semantic Parsing [Neelakantan et al., 2017] Operations Count Select ArgMax ArgMin … … > < Print Neural Network What was the total number of goals scored in 2005 Row Selector Scalar Answer Lookup Answer timestep t Column Selection Data from Table Row Selector from t-1 Operation Selection Table Figure 1: Neural Programmer is a neural network augmented with a set of discrete operations. The model runs for a ﬁxed number of time steps, selecting an operation and a column from the table at every time step. The induced program transfers information across timesteps using the row selector variable while the output of the model is stored in the scalar answer and lookup answer variables. Neural AMR Parsing [Konstas et al., 2017] sviyer,my89,yejin,lsz}@cs.washington.edu en Institute for Artiﬁcial Intelligence, Seattle, WA lukez@allenai.org act models have shown ross a broad range ever, their applica- nerating text using resentation (AMR) o the relatively lim- data and the non- e AMR graphs. We g procedure that can g millions of unla- reful preprocessing r AMR parsing, our titive results of 62.1 best score reported of external seman- MR generation, our ew state-of-the-art 33 8 We present Obama was elected and his voters celebrated Obama elect.01 celebrate.01 vote.01 and * op1 op2 ARG0 poss ARG0 person name name op1 person ARG0-of Figure 1: An example sentence and its cor- responding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies be- tween entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; P l 2017 B di d G k 2016) Neural AMR Parsing [Konstas et al., 2017] Question Answering IR-based Question Answering ! Initial approaches to Q&A: pattern matching, pattern learning, query rewriting, information extraction [Jurafsky-SLP3] 28.1 • IR-BASED FACTOID QUESTION ANSWERING 3 Document Document Document Docume nt Docume nt Docume nt Docume nt Docume nt Question Processing Passage Retrieval Query Formulation Answer Type Detection Question Passage Retrieval Document Retrieval Answer Processing Answer passages Indexing Relevant Docs Document Document Document Figure 28.2 IR-based factoid question answering has three stages: question processing, passage retrieval, and answer processing. also extract a focus, which is the string of words in the question that are likely to be replaced by the answer in any answer string found. Some systems also classify the question type: is this a deﬁnition question a math question a list question? For IR-based Question Answering ! Initial approaches to Q&A: pattern matching, pattern learning, query rewriting, information extraction [Jurafsky-SLP3] 4 CHAPTER 28 • QUESTION ANSWERING NUMERIC ABBREVIATION ENTITY DESCRIPTION LOCATION HUMAN Li & Roth Taxonomy country city state reason definition food currency animal date distance percent size money individual title group expression abbreviation Figure 28.3 A subset of the Li and Roth (2005) answer types. contains 276 hand-written rules associated with the approximately 180 answer types in the typology (Hovy et al., 2002). A regular expression rule for detecting an answer t lik ( hi h th ti h b d tit t d) IR-based Question Answering ! Next came a large-scale, open-domain IE system like IBM Watson [Jurafsky-SLP3] 28.3 Using multiple information sources: IBM’s Watson Of course there is no reason to limit ourselves to just text-based or knowledge-based resources for question answering. The Watson system from IBM that won the Jeop- ardy! challenge in 2011 is an example of a system that relies on a wide variety of resources to answer questions. Document Document Document (1) Question Processing From Text Resources Focus Detection Lexical Answer Type Detection Question Document and Passsage Retrieval passages Document Document Document Question Classification Parsing Named Entity Tagging Relation Extraction Coreference From Structured Data Relation Retrieval DBPedia Freebase (2) Candidate Answer Generation Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer Candidate Answer (3) Candidate Answer Scoring Evidence Retrieval and scoring Answer Extraction Document titles Anchor text Text Evidence Sources (4) Confidence Merging and Ranking Text Evidence Sources Time from DBPedia Space from Facebook Answer Type Answer and Conﬁdence Candidate Answer + Conﬁdence Candidate Answer + Conﬁdence Candidate Answer + Conﬁdence Candidate Answer + Conﬁdence Candidate Answer + Conﬁdence Logistic Regression Answer Ranker Merge Equivalent Answers Figure 28.9 The 4 broad stages of Watson QA: (1) Question Processing, (2) Candidate Answer Generation, (3) Candidate Answer Scoring, and (4) Answer Merging and Conﬁdence Scoring. Figure 28.9 shows the 4 stages of the DeepQA system that is the question an- IR-based Question Answering ! Next came a large-scale, open-domain IE system like IBM Watson Articles Fi 6 D pQA Hi h L l A hit t [Ferrucci et al., 2010] Knowledge Base Q&A (Semantic Parsing) ! Answering question by mapping it to a query (e.g., based on logical forms) executable on a structured database While an enormous amount of information is encoded in the vast amount of text on the web, information obviously also exists in more structured forms. We use the term knowledge-based question answering for the idea of answering a natural language question by mapping it to a query over a structured database. Like the text- based paradigm for question answering, this approach dates back to the earliest days of natural language processing, with systems like BASEBALL (Green et al., 1961) that answered questions from a structured database of baseball games and stats. Systems for mapping from a text string to any logical form are called semantic parsers (???). Semantic parsers for question answering usually map either to some version of predicate calculus or a query language like SQL or SPARQL, as in the examples in Fig. 28.7. Question Logical form When was Ada Lovelace born? birth-year (Ada Lovelace, ?x) What states border Texas? l x.state(x) ^ borders(x,texas) What is the largest state argmax(lx.state(x),lx.size(x)) How many people survived the sinking of the Titanic (count (!fb:event.disaster.survivors fb:en.sinking of the titanic)) Figure 28.7 Sample logical forms produced by a semantic parser for question answering. These range from simple relations like birth-year, or relations normalized to databases like Freebase, to full predicate calculus. The logical form of the question is thus either in the form of a query or can easily be converted into one. The database can be a full relational database, or simpler structured databases like sets of RDF triples. Recall from Chapter 20 that an RDF triple is a 3-tuple, a predicate with two arguments, expressing some simple relation [Jurafsky-SLP3] Semantic Parsing Recap ! Parsing with logic (booleans, individuals, functions) and lambda forms Sentence loves(john,mary) Noun Phrase john Verb Phrase λx.loves(x,mary) Name john Verb λy.λx.loves(x,y) Noun Phrase Name mary “John” john “loves” λy.λx.loves(x,y) “Mary” mary Parse tree with associated semantics [Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Poon and Domingos, 2009; Artzi and Zettlemoyer, 2011, 2013; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Poon 2013; Berant and Liang, 2014; Iyyer et al., 2014] Compositional Semantics Compositional Semantics ! So now we have meanings for the words ! How do we know how to combine words? ! Associate a combination rule with each grammar rule: ! S : β(α) →NP : α VP : β (function application) ! VP : λx . α(x) ∧β(x) →VP : α and : ∅ VP : β (intersection) ! Example: S NP VP Bob VP and sings VP dances bob λy.sings(y) λz.dances(z) λx.sings(x) ∧dances(x) [λx.sings(x) ∧dances(x)](bob) sings(bob) ∧dances(bob) ! Now after we have these meanings for words, we want to combine them into meaning for phrases and sentences ! For this, we associate a combination rule with each grammar rule of the parse tree, e.g.: S: β(α) ! NP: α VP: β (function application) VP: λx . α(x) Λ β(x) ! VP: α and: ∅ VP: β (intersection) CCG Parsing Example CCG Parsing to Prague flights N\N λ λ λ λf.λ λ λ λx.f(x)∧ ∧ ∧ ∧to(x,PRG) N λ λ λ λx.flight(x)∧ ∧ ∧ ∧to(x,PRG) Show me N λ λ λ λx.flight(x) (N\N)/NP λ λ λ λy.λ λ λ λf.λ λ λ λx.f(y)∧ ∧ ∧ ∧to(x,y) NP PRG S/N λ λ λ λf.f S λ λ λ λx.flight(x)∧ ∧ ∧ ∧to(x,PRG) Weak Supervision ! Instead of relying on sentence-logicform pairs as training data, we can learn from query-answer pairs ! Logical forms are latent, and we can check which one gets the correct answer on being executed against a knowledge base (KB) [Clarke et al. 2010; Liang et al. 2011] Weak Supervision Learning from Query Answers What is the largest state that borders Texas? New Mexico argmax(λx.state(x) ^ border(x, TX), λy.size(y)) argmax(λx.river(x) ^ in(x, TX), λy.size(y)) New Mexico Rio Grande [Clarke et al. 2010; Liang et al. 2011] Weak Supervision [Chen and Mooney 2011; Kim and Mooney 2012;Artzi and Zettlemoyer 2013b] ! Learning from Instruction-Demonstration Pairs Weak Supervision [Artzi and Zettlemoyer 2011] ! Learning from Conversation Logs Weak Supervision Learning from Conversation Logs SYSTEM how can I help you ? (OPEN_TASK) USER i ‘ d like to ﬂy to new york SYSTEM ﬂying to new york . (CONFIRM: from(ﬂ, ATL)) leaving what city ? (ASK: λx.from(fl,x)) USER from boston on june seven with american airlines SYSTEM ﬂying to new york . (CONFIRM: to(ﬂ, NYC)) what date would you like to depart boston ? (ASK: λx.date(fl,x)∧to(fl, BOS)) USER june seventh [CONVERSATION CONTINUES] Semantic Parsing on Freebase Mapping questions to answers via latent logical forms. To narrow down the logical predicate space, they use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. [Berant et al., 2013] that on ally earn hal- the for m in ping dge we onal ates. pite Occidental College, Columbia University Execute on Database Type.University u Education.BarackObama Type.University Education BarackObama Which college did Obama go to ? alignment alignment bridging Figure 1: Our task is to map questions to answers via la- tent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Free- Semantic Parsing via Paraphrasing For each candidate logical form (red), they generate canonical utterances (purple). The model is trained to paraphrase the input utterance (green) into the canonical utterances associated with the correct denotation (blue). [Berant and Liang, 2014] What party did Clay establish? paraphrase model What political party founded by Henry Clay? ... What event involved the people Henry Clay? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utter- ances (in purple). The model is trained to paraphrase the in- put utterance (in green) into the canonical utterances associ- ated with the correct denotation (in blue). Reading Comprehension or Passage-based Q&A Passage-based Q&A James the Turtle was always getting in trouble. Sometimes he'd reach into the freezer and empty out all the food. Other times he'd sled on the deck and get a splinter. … He went to the grocery store and pulled all the pudding off the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 bags of fries. Q. What did James pull off of the shelves in the grocery store? (A) pudding, (B) fries, (C) food, (D) splinters Q. Where did James go after eating two jars of pudding? (A) grocery, (B) restaurant, (C) freezer, (D) home CNN/DailyMail RC Datasets [Hermann et al. 2015] Original Version Anonymised Version Context The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the “Top Gear” host, his lawyer said Friday. Clarkson, who hosted one of the most-watched television shows in the world, was dropped by the BBC Wednesday after an internal investigation by the British broad- caster found he had subjected producer Oisin Tymon “to an unprovoked physical and verbal attack.” . . . the ent381 producer allegedly struck by ent212 will not press charges against the “ ent153 ” host , his lawyer said friday . ent212 , who hosted one of the most - watched television shows in the world , was dropped by the ent381 wednesday after an internal investigation by the ent180 broadcaster found he had subjected producer ent193 “ to an unprovoked physical and verbal attack . ” . . . Query Producer X will not press charges against Jeremy Clarkson, his lawyer says. producer X will not press charges against ent212 , his lawyer says . Answer Oisin Tymon ent193 Table 3: Original and anonymised version of a data point from the Daily Mail validation set. The anonymised entity markers are constantly permuted during training and testing. To prevent such degenerate solutions and create a focused task we anonymise and randomise our corpora with the following procedure, a) use a coreference system to establish coreferents in each data point; b) replace all entities with abstract entity markers according to coreference; c) randomly CNN/DailyMail RC Datasets [Hermann et al. 2015] CNN Daily Mail train valid test train valid test # months 95 1 1 56 1 1 # documents 90,266 1,220 1,093 196,961 12,148 10,397 # queries 380,298 3,924 3,198 879,450 64,835 53,182 Max # entities 527 187 396 371 232 245 Avg # entities 26.4 26.5 24.5 26.5 25.5 26.0 Avg # tokens 762 763 716 813 774 780 Vocab size 118,497 208,045 Table 1: Corpus statistics. Articles were collected starting in April 2007 for CNN and June 2010 for the Daily Mail, both until the end of April 2015. Validation data is from March, test data from April 2015. Articles of over 2000 tokens and queries whose answer entity did not appear in the context were ﬁltered out. Top 1 2 3 5 10 Table 2: the corre the top N in a given Attentive/Impatient Readers [Hermann et al. 2015] r s(1)y(1) s(3)y(3) s(2)y(2) u g s(4)y(4) Mary went to X visited England England (a) Attentive Reader. r u r Mary went to X visited England England r g (b) Impatient Reader. Mary went to X visited England England ||| g (c) A two layer Deep LSTM Reader with the question encoded before the document. Figure 1: Document and query embedding models. Attentive/Impatient Readers [Hermann et al. 2015] . . . . . . Figure 3: Attention heat maps from the Attentive Reader for two correctly answered validation set queries (the correct answers are ent23 and ent63, respectively). Both examples require signiﬁcant lexical generalisation and co-reference resolution in order to be answered correctly by a given model. token integrate long range contextual information via the bidirectional LSTM encoders. Figure 3 depicts heat maps for two queries that were correctly answered by the Attentive Reader.7 In both ﬁd l i i h i h d l f b h i iﬁ l i l Attentive/Impatient Readers [Hermann et al. 2015] CNN Daily Mail valid test valid test Maximum frequency 30.5 33.2 25.6 25.5 Exclusive frequency 36.6 39.3 32.7 32.8 Frame-semantic model 36.3 40.2 35.5 35.5 Word distance model 50.5 50.9 56.4 55.5 Deep LSTM Reader 55.0 57.0 63.3 62.2 Uniform Reader 39.0 39.4 34.6 34.4 Attentive Reader 61.6 63.0 70.5 69.0 Impatient Reader 61.8 63.8 69.0 68.0 Table 5: Accuracy of all the models and bench- marks on the CNN and Daily Mail datasets. The Uniform Reader baseline sets all of the m(t) pa- rameters to be equal. Figure 2: models on Feature-based Model [Wang et al. 2015] Figure 1: Example output from SEMAFOR. ! Weighted word overlap between the bag of words constructed from the question/answer and in the window (and their word embedding versions) ! Minimal distance between two word occurrences in the passage that are also contained in the question/answer pair ! Frame semantics (predicates, frames evoked, and predicted argument labels) match between passage sentence and question+answer Feature-based Model [Wang et al. 2015] Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we ﬁrst use rules to transform the ques- rules a is give word a the ﬁrs didate transfo Afte a cand sure it using ! Syntactic dependencies match between passage sentence and ques+ans converted to statement ! Extra features computed after coreference resolution of pronouns/nominals to map to their entity clusters Multi-Hop Memory Models [Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2016; Dhingra et al. 2017] ! Several questions need multi-hop (e.g., path or count-based) reasoning to answer ! Memory models perform multiple passes over the text to collect the multiple evidence pieces ! Some example models: ! End-to-End Memory Networks ! Dynamic Memory Networks ! Gated Attention Readers End-to-End Memory Networks [Sukhbaatar et al., 2015] to produce the predicted label: ˆ a = Softmax(W(o + u)) (3) The overall model is shown in Fig. 1(a). During training, all three embedding matrices A, B and C, as well as W are jointly learned by minimizing a standard cross-entropy loss between ˆ a and the true label a. Training is performed using stochastic gradient descent (see Section 4.2 for more details). Question q Output Input Embedding B Embedding C Weights Softmax Weighted Sum pi ci mi Sentences {xi} Embedding A o W Softmax Predicted Answer a ^ u u Inner Product Out3 In3 B Sentences W a ^ {xi} o1 u1 o2 u2 o3 u3 A1 C1 A3 C3 A2 C2 Question q Out2 In2 Out1 In1 Predicted Answer (a) (b) Figure 1: (a): A single layer version of our model. (b): A three layer version of our model. In practice, we can constrain several of the embedding matrices to be the same (see Section 2.2). 2.2 Multiple Layers We now extend our model to handle K hop operations. The memory layers are stacked in the following way: Dynamic Memory Networks [Kumar et al., 2016] Ask Me Anything: Dynamic Memory Networks for Natural Language Processing Figure 3. Real example of an input list of sentences and the attention gates that are triggered by a speciﬁc question from the bAbI tasks (Weston et al., 2015a). Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. Note that the second iteration has wrongly placed some weight in sentence 2, which makes some intuitive sense, as sentence 2 is another place John had been. of TQ words, hidden states for the question encoder at time t i i b GRU(L[ Q] ) L t th pass. It also allows for a type of transitive inference, since the ﬁrst pass may uncover the need to retrieve additional Gated Attention Readers [Dhingra et al. 2017] Figure 1: Gated-Attention Reader. Dashed lines represent dropout connections. 3.1.2 Gated-Attention Module For brevity, let us drop the superscript k in this subsection as we are focusing on a particular layer. For each token di in D, the GA module forms a answer is then computed by aggregating the prob- abilities of all document tokens which appear in c and renormalizing over the candidates: X Gated Attention Readers [Dhingra et al. 2017] Analysis of AJen)on • Context: “…arrested Illinois governor Rod Blagojevich and his chief of staﬀ John Harris on corrup)on charges … included Blogojevich allegedly conspiring to sell or trade the senate seat leZ vacant by President-elect Barack Obama…” • Query: “President-elect Barack Obama said Tuesday he was not aware of alleged corrup)on by X who was arrested on charges of trying to sell Obama’s senate seat.” • Answer: Rod Blagojevich Layer 1 Layer 2 Code + Data: hJps://github.com/bdhingra/ga-reader Gated Attention Readers [Dhingra et al. 2017] Table 1: Validation/Test accuracy (%) on WDW dataset for both “Strict” and “Relaxed” settings. Results with “†” are cf previously published works. Model Strict Relaxed Val Test Val Test Human † – 84 – – Attentive Reader † – 53 – 55 AS Reader † – 57 – 59 Stanford AR † – 64 – 65 NSE † 66.5 66.2 67.0 66.7 GA-- † – 57 – 60.0 GA (update L(w)) 67.8 67.0 67.0 66.6 GA (ﬁx L(w)) 68.3 68.0 69.6 69.1 GA (+feature, update L(w)) 70.1 69.5 70.9 71.0 GA (+feature, ﬁx L(w)) 71.6 71.2 72.6 72.6 Table 2: Top: Performance of different gating functions. Bottom: Effect of varying the num- ber of hops K. Results on WDW without using the qe-comm feature and with ﬁxed L(w). Gating Function Accuracy Val Test Sum 64.9 64.5 Concatenate 64.4 63.7 Multiply 68.3 68.0 K 1 (AS) † – 57 2 65.6 65.6 3 68.3 68.0 4 68.3 68.2 and CBT, but not for CNN and Daily Mail. This is not surprising given that the latter datasets are larger and less prone to overﬁtting. Comparing with prior work, on the WDW d h b i i f h GA R d 4.3 GA Reader Analysis In this section we do an ablation study to see the effect of Gated Attention. We compare the GA Reader as described here to a model which is ex- Gated Attention Readers [Dhingra et al. 2017] Table 3: Validation/Test accuracy (%) on CNN, Daily Mail and CBT. Results marked with “†” are cf previously published works. Results marked with “‡” were obtained by training on a larger training set. Best performance on standard training sets is in bold, and on larger training sets in italics. Model CNN Daily Mail CBT-NE CBT-CN Val Test Val Test Val Test Val Test Humans (query) † – – – – – 52.0 – 64.4 Humans (context + query) † – – – – – 81.6 – 81.6 LSTMs (context + query) † – – – – 51.2 41.8 62.6 56.0 Deep LSTM Reader † 55.0 57.0 63.3 62.2 – – – – Attentive Reader † 61.6 63.0 70.5 69.0 – – – – Impatient Reader † 61.8 63.8 69.0 68.0 – – – – MemNets † 63.4 66.8 – – 70.4 66.6 64.2 63.0 AS Reader † 68.6 69.5 75.0 73.9 73.8 68.6 68.8 63.4 DER Network † 71.3 72.9 – – – – – – Stanford AR (relabeling) † 73.8 73.6 77.6 76.6 – – – – Iterative Attentive Reader † 72.6 73.3 – – 75.2 68.6 72.1 69.2 EpiReader † 73.4 74.0 – – 75.3 69.7 71.5 67.4 AoA Reader † 73.1 74.4 – – 77.8 72.0 72.2 69.4 ReasoNet † 72.9 74.7 77.6 76.6 – – – – NSE † – – – – 78.2 73.2 74.3 71.9 BiDAF † 76.3 76.9 80.3 79.6 – – – – MemNets (ensemble) † 66.2 69.4 – – – – – – AS Reader (ensemble) † 73.9 75.4 78.7 77.7 76.2 71.0 71.1 68.9 Stanford AR (relabeling,ensemble) † 77.2 77.6 80.2 79.2 – – – – Iterative Attentive Reader (ensemble) † 75.2 76.1 – – 76.9 72.0 74.1 71.0 EpiReader (ensemble) † – – – – 76.6 71.8 73.6 70.6 AS Reader (+BookTest) † ‡ – – – – 80.5 76.2 83.2 80.8 AS Reader (+BookTest,ensemble) † ‡ – – – – 82.3 78.4 85.7 83.7 GA-- 73.0 73.8 76.7 75.7 74.9 69.0 69.0 63.9 GA (update L(w)) 77.9 77.9 81.5 80.9 76.7 70.1 69.8 67.3 GA (ﬁx L(w)) 77.9 77.8 80.4 79.6 77.2 71.4 71.6 68.0 GA (+feature, update L(w)) 77.3 76.9 80.7 80.0 77.2 73.3 73.0 69.8 GA (+feature, ﬁx L(w)) 76.7 77.4 80.0 79.3 78.5 74.9 74.4 70.7 Facebook bAbI Tasks (Synthetic) [Weston et al. 2016] Under review as a conference paper at ICLR 2016 Table 1: Sample statements and questions from tasks 1 to 10. Task 1: Single Supporting Fact Task 2: Two Supporting Facts Mary went to the bathroom. John is in the playground. John moved to the hallway. John picked up the football. Mary travelled to the ofﬁce. Bob went to the kitchen. Where is Mary? A:ofﬁce Where is the football? A:playground Task 3: Three Supporting Facts Task 4: Two Argument Relations John picked up the apple. The ofﬁce is north of the bedroom. John went to the ofﬁce. The bedroom is north of the bathroom. John went to the kitchen. The kitchen is west of the garden. John dropped the apple. What is north of the bedroom? A: ofﬁce Where was the apple before the kitchen? A:ofﬁce What is the bedroom north of? A: bathroom Task 5: Three Argument Relations Task 6: Yes/No Questions Mary gave the cake to Fred. John moved to the playground. Fred gave the cake to Bill. Daniel went to the bathroom. Jeff was given the milk by Bill. John went back to the hallway. Who gave the cake to Fred? A: Mary Is John in the playground? A:no Who did Fred give the cake to? A: Bill Is Daniel in the bathroom? A:yes Task 7: Counting Task 8: Lists/Sets Daniel picked up the football. Daniel picks up the football. Daniel dropped the football. Daniel drops the newspaper. Daniel got the milk. Daniel picks up the milk. Daniel took the apple. John took the apple. How many objects is Daniel holding? A: two What is Daniel holding? milk, football Task 9: Simple Negation Task 10: Indeﬁnite Knowledge Sandra travelled to the ofﬁce. John is either in the classroom or the playground. Fred is no longer in the ofﬁce. Sandra is in the garden. Is Fred in the ofﬁce? A:no Is John in the classroom? A:maybe Is Sandra in the ofﬁce? A:yes Is John in the ofﬁce? A:no Simple Negation and Indeﬁnite Knowledge Tasks 9 and 10 test slightly more complex natural language constructs. Task 9 tests one of the simplest forms of negation, that of supporting facts that Facebook bAbI Tasks (Synthetic) [Weston et al. 2016] Under review as a conference paper at ICLR 2016 Table 2: Sample statements and questions from tasks 11 to 20. Task 11: Basic Coreference Task 12: Conjunction Daniel was in the kitchen. Mary and Jeff went to the kitchen. Then he went to the studio. Then Jeff went to the park. Sandra was in the ofﬁce. Where is Mary? A: kitchen Where is Daniel? A:studio Where is Jeff? A: park Task 13: Compound Coreference Task 14: Time Reasoning Daniel and Sandra journeyed to the ofﬁce. In the afternoon Julie went to the park. Then they went to the garden. Yesterday Julie was at school. Sandra and John travelled to the kitchen. Julie went to the cinema this evening. After that they moved to the hallway. Where did Julie go after the park? A:cinema Where is Daniel? A: garden Where was Julie before the park? A:school Task 15: Basic Deduction Task 16: Basic Induction Sheep are afraid of wolves. Lily is a swan. Cats are afraid of dogs. Lily is white. Mice are afraid of cats. Bernhard is green. Gertrude is a sheep. Greg is a swan. What is Gertrude afraid of? A:wolves What color is Greg? A:white Task 17: Positional Reasoning Task 18: Size Reasoning The triangle is to the right of the blue square. The football ﬁts in the suitcase. The red square is on top of the blue square. The suitcase ﬁts in the cupboard. The red sphere is to the right of the blue square. The box is smaller than the football. Is the red sphere to the right of the blue square? A:yes Will the box ﬁt in the suitcase? A:yes Is the red square to the left of the triangle? A:yes Will the cupboard ﬁt in the box? A:no Task 19: Path Finding Task 20: Agent’s Motivations The kitchen is north of the hallway. John is hungry. The bathroom is west of the bedroom. John goes to the kitchen. The den is east of the hallway. John grabbed the apple there. The ofﬁce is south of the bedroom. Daniel is hungry. How do you go from den to kitchen? A: west, north Where does Daniel go? A:kitchen How do you go from ofﬁce to bathroom? A: north, west Why did John go to the kitchen? A:hungry tests basic induction via inheritance of properties. A full analysis of induction and deduction is Facebook bAbI Tasks (Synthetic) [Weston et al. 2016] function (NL), and combinations thereof. Bold numbers indicate tasks where our extensions achieve ≥95% accuracy but the original MemNN model of Weston et al. (2014) did not. The last two columns (10-11) give extra analysis of the MemNN AM + NG + NL method. Column 10 gives the amount of training data for each task needed to obtain ≥95% accuracy, or FAIL if this is not achievable with 1000 training examples. The ﬁnal column gives the accuracy when training on all data at once, rather than separately. Weakly Uses External Strong Supervision Supervised Resources (using supporting facts) TASK N-gram Classiﬁer LSTM Structured SVM COREF+SRL features MemNN Weston et al. (2014) MemNN ADAPTIVE MEMORY MemNN AM + N-GRAMS MemNN AM + NONLINEAR MemNN AM + NG + NL No. of ex. req. ≥95 MultiTask Training 1 - Single Supporting Fact 36 50 99 100 100 100 100 100 250 ex. 100 2 - Two Supporting Facts 2 20 74 100 100 100 100 100 500 ex. 100 3 - Three Supporting Facts 7 20 17 20 100 99 100 100 500 ex. 98 4 - Two Arg. Relations 50 61 98 71 69 100 73 100 500 ex. 80 5 - Three Arg. Relations 20 70 83 83 83 86 86 98 1000 ex. 99 6 - Yes/No Questions 49 48 99 47 52 53 100 100 500 ex. 100 7 - Counting 52 49 69 68 78 86 83 85 FAIL 86 8 - Lists/Sets 40 45 70 77 90 88 94 91 FAIL 93 9 - Simple Negation 62 64 100 65 71 63 100 100 500 ex. 100 10 - Indeﬁnite Knowledge 45 44 99 59 57 54 97 98 1000 ex. 98 11 - Basic Coreference 29 72 100 100 100 100 100 100 250 ex. 100 12 - Conjunction 9 74 96 100 100 100 100 100 250 ex. 100 13 - Compound Coref. 26 94 99 100 100 100 100 100 250 ex. 100 14 - Time Reasoning 19 27 99 99 100 99 100 99 500 ex. 99 15 - Basic Deduction 20 21 96 74 73 100 77 100 100 ex. 100 16 - Basic Induction 43 23 24 27 100 100 100 100 100 ex. 94 17 - Positional Reasoning 46 51 61 54 46 49 57 65 FAIL 72 18 - Size Reasoning 52 52 62 57 50 74 54 95 1000 ex. 93 19 - Path Finding 0 8 49 0 9 3 15 36 FAIL 19 20 - Agent’s Motivations 76 91 95 100 100 100 100 100 250 ex. 100 Mean Performance 34 49 79 75 79 83 87 93 100 92 Methods The N gram classiﬁer baseline is inspired by the baselines in Richardson et al (2013) Who-did-What (WDW) Dataset [Onishi et al. 2016] ! Solves several issues with CNN/DM dataset: ! Starts with the selection of a question article from Gigaword corpus ! Question is formed by deleting a person named entity from the first sentence of the question article ! An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage ! Forms questions from two distinct articles rather than summary points ! Allows using documents that don’t contain manually-written summaries ! Reduces syntactic similarity between question & relevant passage sentences ! Selectively remove problems so as to suppress four simple baselines — selecting the most mentioned person, the first mentioned person, and two language model baselines ! The resulting dataset yields a larger gap between human and machine performance than existing ones, i.e., humans can answer more questions, while existing state-of-the-art models perform worse! Who-did-What (WDW) Dataset [Onishi et al. 2016] Passage: Britain’s decision on Thursday to drop extradition proceedings against Gen. Augusto Pinochet and allow him to return to Chile is understandably frustrating ... Jack Straw, the home secretary, said the 84-year-old former dictator’s ability to understand the charges against him and to direct his defense had been seriously impaired by a series of strokes. ... Chile’s president-elect, Ricardo Lagos, has wisely pledged to let justice run its course. But the outgoing government of President Eduardo Frei is pushing a constitutional reform that would allow Pinochet to step down from the Senate and retain parliamentary immunity from prosecution. ... Question: Sources close to the presidential palace said that Fujimori declined at the last moment to leave the country and instead he will send a high level delegation to the ceremony, at which Chilean President Eduardo Frei will pass the mandate to XXX. Choices: (1) Augusto Pinochet (2) Jack Straw (3) Ricardo Lagos Passage: Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display which impressed Spurs skipper Robbie Keane. ... Keane scored the ﬁrst goal at the Bloomﬁeld Stadium with Dimitar Berbatov, who insisted earlier on Thursday he was happy at the London club, heading a second. The 26-year-old Berbatov admitted the reports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup in the last two seasons ... Question: Tottenham manager Juande Ramos has hinted he will allow XXX to leave if the Bulgaria striker makes it clear he is unhappy. Choices: (1) Robbie Keane (2) Dimitar Berbatov Table 1: Sample reading comprehension problems from our dataset. the ﬁrst mentioned person, and two language model baselines. This is also intended to produce problems comprehension. The bAbI synthetic question answering dataset Who-did-What (WDW) Dataset [Onishi et al. 2016] set of problems we then lems. Duplication arises s many copies of the same ne is clearly an edited ver- plication-removal process ms have very similar ques- deﬁned as the ratio of the ntersection to the size of taset on the most interest- ome problems to suppress lowing simple baselines: Select the person that ap- e. Select the most frequent t likely answer to ﬁll the anguage model trained on s which are too similar to word overlap and phrase Accuracy Baseline Before After First person in passage 0.60 0.32 Most frequent person 0.61 0.33 n-gram 0.53 0.33 Unigram 0.43 0.32 Random∗ 0.32 0.32 Table 2: Performance of suppressed baselines. ∗Random per- formance is computed as a deterministic function of the number of times each choice set size appears. Many questions have only two choices and there are about three choices on average. relaxed train valid test train # queries 185,978 127,786 10,000 10,000 Avg # choices 3.5 3.5 3.4 3.4 Avg # tokens 378 365 325 326 Vocab size 347,406 308,602 Table 3: Dataset statistics. performance of these baselines before and after sup- Who-did-What (WDW) Dataset [Onishi et al. 2016] feature based system with attention mecha- . ntive reader modiﬁed t al., 2016). r: GRU with a point- c et al., 2016). der: Attention Sum hingra et al., 2016). ce of each system on n and Stanford Read- did-What data by re- tity IDs as in the CNN ns in accuracy when ataset. The Attentive up to 10% and the AS System WDW CNN Word overlap 0.47 – Sliding window 0.48 – Distance 0.46 – Sliding window + Distance 0.51 – Semantic features 0.52 – Attentive Reader 0.53 0.63I Attentive Reader (relaxed train) 0.55 Stanford Reader 0.64 0.73II Stanford Reader (relaxed train) 0.65 AS Reader 0.57 0.70III AS Reader (relaxed train) 0.59 GA Reader 0.57 0.74IV GA Reader (relaxed train) 0.60 Human Performance 84/100 0.75+II Table 4: System performance on test set. Human performance was computed by two annotators on a sample of 100 questions. Result marked I is from (Hermann et al., 2015), results marked II are from (Chen et al., 2016), result marked III is from SQuAd Dataset (100K Manually-Labeled) [Rajpurkar et al. 2016] ! Based on manual annotation from Mturk on Wiki articles, as opposed to cloze/fill-in-the-blank on summaries, etc.; large size (100K+) ! Answer is a span in the document: kar and Jian Zhang and Konstantin Lopyrev and Percy Liang vsr,zjian,klopyrev,pliang}@cs.stanford.edu Computer Science Department Stanford University tract ford Question Answer- a new reading compre- ting of 100,000+ ques- dworkers on a set of here the answer to each t of text from the cor- assage. We analyze the d the types of reason- er the questions, lean- dency and constituency ong logistic regression s an F1 score of 51.0%, ment over a simple base- r, human performance her, indicating that the d challenge problem for ataset is freely available a.com. In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of pre- cipitation include drizzle, rain, sleet, snow, grau- pel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, in- tense periods of rain in scattered locations are called “showers”. What causes precipitation to fall? gravity What is another main form of precipitation be- sides drizzle, rain, snow, sleet and hail? graupel Where do water droplets collide with ice crystals to form precipitation? within a cloud Figure 1: Question-answer pairs for a sample passage in the SQuAd Dataset (100K Manually-Labeled) [Rajpurkar et al. 2016] ataset v1.0 (SQuAD), tanford-qa.com, con- y crowdworkers on a ere the answer to ev- ext, or span, from the ge. SQuAD contains s on 536 articles, and tude larger than previ- asets such as MCTest ts, SQuAD does not ces for each question. e answer from all pos- s needing to cope with didates. While ques- are more constrained uestions found in more we still ﬁnd a rich di- wer types in SQuAD. ues based on distances ify this diversity and Dataset Question source Formulation Size SQuAD crowdsourced RC, spans in passage 100K MCTest (Richardson et al., 2013) crowdsourced RC, multiple choice 2640 Algebra (Kushman et al., 2014) standardized tests computation 514 Science (Clark and Etzioni, 2016) standardized tests reasoning, multiple choice 855 WikiQA (Yang et al., 2015) query logs IR, sentence selection 3047 TREC-QA (Voorhees and Tice, 2000) query logs + human editor IR, free form 1479 CNN/Daily Mail (Hermann et al., 2015) summary + cloze RC, ﬁll in single entity 1.4M CBT (Hill et al., 2015) cloze RC, ﬁll in single word 688K Table 1: A survey of several reading comprehension and ques- tion answering datasets. SQuAD is much larger than all datasets except the semi-synthetic cloze-style datasets, and it is similar to TREC-QA in the open-endedness of the answers. SQuAd Dataset (100K Manually-Labeled) [Rajpurkar et al. 2016] duce the an- et al., 2014). ynthetic RC of reasoning tzioni (2016) ience exams, d knowledge. The goal of stion from a annual eval- nce (TREC) ny advances were used in et al., 2013). the WikiQA kipedia pas- task is sen Fi 2 Th d f i b i t f d t ll t th SQuAd Dataset (100K Manually-Labeled) [Rajpurkar et al. 2016] Reasoning Description Example Percentage Lexical variation (synonymy) Major correspondences between the question and the answer sen- tence are synonyms. Q: What is the Rankine cycle sometimes called? Sentence: The Rankine cycle is sometimes re- ferred to as a practical Carnot cycle. 33.3% Lexical variation (world knowledge) Major correspondences between the question and the answer sen- tence require world knowledge to resolve. Q: Which governing bodies have veto power? Sen.: The European Parliament and the Council of the European Union have powers of amendment and veto during the legislative process. 9.1% Syntactic variation After the question is paraphrased into declarative form, its syntac- tic dependency structure does not match that of the answer sentence even after local modiﬁcations. Q: What Shakespeare scholar is currently on the faculty? Sen.: Current faculty include the anthropol- ogist Marshall Sahlins, ..., Shakespeare scholar David Bevington. 64.1% Multiple sentence reasoning There is anaphora, or higher-level fusion of multiple sentences is re- quired. Q: What collection does the V&A Theatre & Per- formance galleries hold? Sen.: The V&A Theatre & Performance gal- leries opened in March 2009. ... They hold the UK’s biggest national collection of material about live performance. 13.6% Ambiguous We don’t agree with the crowd- workers’ answer, or the question does not have a unique answer. Q: What is the main goal of criminal punishment? Sen.: Achieving crime control via incapacitation and deterrence is a major goal of criminal punish- ment. 6.1% Table 3: We manually labeled 192 examples into one or more of the above categories. Words relevant to the corresponding reasoning type are bolded, and the crowdsourced answer is underlined. Adversarial Examples for Evaluating RC Systems [Jia and Liang, 2017] e that mak- which guage ystems abili- valua- on An- method ques- adver- re au- mputer Article: Super Bowl 50 Paragraph: “Peyton Manning became the ﬁrst quarter- back ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver’s Execu- tive Vice President of Football Operations and General Manager. Quarterback Jeff Dean had jersey number 37 in Champ Bowl XXXIV.” Question: “What is the name of the quarterback who was 38 in Super Bowl XXXIII?” Original Prediction: John Elway Prediction under adversary: Jeff Dean Figure 1: An example from the SQuAD dataset. The BiDAF Ensemble model originally gets the Adversarial Examples for Evaluating RC Systems [Jia and Liang, 2017] Image Reading Classiﬁcation Comprehension Possible Input Tesla moved to the city of Chicago in 1880. Similar Input Tadakatsu moved to the city of Chicago in 1881. Semantics Same Different Model’s Considers the two Considers the two Mistake to be different to be the same Model Overly Overly Weakness sensitive stable Table 1: Adversarial examples in computer vi- sion exploit model oversensitivity to small per- turbations. In contrast, our adversarial examples work because models do not realize that a small perturbation can completely change the meaning of a sentence. Images from Szegedy et al. (2014). the fraction over which the model is robustly cor- semantics-altering per s is compatible, even words in common wi models are bad at dis from sentences that do indicating that they su but from overstability Table 1 summarizes th The decision to alw p is somewhat arbitra it to the beginning, th expectation of the ﬁrst tence. Both are more l of the example than in which runs the risk of Now, we describe adversaries, as well a our main adversary, a that look similar to Adversarial Examples for Evaluating RC Systems [Jia and Liang, 2017] Article: Nikola Tesla Paragraph: ""In January 1880, two of Tesla's uncles put together enough money to help him leave Gospi for Prague where he was to study. Unfortunately, he arrived too late to enroll at Charles-Ferdinand University; he never studied Greek, a required subject; and he was illiterate in Czech, another required subject. Tesla did, however, attend lectures at the university, although, as an auditor, he did not receive grades for the courses."" Question: ""What city did Tesla move to in 1880?"" Answer: Prague Model Predicts: Prague Tadakatsu moved the city of Chicago to in 1881. Chicago What city did Tesla move to in 1880? What city did Tadakatsu move to in 1881? Prague Adversary Adds: Tadakatsu moved to the city of Chicago in 1881. Model Predicts: Chicago (Step 1) Mutate question (Step 3) Convert into statement (Step 4) Fix errors with crowdworkers, verify resulting sentences with other crowdworkers AddSent spring attention income getting reached spring attention income other reached Adversary Adds: tesla move move other george Model Predicts: george Repeat many times Randomly initialize d words: AddAny Greedily change one word (Step 2) Generate fake answer Figure 2: An illustration of the ADDSENT and ADDANY adversaries. Adversarial Examples for Evaluating RC Systems [Jia and Liang, 2017] which iterates andom order. set of candi- ndomly sam- n q. For each h x in the i-th or each j 6= i. aragraph and ability distri- o be the x that F1 score over return imme- tion has 0 F1 ochs, we ran- quences, and ializations in ore model ac- it query the process, but ns a probabil- Table 2: Adversarial evaluation on the Match- LSTM and BiDAF systems. All four systems can be fooled by adversarial examples. Model Original ADDSENT ADDONESENT ReasoNet-E 81.1 39.4 49.8 SEDT-E 80.1 35.0 46.5 BiDAF-E 80.0 34.2 46.9 Mnemonic-E 79.1 46.2 55.3 Ruminating 78.8 37.4 47.7 jNet 78.6 37.9 47.0 Mnemonic-S 78.5 46.6 56.0 ReasoNet-S 78.2 39.4 50.3 MPCM-S 77.0 40.3 50.0 SEDT-S 76.9 33.9 44.8 RaSOR 76.2 39.5 49.5 BiDAF-S 75.5 34.3 45.7 Match-E 75.4 29.4 41.8 Match-S 71.4 27.3 39.0 DCR 69.3 37.8 45.1 Logistic 50.4 23.2 30.4 Table 3: ADDSENT and ADDONESENT on all six- teen models, sorted by F1 score the original exam- ples. S = single, E = ensemble. "
36,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, JurafskyMartin-SLP3, Manning/Socher, others) Lecture 7: Summarization; Guest Talk; Machine Translation 1 Automatic Document Summarization Single-Document Summarization Document Summarization ! Full document to a salient, non-redundant summary of ~100 words Multi-Document Summarization Multi-document Summarization … 27,000+ more ! Several news sources with articles on the same topic (can use overlapping info across articles as a good feature for summarization) Extractive Summarization ! Directly selecting existing sentences from input document instead of rewriting them Extractive Summarizatio Graph-based Extractive Summ mum Marginal Relevance h algorithms s1 s3 s2 s4 Nodes are sentences Edges are similarities Stationary distribution represents node centrality [Mihalcea et al., 2004, 2005; inter alia] Maximize Concept Coverage [Gillick and Favre, 2009] [Gillick and Favre, 2008] Universal health care is a divisive issue. Obama addressed the House on Tuesday. President Obama remained calm. concept concept value value obama 3 health 2 house 1 s1 s2 s3 s4 The health care bill is a major test for the Obama administration. summary summary length length value value {s1, s3} 17 5 {s2, s3, s4} 17 6 Length limit: 18 words greedy optimal Selection Maximize Concept Coverage [Gillick and Favre, 2009] Maximize Concept Coverage [Gillick and Favre 09] Optimization problem: Set Coverage Value of concept c Set of concepts present in summary s Set of extractive summaries of document set D Results 2009 Baseline Bigram Recall 2009 Baseline Pyramid 23.5 35.0 4.00 6.85 ! A set coverage optimization problem Maximize Concept Coverage [Gillick et al., 2008] [Gillick and Favre, 2009] [Gillick, Riedhammer, Favre, Hakkani-Tur, 2008] total concept value summary length limit maintain consistency between selected sentences and concepts nteger Linear Program for the maximum coverage model ! Can be solved using an integer linear program with constraints: Beyond Extraction: Compression [Berg-Kirkpatrick et al., 2011] Problems with Extraction It is therefore unsurprising that Lindsay pleaded not guilty yesterday afternoon to the charges filed against her, according to her publicist. What would a human do? ! If you had to write a concise summary, making effective use of the 100-word limit, you would remove some information from the lengthy sentences in the original article Beyond Extraction: Compression Sentence Rewriting [Berg-Kirkpatrick, Gillick, and Klein 11] [Berg-Kirkpatrick et al., 2011] ! Model should learn the subtree deletions/cuts that allow compression Beyond Extraction: Compression [Berg-Kirkpatrick et al., 2011] Sentence Rewriting [Berg-Kirkpatrick, Gillick, and Klein 11] ! Model should learn the subtree deletions/cuts that allow compression Beyond Extraction: Compression [Berg-Kirkpatrick et al., 2011] Sentence Rewriting [Berg-Kirkpatrick, Gillick, and Klein 11] New Optimization problem: Safe Deletions Set branch cut deletions made in creating summary s Value of deletion d How do we know how much a given deletion costs? ! The new optimization problem looks to maximize the concept values as well as safe deletion values in the candidate summary: ! To decide the value/cost of a deletion, we decide relevant deletion features and the model learns their weights: Sentence Rewriting [Berg-Kirkpatrick, Gillick, and Klein 11] New Optimization problem: Safe Deletions Set branch cut deletions made in creating summary s Value of deletion d How do we know how much a given deletion costs? Beyond Extraction: Compression [Berg-Kirkpatrick et al., 2011] Features COUNT: Bucketed document counts STOP: Stop word indicators POSITION: First document position indicators CONJ: All two- and three-way conjunctions of above BIAS: Always one f(b) Bigram Features Cut Features f(c) COORD: Coordinated phrase, four versions: NP, VP, S, SBAR S-ADJUNCT: Adjunct to matrix verb, four versions: CC, PP, ADVP, SBAR REL-C: Relative clause indicator ATTR-C: Attribution clause indicator ATTR-PP: PP attribution indicator TEMP-PP: Temporal PP indicator TEMP-NP Temporal NP indicator BIAS: Always one ! Some example features for concept bigrams and cuts/deletions: Neural Abstractive Summarization ! Mostly based on sequence-to-sequence RNN models ! Later added attention, coverage, pointer/copy, hierarchical encoder/ attention, metric rewards RL, etc. ! Examples: Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017 Feature-Augmented Encoder-Decoder a- l. al er e d n u- e, e- e a- T) h, e- at we continue to use only word-based embeddings as the representation. Hidden State Input Layer Output Layer W POS NER TF IDF W POS NER TF IDF W POS NER TF IDF W POS NER TF IDF Attention mechanism ENCODER DECODER Figure 1: Feature-rich-encoder: We use one embedding vector each for POS, NER tags and discretized TF and IDF values which are concatenated together with word-based em- [Nallapati et al., 2016] Generation+Copying [Nallapati et al., 2016] ch hi c- p, nd e- si- ple } unseen words although they do not appear in the target vocabulary.1 Hidden State ENCODER DECODER Input Layer Output Layer G P G G G Figure 2: Switching generator/pointer model: When the switch shows ’G’, the traditional generator consisting of the softmax layer is used to produce a word and when it shows Hierarchical Attention [Nallapati et al., 2016] jointly. A graphical representation of this model is displayed in Figure 3. Hidden State Word layer ENCODER DECODER Input Layer Output Layer Hidden State Sentence layer <eos> Sentence-level attention Word-level attention Figure 3: Hierarchical encoder with hierarchical attention: the attention weights at the word level, represented by the dashed arrows are re-scaled by the corresponding sentence- mar W alte 201 fram nati al. to e tent the Gig this volu dec men Pointer-Generator Networks [See et al., 2017] 7IUYIRGIXSWIUYIRGIEXXIRXMSRQSHIP 78%68"" 'SRXI\X:IGXSV +IVQER] :SGEFYPEV] (MWXVMFYXMSR E ^SS %XXIRXMSR (MWXVMFYXMSR FIEX  )RGSHIV ,MHHIR 7XEXIW (IGSHIV ,MHHIR7XEXIW +IVQER]IQIVKIZMGXSVMSYWMR[MREKEMRWX%VKIRXMRESR7EXYVHE] 7SYVGI8I\X [IMKLXIHWYQ [IMKLXIHWYQ 4EVXMEP7YQQEV] Pointer-Generator Networks [See et al., 2017] +IXXSXLITSMRX 7SYVGI8I\X +IVQER]IQIVKIZMGXSVMSYWMR[MREKEMRWX%VKIRXMRESR7EXYVHE] +IVQER]   FIEX %VKIRXMRE  TSMRX TSMRX TSMRX KIRIVEXI  &IWXSJFSXL[SVPHW I\XVEGXMSREFWXVEGXMSR ?A-RGSVTSVEXMRKGST]MRKQIGLERMWQMRWIUYIRGIXSWIUYIRGIPIEVRMRK+YIXEP ?A0ERKYEKIEWEPEXIRXZEVMEFPI (MWGVIXIKIRIVEXMZIQSHIPWJSVWIRXIRGIGSQTVIWWMSR1MESERH&PYRWSQ Pointer-Generator Networks [See et al., 2017] 7SYVGI8I\X +IVQER]IQIVKIZMGXSVMSYWMR[MREKEMRWX%VKIRXMRESR7EXYVHE]  78%68"" +IVQER] :SGEFYPEV] (MWXVMFYXMSR E ^SS FIEX 4EVXMEP7YQQEV] *MREP(MWXVMFYXMSR %VKIRXMRE E ^SS  'SRXI\X:IGXSV %XXIRXMSR (MWXVMFYXMSR )RGSHIV ,MHHIR 7XEXIW (IGSHIV ,MHHIR7XEXIW 4SMRXIVKIRIVEXSVRIX[SVO Coverage for Redundancy Reduction [See et al., 2017] 6IWYPX VITIXMXMSRVEXIVIHYGIHXS PIZIPWMQMPEVXSLYQERWYQQEVMIW 6IHYGMRKVITIXMXMSR[MXLGSZIVEKI 'SZIVEKI!GYQYPEXMZIEXXIRXMSR![LEXLEWFIIRGSZIVIHWSJEV  9WIGSZIVEKIEWI\XVEMRTYXXSEXXIRXMSRQIGLERMWQ  4IREPM^IEXXIRHMRKXSXLMRKWXLEXLEZIEPVIEH]FIIRGSZIVIH ?A1SHIPMRKGSZIVEKIJSVRIYVEPQEGLMRIXVERWPEXMSR8YIXEP ?A'SZIVEKIIQFIHHMRKQSHIPWJSVRIYVEPQEGLMRIXVERWPEXMSR1MIXEP ?A(MWXVEGXMSRFEWIHRIYVEPRIX[SVOWJSVQSHIPMRKHSGYQIRXW'LIRIXEP (SR XEXXIRHLIVI Guest Talk by Ramakanth Pasunuru: “Towards Improving Abstractive Summarization via Entailment Generation” (30 mins) [Pasunuru, Guo, Bansal. New Summarization Frontiers Workshop, EMNLP 2017] Machine Translation Machine Translation ! Useful for tons of companies, online traffic, and our international communication! Statistical Machine Translation Current statistical machine translation syst • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lot Translation Model p(f|e) French à à Pieces of English à Language M p(e) Decoder argmax p(f|e)p(e) à Proper English [Richard Socher CS224d] ! Source language f (e.g., French) ! Target language e (e.g., English) ! We want the best target (English) translation given the source (French) input sentence, hence the probabilistic formulation is: ! Using Bayes rule, we get the following (since p(f) in the denominator is independent of the argmax over e): Current statistical machine translation systems • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lots, free!) T l ti M d l L M d l Statistical Machine Translation [Richard Socher CS224d] ! The first part is known as the ‘Translation Model’ p(f|e) and is trained on parallel corpora of {f,e} sentence pairs, e.g., from EuroParl or Canadian parliament proceedings in multiple languages ! The second part p(e) is the ‘Language Model’ and can be trained on tons more monolingual data, which is much easier to find! y • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lots, free!) 4/26/16 Richard Socher 10 Translation Model p(f|e) French à à Pieces of English à Language Model p(e) Decoder argmax p(f|e)p(e) à Proper English Statistical Machine Translation 1: Alignment know which word or phrases in source language translate to what words or phrases in target ge? à Hard already! Alignments We can factor the translation model P(f | e ) by identifying alignments (correspondences) between words in f and words in e Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes spurious word ! First step in traditional machine translation is to find alignments or translational matchings between the two sentences, i.e., predict which words/phrases in French align to which words/phrases in English. ! Challenging problem: e.g., some words may not have any alignments: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 12 Alignments: harder And the program has been implemented Le programme a été mis en application zero fertility word not translated And the program has been implemented Le programme a été mis en application one-to-many alignment ! One word in the source sentence might align to several words in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 13 Alignments: harder The balance was the territory of the aboriginal people Le reste appartenait aux autochtones many-to-one alignments The balance was the territory of the aboriginal people Le reste appartenait aux autochtones Really hard :/ ! Many words in the source sentence might align to a single word in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 14 Alignments: hardest The poor don’t have any money Les pauvres sont démunis many-to-many alignment The poor dont have any money Les pauvres sont démunis phrase alignment ! And finally, many words in the source sentence might align to many words in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 15 • We could spend an entire lecture on alignment models • Not only single words but could use phrases, syntax • Then consider reordering of translated phrases Example from Philipp Koehn Translation Process • Task: translate this sentence from German into English er geht ja nicht nach hause er geht ja nicht nach hause he does not go home • Pick phrase in input, translate Chapter 6: Decoding 6 ! After learning the word and phrase alignments, the model also needs to figure out the reordering, esp. important in language pairs with very different orders! Statistical Machine Translation After many steps 4/26/16 Richard Socher 16 Each phrase in source language has many possible translations resulting in large search space: Translation Options he er geht ja nicht nach hause it , it , he is are goes go yes is , of course not do not does not is not after to according to in house home chamber at home not is not does not do not home under house return home do not it is he will be it goes he goes is are is after all does to following not after not to , not is not are not is not a • Many translation options to choose from ! After many steps, you get the large ‘phrase table’. Each phrase in the source language can have many possible translations in the target language, and hence the search space can be combinatorially large! Statistical Machine Translation Decode: Search for best of many hypotheses 4/26/16 Richard Socher 17 Hard search problem that also includes language model Decoding: Find Best Path er geht ja nicht nach hause are it he goes does not yes go to home home backtrack from highest scoring complete hypothesis ! Finally, you decode this hard search problem to find the best translation, e.g., using beam search on the several combinatorial paths through this phrase table (and also include the language model p(e) to rerank) Next Week ! IBM Alignment Model Details ! HMM Alignment Model ! Syntactic Models ! Neural Machine Translation (NMT) "
37,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, JurafskyMartin-SLP3, Manning/Socher, others) Machine Translation 2; Guest Task; Coding-HW2 Discussion Machine Translation 2 Announcements ! Robotics+ML talk today from 11am-12pm by Dr. Animesh Garg (Stanford) on “Towards Generalizable Imitation in Robotics”! ! Come back to class at 12.05pm when TA Yixin will present HW2 (on entailment classification) and we will formally release the HW today/ tomorrow. Machine Translation ! Useful for tons of companies, online traffic, and our international communication! Statistical Machine Translation Current statistical machine translation syst • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lot Translation Model p(f|e) French à à Pieces of English à Language M p(e) Decoder argmax p(f|e)p(e) à Proper English [Richard Socher CS224d] ! Source language f (e.g., French) ! Target language e (e.g., English) ! We want the best target (English) translation given the source (French) input sentence, hence the probabilistic formulation is: ! Using Bayes rule, we get the following (since p(f) in the denominator is independent of the argmax over e): Current statistical machine translation systems • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lots, free!) T l ti M d l L M d l Statistical Machine Translation [Richard Socher CS224d] ! The first part is known as the ‘Translation Model’ p(f|e) and is trained on parallel corpora of {f,e} sentence pairs, e.g., from EuroParl or Canadian parliament proceedings in multiple languages ! The second part p(e) is the ‘Language Model’ and can be trained on tons more monolingual data, which is much easier to find! y • Source language f, e.g. French • Target language e, e.g. English • Probabilistic formulation (using Bayes rule) • Translation model p(f|e) trained on parallel corpus • Language model p(e) trained on English only corpus (lots, free!) 4/26/16 Richard Socher 10 Translation Model p(f|e) French à à Pieces of English à Language Model p(e) Decoder argmax p(f|e)p(e) à Proper English Statistical Machine Translation 1: Alignment know which word or phrases in source language translate to what words or phrases in target ge? à Hard already! Alignments We can factor the translation model P(f | e ) by identifying alignments (correspondences) between words in f and words in e Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes spurious word ! First step in traditional machine translation is to find alignments or translational matchings between the two sentences, i.e., predict which words/phrases in French align to which words/phrases in English. ! Challenging problem: e.g., some words may not have any alignments: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 12 Alignments: harder And the program has been implemented Le programme a été mis en application zero fertility word not translated And the program has been implemented Le programme a été mis en application one-to-many alignment ! One word in the source sentence might align to several words in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 13 Alignments: harder The balance was the territory of the aboriginal people Le reste appartenait aux autochtones many-to-one alignments The balance was the territory of the aboriginal people Le reste appartenait aux autochtones Really hard :/ ! Many words in the source sentence might align to a single word in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 14 Alignments: hardest The poor don’t have any money Les pauvres sont démunis many-to-many alignment The poor dont have any money Les pauvres sont démunis phrase alignment ! And finally, many words in the source sentence might align to many words in the target sentence: Statistical Machine Translation Step 1: Alignment 4/26/16 Richard Socher 15 • We could spend an entire lecture on alignment models • Not only single words but could use phrases, syntax • Then consider reordering of translated phrases Example from Philipp Koehn Translation Process • Task: translate this sentence from German into English er geht ja nicht nach hause er geht ja nicht nach hause he does not go home • Pick phrase in input, translate Chapter 6: Decoding 6 ! After learning the word and phrase alignments, the model also needs to figure out the reordering, esp. important in language pairs with very different orders! Statistical Machine Translation After many steps 4/26/16 Richard Socher 16 Each phrase in source language has many possible translations resulting in large search space: Translation Options he er geht ja nicht nach hause it , it , he is are goes go yes is , of course not do not does not is not after to according to in house home chamber at home not is not does not do not home under house return home do not it is he will be it goes he goes is are is after all does to following not after not to , not is not are not is not a • Many translation options to choose from ! After many steps, you get the large ‘phrase table’. Each phrase in the source language can have many possible translations in the target language, and hence the search space can be combinatorially large! Statistical Machine Translation Decode: Search for best of many hypotheses 4/26/16 Richard Socher 17 Hard search problem that also includes language model Decoding: Find Best Path er geht ja nicht nach hause are it he goes does not yes go to home home backtrack from highest scoring complete hypothesis ! Finally, you decode this hard search problem to find the best translation, e.g., using beam search on the several combinatorial paths through this phrase table (and also include the language model p(e) to rerank) Alignment Model Details WordAlignment IBM Model 1 ! Alignments: a hidden vector called an alignment specifies which English source is responsible for each French target word. ! The first, simplest IBM model treated alignment probabilities as roughly uniform: IBMModel1(Brown93)  Alignments:ahiddenvectorcalledanalignment specifieswhichEnglish sourceisresponsibleforeachFrenchtargetword. [Brown et al., 1993] IBM Model 2 (Distortion) [Brown et al., 1993] IBMModel2  Alignmentstendtothediagonal(broadlyatleast)  Otherschemesforbiasingalignmentstowardsthediagonal:  Relativevsabsolutealignment  Asymmetricdistances  Learning a full multinomial over distances ! The next more advanced model captures the notion of ‘distortion’, i.e., how far from the diagonal is the alignment ! Other approaches for biasing alignment towards diagonal include relative vs absolute alignment, asymmetric distances, and learning a full multinomial over distances EMforModels1/2  Model1Parameters: Translationprobabilities(1+2) Distortionparameters(2only)  Startwith uniform,including  Foreachsentence:  ForeachFrenchpositionj  CalculateposterioroverEnglishpositions  (orjustusebestsinglealignment)  Incrementcountofwordfj withwordei bytheseamounts  AlsoreͲestimatedistortionprobabilitiesformodel2  Iterateuntilconvergence IBM Models 1/2 EM Training [Brown et al., 1993] EMforModels1/2  Model1Parameters: Translationprobabilities(1+2) Distortionparameters(2only)  Startwith uniform,including  Foreachsentence:  ForeachFrenchpositionj  CalculateposterioroverEnglishpositions  (orjustusebestsinglealignment)  Incrementcountofwordfj withwordei bytheseamounts  AlsoreͲestimatedistortionprobabilitiesformodel2  Iterateuntilconvergence ! Model Parameters: ! Translational Probabilities: ! Distortion Probabilities: ! Start with uniform P(fj | ei) parameters, including P(fj | null) ! For each sentence in training corpus: ! For each French position j: ! Calculate posterior over English positions using: ! Increment count of word fj with word ei by these amounts ! Similarly re-estimate distortion probabilities for Model2 ! Iterate until convergence HMM Model [Vogel et al., 1996] $ TheHMMModel 7KDQN \RX  , VKDOO GR VR JODGO\              0RGHO3DUDPHWHUV Transitions 3 $ _$  Emissions: 3 ) *UDFLDV_($ 7KDQN *UDFLDV  OR KDUp GH PX\ EXHQ JUDGR      ( ) IBM Models 3/4/5 (Fertility) [Vogel et al., 1996] IBMModels3/4/5 Mary did not slap the green witch Mary not slap slap slap the green witch Mary not slap slap slap NULL the green witch n(3|slap) Mary no daba una botefada a la verde bruja Mary no daba una botefada a la bruja verde P(NULL) t(la|the) d(j|i) [from Al-Onaizan and Knight, 1998] IBM Models 3/4/5 (Fertility) [Vogel et al., 1996] Examples:TranslationandFertility Syntactic Machine Translation Hiero HieroRules )URP>&KLDQJHWDO@ Synchronous Tree-Substitution Grammars [Shieber, 2004; Graehl et al., 2008] Joint Parsing and Alignment NP NP IN PP NP IN PP VBN VP VBD VP NP S JJ NNS ... were established in such places as Quanzhou Zhangzhou etc. 在 泉州 漳州 等 地 !立 了 ... NP P NN NP PP VP VV AS NP VP b8 b7 b4 Sample Synchronization Features NP, b8, NP NN, b7 φ▷ ◁( ) = CoarseSourceTarget⟨phrasal, phrasal⟩: 1 FineSourceTarget⟨NP, NP⟩: 1 φ▷( ) = CoarseSourceAlign⟨pos⟩: 1 FineSourceAlign⟨NN⟩: 1 Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal ITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fully synchronized bispan (b8). The inset provides some examples of active synchronization features (see Section 4.3) on these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by established, and the non-syntactic ITG word aligner misaligned I to such instead of to etc. Our joint model corrected [Burkett et al., 2011] Guest Task by Dr. Animesh Garg (Stanford): “Towards Generalizable Imitation in Robotics” (11am-12pm) Coding-HW2 Presentation by TA Yixin Nie: “Sequence-to-Label Learning for Entailment Recognition” (12.10pm-12.40pm) "
376,"What is Machine Learning? An Overview. Lecture 01 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 About this Course !2 For details -> http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Ofﬁce Hours • Sebastian Raschka: Tue 3:00-4:00, Room MSC 1171 • Shan Lu (TA): Wed 3:00-4:00 pm, Room MSC B248 When • Tue 8:00-9:15 am • Thu 8:00-9:15 am Where • SMI 331 Sebastian Raschka STAT 479: Machine Learning FS 2018 What is Machine Learning? !3 Sebastian Raschka STAT 479: Machine Learning FS 2018 !4 hka STAT479 Fall 2018. Lecture #: Placeholder Page 2 “Machine learning is the hot new thing” — John L. Hennessy, President of Stanford (2000–2016) “A breakthrough in machine learning would be worth ten Microsofts” — Bill Gates, Microsoft Co-Founder Sebastian Raschka STAT 479: Machine Learning FS 2018 !5 “Machine learning is the hot new thing” — John L. Hennessy, President of Stanford (2000–2016) “A breakthrough in machine learning would be worth ten Microsofts” — Bill Gates, Microsoft Co-Founder Sebastian Raschka STAT 479: Machine Learning FS 2018 !6 Department of Statistics University of Wisconsin–Madison http://stat.wisc.edu/⇠sraschka/teaching/stat479-fs2018/ Fall 2018 1 What is Machine Learning? An Overview. 1.1 Machine Learning – The Big Picture We develop (computer) programs to automate various kinds of processes. Originally devel- oped as a subﬁeld of Artiﬁcial Intelligence (AI), one of the goals behind machine learning was to replace the need for developing computer programs ”manually.” If programs are a means to automate processes, we can think of machine learning as ”automating automa- tion.” In other words, machine learning lets computers ”create” programs (often for making predictions) themselves. Machine learning is turning data into programs. It is said that the term machine learning was ﬁrst coined by Arthur Lee Samuel in 19591. One quote that almost every introductory machine learning resource is often accredited to Samuel, an pioneer of the ﬁeld of AI: “Machine learning is the ﬁeld of study that gives computers the ability to learn without being explicitly programmed” — Arthur L. Samuel, AI pioneer, 1959 (This is likely not an original quote but a paraphrased version of Samuel’s sentence ”Pro- gramming computers to learn from experience should eventually eliminate the need for much of this detailed programming e↵ort.”) “The ﬁeld of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience” — Tom Mitchell, former chair of the Machine Learning department of Carnegie Mellon University 1Arthur L Samuel. “Some studies in machine learning using the game of checkers”. In: IBM Journal of research and development 3.3 (1959), pp. 210–229. We develop (computer) programs to automate various kinds of processes. Originally devel- oped as a subﬁeld of Artiﬁcial Intelligence (AI), one of the goals behind machine learning was to replace the need for developing computer programs ”manually.” If programs are a means to automate processes, we can think of machine learning as ”automating automa- tion.” In other words, machine learning lets computers ”create” programs (often for making predictions) themselves. Machine learning is turning data into programs. It is said that the term machine learning was ﬁrst coined by Arthur Lee Samuel in 19591. One quote that almost every introductory machine learning resource is often accredited to Samuel, an pioneer of the ﬁeld of AI: “Machine learning is the ﬁeld of study that gives computers the ability to learn without being explicitly programmed” — Arthur L. Samuel, AI pioneer, 1959 (This is likely not an original quote but a paraphrased version of Samuel’s sentence ”Pro- gramming computers to learn from experience should eventually eliminate the need for much of this detailed programming e↵ort.”) “The ﬁeld of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience” — Tom Mitchell, former chair of the Machine Learning department of Carnegie Mellon University 1Arthur L Samuel. “Some studies in machine learning using the game of checkers”. In: IBM Journal of research and development 3.3 (1959), pp. 210–229. words, machine learning lets computers ”create” programs (often for making emselves. Machine learning is turning data into programs. he term machine learning was ﬁrst coined by Arthur Lee Samuel in 19591. almost every introductory machine learning resource is often accredited to neer of the ﬁeld of AI: “Machine learning is the ﬁeld of study that gives computers the ability to learn without being explicitly programmed” — Arthur L. Samuel, AI pioneer, 1959 not an original quote but a paraphrased version of Samuel’s sentence ”Pro- puters to learn from experience should eventually eliminate the need for much programming e↵ort.”) “The ﬁeld of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience” — Tom Mitchell, former chair of the Machine Learning department of Carnegie Mellon University Sebastian Raschka STAT 479: Machine Learning FS 2018 !7 Sebastian Raschka STAT 479: Machine Learning FS 2018 !8 Sebastian Raschka STAT 479: Machine Learning FS 2018 !9 “A breakthrough in machine learning would be worth ten Microsofts” — Bill Gates, Microsoft Co-Founder “If software ate the world, models will run it” — Steven A. Cohen and Matthew W. Granade, The Wallstreet Journal, 2018 Sebastian Raschka STAT 479: Machine Learning FS 2018 !10 Figure 1: Machine learning vs. ”classic” programming. bit more concrete, Tom Mitchell’s quote from his Machine Learning book2: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” — Tom Mitchell, Professor at Carnegie Mellon University s an example, consider a handwriting recognition learning problem (from Mitchell’s book): • Task T: recognizing and classifying handwritten words within images • Performance measure P : percent of words correctly classiﬁed • Training experience E: a database of handwritten words with given classiﬁcations .2 Applications of Machine Learning mail spam detection 2Tom M Mitchell et al. “Machine learning. 1997”. In: Burr Ridge, IL: McGraw Hill 45.37 (1997), p. 870–877. in T, as measured by P, improves with experience E. — Tom Mitchell, Professor at Carnegie Mellon University As an example, consider a handwriting recognition learning problem (from Mitchell’s book): • Task T: recognizing and classifying handwritten words within images • Performance measure P : percent of words correctly classiﬁed • Training experience E: a database of handwritten words with given classiﬁcations 1.2 Applications of Machine Learning Email spam detection 2Tom M Mitchell et al. “Machine learning. 1997”. In: Burr Ridge, IL: McGraw Hill 45.37 (1997), pp. 870–877. Sebastian Raschka STAT 479: Machine Learning FS 2018 !11 e co c ete, o tc e s quote o s ac e ea g boo “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” — Tom Mitchell, Professor at Carnegie Mellon University ample, consider a handwriting recognition learning problem (from Mitchell’s book): sk T: recognizing and classifying handwritten words within images formance measure P : percent of words correctly classiﬁed aining experience E: a database of handwritten words with given classiﬁcations Applications of Machine Learning pam detection M Mitchell et al. “Machine learning. 1997”. In: Burr Ridge, IL: McGraw Hill 45.37 (1997), 77. A bit more concrete, Tom Mitchell’s quote from his Machin “A computer program is said to learn from some class of tasks T and performance mea in T, as measured by P, improves with exp — Tom Mitchell, Profe As an example, consider a handwriting recognition learning • Task T: recognizing and classifying handwritten word • Performance measure P : percent of words correctly • Training experience E: a database of handwritten wo Handwriting Recognition Example: Sebastian Raschka STAT 479: Machine Learning FS 2018 Some Applications of Machine Learning (1): !12 Sebastian Raschka STAT 479: Machine Learning FS 2018 !13 Some Applications of Machine Learning (2): Sebastian Raschka STAT 479: Machine Learning FS 2018 !14 Categories of Machine Learning Labeled data Direct feedback Predict outcome/future No labels/targets No feedback Find hidden structure in data Decision process Reward system Learn series of actions Reinforcement Learning Unsupervised Learning Supervised Learning Sebastian Raschka STAT 479: Machine Learning FS 2018 Supervised Learning: Classiﬁcation !15 x x 1 2 Sebastian Raschka STAT 479: Machine Learning FS 2018 Supervised Learning: Regression !16 x y Sebastian Raschka STAT 479: Machine Learning FS 2018 !17 Categories of Machine Learning Labeled data Direct feedback Predict outcome/future No labels/targets No feedback Find hidden structure in data Decision process Reward system Learn series of actions Reinforcement Learning Unsupervised Learning Supervised Learning Sebastian Raschka STAT 479: Machine Learning FS 2018 Unsupervised Learning -- Clustering !18 x x 1 2 Sebastian Raschka STAT 479: Machine Learning FS 2018 Unsupervised Learning -- Dimensionality Reduction !19 Sebastian Raschka STAT 479: Machine Learning FS 2018 !20 Categories of Machine Learning Labeled data Direct feedback Predict outcome/future No labels/targets No feedback Find hidden structure in data Decision process Reward system Learn series of actions Reinforcement Learning Unsupervised Learning Supervised Learning Sebastian Raschka STAT 479: Machine Learning FS 2018 Reinforcement Learning !21 Agent Environment Reward State Action Sebastian Raschka STAT 479: Machine Learning FS 2018 Semi-Supervised Learning !22 Sebastian Raschka STAT 479: Machine Learning FS 2018 Supervised Learning (Formal Notation) !23 f(x) = y Unknown function: Hypothesis: h(x) = ̂ y h : ℝm → h : ℝm → ___ ___ Classiﬁcation Regression 풟= { < x[i], y[i] > , i = 1,… , n}, Training set: Sebastian Raschka STAT 479: Machine Learning FS 2018 Supervised Learning Workﬂow -- Overview !24 Machine Learning Algorithm New Data Predictive Model Prediction Labels Training Data Sebastian Raschka STAT 479: Machine Learning FS 2018 Supervised Learning Workﬂow -- More Detailed Overview !25 Labels Raw Data Training Dataset Test Dataset Labels New Data Labels Learning Algorithm Preprocessing Learning Evaluation Prediction Final Model Feature Extraction and Scaling Feature Selection Dimensionality Reduction Sampling Model Selection Cross-Validation Performance Metrics Hyperparameter Optimization Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Representation !26 x = x1 x2 ⋮ xm Feature vector Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Representation !27 Feature vector X = xT 1 xT 2 ⋮ xT n x = x1 x2 ⋮ xm __________ ____________ Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Representation !28 Feature vector X = xT 1 xT 2 ⋮ xT n x = x1 x2 ⋮ xm __________ ____________ X = x[1] 1 x[1] 2 ⋯ x[1] m x[2] 1 x[2] 2 ⋯ x[2] m ⋮ ⋮ ⋱ ⋮ x[n] 1 x[n] 2 ⋯ x[n] m __________ ____________ Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Representation !29 m = _____ n = _____ Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Representation !30 Input features x = x1 x2 ⋮ xm y = y[1] y[2] ⋮ y[n] ______________ Sebastian Raschka STAT 479: Machine Learning FS 2018 Hypothesis Space !31 Entire hypothesis space Hypothesis space a particular learning algorithm can sample Hypothesis space a particular learning algorithm category has access to Particular hypothesis (i.e., a model/classifier) Sebastian Raschka STAT 479: Machine Learning FS 2018 Hypothesis Space Size !32 sepal length < 5 cm sepal width < 5 cm petal length < 5 cm petal width < 5 cm Class Label True True True True Setosa True True True False Versicolor True True False True Setosa ... ... ... ... ... How many possible hypotheses? 4 binary features: __________ diﬀerent feature combinations 3 classes and (Setosa, Versicolor, Virginica) and ____________ rules, that is _______________ potential combinations Sebastian Raschka STAT 479: Machine Learning FS 2018 !33 Classes of Machine Learning Algorithms • Generalized linear models (e.g., • Support vector machines (e.g., • Artiﬁcial neural networks (e.g., • Tree- or rule-based models (e.g., • Graphical models (e.g., • Ensembles (e.g., • Instance-based learners (e.g., Sebastian Raschka STAT 479: Machine Learning FS 2018 5 Steps for Approaching a Machine Learning Application !34 1. Deﬁne the problem to be solved. 2. Collect (labeled) data. 3. Choose an algorithm class. 4. Choose an optimization metric for learning the model. 5. Choose a metric for evaluating the model. Sebastian Raschka STAT 479: Machine Learning FS 2018 !35 Labels Raw Data Training Dataset Test Dataset Labels New Data Labels Learning Algorithm Preprocessing Learning Evaluation Prediction Final Model Feature Extraction and Scaling Feature Selection Dimensionality Reduction Sampling Model Selection Cross-Validation Performance Metrics Hyperparameter Optimization Sebastian Raschka STAT 479: Machine Learning FS 2018 Objective Functions !36 • Maximize the posterior probabilities (e.g., naive Bayes) • Maximize a ﬁtness function (genetic programming) • Maximize the total reward/value function (reinforcement learning) • Maximize information gain/minimize child node impurities (CART decision tree classiﬁcation) • Minimize a mean squared error cost (or loss) function (CART, decision tree regression, linear regression, adaptive linear neurons, ...) • Maximize log-likelihood or minimize cross-entropy loss (or cost) function • Minimize hinge loss (support vector machine) Sebastian Raschka STAT 479: Machine Learning FS 2018 Optimization Methods !37 • Combinatorial search, greedy search (e.g., • Unconstrained convex optimization (e.g., • Constrained convex optimization (e.g., • Nonconvex optimization, here: using backpropagation, chain rule, reverse autodiﬀ. (e.g., • Constrained nonconvex optimization (e.g., Sebastian Raschka STAT 479: Machine Learning FS 2018 Evaluation -- Misclassiﬁcation Error !38 L( ̂ y, y) = { 0 if ̂ y = y 1 if ̂ y ≠y ERR풟test = 1 n n ∑ i=1 L( ̂ y[i], y[i]) Sebastian Raschka STAT 479: Machine Learning FS 2018 Other Metrics in Future Lectures !39 • Accuracy (1-Error) • ROC AUC • Precision • Recall • (Cross) Entropy • Likelihood • Squared Error/MSE • L-norms • Utility • Fitness • ... But more on other metrics in future lectures. Sebastian Raschka STAT 479: Machine Learning FS 2018 !40 Categorizing Machine Learning Algorithms • eager vs lazy; Sebastian Raschka STAT 479: Machine Learning FS 2018 !41 Categorizing Machine Learning Algorithms • eager vs lazy; • batch vs online; Sebastian Raschka STAT 479: Machine Learning FS 2018 !42 Categorizing Machine Learning Algorithms • eager vs lazy; • batch vs online; • parametric vs nonparametric; Sebastian Raschka STAT 479: Machine Learning FS 2018 !43 Categorizing Machine Learning Algorithms • eager vs lazy; • batch vs online; • parametric vs nonparametric; • discriminative vs generative. Sebastian Raschka STAT 479: Machine Learning FS 2018 !44 Pedro Domingo's 5 Tribes of Machine Learning Source: Domingos, Pedro. The master algorithm: How the quest for the ultimate learning machine will remake our world. Basic Books, 2015. Sebastian Raschka STAT 479: Machine Learning FS 2018 !45 rapidly in ﬁelds outside statistics. It can be used both on large data sets and as a more accurate and informative alternativ modeling on smaller data sets. If our goal as a ﬁeld is to us solve problems, then we need to move away from exclusive de on data models and adopt a more diverse set of tools. 1. INTRODUCTION Statistics starts with data. Think of the data as being generated by a black box in which a vector of input variables x (independent variables) go in one side, and on the other side the response variables y come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this: y x nature There are two goals in analyzing the data: Prediction. To be able to predict what the responses are going to be to future input variables; Information. To extract some information about how nature is associating the response variables to the input variables. There are two different approaches toward these goals: The Data Modeling Culture The analysis in this culture starts with assuming a stochastic data model for the inside of the black box. For example, a common data model is that data are generated by independent draws from The values of the para the data and the model and/or prediction. Thus t this: y linear r logistic Cox m Model validation. Yes tests and residual exam Estimated culture popu cians. The Algorithmic Modeli The analysis in this cu the box complex and unk ﬁnd a function f!x""—an x to predict the respons like this: y unk decis neur Model validation. Meas E ti t d lt Abstract. There are two cultures in the use of statistical m reach conclusions from data. One assumes that the data are by a given stochastic data model. The other uses algorithmic treats the data mechanism as unknown. The statistical comm been committed to the almost exclusive use of data models. Th ment has led to irrelevant theory, questionable conclusions, an statisticians from working on a large range of interesting cu lems. Algorithmic modeling, both in theory and practice, has rapidly in ﬁelds outside statistics. It can be used both on lar data sets and as a more accurate and informative alternat modeling on smaller data sets. If our goal as a ﬁeld is to u solve problems, then we need to move away from exclusive d on data models and adopt a more diverse set of tools. 1. INTRODUCTION Statistics starts with data. Think of the data as being generated by a black box in which a vector of input variables x (independent variables) go in one side, and on the other side the response variables y come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this: y x nature There are two goals in analyzing the data: Prediction. To be able to predict what the responses are going to be to future input variables; Information. To extract some information about how nature is associating the response variables to the input variables. The values of the par the data and the mod and/or prediction. Thu this: y linea logis Cox Model validation. Y tests and residual exa Estimated culture po cians. The Algorithmic Mode The analysis in this c the box complex and u ﬁnd a function f!x""—a x to predict the respon lik thi A B 1 Statistics start being generated b input variables x side, and on the o come out. Inside associate the pred variables, so the p y There are two g Prediction. To be are going to be t Information. To how nature is a to the input vari There are two goals: The Data Modelin The analysis in a stochastic data box. For example, are generated by response variable Leo Breiman is P University of Cali Breiman, Leo. ""Statistical modeling: The two cultures (with comments and a rejoinder by the author). "" Statistical science 16.3 (2001): 199-231. Sebastian Raschka STAT 479: Machine Learning FS 2018 !46 in ﬁelds outside statistics. It can be used both on large complex ts and as a more accurate and informative alternative to data ng on smaller data sets. If our goal as a ﬁeld is to use data to roblems, then we need to move away from exclusive dependence models and adopt a more diverse set of tools. DUCTION ata. Think of the data as k box in which a vector of ndent variables) go in one e the response variables y k box, nature functions to ariables with the response s like this: x ure analyzing the data: predict what the responses input variables; some information about ng the response variables approaches toward these re ture starts with assuming or the inside of the black on data model is that data dent draws from The values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is ﬁlled in like this: y x linear regression logistic regression Cox model Model validation. Yes–no using goodness-of-ﬁt tests and residual examination. Estimated culture population. 98% of all statisti- cians. The Algorithmic Modeling Culture The analysis in this culture considers the inside of the box complex and unknown. Their approach is to ﬁnd a function f!x""—an algorithm that operates on x to predict the responses y. Their black box looks like this: y x unknown decision trees neural nets Model validation. Measured by predictive accuracy. act. There are two cultures in the use of statistical modeling to conclusions from data. One assumes that the data are generated iven stochastic data model. The other uses algorithmic models and the data mechanism as unknown. The statistical community has committed to the almost exclusive use of data models. This commit- has led to irrelevant theory, questionable conclusions, and has kept ticians from working on a large range of interesting current prob- Algorithmic modeling, both in theory and practice, has developed y in ﬁelds outside statistics. It can be used both on large complex sets and as a more accurate and informative alternative to data ing on smaller data sets. If our goal as a ﬁeld is to use data to problems, then we need to move away from exclusive dependence ta models and adopt a more diverse set of tools. ODUCTION data. Think of the data as ack box in which a vector of pendent variables) go in one ide the response variables y ack box, nature functions to variables with the response is like this: x ature n analyzing the data: o predict what the responses re input variables; ct some information about ting the response variables The values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is ﬁlled in like this: y x linear regression logistic regression Cox model Model validation. Yes–no using goodness-of-ﬁt tests and residual examination. Estimated culture population. 98% of all statisti- cians. The Algorithmic Modeling Culture The analysis in this culture considers the inside of the box complex and unknown. Their approach is to ﬁnd a function f!x""—an algorithm that operates on x to predict the responses y. Their black box looks B C 1. INTRODUCTION Statistics starts with data. Think of the data as being generated by a black box in which a vector of input variables x (independent variables) go in one side, and on the other side the response variables y come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this: y x nature There are two goals in analyzing the data: Prediction. To be able to predict what the responses are going to be to future input variables; Information. To extract some information about how nature is associating the response variables to the input variables. There are two different approaches toward these goals: The Data Modeling Culture The analysis in this culture starts with assuming a stochastic data model for the inside of the black box. For example, a common data model is that data are generated by independent draws from response variables = f(predictor variables, random noise, parameters) Leo Breiman is Professor, Department of Statistics, U i i f C lif i B k l C lif i 94720 The values of the the data and the m and/or prediction. T this: y li lo C Model validation. tests and residual Estimated culture cians. The Algorithmic Mo The analysis in th the box complex an ﬁnd a function f!x"" x to predict the res like this: y Model validation. M Estimated culture many in other ﬁel In this paper I w statistical communi L d i l Breiman, Leo. ""Statistical modeling: The two cultures (with comments and a rejoinder by the author). "" Statistical science 16.3 (2001): 199-231. Sebastian Raschka STAT 479: Machine Learning FS 2018 !47 ata a to nce s are estimated from used for information ack box is ﬁlled in like x ion ssion using goodness-of-ﬁt on. n. 98% of all statisti- lture considers the inside of . Their approach is to ithm that operates on Their black box looks x es by predictive accuracy. n 2% of statisticians ng to rated s and y has mmit- kept prob- loped mplex data ata to dence ers are estimated from n used for information black box is ﬁlled in like x ssion ression using goodness-of-ﬁt tion. on. 98% of all statisti- Culture e considers the inside of wn. Their approach is to orithm that operates on . Their black box looks C RODUCTION data. Think of the data as ack box in which a vector of pendent variables) go in one ide the response variables y ack box, nature functions to variables with the response e is like this: x nature n analyzing the data: o predict what the responses re input variables; act some information about ting the response variables nt approaches toward these ture ulture starts with assuming l for the inside of the black mmon data model is that data endent draws from (predictor variables, random noise, parameters) or, Department of Statistics, , Berkeley, California 94720- The values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is ﬁlled in like this: y x linear regression logistic regression Cox model Model validation. Yes–no using goodness-of-ﬁt tests and residual examination. Estimated culture population. 98% of all statisti- cians. The Algorithmic Modeling Culture The analysis in this culture considers the inside of the box complex and unknown. Their approach is to ﬁnd a function f!x""—an algorithm that operates on x to predict the responses y. Their black box looks like this: y x unknown decision trees neural nets Model validation. Measured by predictive accuracy. Estimated culture population. 2% of statisticians, many in other ﬁelds. In this paper I will argue that the focus in the statistical community on data models has: • Led to irrelevant theory and questionable sci- Breiman, Leo. ""Statistical modeling: The two cultures (with comments and a rejoinder by the author). "" Statistical science 16.3 (2001): 199-231. Sebastian Raschka STAT 479: Machine Learning FS 2018 !48 Sebastian Raschka STAT 479: Machine Learning FS 2018 !49 Evolved antenna (Source: https://en.wikipedia.org/wiki/Evolved\_antenna) via evolutionary algorithms; used on a 2006 NASA spacecraft. Sebastian Raschka STAT 479: Machine Learning FS 2018 !50 Black Boxes vs Interpretability Sebastian Raschka STAT 479: Machine Learning FS 2018 !51 Black Boxes vs Interpretability Sebastian Raschka STAT 479: Machine Learning FS 2018 !52 Different Motivations for Studying Machine Learning • Engineers: • Mathematicians, computer scientists, and statisticians: • Neuroscientists: Sebastian Raschka STAT 479: Machine Learning FS 2018 The Relationship between Machine Learning and Other Fields !53 Machine Learning and Data Mining Sebastian Raschka STAT 479: Machine Learning FS 2018 Machine Learning, AI, and Deep Learning !54 Machine Learning AI Deep Learning Sebastian Raschka STAT 479: Machine Learning FS 2018 !55 Image by Jake VanderPlas; Source: https://speakerdeck.com/jakevdp/the-state-of-the-stack-scipy-2015-keynote?slide=8) Sebastian Raschka STAT 479: Machine Learning FS 2018 !56 https://www.tiobe.com/tiobe-index/ TIOBE Index for September 2018 https://www.tiobe.com/tiobe-index/programming-languages-deﬁnition/ Programming language ""popularity"" Sebastian Raschka STAT 479: Machine Learning FS 2018 !57 http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/#schedule Roadmap for this Course Sebastian Raschka STAT 479: Machine Learning FS 2018 Reading Assignments !58 • Raschka and Mirjalili: Python Machine Learning, 2nd ed., Ch 1 • Elements of Statistical Learning, Ch 01 (https://web.stanford.edu/~hastie/ElemStatLearn/) "
377,"Nearest Neighbor Methods Lecture 02 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 1-Nearest Neighbor !2 ? ? Sebastian Raschka STAT 479: Machine Learning FS 2018 1-Nearest Neighbor !3 ? ? Sebastian Raschka STAT 479: Machine Learning FS 2018 Training Step !4 ⟨x[i], y[i]⟩∈𝒟 (|𝒟| = n) Sebastian Raschka STAT 479: Machine Learning FS 2018 !5 closest_point := None closest_distance := for : current_distance := if current_distance < closest_distance: closest_distance := current_distance closest_point := return closest_point closest_point is the label of 1-Nearest Neighbor Prediction Step Sebastian Raschka STAT 479: Machine Learning FS 2018 !6 d(x[a], x[b]) = m ∑ j=1 (x[a] j −x[b] j ) 2 Commonly used: Euclidean Distance (L2) Sebastian Raschka STAT 479: Machine Learning FS 2018 !7 Nearest Neighbor Decision Boundary Sebastian Raschka STAT 479: Machine Learning FS 2018 Decision Boundary Between (a) and (b) !8 a b a Sebastian Raschka STAT 479: Machine Learning FS 2018 Decision Boundary Between (a) and (c) !9 a c Sebastian Raschka STAT 479: Machine Learning FS 2018 Decision Boundary Between (a) and (c) !10 a a c d Sebastian Raschka STAT 479: Machine Learning FS 2018 Decision Boundary 1NN !11 a b a c c d Sebastian Raschka STAT 479: Machine Learning FS 2018 Decision Boundary 1-NN !12 Sebastian Raschka STAT 479: Machine Learning FS 2018 Which Point is Closest? !13 ? a b c Euclid distan Sebastian Raschka STAT 479: Machine Learning FS 2018 Depends on the Distance Measure! !14 ? ? a b c Euclidean distance=1 Manhattan distance=1 ? a b ? a b Sebastian Raschka STAT 479: Machine Learning FS 2018 Continuous Distance Measures !15 d(x[a], x[b]) = [ m ∑ j=1 ( x[a] −x[b] ) p ] 1 p Mahalanobis ... Minkowski: Euclidean Manhattan Sebastian Raschka STAT 479: Machine Learning FS 2018 Discrete Distance Measures !16 d(x[a], x[b]) = m ∑ j=1 x[a] −x[b] Jaccard/Tanimoto Cosine similarity Dice ... Hamming: Sebastian Raschka STAT 479: Machine Learning FS 2018 Feature Scaling !17 Euclidean distance=1 Manhattan distance=1 ? a c b ? a c b Euclidean distance=1 Euclidean distance=1 Sebastian Raschka STAT 479: Machine Learning FS 2018 !18 k-Nearest Neighbors Sebastian Raschka STAT 479: Machine Learning FS 2018 !19 Majority vote: Purality vote: Majority vote: Purality vote: None y: y: A B Sebastian Raschka STAT 479: Machine Learning FS 2018 kNN for Classiﬁcation !20 𝒟k = {⟨x[1], f(x[1])⟩, …, ⟨x[k], f(x[k])⟩} 𝒟k ⊆𝒟 Sebastian Raschka STAT 479: Machine Learning FS 2018 kNN for Classiﬁcation !21 𝒟k = {⟨x[1], f(x[1])⟩, …, ⟨x[k], f(x[k])⟩} h(x[q]) = arg max y∈{1,...,t} k ∑ i=1 δ(y, f(x[i])) δ(a, b) = { 1, if a = b, 0, if a ≠b . 𝒟k ⊆𝒟 Sebastian Raschka STAT 479: Machine Learning FS 2018 kNN for Classiﬁcation !22 𝒟k = {⟨x[1], f(x[1])⟩, …, ⟨x[k], f(x[k])⟩} h(x[q]) = arg max y∈{1,...,t} k ∑ i=1 δ(y, f(x[i])) δ(a, b) = { 1, if a = b, 0, if a ≠b . h(x[t]) = mode({f(x[1]), …, f(x[k])}) 𝒟k ⊆𝒟 Sebastian Raschka STAT 479: Machine Learning FS 2018 !23 h(x[t]) = 1 k k ∑ i=1 f(x[i]) kNN for Regression 𝒟k = {⟨x[1], f(x[1])⟩, …, ⟨x[k], f(x[k])⟩} 𝒟k ⊆𝒟 Sebastian Raschka STAT 479: Machine Learning FS 2018 Categories (Last Lecture) !24 • eager vs lazy; • batch vs online; • parametric vs nonparametric; • discriminative vs generative. Sebastian Raschka STAT 479: Machine Learning FS 2018 Big-O !25 tic behavior of functions, i.e., the asymptotic upper bounds. In the context of algorit computer science, it is most commonly use to measure the time complexity or run an algorithm for the worst case scenario. (Often, it is also used to measure mem quirements.) nce Big-O and complexity ﬁeld of research in computer science, we will not go into uch detail in this course. However, you should at leat be familar with the basic conc nce it is import for the study of machine learning algorithms. f(n) Name 1 Constant log n Logarithmic n Linear n log n Log Linear n2 Quadratic n3 Cubic nc Higher-level polynomial 2n Exponential Sebastian Raschka STAT 479: Machine Learning FS 2018 Big-O !26 s used in both mathematics and computer science to study the asymp- tions, i.e., the asymptotic upper bounds. In the context of algorithms it is most commonly use to measure the time complexity or runtime the worst case scenario. (Often, it is also used to measure memory mplexity ﬁeld of research in computer science, we will not go into too ourse. However, you should at leat be familar with the basic concepts, the study of machine learning algorithms. f(n) Name 1 Constant log n Logarithmic n Linear n log n Log Linear n2 Quadratic n3 Cubic nc Higher-level polynomial 2n Exponential Sebastian Raschka STAT 479: Machine Learning FS 2018 Big-O Example 1 !27 f(x) = 14x2 −10x + 25 Sebastian Raschka STAT 479: Machine Learning FS 2018 Big-O Example 2 !28 f(x) = (2x + 8)log2(x + 9) Sebastian Raschka STAT 479: Machine Learning FS 2018 Big-O Example 3 !29 In [16]: In [17]: Out[16]: [[38, 56], [56, 83]] A = [[1, 2, 3], [2, 3, 4]] B = [[5, 8], [6, 9], [7, 10]] def matrixmultiply (A, B): C = [[0 for row in range(len(A))] for col in range(len(B[0]))] for row_a in range(len(A)): for col_b in range(len(B[0])): for col_a in range(len(A[0])): C[row_a][col_b] += \ A[row_a][col_a] * B[col_a][col_b] return C matrixmultiply(A, B) Sebastian Raschka STAT 479: Machine Learning FS 2018 Big O of kNN !30 Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Computational Performance !31 Sebastian Raschka STAT 479: Machine Learning FS 2018 !32 Naive Nearest Neighbor Search O( ____ ) 2.7 Improving Computational Performance 2.7.1 Naive kNN Algorithm in Pseudocode Below are two naive approaches (Variant A and Variant B) for ﬁnding the k nearest neighbors of a query point x[q]. Variant A Dk := {} while |Dk| < k: • closest distance := 1 • for i = 1, ..., n, 8i / 2 Dk: – current distance := d(x[i], x[q]) – if current distance < closest distance: ⇤closest distance := current distance ⇤closest point := x[i] • add closest point to Dk Variant B Dk := D while |Dk| > k: Sebastian Raschka STAT 479: Machine Learning FS 2018 !33 Naive Nearest Neighbor Search O( ____ ) • for i = 1, ..., n, 8i / 2 Dk: – current distance := d(x[i], x[q]) – if current distance < closest distance: ⇤closest distance := current distance ⇤closest point := x[i] • add closest point to Dk Variant B Dk := D while |Dk| > k: • largest distance := 0 • for i = 1, ..., n 8i 2 Dk: – current distance := d(x[i], x[q]) – if current distance > largest distance: ⇤largest distance := current distance ⇤farthest point := x[i] Sebastian Raschka STAT 479: Machine Learning FS 2018 !34 Naive Nearest Neighbor Search O( ____ ) Using a priority queue Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Computational Performance !35 Data Structures Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Computational Performance !36 Dimensionality Reduction Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Computational Performance !37 Editing / ""Pruning"" Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Computational Performance !38 Prototypes Sebastian Raschka STAT 479: Machine Learning FS 2018 Improving Predictive Performance !39 Sebastian Raschka STAT 479: Machine Learning FS 2018 Hyperparameters !40 • Value of k • Scaling of the feature axes • Distance measure • Weighting of the distance measure Sebastian Raschka STAT 479: Machine Learning FS 2018 !41 k ∈{1,3,7} k = _ k = _ k = _ Sebastian Raschka STAT 479: Machine Learning FS 2018 Feature-Weighting via Euclidean Distance !42 dw(x[a], x[b]) = m ∑ j=1 wj(x[a] j −x[b] j ) 2 c = x[a] −x[a], (c, x[a]x[b] ∈ℝm) d(x[a], x[b]) = ctc dw(x[a], x[b]) = cTWc, W ∈ℝm×m = diag(w1, w2, . . . , wm) As a dot product: Sebastian Raschka STAT 479: Machine Learning FS 2018 Distance-weighted kNN !43 h(x[t]) = arg max j∈{1,...,p} k ∑ i=1 w[i]δ(j, f(x[i])) w[i] = 1 d(x[i], x[t])2 Small constant to avoid zero division or set h(x) = f(x) Sebastian Raschka STAT 479: Machine Learning FS 2018 kNN in Python !44 DEMO Sebastian Raschka STAT 479: Machine Learning FS 2018 Reading Assignments !45 • Lecture notes (will be uploaded after this lecture!) • Elements of Statistical Learning, Ch 02, Sections 2.0-2.3 (https://web.stanford.edu/~hastie/ElemStatLearn/) Sebastian Raschka STAT 479: Machine Learning FS 2018 Ungraded Homework Assignment !46 For those who are new to Python, I highly recommend getting some practice. E.g., by solving some interactive learning exercises on https://www.codecademy.com/learn/learn-python "
378,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Data Preprocessing and Machine Learning with Scikit-Learn (Computational Foundations Part 3/3) Lecture 05 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Sebastian Raschka STAT 479: Machine Learning FS 2018 !3 Labels Raw Data Training Dataset Test Dataset Labels New Data Labels Learning Algorithm Preprocessing Learning Evaluation Prediction Final Model Feature Extraction and Scaling Feature Selection Dimensionality Reduction Sampling Model Selection Cross-Validation Performance Metrics Hyperparameter Optimization Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Reading a Dataset from a Tabular Text File Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Iris-Versicolor Iris-Virginica Iris-Setosa Fisher, R.A. ""The use of multiple measurements in taxonomic problems"" Annual Eugenics, 7, Part II, 179-188 (1936); also in ""Contributions to Mathematical Statistics"" (John Wiley, NY, 1950). Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 https://pandas.pydata.org McKinney, Wes. ""Data structures for statistical computing in python."" Proceedings of the 9th Python in Science Conference. Vol. 445. 2010. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 https://pandas.pydata.org Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Basic Data Handling Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Raschka, Sebastian. ""MLxtend: Providing machine learning and data science utilities and extensions to Python’s scientiﬁc computing stack."" The Journal of Open Source Software 3.24 (2018). http://rasbt.github.io/mlxtend/ MLXTEND Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 Python Classes Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 http://scikit-learn.org Pedregosa, Fabian, et al. ""Scikit-learn: Machine learning in Python."" Journal of machine learning research 12.Oct (2011): 2825-2830. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 Training Data Model Training Labels Predicted labels Test Data est.fit(X_train, y_train) est.predict(X_test) ① ② Scikit-learn Estimator API Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 35 Sebastian Raschka STAT 479: Machine Learning FS 2018 !36 or less. Now, further subsampling without replacement alters the statistic (mean, proportion, and variance) of the sample. The degree to which subsampling without replacement affects the statistic of a sample is inversely proportional to the size of the sample. Let us have a look at an example using the Iris dataset 1, which we randomly divide into 2/3 training data and 1/3 test data as illustrated in Figure 1. (The source code for generating this graphic is available on GitHub2.) All samples (n = 150) Training samples (n = 100) Test samples (n = 50) Figure 1: Distribution of Iris ﬂower classes upon random subsampling into training and test sets. Issues with Subsampling Sebastian Raschka STAT 479: Machine Learning FS 2018 !37 Stratiﬁed Split Sebastian Raschka STAT 479: Machine Learning FS 2018 Normalization: Min-Max Scaling !38 x[i] norm = x[i] −xmin xmax −xmin Sebastian Raschka STAT 479: Machine Learning FS 2018 Normalization: Min-Max Scaling !39 x[i] norm = x[i] −xmin xmax −xmin Sebastian Raschka STAT 479: Machine Learning FS 2018 Normalization: Standardization !40 x[i] std = x[i] −μx σx Sebastian Raschka STAT 479: Machine Learning FS 2018 Normalization: Standardization !41 x[i] std = x[i] −μx σx Sebastian Raschka STAT 479: Machine Learning FS 2018 Normalization: Standardization !42 Sebastian Raschka STAT 479: Machine Learning FS 2018 Sample vs Population Standard Deviation !43 sx = 1 n −1 i=1 ∑ n (x[i] −¯ x)2 σx = 1 n i=1 ∑ n (x[i] −μx)2 Sebastian Raschka STAT 479: Machine Learning FS 2018 Sample vs Population Standard Deviation !44 sx = 1 n −1 i=1 ∑ n (x[i] −¯ x)2 σx = 1 n i=1 ∑ n (x[i] −μx)2 Sebastian Raschka STAT 479: Machine Learning FS 2018 !45 Scaling Validation and Test Sets Sebastian Raschka STAT 479: Machine Learning FS 2018 !46 Scaling Validation and Test Sets Given 3 training examples: - example1: 10 cm -> class 2 - example2: 20 cm -> class 2 - example3: 30 cm -> class 1 Estimate: mean: 20 cm standard deviation: 8.2 cm Sebastian Raschka STAT 479: Machine Learning FS 2018 !47 Scaling Validation and Test Sets Given 3 training examples: - example1: 10 cm -> class 2 - example2: 20 cm -> class 2 - example3: 30 cm -> class 1 Estimate: mean: 20 cm standard deviation: 8.2 cm Standardize: - example1: -1.21 -> class 2 - example2: 0.00 -> class 2 - example3: 1.21 -> class 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 !48 Scaling Validation and Test Sets Given 3 training examples: - example1: 10 cm -> class 2 - example2: 20 cm -> class 2 - example3: 30 cm -> class 1 Estimate: mean: 20 cm standard deviation: 8.2 cm Standardize (z scores): - example1: -1.21 -> class 2 - example2: 0.00 -> class 2 - example3: 1.21 -> class 1 h(z) = { 2 z ≤0.6 1 otherwise Sebastian Raschka STAT 479: Machine Learning FS 2018 !49 Scaling Validation and Test Sets Given 3 training examples: - example1: 10 cm -> class 2 - example2: 20 cm -> class 2 - example3: 30 cm -> class 1 Estimate: mean: 20 cm standard deviation: 8.2 cm Standardize (z scores): - example1: -1.21 -> class 2 - example2: 0.00 -> class 2 - example3: 1.21 -> class 1 h(z) = { 2 z ≤0.6 1 otherwise Given 3 NEW examples: - example4: 5 cm -> class ? - example5: 6 cm -> class ? - example6: 7 cm -> class ? Estimate ""new"" mean and std.: - example5: -1.21 -> class 2 - example6: 0.00 -> class 2 - example7: 1.21 -> class 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 !50 Scaling Validation and Test Sets Given 3 training examples: - example1: 10 cm -> class 2 - example2: 20 cm -> class 2 - example3: 30 cm -> class 1 Estimate: mean: 20 cm standard deviation: 8.2 cm Standardize (z scores): - example1: -1.21 -> class 2 - example2: 0.00 -> class 2 - example3: 1.21 -> class 1 h(z) = { 2 z ≤0.6 1 otherwise - example4: 5 cm -> class ? - example5: 6 cm -> class ? - example6: 7 cm -> class ? Estimate ""new"" mean and std.: - example5: -1.21 -> class 2 - example6: 0.00 -> class 2 - example7: 1.21 -> class 1 - example5: -18.37 - example6: -17.15 - example7: -15.92 Sebastian Raschka STAT 479: Machine Learning FS 2018 !51 Training Data Model Transformed Test Data Test Data Transformed Training Data est.fit(X_train) est.transform(X_train) est.transform(X_test) ① ② ③ Scikit-Learn Transformer API Sebastian Raschka STAT 479: Machine Learning FS 2018 !52 Scikit-Learn Transformer API Sebastian Raschka STAT 479: Machine Learning FS 2018 !53 Sebastian Raschka STAT 479: Machine Learning FS 2018 Categorical: Ordinal !54 Sebastian Raschka STAT 479: Machine Learning FS 2018 Categorical: Ordinal !55 Sebastian Raschka STAT 479: Machine Learning FS 2018 Categorical: Nominal !56 Sebastian Raschka STAT 479: Machine Learning FS 2018 One-hot Encoding !57 Sebastian Raschka STAT 479: Machine Learning FS 2018 One-hot Encoding !58 Sebastian Raschka STAT 479: Machine Learning FS 2018 !59 Sebastian Raschka STAT 479: Machine Learning FS 2018 !60 Sebastian Raschka STAT 479: Machine Learning FS 2018 !61 Sebastian Raschka STAT 479: Machine Learning FS 2018 Scikit-Learn Pipelines !62 Training set Test set Scaling Dimensionality Reduction Learning Algorithm .fit(…) & .transform(…) .fit(…) & .transform(…) .fit(…) Predictive Model .transform(…) .transform(…) .predict(…) pipeline.fit(…) Class labels pipeline.predict(…) Class labels (Step 1) (Step 2) Pipeline Sebastian Raschka STAT 479: Machine Learning FS 2018 Scikit-Learn Pipelines !63 Sebastian Raschka STAT 479: Machine Learning FS 2018 Scikit-Learn Pipelines !64 Sebastian Raschka STAT 479: Machine Learning FS 2018 Scikit-Learn Pipelines !65 Training set Test set Scaling Dimensionality Reduction Learning Algorithm .fit(…) & .transform(…) .fit(…) & .transform(…) .fit(…) Predictive Model .transform(…) .transform(…) .predict(…) pipeline.fit(…) Class labels pipeline.predict(…) Class labels (Step 1) (Step 2) Pipeline Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Selection: Simple Holdout Method !66 Original dataset Training set Validation set Test set Training set Test set Machine learning algorithm Predictive model Change hyperparameters and repeat Final performance estimate Fit Evaluate Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Selection: Simple Holdout Method !67 Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Selection: Simple Holdout Method !68 Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Selection: Simple Holdout Method !69 Sebastian Raschka STAT 479: Machine Learning FS 2018 !70 Reading Assignments • Python Machine Learning, 2nd ed.: Ch04 up to ""Selecting Meaningful Features"" (pg 107-123) • Python Machine Learning, 2nd ed.: Ch06 up to ""Debugging Algorithms with Learning and Validation Curves"" (pg 185-194) "
379,"Decision Trees Lecture 06 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ ! 2 Work to do? Stay in Outlook? Sunny Rainy Go to beach Friends busy? Over- cast No Yes Go running Stay in Yes No Go to movies Leaf node Internal node Branch Root node Decision Tree Terminology Decision Trees as Rulesets IF _________________________ ___ ______________________ ___ ______________________ THEN ________ ___ ___________ ! 3 ! 4 • Supervised vs. unsupervised learning algorithm • classiﬁcation vs. regression • Optimization method: ______ • Eager vs. lazy learning algorithm • Batch vs. online learning algorithm • Parametric vs. nonparametric model • Deterministic vs. stochastic Decision Trees and ML Categories ! 5 Recursion / Recursive Algorithms Divide-and-Conquer and Recu Simple recursive algorithms to compute the length of Divide-and-conquer is a concept in computer science divide a problem into subproblems of the same type. Usually, divide-and-conquer can be implemented usin def array_len(x): if x == []: return 0 else: return 1 + array_len(x[1:]) 1 2 3 4 5 some_func some_func What does this function do? ! 6 Divide & Conquer Algorithms def quicksort(array): if len(array) < 2: return array else: pivot = array[0] smaller, bigger = [], [] for ele in array[1:]: if ele <= pivot: smaller.append(ele) else: bigger.append(ele) return quicksort(smaller) + [pivot] + quicksort(bigger) 1 2 3 4 5 6 7 8 9 10 11 12 ! 7 Divide & Conquer Algorithms def quicksort(array): if len(array) < 2: return array else: pivot = array[0] smaller, bigger = [], [] for ele in array[1:]: if ele <= pivot: smaller.append(ele) else: bigger.append(ele) return quicksort(smaller) + [pivot] + quicksort(bigger) 1 2 3 4 5 6 7 8 9 10 11 12 ! 8 Time complexity of quicksort: _____________ (""on average"") def quicksort(array): if len(array) < 2: return array else: pivot = array[0] smaller, bigger = [], [] for ele in array[1:]: if ele <= pivot: smaller.append(ele) else: bigger.append(ele) return quicksort(smaller) + [pivot] + quicksort(bigger) 1 2 3 4 5 6 7 8 9 10 11 12 ! 9 * ""worst"" worst (inversely-sorted array) * http://www.bigocheatsheet.com ! 10 Time Complexity (""Big-O"") Growing the tree: Tip: It can be shown that optimal split is on boundary between adjacent examples (similar feature value) with diﬀerent class labels. Fayyad, Usama Mohammad. ""On the induction of decision trees for multiple concept learning."" (1992). ! 11 Time Complexity (""Big-O"") Querying the tree: ! 12 More formal: GenerateTree( ): if or : return Tree else: Pick best feature : at at return Node( , GenerateTree( ), GenerateTree( )) ! 13 Generic Tree Growing Algorithm 1) Pick the feature that, when parent node is split, results in the largest information gain 2) Stop if child nodes are pure or information gain <= 0 3) Go back to step 1 for each of the two child nodes ! 14 Generic Tree Growing Algorithm 1) Pick the feature that, when parent node is split, results in the largest information gain 2) Stop if child nodes are pure or information gain <= 0 3) Go back to step 1 for each of the two child nodes • How make predictions of features in dataset not suﬃcient to make child nodes pure? ____________________________ ! 15 Design choices • How to split ◦ what measurement/criterion as measure of goodness ◦ binary vs multi-category split • When to stop ◦ if leaf nodes contain only examples of the same class ◦ feature values are all the same for all examples ◦ statistical signiﬁcance test ! 16 ID3 -- Iterative Dichotomizer 3 • one of the earlier/earliest decision tree algorithms • Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1, 1 (Mar. 1986), 81-106. • cannot handle numeric features • no pruning, prone to overﬁtting • short and wide trees (compared to CART) • maximizing information gain/minimizing entropy • discrete features, binary and multi-category features ! 17 C4.5 • continuous and discrete features • Ross Quinlan 1993, Quinlan, J. R. (1993). C4. 5: Programming for machine learning. Morgan Kauﬀmann, 38, 48. • continuous is very expensive, because must consider all possible ranges • handles missing attributes (ignores them in gain compute) • post-pruning (bottom-up pruning) • Gain Ratio ! 18 CART • Breiman, L. (1984). Classiﬁcation and regression trees. Belmont, Calif: Wadsworth International Group. • continuous and discrete features • strictly binary splits (taller trees than ID3, C4.5) • binary splits can generate better trees than C4.5, but tend to be larger and harder to interpret; k-attributes has a ways to create a binary partitioning • variance reduction in regression trees • Gini impurity, twoing criteria in classiﬁcation trees • cost complexity pruning ! 19 Others • CHAID (CHi-squared Automatic Interaction Detector); Kass, G. V. (1980). ""An exploratory technique for investigating large quantities of categorical data"". Applied Statistics. 29 (2): 119– 127. • MARS (Multivariate adaptive regression splines); Friedman, J. H. (1991). ""Multivariate Adaptive Regression Splines"". The Annals of Statistics. 19: 1 • C5.0 (patented) • ... ! 20 x1 x2 x3 y 6 cm 8 cm 9 cm 1 4 cm 11 cm 2 cm 0 6 cm 12 cm 4 cm 0 10 cm 9 cm 3 cm 1 5 cm 7 cm 8 cm 0 8 cm 9 cm 3 cm 1 3 cm 11 cm 5 cm 0 Finding a Decision Rule ! 21 X__ X__ Drawing a Decision Boundary ! 22 GAIN(𝒟, xj) = H(𝒟) − ∑ v∈Values(xj) |𝒟v| |𝒟| H(𝒟v) Information Gain ! 23 Shannon Entropy Refer to lecture notes ! 24 H = −∑ i p(i|xj)log2(p(i|xj)) Entropy ! 25 Gini = 1 −∑ i (p(i|xj)2) Gini Impurity ! 26 ERR = 1 n n ∑ i=1 L( ̂ y[i], y[i]), Misclassiﬁcation Error L( ̂ y, y) = { 0 if ̂ y = y, 1 otherwise. ! 27 Misclassiﬁcation Error ERR = 1 −max i (p(i|xj)) ! 28 ! 29 Why Growing Decision Trees via Entropy instead of Misclassiﬁcation Error? ! 30 Why Growing Decision Trees via Entropy instead of Misclassiﬁcation Error? GAIN(𝒟, xj) = I(𝒟) − ∑ v∈Values(xj) |𝒟v| |𝒟| I(𝒟v) ! 31 40 80 28 42 38 12 28 0 0 42 12 38 0 0 y=1 y=0 x1 = 1 ? No Yes No No Yes Yes x2 = 1 ? x3 = 1 ? ! 32 40 80 28 42 38 12 28 0 0 42 12 38 0 0 Entropy = 0.918 Entropy = 0.0 Entropy = 0.0 Entropy = 0.0 Entropy = 0.795 Entropy = 0.971 Entropy = 0.0 = 0.918 - 70/120 * 0.971 - 50/120 * 0.795 = 0.02 GAIN(D, xj) = H(D) −|Dxj=1| |D| H(Dxj=1) −|Dxj=0| |D| H(Dxj=0) <latexit sha1_base64=""2nDNhLx0zVyYV7+Bdr7p8xHG2M="">ACqHicfVFbS8MwGE3rbc5b1UdfgmPgREcrgr4M5 gVUBJmwi7KOkmbpjEsvJKk4uv42/4Nv/hvTOXBzw8Ch3O+nHw5nxsxKqRpfmn6wuLS8kpuNb+2vrG5ZWzvNEUYc0waOGQhf3KRIwGpCGpZOQp4gT5LiMt3+V6a03wgUNg7ocRKTjo15APYqRVJRjfNxc3D0c2D6SLxix5Do9gu/OawlW4O0kW4 LHENoeRzgZTtBO8u4kr2nFSofplDBM4bTBb2fJtvOZ2z925qzdXDMzLTlGwSybo4KzwBqDAhXzTE+7W6IY58EjMkRNsyI9lJEJcUM5Lm7ViQCOE+6pG2gHyiegko6BTWFRMF3ohVyeQcMRO3kiQL8TAd1VnNq34q2XkPK0dS+8k9AgiUJ8M9 DXsygDG2NdilnGDJBgogzKmaFeIXpPKTard5FYL198uzoHlStsy9XhaqF6O48iBPbAPDoAFzkAV3IaACsFbV7ra419EO9prf059WXRvf2QVTpbvfHYPR4Q=</latexit> <latexit sha1_base64=""2nDNhLx0zVyYV7+Bdr7p8xHG2M="">ACqHicfVFbS8MwGE3rbc5b1UdfgmPgREcrgr4M5 gVUBJmwi7KOkmbpjEsvJKk4uv42/4Nv/hvTOXBzw8Ch3O+nHw5nxsxKqRpfmn6wuLS8kpuNb+2vrG5ZWzvNEUYc0waOGQhf3KRIwGpCGpZOQp4gT5LiMt3+V6a03wgUNg7ocRKTjo15APYqRVJRjfNxc3D0c2D6SLxix5Do9gu/OawlW4O0kW4 LHENoeRzgZTtBO8u4kr2nFSofplDBM4bTBb2fJtvOZ2z925qzdXDMzLTlGwSybo4KzwBqDAhXzTE+7W6IY58EjMkRNsyI9lJEJcUM5Lm7ViQCOE+6pG2gHyiegko6BTWFRMF3ohVyeQcMRO3kiQL8TAd1VnNq34q2XkPK0dS+8k9AgiUJ8M9 DXsygDG2NdilnGDJBgogzKmaFeIXpPKTard5FYL198uzoHlStsy9XhaqF6O48iBPbAPDoAFzkAV3IaACsFbV7ra419EO9prf059WXRvf2QVTpbvfHYPR4Q=</latexit> <latexit sha1_base64=""2nDNhLx0zVyYV7+Bdr7p8xHG2M="">ACqHicfVFbS8MwGE3rbc5b1UdfgmPgREcrgr4M5 gVUBJmwi7KOkmbpjEsvJKk4uv42/4Nv/hvTOXBzw8Ch3O+nHw5nxsxKqRpfmn6wuLS8kpuNb+2vrG5ZWzvNEUYc0waOGQhf3KRIwGpCGpZOQp4gT5LiMt3+V6a03wgUNg7ocRKTjo15APYqRVJRjfNxc3D0c2D6SLxix5Do9gu/OawlW4O0kW4 LHENoeRzgZTtBO8u4kr2nFSofplDBM4bTBb2fJtvOZ2z925qzdXDMzLTlGwSybo4KzwBqDAhXzTE+7W6IY58EjMkRNsyI9lJEJcUM5Lm7ViQCOE+6pG2gHyiegko6BTWFRMF3ohVyeQcMRO3kiQL8TAd1VnNq34q2XkPK0dS+8k9AgiUJ8M9 DXsygDG2NdilnGDJBgogzKmaFeIXpPKTard5FYL198uzoHlStsy9XhaqF6O48iBPbAPDoAFzkAV3IaACsFbV7ra419EO9prf059WXRvf2QVTpbvfHYPR4Q=</latexit> <latexit sha1_base64=""2nDNhLx0zVyYV7+Bdr7p8xHG2M="">ACqHicfVFbS8MwGE3rbc5b1UdfgmPgREcrgr4M5 gVUBJmwi7KOkmbpjEsvJKk4uv42/4Nv/hvTOXBzw8Ch3O+nHw5nxsxKqRpfmn6wuLS8kpuNb+2vrG5ZWzvNEUYc0waOGQhf3KRIwGpCGpZOQp4gT5LiMt3+V6a03wgUNg7ocRKTjo15APYqRVJRjfNxc3D0c2D6SLxix5Do9gu/OawlW4O0kW4 LHENoeRzgZTtBO8u4kr2nFSofplDBM4bTBb2fJtvOZ2z925qzdXDMzLTlGwSybo4KzwBqDAhXzTE+7W6IY58EjMkRNsyI9lJEJcUM5Lm7ViQCOE+6pG2gHyiegko6BTWFRMF3ohVyeQcMRO3kiQL8TAd1VnNq34q2XkPK0dS+8k9AgiUJ8M9 DXsygDG2NdilnGDJBgogzKmaFeIXpPKTard5FYL198uzoHlStsy9XhaqF6O48iBPbAPDoAFzkAV3IaACsFbV7ra419EO9prf059WXRvf2QVTpbvfHYPR4Q=</latexit> ! 33 40 80 28 42 38 12 28 0 0 42 12 38 0 0 Error = 40/120 Error = 0.0 Error = 0.0 Error = 0.0 Error = 12/50 Error = 28/70 Error = 0.0 = 40/120 - 70/120 * 28/70 - 50/120 * 12/50 = 0 GAIN(D, xj) = ERR(D) −|Dxj=1| |D| ERR(Dxj=1) −|Dxj=0| |D| ERR(Dxj=0) <latexit sha1_base64=""p+PtJ4mY9qI7fMGL10cmuxMHAeo="">ACrnicfVFbS8MwGE3rfd6mPvoSHIJDHakI+jKYN9QXUXFTX EtJs3SLpheSVBxdf5/wDf/jekcODf1g8DhnC8nX87nxZxJhdCHYU5MTk3PzM4V5hcWl5aLK6sNGSWC0DqJeCQePCwpZyGtK6Y4fYgFxYH6b3fJLr9y9USBaFd6obUyfA7ZD5jGClKbf4dn50ebVlB1h1CObpabYDX92nMqzCs9vbYb4MdyG0fYFJ2hui3fTVTZ+yqpX 1sh9CL4OjFt+9Zdsu5H7/GKJxwz/sUFZ2iyVUQf2C48AagBIY1LVbfLdbEUkCGirCsZRNC8XKSbFQjHCaFexE0hiTZ9ymTQ1DHFDpP24M7ipmRb0I6FPqGCfHb6R4kDKbuDpznxaOarl5G9aM1H+oZOyME4UDcnXQ37CoYpgvjvYoISxbsaYCKYnhWSDtYJKr3hg7BG v3yOGjsVSxUsW72S7XjQRyzYB1sgC1gQNQAxfgGtQBMbaNG+PRaJrIbJiO6X61msbgzhr4UWbnE2ie1A=</latexit> <latexit sha1_base64=""p+PtJ4mY9qI7fMGL10cmuxMHAeo="">ACrnicfVFbS8MwGE3rfd6mPvoSHIJDHakI+jKYN9QXUXFTX EtJs3SLpheSVBxdf5/wDf/jekcODf1g8DhnC8nX87nxZxJhdCHYU5MTk3PzM4V5hcWl5aLK6sNGSWC0DqJeCQePCwpZyGtK6Y4fYgFxYH6b3fJLr9y9USBaFd6obUyfA7ZD5jGClKbf4dn50ebVlB1h1CObpabYDX92nMqzCs9vbYb4MdyG0fYFJ2hui3fTVTZ+yqpX 1sh9CL4OjFt+9Zdsu5H7/GKJxwz/sUFZ2iyVUQf2C48AagBIY1LVbfLdbEUkCGirCsZRNC8XKSbFQjHCaFexE0hiTZ9ymTQ1DHFDpP24M7ipmRb0I6FPqGCfHb6R4kDKbuDpznxaOarl5G9aM1H+oZOyME4UDcnXQ37CoYpgvjvYoISxbsaYCKYnhWSDtYJKr3hg7BG v3yOGjsVSxUsW72S7XjQRyzYB1sgC1gQNQAxfgGtQBMbaNG+PRaJrIbJiO6X61msbgzhr4UWbnE2ie1A=</latexit> <latexit sha1_base64=""p+PtJ4mY9qI7fMGL10cmuxMHAeo="">ACrnicfVFbS8MwGE3rfd6mPvoSHIJDHakI+jKYN9QXUXFTX EtJs3SLpheSVBxdf5/wDf/jekcODf1g8DhnC8nX87nxZxJhdCHYU5MTk3PzM4V5hcWl5aLK6sNGSWC0DqJeCQePCwpZyGtK6Y4fYgFxYH6b3fJLr9y9USBaFd6obUyfA7ZD5jGClKbf4dn50ebVlB1h1CObpabYDX92nMqzCs9vbYb4MdyG0fYFJ2hui3fTVTZ+yqpX 1sh9CL4OjFt+9Zdsu5H7/GKJxwz/sUFZ2iyVUQf2C48AagBIY1LVbfLdbEUkCGirCsZRNC8XKSbFQjHCaFexE0hiTZ9ymTQ1DHFDpP24M7ipmRb0I6FPqGCfHb6R4kDKbuDpznxaOarl5G9aM1H+oZOyME4UDcnXQ37CoYpgvjvYoISxbsaYCKYnhWSDtYJKr3hg7BG v3yOGjsVSxUsW72S7XjQRyzYB1sgC1gQNQAxfgGtQBMbaNG+PRaJrIbJiO6X61msbgzhr4UWbnE2ie1A=</latexit> <latexit sha1_base64=""p+PtJ4mY9qI7fMGL10cmuxMHAeo="">ACrnicfVFbS8MwGE3rfd6mPvoSHIJDHakI+jKYN9QXUXFTX EtJs3SLpheSVBxdf5/wDf/jekcODf1g8DhnC8nX87nxZxJhdCHYU5MTk3PzM4V5hcWl5aLK6sNGSWC0DqJeCQePCwpZyGtK6Y4fYgFxYH6b3fJLr9y9USBaFd6obUyfA7ZD5jGClKbf4dn50ebVlB1h1CObpabYDX92nMqzCs9vbYb4MdyG0fYFJ2hui3fTVTZ+yqpX 1sh9CL4OjFt+9Zdsu5H7/GKJxwz/sUFZ2iyVUQf2C48AagBIY1LVbfLdbEUkCGirCsZRNC8XKSbFQjHCaFexE0hiTZ9ymTQ1DHFDpP24M7ipmRb0I6FPqGCfHb6R4kDKbuDpznxaOarl5G9aM1H+oZOyME4UDcnXQ37CoYpgvjvYoISxbsaYCKYnhWSDtYJKr3hg7BG v3yOGjsVSxUsW72S7XjQRyzYB1sgC1gQNQAxfgGtQBMbaNG+PRaJrIbJiO6X61msbgzhr4UWbnE2ie1A=</latexit> ! 34 ! 35 Gain Ratio GainRatio(𝒟, xj) = Gain(𝒟, xj) SplitInfo(𝒟, xj) where the split information measures the entropy of the feature: SplitInfo(𝒟, xj) = −∑ v∈xj |𝒟v| |𝒟| log2 |𝒟v| |𝒟| Penalizes splitting categorical attributes with many values (e.g., think date column, or really bad: row ID) via the split information Quinlan 1986 ! 36 Shortcomings How would the decision tree split look like? ! 37 Overﬁtting Q: Why (when) does the accuracy start at ~50%? ! 38 • Set a depth cut-oﬀ (maximum tree depth) a priori • Cost-complexity pruning: , where is an impurity measure, is a tuning parameter, and is the total number of nodes. • Stop growing if split is not statistically signiﬁcant (e.g., Chi^2 test) • Set a minimum number of data points for each node Pre-Pruning ! 39 • Grow full tree ﬁrst, then remove nodes, in C4.5 • Reduced-error pruning, remove nodes via validation set eval. (problematic for limited data) • Can also convert trees to rules ﬁrst and then prune the rules Post-Pruning ! 40 Post-Pruning ! 41 Regression Trees ! 42 Summary: Pros and Cons • (+) Easy to interpret and communicate • (+) Can represent ""complete"" hypothesis space • (-) Easy to overﬁt • (-) Elaborate pruning required • (-) Expensive to just ﬁt a ""diagonal line"" • (-) Output range is bounded (dep. on training examples) in regression trees ! 43 Reading Assignments Python Machine Learning, 2nd Ed., Ch03 pg. 88-104 ! 44 Demo 06_trees_demo.ipynb "
38,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal (various slides adapted/borrowed from courses by Dan Klein, JurafskyMartin-SLP3, Manning/Socher, others) Machine Translation 3 (Neural); Dialogue Models Statistical Machine Translation Recap IBM Model 1 ! Alignments: a hidden vector called an alignment specifies which English source is responsible for each French target word. ! The first, simplest IBM model treated alignment probabilities as roughly uniform: IBMModel1(Brown93)  Alignments:ahiddenvectorcalledanalignment specifieswhichEnglish sourceisresponsibleforeachFrenchtargetword. [Brown et al., 1993] IBM Model 2 (Distortion) [Brown et al., 1993] IBMModel2  Alignmentstendtothediagonal(broadlyatleast)  Otherschemesforbiasingalignmentstowardsthediagonal:  Relativevsabsolutealignment  Asymmetricdistances  Learning a full multinomial over distances ! The next more advanced model captures the notion of ‘distortion’, i.e., how far from the diagonal is the alignment ! Other approaches for biasing alignment towards diagonal include relative vs absolute alignment, asymmetric distances, and learning a full multinomial over distances IBM Models 3/4/5 (Fertility) [Vogel et al., 1996] IBMModels3/4/5 Mary did not slap the green witch Mary not slap slap slap the green witch Mary not slap slap slap NULL the green witch n(3|slap) Mary no daba una botefada a la verde bruja Mary no daba una botefada a la bruja verde P(NULL) t(la|the) d(j|i) [from Al-Onaizan and Knight, 1998] Synchronous Tree-Substitution Grammars [Shieber, 2004; Graehl et al., 2008] Neural Machine Translation Traditional Stat. Machine Translation ! Lots of feature engineering ! Very complex pipeline systems with multiple steps to generate the final huge phrase table! ! Incentive to do it end-to-end and jointly ! Can neural models be a powerful enough alternative to do so? Machine Translation Progress 7 NMT slides from ACL 2016 Tutorial (Luong, Cho, Manning) Neural Machine Translation Die Proteste waren am Wochenende eskaliert <EOS> The protests escalated over the weekend 0.2 0.6 -0.1 -0.7 0.1 0.4 -0.6 0.2 -0.3 0.4 0.2 -0.3 -0.1 -0.4 0.2 0.2 0.4 0.1 -0.5 -0.2 0.4 -0.2 -0.3 -0.4 -0.2 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 -0.1 0.3 -0.1 -0.7 0.1 -0.2 0.6 0.1 0.3 0.1 -0.4 0.5 -0.5 0.4 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 -0.2 -0.1 0.1 0.1 0.2 0.6 -0.1 -0.7 0.1 0.1 0.3 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.4 0.1 0.2 -0.8 -0.1 -0.5 0.1 0.2 0.6 -0.1 -0.7 0.1 -0.4 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 0.3 0.1 -0.1 0.6 -0.1 0.3 0.1 0.2 0.4 -0.1 0.2 0.1 0.3 0.6 -0.1 -0.5 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 -0.1 -0.1 -0.7 0.1 0.1 0.3 0.1 -0.4 0.2 0.2 0.6 -0.1 -0.7 0.1 0.4 0.4 0.3 -0.2 -0.3 0.5 0.5 0.9 -0.3 -0.2 0.2 0.6 -0.1 -0.5 0.1 -0.1 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.3 0.6 -0.1 -0.7 0.1 0.4 0.4 -0.1 -0.7 0.1 -0.2 0.6 -0.1 -0.7 0.1 -0.4 0.6 -0.1 -0.7 0.1 -0.3 0.5 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 The protests escalated over the weekend <EOS> Modern Sequence Models for NMT [Sutskever et al. 2014, Bahdanau et al. 2014, et seq.] following [Jordan 1986] and more closely [Elman 1990] Sentence meaning is built up Source sentence Translation generated Feeding in last word A deep recurrent neural network ! Encoder-Decoder RNN models: Initial Improvement Sources ! Stacking multiple layers ! Bidirectionality ! Better memory units, e.g., GRUs ! Pre-trained language models on tons of monolingual data ! Ensembles ! Attention/Alignment models Alignment/Attention Models ! Translating longer sentences better, e.g., via attention/alignment module between encoder and decoder to jointly learn alignments and translations end-to-end • Simplified mechanism & more functions: Bilinear form: well-adopted. Attention Mechanisms+ Alignment/Attention Models ! Translating longer sentences better, e.g., via attention/alignment module between encoder and decoder to jointly learn alignments and translations end-to-end Dzmitry Bahdanau, KyungHuyn Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Translate and Align. ICLR’15. 132 Learning both translation & alignment Linguistic Insights in NMT Constraints on “distortion” (displacement) and fertility  Constraints on attention [Cohn, Hoang, Vymolova, Yao, Dyer & Haffari NAACL 2016; Feng, Liu, Li, Zhou 2016 arXiv; Yang, Hu, Deng, Dyer, Smola 2016 arXiv]. Linguistic Insights in NMT Extend to NMT – Linguistic insights • [Cohn, Hoang, Vymolova, Yao, Dyer, Haffari, NAACL’16]: position (IBM2) + Markov (HMM) + fertility (IBM3-5) + alignment symmetry (BerkeleyAligner). • [Tu, Lu, Liu, Liu, Li, ACL’16]: linguistic & NN-based coverage models. 151 Source word fertility Per source word Other New Ideas/Improvements ! Extending vocabulary coverage and handling rare/unseen words ! Handling more language variations, e.g., via character-level models to capture morphology ! Utilize more data resources, e.g., multilingual models (one to many, many to one, many to many), multi-task learning (combine with other encoder-decoder tasks with shared sides) ! Zero-shot translation ! See ACL 2016 tutorial: https://sites.google.com/site/acl16nmt/ Hybrid Char-Word NMT g, g}@ on neural ma- has used quite erhaps with a h in unknown a novel word- ving open vo- hybrid systems he word level omponents for er-level recur- te source word unknown tar- The twofold d approach is easier to train at the same nknown words [Luong and Manning, 2016] Char-level NMT with CNN Encoder [Lee et al., 2016; Gehring et al., 2016] _ _ T h e s e c o n d p e r s o n _ _ Single-layer Convolution + ReLU Max Pooling with Stride 5 Four-layer Highway Network Single-layer Bidirectional GRU Character Embeddings ℝ#×% & ℝ()×(% &+,-#) ℝ/×% & ℝ/×(% & 0 ⁄ ) ℝ/×(% & 0 ⁄ ) Segment Embeddings 1: Encoder architecture schematics. Underscore denotes padding. A dotted vertical line delimits each s ride of pooling s is 5 in the diagram. F = {f1 fm} where fi = Rdc⇥i⇥ni is at increased training time We chose s = ! Later extended to convolutions for both encoder and decoder! Google’s Zero-Shot Machine Translation [Johnson et al., 2016] ! Play above gif video at https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html Google’s Zero-Shot Machine Translation [Johnson et al., 2016] igure 1: The model architecture of the Multilingual GNMT system. In addition to what is described in [ ur input has an artiﬁcial token to indicate the required target language. In this example, the token “<2e ndicates that the target sentence is in Spanish, and the source sentence is reversed as a processing step. most of our experiments we also used direct connections between the encoder and decoder although we la Dialogue Models Examples of a sentence, although it might be as short as a single word or as long as multiple sentences. The simplest such systems generally handle a single turn from the user, acting more like question-answering or command-and-control systems. This is espe- cially common with digital assistants. For example Fig. 29.1 shows screen captures from an early version of Apple’s Siri personal assistant from 2014, demonstrating this kind of single-query behavior. (a) (b) Figure 29.1 Two sets of interactions with Siri in 2014. (a) A question (”Find restaurants near me”) returns restaurants, but the system was unable to interpret a follow-up question (“Are any of them Italian?”). (b) An alternative followup (“Tell me more about the second one”) similarly fails. This early system’s confusion at follow-up questions suggests that it is mainly designed for a single interaction. By contrast, Fig. 29.2 shows that a 2017 version of the Siri digital assistant can handle slightly longer dialogs, handling a second turn with a follow-up question. While spoken dialogs with mobile phone digital assistants tend to be short, some tasks do require longer dialogs. One such task is travel planning and management, [Jurafsky SLP3 Book Chapter 29] Examples 3 (a) (b) Figure 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses when posed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 Examples (a) (b) 29.2 The same follow-up questions that Siri couldn’t answer in 2014 receive appropriate responses osed to Siri in 2017. Tutor: What will the velocity of the object be a second after that (where the initial velocity is 9.8 m/s and the acceleration is 9.8 m/s2)? Student: Is it 19.6 m/s? Tutor: Good. A second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s This is because its acceleration tells us that every second, its velocity increases by 9.8 m/s. So what’ll its velocity be, a second after that? Student: Um. I’m thinking. 29.4 m/s Tutor: Right. Now back to the man and his keys. We’ve shown that they both have an acceleration of 9.8 m/ss. So, how do their velocities compare with each other, at every second during the fall? Figure 29.3 The tutoring domain: An excerpt from a tutorial interaction between a physics student and the ITSPOKE system of Forbes-Riley and Litman (2011) based on the Why2- Atlas text-based tutoring system (VanLehn et al., 2002). of conversations shown in Fig. 29.4. User: Do you like Doctor Who? Cleverbot: Yes, so funny. User: What’s your favorite show? Cleverbot: I don’t have one. Examples every second during the fall? Figure 29.3 The tutoring domain: An excerpt from a tutorial interaction between a physics student and the ITSPOKE system of Forbes-Riley and Litman (2011) based on the Why2- Atlas text-based tutoring system (VanLehn et al., 2002). of conversations shown in Fig. 29.4. User: Do you like Doctor Who? Cleverbot: Yes, so funny. User: What’s your favorite show? Cleverbot: I don’t have one. User: What’s your name? Cleverbot: Jason I already told you. User: No your real name. Cleverbot: Stephanie IS my real name. User: I thought it was Jason? Cleverbot: How did you come to that conclusion. User: Because you said so. Cleverbot: I have to do my homework. Figure 29.4 Chatbots: A conversation between a user and Cleverbot, an IR-based chatbot (Carpenter, 2017). We begin in the next section with chatbots, introducing the three major chatbot architectures: rule-based systems, information retrieval systems, and transduction models. We then turn to task-oriented dialog, introducing the frame-based architec- ture (the GUS architecture) that underlies most modern task-based dialog systems, the VoiceXML system that is a simple industrial instantiation of the architecture, Rule-based Chatbots (e.g., ELIZA) nothing of the real world”. If a patient says “I went for a long boat ride” and the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what a boat is, but rather assume she had some conversational goal. Most chatbots trying to pass Turing test choose a domain with similar properties. Let’s look at the most famous ELIZA conversation, excerpted (deleting some material with ...) from a full interaction given in the original paper: Men are all alike. IN WHAT WAY They’re always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well my boyfriend made me come here. YOUR BOYFRIEND MADE YOU COME HERE He says I’m depressed much of the time. I AM SORRY TO HEAR YOU ARE DEPRESSED .. .. WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU? My father YOUR FATHER You are like my father in some ways. 29.1 • CHATBOTS 5 WHAT RESEMBLANCE DO YOU SEE You are not very aggressive but I think you don’t want me to notice that. WHAT MAKES YOU THINK I AM NOT AGGRESSIVE You don’t argue with me. WHY DO YOU THINK I DON’T ARGUE WITH YOU You are afraid of me. DOES IT PLEASE YOU TO BELIEVE I’M AFRAID OF YOU My father is afraid of everybody. WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER Bullies. DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HERE As we summarized brieﬂy in Chapter 1, ELIZA worked by pattern/transform rules like the following one: Rule-based Chatbots (e.g., ELIZA) y y y WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER Bullies. DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YO As we summarized brieﬂy in Chapter 1, ELIZA worked by pattern/transform rules like the following one: (0 YOU 0 ME) [pattern] -> (WHAT MAKES YOU THINK I 3 YOU) [transform] In the ELIZA pattern, 0 means Kleene*, and in the transform rules the numbers are the index of the constituent in the pattern. Thus the number 3 refers to the second 0 in the ﬁrst pattern. This rule would transfer You hate me into: WHAT MAKES YOU THINK I HATE YOU Each ELIZA pattern/rule is linked to a keyword that might occur in a user sen- tence. The architecture is summarized in Fig. 29.5. function ELIZA GENERATOR(user sentence) returns response Find the word w in sentence that has the highest keyword rank if w exists Choose the highest ranked rule r for w that matches sentence response Apply the transform in r to sentence if w = ’my’ f t A l t f ti f th ‘ ’ l li t t t a e t e de o t e co st tue t t e patte . us t e u be 3 e e s to t e seco d 0 in the ﬁrst pattern. This rule would transfer You hate me into: WHAT MAKES YOU THINK I HATE YOU Each ELIZA pattern/rule is linked to a keyword that might occur in a user sen- tence. The architecture is summarized in Fig. 29.5. function ELIZA GENERATOR(user sentence) returns response Find the word w in sentence that has the highest keyword rank if w exists Choose the highest ranked rule r for w that matches sentence response Apply the transform in r to sentence if w = ’my’ future Apply a transformation from the ‘memory’ rule list to sentence Push future onto memory stack else (no keyword applies) either response Apply the transform for the NONE keyword to sentence or response Pop the top response from the memory stack return(response) Figure 29.5 A simpliﬁed sketch of the ELIZA algorithm. The power of the algorithm come from the particular transforms associated with each keyword. IR-based Chatbots Another approach is to use corpora of movie dialog. Once a chatbot has been put into practice, the turns that humans use to respond to the chatbot can be used as additional conversational data for training. Given the corpus and the user’s sentence, IR-based systems can use any retrieval algorithm to choose an appropriate response from the corpus. The two simplest methods are the following: 1. Return the response to the most similar turn: Given user query q and a con- versational corpus C, ﬁnd the turn t in C that is most similar to (q) (for example has the highest cosine with q) and return the following turn, i.e. the human response to t in C: r = response ✓ argmax t2C qTt ||q||t|| ◆ (29.1) The idea is that we should look for a turn that most resembles the user’s turn, and return the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011). 2. Return the most similar turn: Given user query q and a conversational corpus C, return the turn t in C that is most similar to (q) (for example has the highest cosine with q): r = argmax t2C qTt ||q||t|| (29.2) The idea here is to directly match the users query q with turns from C, since a d ill ft h d ti ith th i t into practice, the turns that humans use to respond to the chatbot can be used as additional conversational data for training. Given the corpus and the user’s sentence, IR-based systems can use any retrieval algorithm to choose an appropriate response from the corpus. The two simplest methods are the following: 1. Return the response to the most similar turn: Given user query q and a con- versational corpus C, ﬁnd the turn t in C that is most similar to (q) (for example has the highest cosine with q) and return the following turn, i.e. the human response to t in C: r = response ✓ argmax t2C qTt ||q||t|| ◆ (29.1) The idea is that we should look for a turn that most resembles the user’s turn, and return the human response to that turn (Jafarpour et al. 2009, Leuski and Traum 2011). 2. Return the most similar turn: Given user query q and a conversational corpus C, return the turn t in C that is most similar to (q) (for example has the highest cosine with q): r = argmax t2C qTt ||q||t|| (29.2) The idea here is to directly match the users query q with turns from C, since a good response will often share words or semantics with the prior turn. ! Retrieval systems use two major approaches to “extract” the best response from a dialogue corpus, given the new, test-time user utterance: ! 1) Return Response of Most Similar Turn: Find conversation turn t (in corpus C) which is most similar to the given user utterance/query q, and return the following turn/response r of that most-similar utterance: ! 2) Return Most Similar Turn: Instead of returning the following turn of the most similar utterance, we return this most similar utterance itself, with the intuition that a good response often shared words/semantics with the prior turn: Seq-to-Seq Chatbots p g to align well with each other; but a user utterance may share no words or phrases with a coherent response. Instead, (roughly contemporaneously by Shang et al. 2015, Vinyals and Le 2015, and Sordoni et al. 2015) transduction models for response generation were modeled instead using sequence to sequence (seq2seq) models (Chapter 25). How are you ? I’m fine . EOS Encoding Decoding EOS I’m fine . Figure 29.6 A sequence to sequence model for neural response generation in dialog. A number of modiﬁcations are required to the basic seq2seq model to adapt it for the task of response generation. For example basic seq2seq models have a tendency to produce predictable but repetitive and therefore dull responses like “I’m OK” or “I don’t know” that shut down the conversation. This can be addressed by changing the objective function for seq2seq model training to a mutual information objective, b dif i b d d t k di i th b (Li [Shang et al. 2015; Vinyals and Le, 2015; Sordoni et al., 2015] Evaluating Chatbots ! Automatic metrics based on word/phrase overlap not very useful because so many responses might be correct/appropriate for chitchat ! Human evaluation most meaningful/common (but time-consuming) ! Can’t do slot-filling techniques because this is not task-oriented dialogue with a specific goal or success metric ! Engagement or length of conversation in real human-based setup? ! Some new automatic classification approaches like ADEM [Lowe et al., 2017] to classify appropriateness of response, and Adversarial evaluation [Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017] to fool a classifier that distinguishes between human and machine generated responses Some Advanced Seq-to-Seq Models ! Hierarchical Recurrent Encoder-Decoder 1: The computational graph of the HRED architecture for a dialogue composed of three turns. Each uttera d into a dense vector and then mapped into the dialogue context, which is used to decode (generate) the tokens erance. The encoder RNN encodes the tokens appearing within the utterance, and the context RNN encodes the tem [Serban et al., 2015] Some Advanced Seq-to-Seq Models ! Attention-RNN Language Model [Mei et al., 2016] Output (a) RNN seq2seq (encoder-decoder) model (b) RNN language model Attention RNN (c) Attention seq2seq (encoder-decoder) model (d) Attention language model Figure 1: Comparing RNN language models to RNN sequence-to-sequence model, with and without attention. conversational corpora that are becoming increasingly avail- still maintaining a relatively simple architecture. We show Some Advanced Seq-to-Seq Models ! Persona-based Language Models [Li et al., 2016] EOS Rob Word embeddings (50k) england london u.s. great good stay live okay monday tuesday Speaker embeddings (70k) Rob_712 where do you live in in Rob england Rob england . Rob . EOS Source Target skinnyoflynny2 Tomcoatez Kush_322 D_Gomes25 Dreamswalls kierongillen5 TheCharlieZ The_Football_Bar This_Is_Artful DigitalDan285 Jinnmeow3 Bob_Kelly2 Figure 1: Illustrative example of the Speaker Model introduced in this work. Speaker IDs close in embedding space tend to respond in the same manner. These speaker embeddings are learned jointly with word embeddings and all other parameters of the neural model via backpropagation. In this example, say Rob is a speaker clustered with people who often mention England in the training data, then the generation of the token ‘england’ at time t = 2 would be much more likely than that of ‘u.s.’. A non-persona model would prefer generating in the u.s. if ‘u.s.’ is more represented in the training data across all speakers. Frame-based & KB-based Dialog Agents [Jurafsky SLP3] gy slot deﬁnes the values that each slot can take. This frame-based architecture was ﬁrst introduced in 1977 in the inﬂuential GUS system for travel planning (Bobrow et al., 1977), and has been astonishingly long-lived, underlying most modern commercial digital assistants. Figure 29.8 shows a conversation with the original GUS system. GUS: Hello. My name is GUS. I can help you plan a simple trip by air. Where do you want to go ? Client: I want to go to San Diego on May 28 GUS: What time do you want to leave? Client: I must be in San Diego before 10 am GUS: Would you like P.S.A. ﬂight 102 that arrives at 9.15 am ? Client: What is the next ﬂight ? GUS: Air California ﬂight 310 that leaves at 8.30am and arrives at 10.00am Client: I’ll take the ﬁrst one GUS: In what name should I make a reservation, please? Client: Dan Bobrow GUS: I have conﬁrmed the following ﬂight: Client: P.S.A. ﬂight 102 on Wednesday May 28 from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am. GUS: What date do you want to return on ? Client: On Friday in the evening. GUS: Would you like the ﬂight that leaves at 7.45 pm ? Client: That’s ﬁne. GUS: I have conﬁrmed the following ﬂight: P.S.A. ﬂight 307 on Friday May 30 from San Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank you for calling. Goodbye Figure 29.8 The travel domain: A transcript of an actual dialog with the GUS system of Bobrow et al. (1977). P.S.A. and Air California were airlines of that period. The set of slots in a GUS-style frame speciﬁes what the system needs to know, Frame-based & KB-based Dialog Agents Bobrow et al. (1977). P.S.A. and Air California were airlines of that period. The set of slots in a GUS-style frame speciﬁes what the system needs to know, and the ﬁller of each slot is constrained to values of a particular semantic type. In the travel domain, for example, a slot might be of type city (hence take on values like San Francisco, or Hong Kong) or of type date, airline, or time: Slot Type ORIGIN CITY city DESTINATION CITY city DEPARTURE TIME time DEPARTURE DATE date ARRIVAL TIME time ARRIVAL DATE date Types in GUS, as in modern frame-based dialog agents, may have hierarchical structure; for example the date type in GUS is itself a frame with slots with types like integer or members of sets of weekday names: DATE MONTH NAME DAY (BOUNDED-INTEGER 1 31) YEAR INTEGER WEEKDAY (MEMBER (SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY)] the travel domain, for example, a slot might be of type city (hence take on values like San Francisco, or Hong Kong) or of type date, airline, or time: Slot Type ORIGIN CITY city DESTINATION CITY city DEPARTURE TIME time DEPARTURE DATE date ARRIVAL TIME time ARRIVAL DATE date Types in GUS, as in modern frame-based dialog agents, may have hierarchical structure; for example the date type in GUS is itself a frame with slots with types like integer or members of sets of weekday names: DATE MONTH NAME DAY (BOUNDED-INTEGER 1 31) YEAR INTEGER WEEKDAY (MEMBER (SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY)] Do you want to go from <FROM> to <TO> on <DATE>? Yes Where are you going? What date do you want to leave? Is it a one-way trip? What date do you want to return? Do you want to go from <FROM> to <TO> on <DATE> returning on <RETURN>? No No Yes Yes No Book the flight Figure 29.9 A simple ﬁnite-state automaton architecture for frame-based dialog. Consider the very simple ﬁnite-state control architecture shown in Fig. 29.9, implementing a trivial airline travel system whose job is to ask the user for the information for 4 slots: departure city, a destination city, a time, and whether the trip is one-way or round-trip. Let’s ﬁrst associate with each slot a question to ask the user: Slot Question ORIGIN CITY “From what city are you leaving?” DESTINATION CITY “Where are you going?” DEPARTURE TIME “When would you like to leave?” ARRIVAL TIME “When do you want to arrive?” Figure 29.9 shows a sample dialog manager for such a system. The states of the FSA correspond to the slot questions, user, and the arcs correspond to actions to take depending on what the user responds. This system completely controls the conversation with the user. It asks the user a series of questions, ignoring (or misin- i ) hi h i di h i d h i Frame-based & KB-based Dialog Agents The control architecture of frame-based dialog systems is designed around the frame. The goal is to ﬁll the slots in the frame with the ﬁllers the user intends, and then per- form the relevant action for the user (answering a question, or booking a ﬂight). Most frame-based dialog systems are based on ﬁnite-state automata that are hand- designed for the task by a dialog designer. What city are you leaving from? Do you want to go from <FROM> to <TO> on <DATE>? Yes Where are you going? What date do you want to leave? Is it a one-way trip? What date do you want to return? Do you want to go from <FROM> to <TO> on <DATE> returning on <RETURN>? No No Yes Yes No Book the flight Figure 29.9 A simple ﬁnite-state automaton architecture for frame-based dialog. Consider the very simple ﬁnite-state control architecture shown in Fig. 29.9, i l ti t i i l i li t l t h j b i t k th f th Frame-based & KB-based Dialog Agents what general task or goal is the user trying to accomplish? For example the task could be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment. Finally, we need to do slot ﬁlling: extract the particular slots and ﬁllers that the user ﬁlling intends the system to understand from their utterance with respect to their intent. From a user utterance like this one: Show me morning flights from Boston to San Francisco on Tuesday a system might want to build a representation like: DOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS ORIGIN-CITY: Boston ORIGIN-DATE: Tuesday ORIGIN-TIME: morning DEST-CITY: San Francisco while an utterance like Wake me tomorrow at 6 should give an intent like this: DOMAIN: ALARM-CLOCK INTENT: SET-ALARM TIME: 2017-07-01 0600-0800 The task of slot-ﬁlling, and the simpler tasks of domain and intent classiﬁcation, are special cases of the task of semantic parsing discussed in Chapter ??. Dialogue agents can thus extract slots domains and intents from user utterances by applying Frame-based & KB-based Dialog Agents | | HOUR ! one|two|three|four...|twelve (AMPM) FLIGHTS ! (a) ﬂight | ﬂights AMPM ! am | pm ORIGIN ! from CITY DESTINATION ! to CITY CITY ! Boston | San Francisco | Denver | Washington Semantic grammars can be parsed by any CFG parsing algorithm (see Chap- ter 12), resulting in a hierarchical labeling of the input string with semantic node labels, as shown in Fig. 29.10. S DEPARTTIME morning DEPARTDATE Tuesday on DESTINATION Francisco San to ORIGIN Boston from FLIGHTS ﬂights SHOW me Show Figure 29.10 A semantic grammar parse for a user sentence, using slot names as the internal parse tree nodes. Whether regular expressions or parsers are used, it remains only to put the ﬁllers into some sort of canonical form, for example by normalizing dates as discussed in Chapter 20. A number of tricky issues have to be dealt with. One important issue is negation; if a user speciﬁes that they “can’t ﬂy Tuesday morning”, or want a meeting ”any time except Tuesday morning”, a simple system will often incorrectly extract “Tuesday morning” as a user goal, rather than as a negative constraint. Speech recognition errors must also be dealt with. One common trick is to make Frame-based & KB-based Dialog Agents TER 29 • DIALOG SYSTEMS AND CHATBOTS h0 h1 h2 hn hn+1 w0 w1 w2 wn <EOS> s0 s1 s2 sn d+i Figure 29.11 An LSTM architecture for slot ﬁlling, mapping the words in the input (repre- sented as 1-hot vectors or as embeddings) to a series of IOB tags plus a ﬁnal state consisting of a domain concatenated with an intent. In industrial contexts, machine learning-based systems for slot-ﬁlling are often b d f l b d i i i d l i A l Frame-based & KB-based Dialog Agents B: anyone went to columbia? columbia google KB + Dialogue history Dynamic knowledge graph Graph embedding Generator Name School Company Jessica Columbia Google Josh Columbia Google Item 1 Item 2 2 1 josh jessica S N C Message passing path of columbia anyone went columbia … … columbia google jessica josh … … Yes and josh jessica Attention + Copy Figure 3: Overview of our approach. First, the KB and dialogue history (entities in bold) is mapped to a graph. Here, an item node is labeled by the item ID and an attribute node is labeled by the attribute’s ﬁrst letter. Next, each node is embedded using relevant utterance embeddings through message passing. Finally, an LSTM generates the next utterance based on attention over the node embeddings. model consists of three components shown in Fig- ure 3: (i) a dynamic knowledge graph which rep- is columbia. An example graph is shown in Fig- ure 3 The graph Gt is updated based on utterance [He et al., 2017] Frame-based & KB-based Dialog Agents [Eric et al., 2017] Figure 2: Key-value retrieval network. For each time-step of decoding, the cell state is used to compute an attention over the encoder states and a separate attention over the key of each entry in the KB. The attentions over the encoder are used to generate a context vector which is combined with the cell state to "
380,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Ensemble Methods Lecture 07 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Overview Ensemble Methods Majority Voting Bagging Boosting Random Forests Stacking Sebastian Raschka STAT 479: Machine Learning FS 2018 Majority Voting ! 3 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Unanimity Majority Plurality Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 hi(x) = ̂ yi Training set h1 h2 hn . . . y1 y2 yn . . . Voting yf New data Classification models Predictions Final prediction where Majority Vote Classiﬁer ̂ yf = mode{h1(x), h2(x), . . . hn(x)} Sebastian Raschka STAT 479: Machine Learning FS 2018 • assume n independent classiﬁers with a base error rate • here, independent means that the errors are uncorrelated • assume a binary classiﬁcation task • assume the error rate is better than random guessing (i.e., lower than 0.5 for binary classiﬁcation) ∀ϵi ∈{ϵ1, ϵ2, . . . , ϵn}, ϵi < 0.5 ϵ Why Majority Vote? Sebastian Raschka STAT 479: Machine Learning FS 2018 • assume n independent classiﬁers with a base error rate • here, independent means that the errors are uncorrelated • assume a binary classiﬁcation task • assume the error rate is better than random guessing (i.e., lower than 0.5 for binary classiﬁcation) ∀ϵi ∈{ϵ1, ϵ2, . . . , ϵn}, ϵi < 0.5 ϵ k > ⌈n/2⌉ Why Majority Vote? P(k) = ( n k)ϵk(1 −ϵ)n−k The probability that we make a wrong prediction via the ensemble if k classiﬁers predict the same class label Sebastian Raschka STAT 479: Machine Learning FS 2018 k > ⌈n/2⌉ Why Majority Vote? P(k) = ( n k)ϵk(1 −ϵ)n−k The probability that we make a wrong prediction via the ensemble if k classiﬁers predict the same class label Ensemble error: ϵens = n ∑ k ( n k)ϵk(1 −ϵ)n−k ϵens = 11 ∑ k=6 ( 11 k )0.25k(1 −0.25)11−k = 0.034 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 ϵens = n ∑ k ( n k)ϵk(1 −ϵ)n−k Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 ""Soft"" Voting ̂ y = arg max j n ∑ i=1 wipi,j wj optional weighting parameter, default wi = 1/n, ∀wi ∈{w1, . . . , wn} pi,j predicted class membership probability of the ith classiﬁer for class label j : : Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 ""Soft"" Voting ̂ y = arg max j n ∑ i=1 wipi,j wj optional weighting parameter, default wi = 1/n, ∀wi ∈{w1, . . . , wn} pi,j predicted class membership probability of the ith classiﬁer for class label j : : Use only for well-calibrated classiﬁers! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 ""Soft"" Voting ̂ y = arg max j n ∑ i=1 wipi,j Binary classiﬁcation example j ∈{0,1} hi(i ∈{1,2,3}) h1(x) →[0.9,0.1] h2(x) →[0.8,0.2] h3(x) →[0.4,0.6] Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 ""Soft"" Voting ̂ y = arg max j n ∑ i=1 wipi,j Binary classiﬁcation example j ∈{0,1} hi(i ∈{1,2,3}) h1(x) →[0.9,0.1] h2(x) →[0.8,0.2] h3(x) →[0.4,0.6] p(j = 0|x) = 0.2 ⋅0.9 + 0.2 ⋅0.8 + 0.6 ⋅0.4 = 0.58 p(j = 1|x) = 0.2 ⋅0.1 + 0.2 ⋅0.2 + 0.6 ⋅0.6 = 0.42 ̂ y = arg max j {p(j = 0|x), p(j = 1|x)} Sebastian Raschka STAT 479: Machine Learning FS 2018 Bagging ! 14 (Bootstrap Aggregating) Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Algorithm 1 Bagging 1: Let n be the number of bootstrap samples 2: 3: for i=1 to n do 4: Draw bootstrap sample of size m, Di 5: Train base classiﬁer hi on Di 6: ˆ y = mode{h1(x), ..., hn(x)} Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 x1 x1 x1 x1 x1 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x2 x2 x2 x2 x8 x8 x10 x7 x3 x6 x9 x3 x8 x10 x7 x2 x2 x9 x6 x6 x4 x4 x5 x10 x8 x7 x5 x4 x3 x4 x5 x6 x8 x9 Training Sets Test Sets Bootstrap 1 Bootstrap 2 Bootstrap 3 Original Dataset Bootstrap Sampling Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 P(not chosen) = (1 −1 n) n , 1 e ≈0.368, n →∞. Bootstrap Sampling Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 P(not chosen) = (1 −1 n ) n , 1 e ≈0.368, n →∞. P(chosen) = 1 −(1 −1 n ) n ≈0.632 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Training example indices Bagging round 1 Bagging round 2 … 1 2 7 … 2 2 3 … 3 1 2 … 4 3 1 … 5 7 1 … 6 2 7 … 7 4 7 … h1 h2 hn Bootstrap Sampling Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 h1 h2 hn . . . y1 y2 yn . . . Voting yf New data Classification models Predictions Final prediction . . . T2 Training set Tn T1 T2 Bootstrap samples hi(x) = ̂ yi where ̂ yf = mode{h1(x), h2(x), . . . hn(x)} Bagging Classiﬁer Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Bias-Variance Decomposition Loss = Bias + Variance + Noise (more technical details in next lecture on model evaluation) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 Bias and Variance Example where f(x) is some true (target) function Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 where f(x) is some true (target) function the blue dots are a training dataset; here, I added some random Gaussian noise Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 where f(x) is some true (target) function the blue dots are a training dataset; here, I added some random Gaussian noise here, suppose I ﬁt a simple linear model (linear regression) or a decision tree stump High Bias Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 where f(x) is some true (target) function the blue dots are a training dataset; here, I added some random Gaussian noise here, suppose I ﬁt an unpruned decision tree High Variance Why? Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 where f(x) is some true (target) function suppose we have multiple training sets Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 High Variance Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 So, why does bagging work/what does it do? Sebastian Raschka STAT 479: Machine Learning FS 2018 Boosting ! 34 Sebastian Raschka STAT 479: Machine Learning FS 2018 Adaptive Boosting ! 35 Gradient Boosting e.g., AdaBoost (here!) e.g., XGBoost Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139. Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232. Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International conference on knowledge discovery and data mining (pp. 785-794). ACM. Sebastian Raschka STAT 479: Machine Learning FS 2018 Adaptive Boosting ! 36 Gradient Boosting e.g., AdaBoost (here!) e.g., XGBoost Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139. Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189-1232. Chen, T., & Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International conference on knowledge discovery and data mining (pp. 785-794). ACM. Diﬀer mainly in terms of how • weights are updated • classiﬁers are combined Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 37 General Boosting Training Sample Weighted Training Sample Weighted Training Sample h1(x) h2(x) hm(x) hm(x) = sign( m ∑ j=1 wj hj(x)) hm(x) = arg max i ( m ∑ j=1 wj 1[hj(x) = i]) for h(x) ∈{−1,1} or h(x) = i, i ∈{1,...,n} for Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 38 General Boosting ‣ Initialize a weight vector with uniform weights ‣ Loop: ‣ Apply weak learner* to weighted training examples (instead of orig. training set, may draw bootstrap samples with weighted probability) ‣ Increase weight for misclassiﬁed examples ‣ (Weighted) majority voting on trained classiﬁers * a learner slightly better than random guessing Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 39 AdaBoost Algorithm 1 AdaBoost 1: Initialize k: the number of AdaBoost rounds 2: Initialize D: the training dataset, D = {hx[1], y[1]i, ..., x[n], y[n]i} 3: Initialize w1(i) = 1/n, i = 1, ..., n, w1 2 Rn 4: 5: for r=1 to k do 6: For all i : wr(i) := wr(i)/ P i wr(i) [normalize weights] 7: hr := FitWeakLearner(D, wr) 8: ✏r := P i wr(i) 1(hr(i) 6= yi) [compute error] 9: if ✏r > 1/2 then stop 10: ↵r := 1 2 log[(1 −✏r)/✏r] [small if error is large and vice versa] 11: wr+1(i) := wr(i)⇥ ( e−↵r if hr(x[i]) = y[i] e↵r if hr(x[i]) 6= y[i] 12: Predict: hk(x) = arg maxj P r ↵r1[hr(x) = j] 13: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 40 Modify AdaBoost w. Bootstrap Algorithm 1 AdaBoost 1: Initialize k: the number of AdaBoost rounds 2: Initialize D: the training dataset, D = {hx[1], y[1]i, ..., x[n], y[n]i} 3: Initialize w1(i) = 1/n, i = 1, ..., n, w1 2 Rn 4: 5: for r=1 to k do 6: For all i : wr(i) := wr(i)/ P i wr(i) [normalize weights] 7: hr := FitWeakLearner(D, wr) 8: ✏r := P i wr(i) 1(hr(i) 6= yi) [compute error] 9: if ✏r > 1/2 then stop 10: ↵r := 1 2 log[(1 −✏r)/✏r] [small if error is large and vice versa] 11: wr+1(i) := wr(i)⇥ ( e−↵r if hr(x[i]) = y[i] e↵r if hr(x[i]) 6= y[i] 12: Predict: hk(x) = arg maxj P r ↵r1[hr(x) = j] 13: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 41 Decision Tree Stumps Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 42 x1 x1 x1 x1 x2 x2 x2 x2 2 4 3 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 43 x1 x1 x1 x1 x2 x2 x2 x2 2 4 3 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 44 x1 x1 x1 x1 x2 x2 x2 x2 2 4 3 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 45 x1 x1 x1 x1 x2 x2 x2 x2 2 4 3 1 Sebastian Raschka STAT 479: Machine Learning FS 2018 Random Forests ! 46 Sebastian Raschka STAT 479: Machine Learning FS 2018 Random Forests = Bagging w. trees + random feature subsets ! 47 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 48 Tin Kam Ho used the “random subspace method,” where each tree got a random subset of features. “Our method relies on an autonomous, pseudo-random procedure to select a small number of dimensions from a given feature space …” • Ho, Tin Kam. “The random subspace method for constructing decision forests.” IEEE transactions on pattern analysis and machine intelligence 20.8 (1998): 832-844. “Trademark” random forest: “… random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on.” • Breiman, Leo. “Random Forests” Machine learning 45.1 (2001): 5-32. Random Feature Subset for each Tree or Node? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 49 Tin Kam Ho used the “random subspace method,” where each tree got a random subset of features. “Our method relies on an autonomous, pseudo-random procedure to select a small number of dimensions from a given feature space …” • Ho, Tin Kam. “The random subspace method for constructing decision forests.” IEEE transactions on pattern analysis and machine intelligence 20.8 (1998): 832-844. “Trademark” random forest: “… random forest with random features is formed by selecting at random, at each node, a small group of input variables to split on.” • Breiman, Leo. “Random Forests” Machine learning 45.1 (2001): 5-32. Random Feature Subset for each Tree or Node? num features = log2 m + 1 where m is the number of input features Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 50 In contrast to the original publication [Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001] the scikit-learn implementation combines classiﬁers by averaging their probabilistic prediction, instead of letting each classiﬁer vote for a single class. ""Soft Voting"" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 51 Will discuss Random Forests and feature importance in Feature Selection lecture Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 52 PE ≤¯ ρ ⋅(1 −s2) s2 (Loose) Upper Bound for the Generalization Error Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001 ¯ ρ : Average correlation among trees : ""Strength"" of the ensemble s Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 53 Extremely Randomized Trees (ExtraTrees) ExtraTrees algorithm adds one more random component Random Forest random components: 1) _____________ 2) _____________ 3) _____________ Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine learning, 63(1), 3-42. Sebastian Raschka STAT 479: Machine Learning FS 2018 Stacking ! 54 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 55 Tang, J., S. Alelyani, and H. Liu. ""Data Classiﬁcation: Algorithms and Applications."" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500. Wolpert, David H. ""Stacked generalization."" Neural networks 5.2 (1992): 241-259. Stacking Algorithm Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 56 Stacking Algorithm Training set h1 h2 hn . . . y1 y2 yn . . . Meta-Classifier yf New data Classification models Predictions Final prediction Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 57 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Holdout Method 2-Fold Cross-Validation Training Evaluation Cross-Validation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 58 1st 2nd 3rd 4th 5th K Iterations (K-Folds) Validation Fold Training Fold Training Fold Data Training Fold Labels Prediction Validation Fold Data Performance Performance Performance Performance Performance 1 2 3 4 5 Performance 1 5 ∑ 5 i =1 Performancei = A B C k-fold Cross-Validation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 59 1st 2nd 3rd 4th 5th K Iterations (K-Folds) Validation Fold Training Fold Learning Algorithm Hyperparameter Values Model Training Fold Data Training Fold Labels Prediction Performance Model Validation Fold Data Validation Fold Labels Performance Performance Performance Performance Performance 1 2 3 4 5 Performance 1 5 ∑ 5 i =1 Performancei = A B C Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 60 Stacking Algorithm with Cross-Validation Wolpert, David H. ""Stacked generalization."" Neural networks 5.2 (1992): 241-259. Tang, J., S. Alelyani, and H. Liu. ""Data Classiﬁcation: Algorithms and Applications."" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 61 Stacking Algorithm with Cross-Validation Training set h1 h2 hn . . . y1 y2 yn . . . Meta-Classifier yf Base Classifiers Level-1 predictions in k-th iteration Final prediction Training folds Validation fold Repeat k times All level-1 predictions Train Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 62 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Training evaluation Leave-One-Out CV Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 63 Demos http://rasbt.github.io/mlxtend/user_guide/classiﬁer/EnsembleVoteClassiﬁer/ http://rasbt.github.io/mlxtend/user_guide/classiﬁer/StackingClassiﬁer/ http://rasbt.github.io/mlxtend/user_guide/classiﬁer/StackingCVClassiﬁer/ http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassiﬁer.html http://scikit-learn.org/stable/auto_examples/ensemble/ plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py http://scikit-learn.org/stable/auto_examples/ensemble/ plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost- hastie-10-2-py Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 64 Reading Assignments Python Machine Learning, 2nd Ed., Ch07 "
381,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Evaluation 1: Introduction to Overﬁtting and Underﬁtting Lecture 08 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Overview Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Overﬁtting and Underﬁtting Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Overﬁtting and Underﬁtting • Want a model to ""generalize"" well to unseen data (""high generalization accuracy"" or ""low generalization error"") ""Generalization Performance"" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Overﬁtting and Underﬁtting Assumptions • i.i.d. assumption: inputs are independent, and training and test examples are identically distributed (drawn from the same probability distribution) • The training error or accuracy provides an (optimistically) biased estimate of the generalization performance • For some random model that has not been ﬁtted to the training set, we expect both the training and test error to be equal Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 • Underﬁtting: both training and test error are large • Overﬁtting: gap between training and test error (where test error is higher) • Large hypothesis space being searched by a learning algorithm -> high tendency to overﬁt Overﬁtting and Underﬁtting Model Capacity Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 Overﬁtting and Underﬁtting Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 ""[...] model has high bias/variance"" -- What does that mean? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 ""[...] model has high bias/variance"" -- What does that mean? Originally formulated for regression Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Bias-Variance Decomposition and Trade-oﬀ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Bias-Variance Decomposition • Decomposition of the loss into bias and variance help us understand learning algorithms, concepts are correlated to underﬁtting and overﬁtting • Helps explain why ensemble methods (last lecture) might perform better than single models Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Low Variance (Precise) High Variance (Not Precise) Low Bias (Accurate) High Bias (Not Accurate) Bias-Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 Bias and Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Bias and Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Bias and Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Bias and Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Bias and Variance Intuition High bias (There are two points where the bias is zero) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 (here, I ﬁt an unpruned decision tree) High Variance Why? Bias and Variance Intuition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 where f(x) is some true (target) function suppose we have multiple training sets Bias and Variance Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Bias and Variance Example High variance Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Bias and Variance Example High variance What happens if we take the average? Does this remind you of something? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Terminology ̂ θ Point estimator of some parameter θ (could also be a function, e.g., the hypothesis is an estimator of some target function) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Terminology ̂ θ Point estimator of some parameter θ (could also be a function, e.g., the hypothesis is an estimator of some target function) Bias = E[ ̂ θ] −θ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Bias-Variance Decomposition Bias( ̂ θ) = E[ ̂ θ] −θ Var( ̂ θ) = E[ ̂ θ2] −(E[ ̂ θ]) 2 General Deﬁnition: Var( ̂ θ) = E[(E[ ̂ θ] −̂ θ)2] Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 Bias-Variance Decomposition Bias( ̂ θ) = E[ ̂ θ] −θ Var( ̂ θ) = E[ ̂ θ2] −(E[ ̂ θ]) 2 General Deﬁnition: Intuition: Var( ̂ θ) = E[(E[ ̂ θ] −̂ θ)2] (we ignore noise in this lecture for simplicity) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Bias-Variance Decomposition of Squared Error Bias( ̂ θ) = E[ ̂ θ] −θ Var( ̂ θ) = E[ ̂ θ2] −(E[ ̂ θ]) 2 General Deﬁnition: The variance provides an estimate of how much the estimate varies as we vary the training data (e.g,. by resampling). Intuition: Bias is the diﬀerence between the average estimator from diﬀerent training samples and the true value. (The expectation is over the training sets.) Var( ̂ θ) = E[(E[ ̂ θ] −̂ θ)2] Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 Bias-Variance Decomposition Loss = Bias + Variance + Noise Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 Bias-Variance Decomposition of Squared Error Bias( ̂ θ) = E[ ̂ θ] −θ Var( ̂ θ) = E[ ̂ θ2] −(E[ ̂ θ]) 2 General Deﬁnition: Var( ̂ θ) = E[(E[ ̂ θ] −̂ θ)2] y = f(x) ""ML notation"" for the Squared Error Loss: S = (y −̂ y)2 ̂ y = ̂ f(x) = h(x) (target, target function) (For the sake of simplicity, we ignore the noise term in this lecture) (Next slides: the expectation is over the training data, i.e, the average estimator from diﬀerent training samples) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 Bias-Variance Decomposition of Squared Error y = f(x) S = (y −̂ y)2 ̂ y = ̂ f(x) = h(x) (target, target function) S = (y −̂ y)2 (y −̂ y)2 = (y −E[ ̂ y] + E[ ̂ y] −̂ y)2 = (y −E[ ̂ y])2 + (E[ ̂ y] −y)2 + 2(y −E[ ̂ y])(E[ ̂ y] −̂ y) ""ML notation"" for the Squared Error Loss: (x is a particular data point e.g,. in the test set; the expectation is over training sets) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 Bias-Variance Decomposition of Squared Error S = (y −̂ y)2 (y −̂ y)2 = (y −E[ ̂ y] + E[ ̂ y] −̂ y)2 = (y −E[ ̂ y])2 + (E[ ̂ y] −y)2 + 2(y −E[ ̂ y])(E[ ̂ y] −̂ y) E[S] = E[(y −̂ y)2] E[(y −̂ y)2] = (y −E[ ̂ y])2 + E[(E[ ̂ y] −̂ y)2] = [Bias of the ﬁt]2 + Variance of the ﬁt (The expectation is over the training data, i.e, the average estimator from diﬀerent training samples) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 Bias-Variance Decomposition of Squared Error S = (y −̂ y)2 (y −̂ y)2 = (y −E[ ̂ y] + E[ ̂ y] −̂ y)2 = (y −E[ ̂ y])2 + (E[ ̂ y] −y)2 + 2(y −E[ ̂ y])(E[ ̂ y] −̂ y) E[S] = E[(y −̂ y)2] E[(y −̂ y)2] = (y −E[ ̂ y])2 + E[(E[ ̂ y] −̂ y)2] = [Bias]2 + Variance ??? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 Bias-Variance Decomposition of Squared Error S = (y −̂ y)2 (y −̂ y)2 = (y −E[ ̂ y] + E[ ̂ y] −̂ y)2 = (y −E[ ̂ y])2 + (E[ ̂ y] −y)2 + 2(y −E[ ̂ y])(E[ ̂ y] −̂ y) ??? E[2(y −E[ ̂ y])(E[ ̂ y] −̂ y)] = 2E[(y −E[ ̂ y])(E[ ̂ y] −̂ y)] = 2(y −E[ ̂ y])E[(E[ ̂ y] −̂ y)] = 2(y −E[ ̂ y])(E[E[ ̂ y]] −E[ ̂ y]) = 2(y −E[ ̂ y])(E[ ̂ y] −E[ ̂ y]) = 0 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 ""several authors have proposed bias-variance decompositions related to zero- one loss (Kong & Dietterich, 1995; Breiman, 1996b; Kohavi & Wolpert, 1996; Tibshirani, 1996; Friedman, 1997). However, each of these decompositions has signiﬁcant shortcomings."" Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 35 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Squared Loss Generalized Loss (y −̂ y)2 L(y, ̂ y) E[(y −̂ y)2] E[L(y, ̂ y)] Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 36 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Squared Loss Generalized Loss (y −̂ y)2 L(y, ̂ y) E[(y −̂ y)2] E[L(y, ̂ y)] Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). E[(y −̂ y)2] = (y −E[ ̂ y])2 + E[(E[ ̂ y] −̂ y)2] Bias2 Variance + Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 37 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Squared Loss Generalized Loss (y −̂ y)2 L(y, ̂ y) E[(y −̂ y)2] E[L(y, ̂ y)] Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). E[(y −̂ y)2] = (y −E[ ̂ y])2 + E[(E[ ̂ y] −̂ y)2] Bias2 Variance + Bias2: (y −E[ ̂ y])2 L(y, E[ ̂ y]) Variance: E[L( ̂ y, E[ ̂ y])] E[(E[ ̂ y] −̂ y)2] Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 38 Deﬁne ""Main Prediction"" Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). The main prediction is the prediction that minimizes the average loss E[L( ̂ y, ̂ y′!)] ¯̂ y = argmin ̂ y′! For squared loss -> Mean For 0-1 loss -> Mode Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 39 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Squared Loss 0-1 Loss (y −̂ y)2 L(y, ̂ y) E[(y −̂ y)2] E[L(y, ̂ y)] Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). E[(y −̂ y)2] = (y −E[ ̂ y])2 + E[(E[ ̂ y] −̂ y)2] Bias2 Variance + Bias2: (y −E[ ̂ y])2 L(y, E[ ̂ y]) Variance: E[L( ̂ y, E[ ̂ y])] E[(E[ ̂ y] −̂ y)2] Main prediction -> Mean Main prediction -> Mode Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 40 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Squared Loss 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias2: (y −E[ ̂ y])2 L(y, E[ ̂ y]) Variance: E[L( ̂ y, E[ ̂ y])] E[(E[ ̂ y] −̂ y)2] Main prediction -> Mean Main prediction -> Mode Bias = { 1 if y ≠¯̂ y 0 otherwise Variance = P( ̂ y ≠̂ ¯ y) E[(y −̂ y)2] E[L(y, ̂ y)] P(y ≠̂ y) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 41 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = Bias + Variance = P( ̂ y ≠y) Loss = Variance = Variance = P( ̂ y ≠̂ ¯ y) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 42 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = Bias + Variance = P( ̂ y ≠y) Loss = Variance = Variance = P( ̂ y ≠̂ ¯ y) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 43 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = P( ̂ y ≠y) = 1 −P( ̂ y = y) Loss = Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 44 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = P( ̂ y ≠y) = 1 −P( ̂ y = y) = 1 −P( ̂ y ≠¯̂ y) Loss = Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 45 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = P( ̂ y ≠y) = 1 −P( ̂ y = y) = 1 −P( ̂ y ≠¯̂ y) Loss = Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 46 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = P( ̂ y ≠y) = 1 −P( ̂ y = y) = 1 −P( ̂ y ≠¯̂ y) Loss = Loss = Bias - Variance Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 47 Bias-Variance Decomposition of 0-1 Loss Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0-1 Loss Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). Bias = { 1 if y ≠¯̂ y 0 otherwise P( ̂ y ≠y) Loss = P( ̂ y ≠y) = 1 −P( ̂ y = y) = 1 −P( ̂ y ≠¯̂ y) Loss = Loss = Bias - Variance Variance can improve loss!! Why is that so? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 48 Statistical Bias vs ""Machine Learning Bias"" • ""Machine learning bias"" sometimes also called ""inductive bias"" • e.g., decision tree algorithms consider small trees before they consider large trees (if training data can be classiﬁed by small tree, large trees are not considered) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 49 Hypothesis Space Entire hypothesis space Hypothesis space a particular learning algorithm can sample Hypothesis space a particular learning algorithm category has access to Particular hypothesis (i.e., a model/classifier) (From Lecture 1) e.g,. decision tree + KNN e.g,. decision tree e.g,. ID3 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 50 Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 51 Bias-Variance Simulation of C 4.5 Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 x2 x1 class c0 class c1 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 x1 • simulation on 200 training sets with 200 examples each (0-1 labels) • 200 hypotheses • test set: 22,801 examples (1 data point for each grid point) • mean error rate is 536 errors (out of the 22,801 test examples) • 297 as a result of bias • 239 as a result of variance (remember that trees use a ""staircase"" to approximate diagonal boundaries) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 52 Bias-Variance Simulation of C 4.5 Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. errors due to bias: 1788 errors due to variance:1046 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 53 Bias-Variance Simulation of C 4.5 Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. errors due to bias: 1788 errors due to variance: 1046 why? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 54 Bias-Variance Simulation of C 4.5 Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. errors due to bias: 0 errors due to variance: 17 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 55 Bias-Variance Simulation of C 4.5 Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). 0 5 10 15 20 25 30 35 40 45 0 5 10 15 20 Loss (%) k L B V Vu Vb 0 2 4 6 8 10 12 14 0 5 10 15 20 Loss (%) k L B V Vu Vb Figure 4: Eﬀect of varying k in k-nearest neighbor: audiology (top) and chess (bottom). L = Loss B = Bias V = (net) Variance Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 56 Recommended Reading Resources for Bias-Decomposition or more precisely Dietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. Technical report, Department of Computer Science, Oregon State University. Domingos, P. (2000). A unified bias-variance decomposition. In Proceedings of 17th International Conference on Machine Learning (pp. 231-238). 0-1 loss includes noise and more general: Loss = Bias + c Variance c1N(x) + B(x) + c2V(x) c1 = c2 = 1 where, e.g., for squared loss Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 57 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Next Lecture "
382,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Evaluation 2: Conﬁdence Intervals Lecture 09 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics This Lecture Overview Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 • Concepts ﬁrst • (More) Code at the end of the lecture Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Main points why we evaluate the predictive performance of a model: 1. Want to estimate the generalization performance, the predictive performance of our model on future (unseen) data. 2. Want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space. 3. Want to identify the ML algorithm that is best-suited for the problem at hand; thus, we want to compare diﬀerent algorithms, selecting the best-performing one as well as the best performing model from the algorithm’s hypothesis space. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 • Training set error is an optimistically biased estimator of the generalization error • Test set error is an unbiased estimator of the generalization error (test sample and hypothesis chosen independently) • (in practice, it is actually pessimistically biased; why?) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Holdout Method 2-Fold Cross-Validation Training Evaluation Sebastian Raschka STAT 479: Machine Learning FS 2018 Often using the holdout method is not a good idea ... ! 7 Sebastian Raschka STAT 479: Machine Learning FS 2018 Often using the holdout method is not a good idea ... ! 8 Test set error as generalization error estimator is pessimistically biased (not so bad) But it does not account for variance in the training data (bad) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Suppose we have the following ranking based on accuracy: h2: 75% > h1: 70% > h3: 65%, we would still rank them the same way if we add a 10% pessimistic bias: h2: 65% > h1: 60% > h3: 55%. Why is pessimistic bias not ""so bad""? Sebastian Raschka STAT 479: Machine Learning FS 2018 Often using the holdout method is not a good idea ... ! 10 • Test set error as generalization error estimator is pessimistically biased (not so bad) • Does not account for variance in the training data (bad) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 We have to keep in mind that a dataset represents a random sample drawn from a probability distribution, and we typically assume that this sample is representative of the true population – more or less. Now, further subsampling without replacement alters the statistic (mean, proportion, and variance) of the sample. The degree to which subsampling without replacement affects the statistic of a sample is inversely proportional to the size of the sample. Let us have a look at an example using the Iris dataset 1, which we randomly divide into 2/3 training data and 1/3 test data as illustrated in Figure 1. (The source code for generating this graphic is available on GitHub2.) All samples (n = 150) Training samples (n = 100) Test samples (n = 50) Figure 1: Distribution of Iris ﬂower classes upon random subsampling into training and test sets. 1https://archive.ics.uci.edu/ml/datasets/iris 2https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/iris-random-dist.ipynb 6 Issues with Subsampling (Independence violation) The Iris dataset consists of 50 Setosa, 50 Versicolor, and 50 Virginica ﬂowers; the ﬂower species are distributed uniformly: • 33.3% Setosa • 33.3% Versicolor • 33.3% Virginia If our random function assigns 2/3 of the ﬂowers (100) to the training set and 1/3 of the ﬂowers (50) to the test set, it may yield the following: • training set → 38 x Setosa, 28 x Versicolor, 34 x Virginica • test set → 12 x Setosa, 22 x Versicolor, 16 x Virginica Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Learning Algorithm Hyperparameter Values Model Prediction Test Labels Performance Model Learning Algorithm Hyperparameter Values Final Model 2 3 4 1 Test Labels Test Data Training Data Training Labels Data Labels Data Labels Training Data Training Labels Test Data Holdout evaluation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 Can we use the holdout method for model selection? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Holdout validation (hyperparam. tuning) 2 1 Data Labels Training Data Validation Data Validation Labels Test Data Test Labels Training Labels Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Best Model Learning Algorithm Hyperparameter values Model Hyperparameter values Hyperparameter values Model Model Training Data Training Labels Learning Algorithm Best Hyperparameter Values Final Model 6 Data Labels 3 Best Hyperparameter values Prediction Test Labels Performance Model 4 Test Data Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels 5 Validation Data Validation Labels Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Holdout validation (hyperparam. tuning) 2 1 Data Labels Training Data Validation Data Validation Labels Test Data Test Labels Training Labels Performance Model Validation Data Validation Labels Prediction Validation Data Prediction Learning Algorithm Hyperparameter values Model Hyperparameter values Hyperparameter values Model Model Training Data Training Labels 3 Best Hyperparameter values Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Holdout validation (hyperparam. tuning) Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Best Model Hyperparameter values Model 3 Best Hyperparameter values Prediction Test Labels Performance Model 4 Test Data Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels 5 Validation Data Validation Labels Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Holdout validation (hyperparam. tuning) Performance Model Validation Labels Learning Algorithm Best Hyperparameter Values Final Model 6 Data Labels Prediction Test Labels Performance Model 4 Test Data Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels 5 Validation Data Validation Labels Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Cross-Validation is generally better ... but ... Bengio, Y., & Grandvalet, Y. (2004). No unbiased estimator of the variance of k-fold cross-validation. Journal of machine learning research, 5(Sep), 1089-1105. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 𝒩(μ, σ2) Bias of Estimators Example Normal Distribution: f(x[i]; μ, σ2) = 1 2πσ2 exp( −1 2 (x[i] −μ)2 σ2 ) Probability density function: Is the sample mean an unbiased estimator of the mean of the Gaussian? ̂ μ = 1 n ∑ i x[i] Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Is the sample mean an unbiased estimator of the mean of the Gaussian? ̂ μ = 1 n ∑ i x[i] Bias( ̂ μ) = E[ ̂ μ] −μ = E[ 1 n ∑ i x[i]] −μ = 1 n ∑ i E[x[i]] −μ = 1 n ∑ i μ −μ = μ −μ = 0 Bias of Estimators Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 ̂ σ2 = 1 n ∑ i (x[i] − ̂ μ)2 Bias( ̂ σ2) = E[ ̂ σ2] −σ2 = E[ 1 n ∑ i (x[i] − ̂ μ) 2 ] −σ2 = . . . = m −1 m σ2 −σ2 Is the sample variance an unbiased estimator of the mean of the Gaussian Bias of Estimators Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 ̂ σ2 = 1 n ∑ i (x[i] − ̂ μ)2 Bias( ̂ σ2) = E[ ̂ σ2] −σ2 = E[ 1 n ∑ i (x[i] − ̂ μ) 2 ] −σ2 = . . . = m −1 m σ2 −σ2 ̂ σ′!2 = 1 n −1 ∑ i (x[i] − ̂ μ)2 The unbiased estimator is actually Is the sample variance an unbiased estimator of the mean of the Gaussian? Bias of Estimators Example Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 (Image credit: Screenshot from https://en.wikipedia.org/wiki/Binomial_distribution_) Binomial distribution Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 • coin lands on head (""success"") p • probability of success ERRS(h) = 1 n ∑x∈S δ( f(x), h(x)) ERR𝒟(h) = Pr x∈𝒟[ f(x) ≠h(x)] • true error Coin Flip (Bernoulli Trial) 0-1 Loss • example misclassiﬁed (0-1 loss) Pr(k) = n! k!(n −k)! pk(1 −p)n−k . Binomial distribution • sample (test set) error • , estimator of k n p • mean, number of successes μk = np Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Coin Flip (Bernoulli Trial) 0-1 Loss Pr(k) = n! k!(n −k)! pk(1 −p)n−k . Binomial distribution • mean, number of successes μk = np • variance σk = np(1 −p) • standard deviation σ2 k = np(1 −p) ERRS(h) = 1 n ∑x∈S δ( f(x), h(x)) σERRS(h) = σk n = np(1 −p) n = p(1 −p) n We are interested in proportions! σERRS(h) ≈ ERRS(h)(1 −ERRS(h)) n Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 Conﬁdence Intervals By deﬁnition, a XX% conﬁdence interval of some parameter p is an interval that is expected to contain p with probability XX% Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 Normal Approximation • Less tedious than conﬁdence interval for Binomial distribution and hence often used in (ML) practice for large n • Rule of thumb: if n larger than 40, the Binomial distribution can be reasonably approximated by a Normal distribution; and np and n(1 - p) should be greater than 5 CI = ERRS(h) ± z ERRS(h)(1 −ERRS(h)) n Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 The z constant for diﬀerent conﬁdence intervals: • 99%: z=2.58 • 95%: z=1.96 • 90%: z=1.64 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 Proportionally large test sets increase the pessimistic bias if a model has not reached its full capacity, yet. • To produce the plot above, I took 500 random samples of each of the ten classes from MNIST • The sample was then randomly divided into a 3500-example training subset and a test set (1500 examples) via stratiﬁcation. • Even smaller subsets of the 3500-sample training set were produced via randomized, stratiﬁed splits, and I used these subsets to ﬁt softmax classiﬁers and used the same 1500- sample test set to evaluate their performances; samples may overlap between these training subsets. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 Decreasing the size of the test set brings up another problem: It may result in a substantial variance increase of our model’s performance estimate. Dataset Distribution Sample 1 Sample 2 Sample 3 Train (70%) Test (30%) Train (70%) Test (30%) n=1000 n=100 Real World Distribution Resampling The reason is that it depends on which instances end up in training set, and which particular instances end up in test set. Keeping in mind that each time we resample our data, we alter the statistics of the distribution of the sample. Here, I repeatedly subsampled a two-dimensional Gaussian Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 Repeated Holdout: Estimate Model Stability ACCavg = 1 k k ∑ j=1 ACCj, ACCj = 1 −1 n n ∑ i=1 L(h(x[i]), f(x[i])) . Average performance over k repetitions where ACCj is the accuracy estimate of the jth test set of size m, (also called Monte Carlo Cross-Validation) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 Repeated Holdout: Estimate Model Stability 50/50 Train-Test Split Avg. Acc. 0.95 90/10 Train-Test Split Avg. Acc. 0.96 Left: I performed 50 stratiﬁed training/test splits with 75 samples in the test and training set each; a K- nearest neighbors model was ﬁt to the training set and evaluated on the test set in each repetition. Right: Here, I repeatedly performed 90/10 splits, though, so that the test set consisted of only 15 samples. How repeated holdout validation may look like for different training- test split using the Iris dataset to ﬁt to 3-nearest neighbors classiﬁers: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 35 The Bootstrap Method and Empirical Conﬁdence Intervals Circa 1900, to pull (oneself) up by (one’s) bootstraps was used ﬁguratively of an impossible task (Among the “practical questions” at the end of chapter one of Steele’s “Popular Physics” schoolbook (1888) is, “30. Why can not a man lift himself by pulling up on his boot-straps?”). By 1916 its meaning expanded to include “better oneself by rigorous, unaided effort.” The meaning “ﬁxed sequence of instructions to load the operating system of a computer” (1953) is from the notion of the ﬁrst-loaded program pulling itself, and the rest, up by the bootstrap. (Source: Online Etymology Dictionary) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 36 The Bootstrap Method and Empirical Conﬁdence Intervals • The bootstrap method is a resampling technique for estimating a sampling distribution • Here, we are particularly interested in estimating the uncertainty of our performance estimate • The bootstrap method was introduced by Bradley Efron in 1979 [1] • About 15 years later, Bradley Efron and Robert Tibshirani even devoted a whole book to the bootstrap, “An Introduction to the Bootstrap” [2] • In brief, the idea of the bootstrap method is to generate new data from a population by repeated sampling from the original dataset with replacement — in contrast, the repeated holdout method can be understood as sampling without replacement. [1] Efron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1). Institute of Mathematical Statistics: 1–26. doi:10.1214/aos/1176344552. [2] Efron, Bradley, and Robert Tibshirani. 1994. An Introduction to the Bootstrap. Chapman & Hall. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 37 The Bootstrap Method and Empirical Conﬁdence Intervals 1. We are given a dataset of size n. 2. For b bootstrap rounds: 1. We draw one single instance from this dataset and assign it to our jth bootstrap sample. We repeat this step until our bootstrap sample has size n (the size of the original dataset). Each time, we draw samples from the same original dataset so that certain samples may appear more than once in our bootstrap sample and some not at all. 3. We ﬁt a model to each of the b bootstrap samples and compute the resubstitution accuracy. 4. We compute the model accuracy as the average over the b accuracy estimates ACCboot = 1 b b ∑ j=1 1 n n ∑ i=1 (1 −L(h(x[i]), f(x[i])) . Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 38 The Bootstrap Method and Empirical Conﬁdence Intervals • As we discussed previously, the resubstitution accuracy usually leads to an extremely optimistic bias, since a model can be overly sensible to noise in a dataset. • Originally, the bootstrap method aims to determine the statistical properties of an estimator when the underlying distribution was unknown and additional samples are not available. • So, in order to exploit this method for the evaluation of predictive models, such as hypotheses for classiﬁcation and regression, we may prefer a slightly different approach to bootstrapping using the so- called Leave-One-Out Bootstrap (LOOB) technique. • Here, we use out-of-bag samples as test sets for evaluation instead of evaluating the model on the training data. Out-of-bag samples are the unique sets of instances that are not used for model ﬁtting Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 39 x1 x1 x1 x1 x1 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x2 x2 x2 x2 x8 x8 x10 x7 x3 x6 x9 x3 x8 x10 x7 x2 x2 x9 x6 x6 x4 x4 x5 x10 x8 x7 x5 x4 x3 x4 x5 x6 x8 x9 Training Sets Test Sets Bootstrap 1 Bootstrap 2 Bootstrap 3 Original Dataset Bootstrap Sampling Out-of-bag samples Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 40 The Bootstrap Method and Empirical Conﬁdence Intervals We can compute the 95% conﬁdence interval of the bootstrap estimate as ACCboot = 1 b b ∑ i=1 ACCi and use it to compute the standard error (In practice, at least 200 bootstrap rounds are recommended) SEboot = 1 b −1 b ∑ i=1 (ACCi −ACCboot)2 . Finally, we can then compute the conﬁdence interval around the mean estimate as ACCboot ± t × SEboot . For instance, given a sample with n=100, we ﬁnd that t95 = 1.984 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 41 The Bootstrap Method and Empirical Conﬁdence Intervals And if our samples do not follow a normal distribution? A more robust, yet computationally straight-forward approach is the percentile method as described by B. Efron (Efron, 1981). Here, we pick our lower and upper conﬁdence bounds as follows: ACClower = α1th percentile of the ACCboot distribution ACCupper = α1th percentile of the ACCboot distribution where and and is our degree of conﬁdence to compute the conﬁdence interval. α1 = α α2 = 1 −α α 100 × (1 −2 × α) For instance, to compute a 95% conﬁdence interval, we pick to obtain the 2.5th and 97.5th percentiles of the b bootstrap samples distribution as our upper and lower conﬁdence bounds. α = 0.025 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 42 The Bootstrap Method and Empirical Conﬁdence Intervals In the left subplot, I applied the Leave-One-Out Bootstrap technique to evaluate 3-nearest neighbors models on Iris, and the right subplot shows the results of the same model evaluation approach on MNIST, using the same softmax algorithm as mentioned earlier.   Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 43 The 0.632 Bootstrap Method • In 1983, Bradley Efron described the .632 Estimate, a further improvement to address the pessimistic bias of the bootstrap [1]. • The pessimistic bias in the “classic” bootstrap method can be attributed to the fact that the bootstrap samples only contain approximately 63.2% of the unique samples from the original dataset. [1] Efron, Bradley. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 44 P(not chosen) = (1 −1 n) n , 1 e ≈0.368, n →∞. Bootstrap Sampling Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 45 P(not chosen) = (1 −1 n ) n , 1 e ≈0.368, n →∞. P(chosen) = 1 −(1 −1 n ) n ≈0.632 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 46 The .632 Bootstrap Method [1] Efron, Bradley. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. The .632 Estimate, is computed via the following equation: ACCboot = 1 b b ∑ i=1 (0.632 ⋅ACCh,i + 0.368 ⋅ACCr,i), ACCr,i ACCh,i where is the resubstitution accuracy is the accuracy on the out-of-bag sample. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 47 The .632+ Bootstrap Method Now, while the .632 Boostrap attempts to address the pessimistic bias of the estimate, an optimistic bias may occur with models that tend to overﬁt so that Bradley Efron and Robert Tibshirani proposed the The .632+ Bootstrap Method [1]. Instead of using a ﬁxed “weight” ω = 0.632 ACCboot = 1 b b ∑ i=1 (ω ⋅ACCh,i + (1 −ω) ⋅ACCr,i), in we compute the weight as ω = 0.632 1 −0.368 × R , where R is the relative overﬁtting rate R = (−1) × (ACCh,i −ACCr,i) γ −(1 −ACCh,i) . [1] Efron, Bradley, and Robert Tibshirani. 1997. “Improvements on Cross-Validation: The .632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 48 The .632+ Bootstrap Method R is the relative overﬁtting rate R = (−1) × (ACCh,i −ACCr,i) γ −(1 −ACCh,i) . [1] Efron, Bradley, and Robert Tibshirani. 1997. “Improvements on Cross-Validation: The .632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. Now, we need to determine the no-information rate γ in order to compute R. For instance, we can compute γ by ﬁtting a model to a dataset that contains all possible combinations between the examples and target class labels: γ = 1 n2 n ∑ i=1 n ∑ i′!=1 (1 −L(h(x[i]), f(x[i])) . Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 49 The .632+ Bootstrap Method R is the relative overﬁtting rate R = (−1) × (ACCh,i −ACCr,i) γ −(1 −ACCh,i) . [1] Efron, Bradley, and Robert Tibshirani. 1997. “Improvements on Cross-Validation: The .632+ Bootstrap Method.” Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. Now, we need to determine the no-information rate γ in order to compute R. For instance, we can compute γ by ﬁtting a model to a dataset that contains all possible combinations between the examples and target class labels: γ = 1 n2 n ∑ i=1 n ∑ i′!=1 (1 −L(h(x[i]), f(x[i])) . Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 50 Code Examples https://github.com/rasbt/stat479-machine-learning-fs18/ blob/master/09_eval-ci/09_eval-ci_code.ipynb Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 51 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Next Lecture Overview "
383,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Evaluation 3: Cross Validation Lecture 10 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics This Lecture Overview Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Hyperparameters Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Hyperparameters ? k = 5 k = 3 k = 1 feature 1 feature 2 nonparametric model: k-nearest neighbors Hyperparameter k=1 k=3 k=5 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Hyperparameters parametric model: logistic regression Hyperparameter Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 3-Way Holdout instead of ""regular"" holdout to avoid ""data leakage"" during hyperparameter optimization Learning Algorithm Best Hyperparameter Values Final Model 6 Data Labels Prediction Test Labels Performance Model 4 Test Data Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels 5 Validation Data Validation Labels 2 1 Data Labels Training Data Validation Data Validation Labels Test Data Test Labels Training Labels Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Performance Model Validation Data Validation Labels Prediction Best Model Learning Algorithm Hyperparameter values Model Hyperparameter values Hyperparameter values Model Model Training Data Training Labels 3 Best Hyperparameter values Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 Main points why we evaluate the predictive performance of a model: 1. Want to estimate the generalization performance, the predictive performance of our model on future (unseen) data. 2. Want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space. 3. Want to identify the ML algorithm that is best-suited for the problem at hand; thus, we want to compare diﬀerent algorithms, selecting the best-performing one as well as the best performing model from the algorithm’s hypothesis space. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 k-Fold Cross-Validation Part 1 Model Evaluation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 k-Fold Cross-Validation 1st 2nd 3rd 4th 5th K Iterations (K-Folds) Validation Fold Training Fold Learning Algorithm Hyperparameter Values Model Training Fold Data Training Fold Labels Prediction Performance Model Validation Fold Data Validation Fold Labels Performance Performance Performance Performance Performance 1 2 3 4 5 Performance 1 5 ∑ 5 i =1 Performancei = A B C Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 k-Fold Cross-Validation - non-overlapping test folds; utilizes all data for testing - overlapping training folds - some variance estimate from diﬀerent training sets, (but no unbiased estimate) - more pessimistic for small k because we withhold data from ﬁtting 1st 2nd 3rd 4th 5th K Iterations (K-Folds) Validation Fold Training Fold Learning Algorithm Hyperparameter Values Model Training Fold Data Training Fold Labels Prediction Performance Model Validation Fold Data Validation Fold Labels Performance Performance Performance Performance Performance 1 2 3 4 5 Performance 1 5 ∑ 5 i =1 Performancei = A B C Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 k-Fold CV special cases: k=2 & k=n 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 ... 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Holdout Method 2-Fold Cross-Validation Repeated Holdout Training Evaluation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 k-Fold CV special cases: k=2 & k=n 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 ... 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Holdout Method 2-Fold Cross-Validation Repeated Holdout Training Evaluation 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 1 2 3 4 5 6 7 8 10 9 Training evaluation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 k-Fold Cross-Validation ""[...] where available sample sizes are modest, holding back compounds for model testing is ill-advised. This fragmentation of the sample harms the calibration and does not give a trustworthy assessment of ﬁt anyway. It is better to use all data for the calibration step and check the ﬁt by cross-validation, making sure that the cross- validation is carried out correctly. [...] The only motivation to rely on the holdout sample rather than cross-validation would be if there was reason to think the cross-validation not trustworthy -- biased or highly variable. But neither theoretical results nor the empiric results sketched here give any reason to disbelieve the cross-validation results."" [1] 1. Hawkins, D. M., Basak, S. C., & Mills, D. (2003). Assessing model fit by cross-validation. Journal of chemical information and computer sciences, 43(2), 579-586. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 LOOCV vs Holdout The reported ""mean"" refers to the averaged diﬀerence between the true coeﬃcients of determination (R2 ) and the coeﬃcients obtained via LOOCV (here called q2) after repeating this procedure on diﬀerent 100-example training Sebastian Raschka STAT479 FS18. L10: Cross Validation Page 10 Table 1: Summary of the ﬁndings from the LOOCV vs. holdout comparison study conducted by Hawkins and others Douglas M Hawkins, Subhash C Basak, and Denise Mills. “Assessing model ﬁt by cross-validation”. In: Journal of Chemical Information and Computer Sciences 43.2 (2003), pp. 579–586. See text for details. Experiment Mean Standard deviation True R2 — q2 0.010 0.149 True R2 — hold 50 0.028 0.184 True R2 — hold 20 0.055 0.305 True R2 — hold 10 0.123 0.504 sets and averaging the results. In rows 2-4, the researchers used the holdout method for ﬁtting models to the 100-example training sets, and they evaluated the performances on holdout sets of sizes 10, 20, and 50 samples. Each experiment was repeated 75 times, and the mean column shows the average di↵erence between the estimated R2 and the true R2 values. As we can see, the estimates obtained via LOOCV (q2) are the closest to the true R2 on average. The estimates obtained from the 50-example test set via the holdout method are also passable, though. Based on these particular experiments, we may agree with the researchers’ conclusion: Taking the third of these points, if you have 150 or more compounds available, then you can certainly make a random split into 100 for calibration and 50 or more for testing. However it is hard to see why you would want to do this. Douglas M Hawkins, Subhash C Basak, and Denise Mills. “Assessing model ﬁt by cross-validation”. In: Journal of Chemical Information and Computer Sciences 43.2 (2003), pp. 579–586 One reason why we may prefer the holdout method may be concerns about computational eﬃciency, if the dataset is suﬃciently large. As a rule of thumb, we can say that the 1. Hawkins, D. M., Basak, S. C., & Mills, D. (2003). Assessing model fit by cross-validation. Journal of chemical information and computer sciences, 43(2), 579-586. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 LOOCV vs Holdout The reported ""mean"" refers to the averaged diﬀerence between the true coeﬃcients of determination (R2 ) and the coeﬃcients obtained via LOOCV (here called q2) after repeating this procedure on diﬀerent 100-example training Sebastian Raschka STAT479 FS18. L10: Cross Validation Page 10 Table 1: Summary of the ﬁndings from the LOOCV vs. holdout comparison study conducted by Hawkins and others Douglas M Hawkins, Subhash C Basak, and Denise Mills. “Assessing model ﬁt by cross-validation”. In: Journal of Chemical Information and Computer Sciences 43.2 (2003), pp. 579–586. See text for details. Experiment Mean Standard deviation True R2 — q2 0.010 0.149 True R2 — hold 50 0.028 0.184 True R2 — hold 20 0.055 0.305 True R2 — hold 10 0.123 0.504 sets and averaging the results. In rows 2-4, the researchers used the holdout method for ﬁtting models to the 100-example training sets, and they evaluated the performances on holdout sets of sizes 10, 20, and 50 samples. Each experiment was repeated 75 times, and the mean column shows the average di↵erence between the estimated R2 and the true R2 values. As we can see, the estimates obtained via LOOCV (q2) are the closest to the true R2 on average. The estimates obtained from the 50-example test set via the holdout method are also passable, though. Based on these particular experiments, we may agree with the researchers’ conclusion: Taking the third of these points, if you have 150 or more compounds available, then you can certainly make a random split into 100 for calibration and 50 or more for testing. However it is hard to see why you would want to do this. Douglas M Hawkins, Subhash C Basak, and Denise Mills. “Assessing model ﬁt by cross-validation”. In: Journal of Chemical Information and Computer Sciences 43.2 (2003), pp. 579–586 One reason why we may prefer the holdout method may be concerns about computational eﬃciency, if the dataset is suﬃciently large. As a rule of thumb, we can say that the 1. Hawkins, D. M., Basak, S. C., & Mills, D. (2003). Assessing model fit by cross-validation. Journal of chemical information and computer sciences, 43(2), 579-586. In rows 2-4, the researchers used the holdout method for ﬁtting models to the 100- example training sets, and they evaluated the performances on holdout sets of sizes 10, 20, and 50 samples. Each experiment was repeated 75 times, and the mean column shows the average diﬀerence between the estimated R2 and the true R2 values. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Problems with LOOCV for Classiﬁcation • While LOOCV is almost unbiased, one downside of using LOOCV over k-fold cross-validation with k < n is the large variance of the LOOCV estimate. • LOOCV is ""defect"" when using a discontinuous loss-function such as the 0-1 loss in classiﬁcation or even in continuous loss functions such as the mean-squared-error. • LOOCV has high variance because the test set only contains one sample Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Problems with LOOCV for Classiﬁcation ""With k=n, the cross-validation estimator is approximately unbiased for the true (expected) prediction error, but can have high variance because the n ""training sets"" are so similar to one another."" [1] [1] Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York, NY, USA:: Springer series in statistics. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 For correlated variables, the variance of their sum is the sum of their covariances Var ( n ∑ i=1 Xi) = n ∑ i=1 n ∑ j=1 Cov(Xi, Xj) = n ∑ i=1 Var(Xi) + 2 ∑ 1≤i<j≤n Cov(Xi, Xj) Or in other words, we can attribute the high variance to the fact that the mean of highly correlated variables has a higher variance than the mean of variables that are not highly correlated? Problems with LOOCV for Classiﬁcation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Empirical Study and Recommendation 2 5 10 20 -5 -2 -1 folds 45 50 55 60 65 70 75 % acc Soybean Vehicle Rand 2 5 10 20 -5 -2 -1 folds 96 96.5 97 97.5 98 98.5 99 99.5 100 % acc Chess Hypo Mushroom 1 2 5 10 20 50 100 samples 45 50 55 60 65 70 75 % acc Soybean Vehicle Rand Estimated Soybean Vehicle Rand 1 2 5 10 20 50 100 samples 96 96.5 97 97.5 98 98.5 99 99.5 100 % acc Chess Hypo Mushroom 2 5 10 20 -5 -2 -1 folds C4.5 0 1 2 3 4 5 6 7 std dev Mushroom Chess Hypo Breast Vehicle Soybean Rand 2 5 10 20 -5 -2 -1 folds Naive-Bayes 0 1 2 3 4 5 6 7 std dev Mushroom Chess Hypo Breast Vehicle Soybean Rand Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 • The bias of the performance estimator decreases (more accurate) • The variance of the performance estimators increases (more variability) • The computational cost increases (more iterations, larger training sets during ﬁtting) • Exception: decreasing the value of k in k-fold cross-validation to small values (for example, 2 or 3) also increases the variance on small datasets due to random sampling eﬀects. Summarizing k-Fold CV for Model Evaluation What happens if we increase k? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 k-Fold Cross-Validation Part 2 Model Selection Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Test Labels Test Data Training Data Training Labels Data Labels Learning Algorithm Hyperparameter values Hyperparameter values Hyperparameter values Training Data Training Labels Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels Prediction Test Labels Performance Model Test Data Learning Algorithm Best Hyperparameter Values Final Model Data Labels 2 1 3 4 5 Performance Performance Performance Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Test Labels Test Data Training Data Training Labels Data Labels Learning Algorithm Hyperparameter values Hyperparameter values Hyperparameter values Training Data Training Labels Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels 2 1 3 Performance Performance Performance Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Learning Algorithm Best Hyperparameter Values Model Training Data Training Labels Prediction Test Labels Performance Model Test Data Learning Algorithm Best Hyperparameter Values Final Model Data Labels 3 4 5 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 The Law of Parsimony Occam's Razor: ""Among competing hypotheses, the one with the fewest assumptions should be selected."" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 The Law of Parsimony ""Simpler models are more accurate. This belief is sometimes equated with Occam's razor, but the razor only says that simpler explanations are preferable, not why. They're preferable because they're easier to understand, remember, and reason with. Sometimes the simplest hypothesis consistent with the data is less accurate for prediction than a more complicated one. Some of the most powerful learning algorithms output models that seem gratuitously elaborate -- sometimes even continuing to add to them after they've perfectly ﬁt the data -- but that's how they beat the less powerful ones."" https://medium.com/@pedromdd/ten-myths-about-machine-learning-d888b48334a3 Pedro Domingos: ""Then Myths about Machine Learning"" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 The 1-standard error method ... However, if two models perform equally well, the simpler one seems more likely (among other advantages) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 The 1-standard error method ... However, if two models perform equally well, the simpler one seems more likely (among other advantages) 1. Consider the numerically optimal estimate and its standard error. 2. Select the model whose performance is within one standard error of the value obtained in step 1. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 The 1-standard error method (Some toy data I generated via scikit-learn) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 The 1-standard error method Consider a RBF-kernel SVM, where gamma controls the inﬂuence of the training points (don't need to know the details, yet) K(xi, xj) = exp(−γ||xi −xj||2), γ > 0. Gaussian/RBF-kernel: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 The 1-standard error method = 0.001 (note: here I used 10-fold CV) Which parameter would you select? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 The 1-standard error method = 0.1 = 0.001 = 10.0 (note: here I used 10-fold CV) Which parameter would you select? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 Code Examples https://github.com/rasbt/stat479-machine-learning-fs18/blob/ master/10_eval-cv/10_eval-cv_code.ipynb Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Overview Next Lecture "
384,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Evaluation 4: Algorithm Comparisons Lecture 11 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics This Lecture Overview Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Performance estimation Model selection (hyperparameter optimization) and performance estimation Large dataset ▪ 2-way holdout method (train/test split) ▪ Conﬁdence interval via normal approximation Small dataset ▪ 3-way holdout method (train/validation/test split) ▪ (Repeated) k-fold cross-validation without independent test set ▪ Leave-one-out cross-validation without independent test set ▪ Conﬁdence interval via 0.632(+) bootstrap Model & algorithm comparison ▪ Disjoint training sets + test set (algorithm comparison, AC) ▪ McNemar test (model comparison, MC) ▪ Cochran’s Q + McNemar test (MC) ▪ Combined 5x2cv F test (AC) ▪ Nested cross-validation (AC) Large dataset Small dataset Large dataset Small dataset ▪ (Repeated) k-fold cross-validation with independent test set ▪ Leave-one-out cross-validation with independent test set Overview, (my) ""recommendations"" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Comparing two machine learning classiﬁers -- McNemar's Test McNemar's test, introduced by Quinn McNemar in 1947 [1], is a non-parametric statistical test for paired comparisons that can be applied to compare the performance of two machine learning classiﬁers: This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. Task Gaussian data … Paired nominal data Compare a group to a reference value Compare a pair of groups Compare two unpaired groups Binomial test McNemar’s test test, Fisher’s exact test [1] McNemar, Quinn. ""Note on the sampling error of the diﬀerence between correlated proportions or percentages."" Psychometrika 12.2 (1947): 153-157. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Comparing two machine learning classiﬁers -- McNemar's Test • Also referred to as ""within-subjects chi-squared test"" • Applied to paired nominal data based on a version of a 2x2 confusion matrix • Compares the predictions of two models to each other rather than listing false positive, true positive, false negative, and true negative counts of a single model • The layout of the 2x2 confusion matrix suitable for McNemar's test is shown in the following ﬁgure: This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. A B C D Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 Comparing two machine learning classiﬁers -- McNemar's Test This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. A B C D Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong • Given such a 2x2 confusion matrix as shown in the previous ﬁgure, we can compute the accuracy of a Model 1 via (A+B) / (A+B+C+D) • Similarly, we can compute the accuracy of Model 2 as (A+C) / N • Cells B and C (the oﬀ-diagonal entries) tell us how the models diﬀer Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 Comparing two machine learning classiﬁers -- McNemar's Test • Let's take a look at the following example: 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B • What is the prediction accuracy of models 1 and 2? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 Comparing two machine learning classiﬁers -- McNemar's Test 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B • What is the prediction accuracy of models 1 and 2? In both subpanel A and B, the accuracy of Model 1 and Model 2 are ???% and ???%, respectively. • Model 1 accuracy subpanel A: • Model 1 accuracy subpanel B: • Model 2 accuracy subpanel A: • Model 2 accuracy subpanel B: (???)/10000 × 100 % = ??? % (???)/10000 × 100 % = ??? % (???)/10000 × 100 % = ??? % (???)/10000 × 100 % = ??? % Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Comparing two machine learning classiﬁers -- McNemar's Test 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B • What is the prediction accuracy of models 1 and 2? In both subpanel A and B, the accuracy of Model 1 and Model 2 are 99.7% and 99.6%, respectively. • Model 1 accuracy subpanel A: • Model 1 accuracy subpanel B: • Model 2 accuracy subpanel A: • Model 2 accuracy subpanel B: (9959 + 1)/10000 × 100 % = 99.6 % (9945 + 15)/10000 × 100 % = 99.6 % (9959 + 11)/10000 × 100 % = 99.7 % (9945 + 25)/10000 × 100 % = 99.7 % Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Comparing two machine learning classiﬁers -- McNemar's Test 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B In both subpanel A and B, the accuracy of Model 1 and Model 2 are 99.7% and 99.6%, respectively. In subpanel A: • Model 1 got 11 predictions right that Model 2 got wrong • Model 2 got 1 prediction right that Model 1 got wrong • Based on this 11:1 ratio (based on our intuition), does Model 1 perform substantially better than Model 2? In subpanel B: • The Model 1:Model 2 ratio is 25:15 • This is less conclusive about which model is the better one to choose. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Comparing two machine learning classiﬁers -- McNemar's Test 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B In both subpanel A and B, the accuracy of Model 1 and Model 2 are 99.7% and 99.6%, respectively. In McNemar's Test, we formulate the • null hypothesis: the probabilities p(B) and p(C) are the same • alternative hypothesis: the performances of the two models are not equal This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. A B C D Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Comparing two machine learning classiﬁers -- McNemar's Test 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B In both subpanel A and B, the accuracy of Model 1 and Model 2 are 99.7% and 99.6%, respectively. In McNemar's Test, we formulate the • null hypothesis: the probabilities p(B) and p(C) are the same • alternative hypothesis: the performances of the two models are not equal This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. A B C D Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong The McNemar test statistic (""chi-squared"") can be computed as follows: χ2 = (B −C)2 B + C Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 Comparing two machine learning classiﬁers -- McNemar's Test The McNemar test statistic (""chi-squared"") can be computed as follows: χ2 = (B −C)2 B + C • Set a signiﬁcance threshold, for example, • Compute the p-value -- assuming that the null hypothesis is true, the p-value is the probability of observing the given empirical (or a larger) chi-squared value (chi^2 distribution with 1 degree of freedom, and relatively large numbers in cells B and C, say > 25) • If the p-value is lower than our chosen signiﬁcance level, we can reject the null hypothesis that the two model's performances are equal α = 0.05 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Comparing two machine learning classiﬁers -- McNemar's Test • If we did this for scenario B in the previous ﬁgure (chi^2=2.5), we would obtain a p-value of 0.1138, which is larger than our signiﬁcance threshold, and thus, we cannot reject the null hypothesis. • If we computed the p-value for scenario A (chi^2=8.3), we would obtain a p-value of 0.0039, which is below the set signiﬁcance threshold (alpha=0.05) and leads to the rejection of the null hypothesis; we can conclude that the models' performances are diﬀerent (for instance, Model 1 performs better than Model 2). 9945 25 15 15 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 9959 11 1 29 Model 2 correct Model 2 wrong Model 1 correct Model 1 wrong A B Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Comparing two machine learning classiﬁers -- McNemar's Test Approximately 1 year after Quinn McNemar published the McNemar Test (McNemar 1947), Allen L. Edwards [1] proposed a continuity corrected version, which is the more commonly used variant today: Continuity Correction χ2 = (|B −C| −1) 2 B + C . ""This correction will have the obvious result of reducing the absolute value of the diﬀerence, [B - C], by unity."" [1] [1] Edwards, Allen L. ""Note on the “correction for continuity” in testing the signiﬁcance of the diﬀerence between correlated proportions."" Psychometrika 13.3 (1948): 185-187. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Comparing two machine learning classiﬁers -- McNemar's Test Exact p-values via the Binomial test • McNemar's test approximates the p-values reasonably well if the values in cells B and C are larger than 50 • But it makes sense to use a computationally more expensive binomial test to compute the exact p-values (esp. if B and C are relatively small) -- since the chi- squared value from McNemar's test may not be well-approximated by the chi- squared distribution Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Comparing two machine learning classiﬁers -- McNemar's Test Exact p-values via the Binomial test • McNemar's test approximates the p-values reasonably well if the values in cells B and C are larger than 50 • But it makes sense to use a computationally more expensive binomial test to compute the exact p-values (esp. if B and C are relatively small) -- since the chi- squared value from McNemar's test may not be well-approximated by the chi- squared distribution The exact p-value can be computed as follows: p = 2 n ∑ i=B ( n i ) 0.5i(1 −0.5)n−i, where n=b+c, and the factor 2 is used to compute the two-sided p-value. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Comparing two machine learning classiﬁers -- McNemar's Test Exact p-values via the Binomial test • The following heat map illustrates the diﬀerences between the McNemar approximation of the chi-squared value (with and without Edward's continuity correction) to the exact p-values computed via the binomial test: 0 20 40 60 80 100 B 0 20 40 60 80 100 C 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 This work by Sebastian Raschka is licensed under a Creative Commons Attribution 4.0 International License. 0 20 40 60 80 100 B 0 20 40 60 80 100 C 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Uncorrected vs. exact Corrected vs. exact absolute p-value diﬀerence absolute p-value diﬀerence (As we can see in this heat map, the p-values from the continuity-corrected version of McNemar's test are almost identical to the p-values from a binomial test if both B and C are larger than 50.) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Multiple Hypothesis Testing Issue 1. Conduct an omnibus test under the null hypothesis that there is no diﬀerence between the classiﬁcation accuracies 2. If the omnibus test led to the rejection of the null hypothesis, conduct pairwise post hoc tests, with adjustments for multiple comparisons, to determine where the diﬀerences between the model performances occurred Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Multiple Hypothesis Testing Issue 1. Conduct an omnibus test under the null hypothesis that there is no diﬀerence between the classiﬁcation accuracies (Cochran's Q test would be a good choice, which is a generalized version of McNemar's test for three or more models) 2. If the omnibus test led to the rejection of the null hypothesis, conduct pairwise post hoc tests, with adjustments for multiple comparisons, to determine where the diﬀerences between the model performances occurred (McNemar's Test would be a candidate here) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Cochran's Q Test • Cochran's Q test is analogous to ANOVA for binary outcomes • The test statistic is approximately (similar to McNemar's test) distributed as chi- squared with L−1 degrees of freedom, where L is the number of models we evaluate (since L=2 for McNemar's test, McNemars test statistic approximates a chi-squared distribution with one degree of freedom) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Cochran's Q Test • Cochran's Q test is analogous to ANOVA for binary outcomes • The test statistic is approximately (similar to McNemar's test) distributed as chi- squared with L−1 degrees of freedom, where L is the number of models we evaluate (since L=2 for McNemar's test, McNemars test statistic approximates a chi-squared distribution with one degree of freedom) pi : H0 = p1 = p2 = ⋯= pL . More formally, Cochran's Q test tests the hypothesis that there is no diﬀerence between the classiﬁcation accuracies Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Cochran's Q Test Let be a set of classiﬁers who have all been tested on the same dataset. If the L classiﬁers don't perform diﬀerently, then the following Q statistic is distributed approximately as ""chi-squared"" with L-1 degrees of freedom {C1, …, CL} QC = (L −1) L∑L i=1 G2 i −T2 LT −∑Nts j=1 (Lj)2 . Gi is the number of objects out of Nts correctly classiﬁed by Ci = 1,…L Lj is the number of classiﬁers out of L that correctly classiﬁed object zj ∈Zts Zts = {z1, . . . zNts} where is the test dataset on which the classiﬁers are tested on; and T is the total number of correct number of votes among the L classiﬁers T = L ∑ i=1 Gi = Nts ∑ j=1 Lj . Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 McNemar's Test with Bonferroni Correction to counteract the problem of multiple comparisons Unfortunately, the problem of multiple comparisons receives little attention in literature. However, Peter H. Westfall, James F . Troendl, and Gene Pennello wrote a nice article on how to approach such situations where we want to compare multiple models to each other if you are interested: • Westfall, Peter H., James F . Troendle, and Gene Pennello. ""Multiple mcnemar tests."" Biometrics 66.4 (2010): 1185-1191. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 McNemar's Test with Bonferroni Correction to counteract the problem of multiple comparisons Perneger, Thomas V. ""What’s wrong with Bonferroni adjustments."" BMJ: British Medical Journal 316.7139 (1998): 1236: ""Type I errors [False Positives] cannot decrease (the whole point of Bonferroni adjustments) without inflating type II errors (the probability of accepting the null hypothesis when the alternative is true) (Rothman, 1990). And type II errors [False Negatives] are no less false than type I errors."" Eventually, once more it comes down to the ""no free lunch"" -- in this context, let us refer of it as the ""no free lunch theorem of statistical tests."" ""The answer is that such adjustments are correct in the original framework of statistical test theory, proposed by Neyman and Pearson in the 1920s (Neyman, 1928). This theory was intended to aid decisions in repetitive situations."" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Algorithm Selection Aside from publishing papers, what would be a real-world application (vs. model evaluation)? Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural computation, 10(7), 1895-1923: 1. McNemar's test - low false positive rate - fast, only needs to be executed once 2. Diﬀerence in proportions, by Snedecor and Cochran - high false positive rate (here, incorrectly detect diﬀerence when there is none) - cheap to compute though 3. Resampled paired t-test - high false positive rate - computationally very expensive 4. k-fold cross-validated t-test - somewhat elated false positive rate 5. 5x2cv paired t-test - low false positive rate (similar to McNemarr) - slightly more powerful than McNemar; recommended if computational eﬃciency (runtime) is not an issue (10 times more computations than McNemar) Summary: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 Resampled paired t test H0: equal accuracies t = ΔACCavg k ∑k i=1 (ΔACCi −ΔACCavg)2/(k −1) ΔACCavg = 1 k k ∑ i=1 ΔACCi ΔACCi = ACCA i −ACCB i Here, k is the number of times we split the set into train/test sets Two independence violations!!! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 K-fold cross-validation with paired t test H0: equal accuracies t = ΔACCavg k ∑k i=1 (ΔACCi −ΔACCavg)2/(k −1) ΔACCavg = 1 k k ∑ i=1 ΔACCi ΔACCi = ACCA i −ACCB i Here, k is the number of folds we use Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 5x2 CV Cross-Validation + paired t test Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural computation, 10(7), 1895-1923: Argument: independent training sets for 2-fold Now we get 2 diﬀerences, since we use 2-fold cross-validation: ΔACC(1) i = ACCA(1) i −ACCB(1) i ΔACC(2) i = ACCA(2) i −ACCB(2) i ΔACCavg,i = (ΔACC(1) i + ΔACC(2) i )/2 est. variance: s2 i = (ACC(1) −ΔACCavg,i)2 + (ACC(2) −ΔACCavg,i)2 t = ΔACC(1) 1 (1/5)∑5 i=1 s2 i (note that the subscript 1 in denominator is not a typo, it only refers to the ﬁrst run) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 F Test for classifiers Looney, S. W. (1988). A statistical technique for comparing the accuracies of several classiﬁers. Pattern Recognition Letters, 8(1), 5-9. Assume 1 test set and L independent classiﬁers with accuracies ACC1, . . . ACCL SSC = Nts L ∑ i=1 ACC2 i −Nts ⋅L ⋅ACC2 avg SSO = 1 L Nts ∑ j=1 (Lj)2 −Nts ⋅L ⋅ACC2 avg (where Lj is the number of classiﬁers that correctly classiﬁed the jth example) SST = Nts ⋅L ⋅ACCavg(1 −ACCavg) SSCOMB = SST −SSC −SSO MSC = SSC L −1 MSCOMB = SSCOMB (L −1)(Nts −1) F = MSC MSCOMB Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 Combined 5 × 2 cv F Test for Comparing Supervised Classiﬁcation Learning Algorithms More robust than Dietterich 1998's 5x2 CV + t test Alpaydin, Ethem. ""Combined 5×2 cv F test for comparing supervised classification learning algorithms."" Neural computation 11.8 (1999): 1885-1892. f = ∑5 i=1 ∑2 j=1 (ΔACCj i)2 2∑5 i=1 s2 i Approximately F-distributed with 10 and 5 degrees of freedom. Sebastian Raschka STAT 479: Machine Learning FS 2018 Back to ""Computational/Empirical"" Methods ! 33 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 Recap: Model Selection with 3-way Holdout Original dataset Training set Validation set Test set Training set Test set Machine learning algorithm Predictive model Change hyperparameters and repeat Final performance estimate Fit Evaluate Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 35 Recap: Model Selection with k-fold Cross.-Val. Training set Test set Training set Validation set Test set Model Model Model Model Model Training set Training set Test set Model Model Model Model Training set Training set … Model Model Model Model Training Evaluation Training Selection & Evaluation Training Selection Evaluation Model Model Selection Test set Evaluation K-FOLD CROSS- VALIDATION good bad or ? good bad or ? good bad or ? good bad or ? 1) 2) 3) 4) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 36 Recap: Model Selection with k-fold Cross.-Val. Training set Test set Training set Validation set Test set Model Model Model Model Model Training set Training set Test set Model Model Model Model Training set Training set … Model Model Model Model Training Evaluation Training Selection & Evaluation Training Selection Evaluation Model Model Selection Test set Evaluation K-FOLD CROSS- VALIDATION 1) 2) 3) 4) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 37 Nested Cross-Validation for Algorithm Selection Main Idea: • Outer loop: purpose related to train/test split • Inner loop: like k-fold cross-validation for tuning Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 38 Nested Cross-Validation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 39 Nested Cross-Validation for Algorithm Selection • Outer loop: use average performance as generalization performance check for ""model stability"" • Finally: as usual, ﬁt model on whole dataset for deployment Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 40 Performance estimation Model selection (hyperparameter optimization) and performance estimation Large dataset ▪ 2-way holdout method (train/test split) ▪ Conﬁdence interval via normal approximation Small dataset ▪ 3-way holdout method (train/validation/test split) ▪ (Repeated) k-fold cross-validation without independent test set ▪ Leave-one-out cross-validation without independent test set ▪ Conﬁdence interval via 0.632(+) bootstrap Model & algorithm comparison ▪ Disjoint training sets + test set (algorithm comparison, AC) ▪ McNemar test (model comparison, MC) ▪ Cochran’s Q + McNemar test (MC) ▪ Combined 5x2cv F test (AC) ▪ Nested cross-validation (AC) Large dataset Small dataset Large dataset Small dataset ▪ (Repeated) k-fold cross-validation with independent test set ▪ Leave-one-out cross-validation with independent test set Conclusions, (my) ""recommendations"" Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 41 Code Examples https://github.com/rasbt/stat479-machine-learning-fs18/blob/master/ 11_eval-algo/11_eval-algo_code.ipynb Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 42 Model Eval Lectures Basics Bias and Variance Overﬁtting and Underﬁtting Holdout method Conﬁdence Intervals Resampling methods Repeated holdout Empirical conﬁdence intervals Cross-Validation Hyperparameter tuning Model selection Algorithm Selection Statistical Tests Evaluation Metrics Overview Next Lecture "
385,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Model Evaluation 5: Performance Metrics Lecture 12 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Training set Test set Scaling Dimensionality Reduction Learning Algorithm .fit(…) & .transform(…) .fit(…) & .transform(…) .fit(…) Predictive Model .transform(…) .transform(…) .predict(…) pipeline.fit(…) Class labels pipeline.predict(…) Class labels (Step 1) (Step 2) Pipeline Recap Source: Python Machine Learning, 2nd Edition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Accuracy Number of training samples Training accuracy Validation accuracy High bias High variance Good bias-variance trade-off 1.0 Accuracy Accuracy Number of training samples Number of training samples Desired accuracy Recap Source: Python Machine Learning, 2nd Edition Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 mance of a classiﬁer for binary classiﬁcation tasks. This square matrix consis of columns and rows that list the number of instances as absolute or relati ”actual class” vs. ”predicted class” ratios. Let P be the label of class 1 and N be the label of a second class or the lab of all classes that are not class 1 in a multi-class setting. Actual Class Predicted class P N P True Positives (TP) False Negatives (FN) N False Positives (FP) True Negatives (TN) The following equations are based on An introduction to ROC analysis by To Fawcett [1]. 2 Prediction Error and Accuracy Both the prediction error (ERR) and accuracy (ACC) provide general inform tion about how many samples are misclassiﬁed. The error can be understoo Actual Class P Positives (TP) Negatives (FN) N False Positives (FP) True Negatives (TN) The following equations are based on An introduction to ROC analysis by Tom Fawcett [1]. 2 Prediction Error and Accuracy Both the prediction error (ERR) and accuracy (ACC) provide general informa- tion about how many samples are misclassiﬁed. The error can be understood as the sum of all false predictions divided by the number of total predications, and the the accuracy is calculated as the sum of correct predictions divided by the total number of predictions, respectively. ERR = FP + FN FP + FN + TP + TN = 1 −ACC (1) ACC = TP + TN FP + FN + TP + TN = 1 −ERR (2) 2x2 Confusion Matrix Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Confusion Matrix for Multi-Class Settings Confusions matrices are traditionally for binary class problems but we can easily generalize it to multi-class settings Class 0 Class 1 Class 2 Class 0 T(0,0) Class 1 T(1,1) Class 2 T(2,2) Predicted Labels True Labels Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 False Positive Rate and False Negative Rate sages that were incorrectly classiﬁed as spam (False Positives): A situation re a person misses an important message is considered as ”worse” than a sit- on where a person ends up with a few spam messages in his e-mail inbox. In rast to the FPR, the True Positive Rate provides useful information about fraction of positive (or relevant) samples that were correctly identiﬁed out he total pool of Positives. FPR = FP N = FP FP + TN (3) TPR = TP P = TP FN + TP (4) Precision, Recall, and the F1-Score ision (PRE) and Recall (REC) are metrics that are more commonly used in rmation Technology and related to the False and True Prositive Rates. In Recall is synonymous to the True Positive Rate and also sometimes called itivity. The F1-Score can be understood as a combination of both Precision Recall [2]. Precision (PRE) and Recall (REC) are metrics that are more commonly used nformation Technology and related to the False and True Prositive Rates. act, Recall is synonymous to the True Positive Rate and is sometimes also d Sensitivity. The F1-Score can be understood as a combination of both ision and Recall [2]. • Think of it in a spam classiﬁcation problem (what are true positives, and if you had to pick one at the expense of the other: would you rather decrease the FPR or increase the TPR?) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 False Positive Rate and False Negative Rate Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 Precision, Recall, and F1 Score fact, Recall is synonymous to the True Positive Rate and also sometimes called Sensitivity. The F1-Score can be understood as a combination of both Precision and Recall [2]. Precision (PRE) and Recall (REC) are metrics that are more commonly used in Information Technology and related to the False and True Prositive Rates. In fact, Recall is synonymous to the True Positive Rate and is sometimes also called Sensitivity. The F1-Score can be understood as a combination of both Precision and Recall [2]. PRE = TP TP + FP (5) REC = TPR = TP P = TP FN + TP (6) F1 = 2 · PRE · REC PRE + REC (7) 3 • Terms that are more popular in Information Technology • Recall is actually just another term for True Positive Rate (or ""sensitivity"") Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Precision and Recall https://en.wikipedia.org/wiki/Precision_and_recall Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Sensitivity and Speciﬁcity Sensitivity measures the recovery rate of the Positives and complimentary, the Speciﬁcity measures the recovery rate of the Negatives. 5 Sensitivity and Speciﬁcity Sensitivity (SEN) is synonymous to Recall and the True Positive Rate whereas Speciﬁcity (SPC) is synonymous to the True Negative Rate — Sensitivity mea- sures the recovery rate of the Positives and complimentary, the Speciﬁcity mea- sures the recovery rate of the Negatives. SEN = TPR = REC = TP P = TP FN + TP (8) SPC = TNR = TN N = TN FP + TN (9) 6 Matthews correlation coeﬃcient Matthews correlation coeﬃcient (MCC) was ﬁrst formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a speciﬁc case of a linear correlation coeﬃcient (Pearson r) for a binary classiﬁcation setting and is considered as especially use- ful in unbalanced class settings. The previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) — a value of 0 denotes a random prediction. TP TN FP FN Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Matthew's Correlation Coeﬃcient P FN + TP SPC = TNR = TN N = TN FP + TN (9) 6 Matthews correlation coeﬃcient Matthews correlation coeﬃcient (MCC) was ﬁrst formulated by Brian W. Matthews [3] in 1975 to assess the performance of protein secondary structure predictions. The MCC can be understood as a speciﬁc case of a linear correlation coeﬃcient (Pearson r) for a binary classiﬁcation setting and is considered as especially use- ful in unbalanced class settings. The previous metrics take values in the range between 0 (worst) and 1 (best), whereas the MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) — a value of 0 denotes a random prediction. MCC = TP · TN −FP · FN p (TP + FP)(TP + FN)(TN + FP)(TN + FN) (10) 7 Receiver Operator Characteristic (ROC) Receiver Operator Characteristics (ROC) graphs are useful tools to select clas- siﬁcation models based on their performance with respect to the False Positive and True Positive rates. The diagonal of a ROC graph can be interpreted as random guessing and classiﬁcation models that fall below the diagonal are considered as worse than • Matthews correlation coeﬃcient (MCC) was ﬁrst formulated by Brian W. Matthews [1] in 1975 to assess the performance of protein secondary structure predictions • The MCC can be understood as a speciﬁc case of a linear correlation coeﬃcient (Pearson r) for a binary classiﬁcation setting • Considered as especially useful in unbalanced class settings. • The previous metrics take values in the range between 0 (worst) and 1 (best) • The MCC is bounded between the range 1 (perfect correlation between ground truth and predicted outcome) and -1 (inverse or negative correlation) — a value of 0 denotes a random prediction. [1] Brian W Matthews. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophysica Acta (BBA)- Protein Structure, 405(2):442–451, 1975. Sebastian Raschka STAT 479: Machine Learning FS 2018 Balanced Accuracy / Average Per-Class Accuracy Class 0 Class 1 Class 2 Class 0 T(0,0) Class 1 T(1,1) Class 2 T(2,2) Predicted Labels True Labels Class 0 Class 1 Class 2 Class 0 3 0 0 Class 1 7 50 12 Class 2 0 0 18 Predicted Labels True Labels ACC = T n ACC = 3 + 50 + 18 90 ≈0.79 APC ACC = 83/90 + 71/90 + 78/90 3 ≈0.86 Sebastian Raschka STAT 479: Machine Learning FS 2018 Balanced Accuracy / Average Per-Class Accuracy Class 0 Class 1 Class 2 Class 0 3 0 0 Class 1 7 50 12 Class 2 0 0 18 Predicted Labels True Labels APC ACC = 83/90 + 71/90 + 78/90 3 ≈0.86 Class 0 Neg Class Class 0 3 0 Neg Class 7 80 Predicted Labels True Labels Class 1 Neg Class Class 1 50 19 Neg Class 0 21 Predicted Labels True Labels Class 2 Neg Class Class 2 18 0 Neg Class 12 60 Predicted Labels True Labels Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Receiver Operating Characteristic curve (ROC curve) • Trade-oﬀ between True Positive Rate and False Positive Rate • ROC can be plotted by changing the prediction threshold • ROC term comes from ""Radar Receiver Operators"" (analysis of radar [RAdio Direction And Ranging] images) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 • ?.? = Perfect Prediction • ?.? = Random Prediction Receiver Operating Characteristic curve (ROC curve) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 ROC Area Under the Curve (AUC) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 ROC and k-Fold Cross-Validation Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Binary Classiﬁers and One-vs-Rest (OvR) / One-vs-All (OvA) Big O: O( ? ) Choose the class with the highest conﬁdence score Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Binary Classiﬁers and One-vs-One (OvO) / All-vs-All (AvA) num_classes x (num_classes - 1) / 2 Big O: O( ? ) Select the class by majority vote (and use conﬁdence score in case of ties) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Macro and Micro Averaging PREmicro = TP1 + … + TPc TP1 + … + TPc + FP1 + … + FPc PREmacro = PRE1 + … + PREc c Micro-averaging is useful if we want to weight each instance or prediction equally, whereas macro-averaging weights all classes equally to evaluate the overall performance of a classiﬁer with regard to the most frequent class labels. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Dealing with Class Imbalance https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Reading Assignment • Python Machine Learning, 2nd Edition Chapter 6: Learning Best Practices for Model Evaluation and Hyperparameter Tuning "
386,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Dimensionality Reduction I: Feature Selection Lecture 13 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Dimensionality Reduction Feature Selection Feature Extraction Today Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Dimensionality Reduction Feature Extraction Filter Methods Wrapper Methods Embedded Methods Feature Selection Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 4 Dimensionality Reduction Feature Extraction Filter Methods Wrapper Methods Embedded Methods Feature Selection • Information gain • Correlation with target • Pairwise correlation • Variance threshold • ... • L1 (LASSO) regularization • Decision tree • ... • Recursive Feature Elimination (RFE) • Sequential Feature Selection (SFS) • Permutation importance • ... Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 5 Variance Threshold (Filter) • Compute the variance of each feature • Assume that features with a higher variance may contain more useful information • Select the subset of features based on a user-speciﬁed threshold (""keep if greater or equal to x” or “keep the the top k features with largest variance”) • Good: fast! • Bad: does not take the relationship among features into account Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 6 Hyperparameters parametric model: logistic regression Hyperparameter Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 7 Hyperparameter L1 Regularization / LASSO (Embedded) Least Absolute Shrinkage and Selection Operator λ||w||1 = λ m ∑ j |w| L1 norm: Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 Least Absolute Shrinkage and Selection Operator L1 Regularization / LASSO (Embedded) Cost minimum (regularized estimate) w2 w1 Overall Goal: Minimize cost + penalty Goal 1: Minimize cost Goal 2) Minimize penalty Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Least Absolute Shrinkage and Selection Operator Wine Dataset https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data 'Class label' 'Alcohol' 'Malic acid' 'Ash' Alcalinity of ash' 'Magnesium' 'Total phenols' 'Flavanoids' 'Nonﬂavanoid phenols' 'Proanthocyanins' 'Color intensity' 'Hue' OD280/OD315 of diluted wines' 'Proline' 1 14.23 1.71 2.43 15.6 127 2.8 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 13.2 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.4 1050 1 13.16 2.36 2.67 18.6 101 2.8 3.24 0.3 2.81 5.68 1.03 3.17 1185 3 13.27 4.28 2.26 20 120 1.59 0.69 0.43 1.35 10.2 0.59 1.56 835 3 13.17 2.59 2.37 20 120 1.65 0.68 0.53 1.46 9.3 0.6 1.62 840 3 14.13 4.1 2.74 24.5 96 2.05 0.76 0.56 1.35 9.2 0.61 1.6 560 L1 Regularization / LASSO (Embedded) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Least Absolute Shrinkage and Selection Operator Wine Dataset https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data LASSO Path (don't forget to normalize/standardize features) L1 Regularization / LASSO (Embedded) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Recursive Feature Elimination (Wrapper) Consider a (generalized) linear model: 1. Fit model to dataset 2. Eliminate feature with the smallest coeﬃcient (""most unimportant"") 3. Repeat steps 1-2 until desired number of features is reached Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Random Forest Feature Importance • Usually measured as • impurity decrease (Gini, Entropy) for a given node/feature decision • weighted by number of examples at that node • averaged over all trees • then normalize so that sum of feature importances sum to 1 • (Unfair for variables with many vs few values) ""Method A"" (used in scikit-learn) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 Permutation Test (Interlude) • A nonparametric test procedure to test the null hypothesis that two diﬀerent groups come from the same distribution • Can be used for signiﬁcance or hypothesis testing w/o requiring to make any assumptions about the sampling distribution (e.g., it doesn't require the samples to be normal distributed). • Under the null hypothesis (treatment = control), any permutations are equally likely • Note that there are (n+m)! permutations, where n is the number of records in the treatment sample, and m is the number of records in the control sample • For a two-sided test, we deﬁne the alternative hypothesis that the two samples are diﬀerent (e.g., treatment != control) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Permutation Test 1. Compute the diﬀerence (here: mean) of sample x (size n) and sample y (size m) 2. Combine all measurements into a single dataset 3. Draw a permuted dataset from all possible permutations of the dataset in 2. 4. Divide the permuted dataset into two datasets x' and y' of size n and m, respectively 5. Compute the diﬀerence (here: mean) of sample x' and sample y' and record this diﬀerence 6. Repeat steps 3-5 until all permutations are evaluated 7. Return the p-value as the number of times the recorded diﬀerences were more extreme than the original diﬀerence from 1., then divide this number by the total number of permutations Here, the p-value is deﬁned as the probability, given the null hypothesis (no diﬀerence between the samples) is true, that we obtain results that are at least as extreme as the results we observed (i.e., the sample diﬀerence from 1.). Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Permutation Test 1. Compute the diﬀerence (here: mean) of sample x (size n) and sample y (size m) 2. Combine all measurements into a single dataset 3. Draw a permuted dataset from all possible permutations of the dataset in 2. 4. Divide the permuted dataset into two datasets x' and y' of size n and m, respectively 5. Compute the diﬀerence (here: mean) of sample x' and sample y' and record this diﬀerence 6. Repeat steps 3-5 until all permutations are evaluated 7. Return the p-value as the number of times the recorded diﬀerences were more extreme than the original diﬀerence from 1., then divide this number by the total number of permutations Here, the p-value is deﬁned as the probability, given the null hypothesis (no diﬀerence between the samples) is true, that we obtain results that are at least as extreme as the results we observed (i.e., the sample diﬀerence from 1.). Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 Permutation Test Here, the p-value is deﬁned as the probability, given the null hypothesis (no diﬀerence between the samples) is true, that we obtain results that are at least as extreme as the results we observed (i.e., the sample diﬀerence from 1.). p(t > t0) = 1 (n + m)! (n+m)! ∑ j=1 I(tj > t0), where t0 is the observed value of the test statistic, and t is the t-value, the statistic computed from the resamples, and I is the indicator function. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Feature Importance Through Permutation (Wrapper) 1. Take a model that was ﬁt to the training set 2. Estimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance 3. For each feature i: a. randomly permute feature column i in the original dataset b. record the predictive performance of the model on the dataset with the permuted column c. compute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset Repeat a-c exhaustively (all combinations) or a large number of times and compute the feature importance as the average difference intuitive & model-agnostic Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Feature Importance Through Permutation (Wrapper) intuitive & model-agnostic Column-Drop variant: For each feature column i: 1.temporarily remove column 2.ﬁt model to reduced dataset 3.compute validation set performance and compare to before Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 Random Forest Importance vs Permutation from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier # Build a classification task using 3 informative features X, y = make_classification(n_samples=10000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False) • Permutation performance is a universal method, RF just shown as an example • Permutation performance much more expensive, and in case of RF , usually computationally wasteful but can be more robust • In the special case of RF , we can also use the OOB examples instead of a validation set (next slide) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 Random Forest Feature Importance ""Method B"" Out-of-bag accuracy: • During training, for each tree, make prediction for OOB sample (1/3 of the training data) • Based on those predictions where example i was OOB, compute label via majority vote • The proportion over all examples where the majority vote is wrong is the OOB accuracy estimate Out-of-bag feature importance via permutation: • Count votes for correct class • Given feature i, permute this feature in OOB examples of a tree • Compute the number of correct votes after permutation from the number of votes before permutation for given tree • Repeat for all trees in the random forest and average the importance • Repeat for other features Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 Sequential Forward Selection (Wrapper) The SFAs are outlined in pseudo code below: Sequential Forward Selection (SFS) Input: The SFS algorithm takes the whole -dimensional feature set as input. Output: , where SFS returns a subset of features; the number of selected features , where , has to be speciﬁed a priori. Initialization: , Y = { , , . . . , } y1 y2 yd d = { | j = 1, 2, . . . , k; ∈Y} Xk xj xj k = (0, 1, 2, . . . , d) k k < d = ∅ X0 k = 0 We initialize the algorithm with an empty set (""null set"") so that (where is the size of the subset). Step 1 (Inclusion): Go to Step 1 in this step, we add an additional feature, , to our feature subset . is the feature that maximizes our criterion function, that is, the feature that is associated with the best classiﬁer performance if it is added to . We repeat this procedure until the termination criterion is satisﬁed. Termination: We add features from the feature subset until the feature subset of size contains the number of desired features that we speciﬁed a priori. Sequential Backward Selection (SBS) ∅ k = 0 k = arg max J( + x), where x ∈Y − x+ xk Xk = + Xk+1 Xk x+ k = k + 1 x+ Xk x+ Xk k = p Xk k p Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Sequential Backward Selection We add features from the feature subset until the feature subset of size contains the number of desired features that we speciﬁed a priori. Sequential Backward Selection (SBS) Input: the set of all features, The SBS algorithm takes the whole feature set as input. Output: , where SBS returns a subset of features; the number of selected features , where , has to be speciﬁed a priori. Initialization: , We initialize the algorithm with the given feature set so that the . Step 1 (Exclusion): Go to Step 1 In this step, we remove a feature, from our feature subset . is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classiﬁer performance if it is removed from . We repeat this procedure until the termination criterion is satisﬁed. Termination: We add features from the feature subset until the feature subset of size contains the number of desired features that we speciﬁed a priori. Sequential Backward Floating Selection (SBFS) Xk k p Y = { , , . . . , } y1 y2 yd = { | j = 1, 2, . . . , k; ∈Y} Xk xj xj k = (0, 1, 2, . . . , d) k k < d = Y X0 k = d k = d = arg max J( −x), where x ∈ x− xk Xk = − Xk−1 Xk x− k = k −1 x− Xk x− Xk k = p Xk k p Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 Sequential Floating Forward Selection Sequential Forward Floating Selection (SFFS) Input: the set of all features, The SFFS algorithm takes the whole feature set as input, if our feature space consists of, e.g. 10, if our feature space consists of 10 dimensions (d = 10). Output: a subset of features, , where The returned output of the algorithm is a subset of the feature space of a speciﬁed size. E.g., a subset of 5 features from a 10-dimensional feature space (k = 5, d = 10). Initialization: , We initialize the algorithm with an empty set (""null set"") so that the k = 0 (where k is the size of the subset) Step 1 (Inclusion): Go to Step 2 Step 2 (Conditional Exclusion): Y = { , , . . . , } y1 y2 yd = { | j = 1, 2, . . . , k; ∈Y} Xk xj xj k = (0, 1, 2, . . . , d) = Y X0 k = d = arg max J( + x), where x ∈Y − x+ xk Xk = + Xk+1 Xk x+ k = k + 1 = arg max J( −x), where x ∈ x− xk Xk : Go to Step 1 In step 1, we include the feature from the feature space that leads to the best performance increase for our feature subset (assessed by the criterion function). Then, we go over to step 2 In step 2, we only remove a feature if the resulting subset would gain an increase in performance. If or an improvement cannot be made (i.e., such feature cannot be found), go back to step 1; else, repeat this step. Steps 1 and 2 are repeated until the Termination criterion is reached. Termination: stop when k equals the number of desired features if J( −x) > J( −x) xk xk = − Xk−1 Xk x− k = k −1 k = 2 x+ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Sequential Floating Backward Selection Sequential Backward Floating Selection (SBFS) Input: the set of all features, The SBFS algorithm takes the whole feature set as input. Output: , where SBFS returns a subset of features; the number of selected features , where , has to be speciﬁed a priori. Initialization: , Y = { , , . . . , } y1 y2 yd = { | j = 1, 2, . . . , k; ∈Y} Xk xj xj k = (0, 1, 2, . . . , d) k k < d = Y X0 k = d We initialize the algorithm with the given feature set so that the . Step 1 (Exclusion): Go to Step 2 In this step, we remove a feature, from our feature subset . is the feature that maximizes our criterion function upon re,oval, that is, the feature that is associated with the best classiﬁer performance if it is removed from . Step 2 (Conditional Inclusion): if J(x_k + x) > J(x_k + x): Go to Step 1 In Step 2, we search for features that improve the classiﬁer performance if they are added back to the feature subset. If such features exist, we add the feature for which the performance improvement is maximized. If or an improvement cannot be made (i.e., such feature cannot be found), go back to step 1; else, repeat this step. Termination: We add features from the feature subset until the feature subset of size contains the number of desired features that we speciﬁed a priori. k = d = arg max J( −x), where x ∈ x− xk Xk = − Xk−1 Xk x− k = k −1 x− Xk x− Xk = arg max J( + x), where x ∈Y − x+ xk Xk = + Xk+1 Xk x+ k = k + 1 x+ k = 2 x+ k = p Xk k p Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 Pudil, P., Novovičová, J., & Kittler, J. (1994). ""Floating search methods in feature selection."" Pattern recognition letters 15.11 (1994): 1119-1125. Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Ferri, F. J., Pudil P., Hatef, M., Kittler, J. (1994). ""Comparative study of techniques for large-scale feature selection."" Pattern Recognition in Practice IV : 403-413. "
387,"Sebastian Raschka STAT 479: Machine Learning FS 2018 Dimensionality Reduction II: Feature Extraction Lecture 14 ! 1 STAT 479: Machine Learning, Fall 2018 Sebastian Raschka http://stat.wisc.edu/~sraschka/teaching/stat479-fs2018/ short version Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 2 Dimensionality Reduction Feature Selection Feature Extraction Today Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 3 Dimensionality Reduction Feature Selection Linear Methods Nonlinear Methods Feature Extraction • Principal Component Analysis (PCA) • Independent Component Analysis (ICA) • Autoencoders (linear act. func.) • Singular Vector Decomposition (SVD) • Linear Discriminant Analysis (LDA) (Supervised) • ... • t-Distr. Stochastic Neigh. Emb. (t-SNE) • Uniform Manifold Approx. & Proj. (UMAP) • Kernel PCA • Spectral Clustering • Autoencoders (non-linear act. func.) • ... Sebastian Raschka STAT 479: Machine Learning FS 2018 Goals of Dimensionality Reduction • Reduce Curse of Dimensionality problems • Increase storage and computational eﬃciency • Visualize Data in 2D or 3D ! 4 Sebastian Raschka STAT 479: Machine Learning FS 2018 Principal Component Analysis (PCA) x2 PC1 PC2 x1 x2 PC2 PC2 PC1 1) Find directions of maximum variance ! 5 Sebastian Raschka STAT 479: Machine Learning FS 2018 Principal Component Analysis (PCA) x2 PC1 PC2 x1 x2 PC1 PC2 PC2 PC1 2) Transform features onto directions of maximum variance x2 PC1 PC2 x1 x2 PC1 PC2 PC2 PC1 ! 6 Sebastian Raschka STAT 479: Machine Learning FS 2018 Principal Component Analysis (PCA) 3) Usually consider a subset of vectors of most variance (dimensionality reduction) x1 x2 PC1 PC2 PC2 PC1 x2 PC1 PC2 PC1 ! 7 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 8 Principal Component Analysis (PCA) Given design matrix ﬁnd vector X ∈ℝn×m αi with maximum variance repeat: ﬁnd αi+1 with maximum variance uncorrelated with αi (repeat k times, where k is the desired number of dimensions; ) k ≤m (in a nutshell) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 9 Two approaches to solve PCA (on standardized data): 1. Constrained maximization (e.g., Lagrange multipliers) 2. Eigen-decomposition of covariance matrix directly Principal Component Analysis (PCA) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 10 Principal Component Analysis (PCA) (in a nutshell) αi Collect vectors in a projection matrix Compute projected data points: A ∈ℝm×k Z = XA (Sorted from highest to lowest associated eigenvalue) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 11 Principal Component Analysis (PCA) Usually useful to plot the explained variance (normalized eigenvalues) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 12 Principal Component Analysis (PCA) Keep in mind that PCA is unsupervised! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 13 PCA Factor Loadings • The loadings are the unstandardized values of the eigenvectors • We can interpret the loadings as the covariances (or correlation in case we standardized the input features) between the input features and the and the principal components (or eigenvectors), which have been scaled to unit length Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 14 Mirrored Results in PCA Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 15 Mirrored Results in PCA • Not due to an error; reason for this diﬀerence is that, depending on the eigensolver, eigenvectors can have either negative or positive signs For instance, if v is an eigenvector of a matrix Σ , we have Σv = λv, is the eigenvalue λ where is also an eigenvalue of the same value −λ then Σ(−v) = −Σv = −λv = λ(−v) since Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 16 t-Distributed Stochastic Neighbor Embedding (t-SNE) Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605. VAN DER MAATEN AND HINTON 0 1 2 3 4 5 6 7 8 9 (a) Visualization by t-SNE. Shown are 6000 images from MNIST projected in 2D Note that MNIST has 28 x 28 = 784 dimensions (t-SNE is only meant for visualization not for preparing datasets!) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 17 Stochastic Nearest Neighbor Embeddings (SNE) Given high-dimensional datapoints, x1, . . . , xn ∈ℝm represent intrinsic structure of the data in 1D, 2D, or 3D (for visualization) How? 1) Model neighboring datapoint pairs based on the distance of those points in the high-dimensional space 2) Find a probability distribution of the pairwise distances in the low dimensional space that is as close as possible as the original probability distribution Main Idea: Map points near on a manifold to a near position in low-dimensional space Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 18 Stochastic Nearest Neighbor Embeddings (SNE) pj|i = exp( −||xi −xj||2/2σ2 i ) ∑k≠i exp( −||xi −xk||2/2σ2 i ) neighborhood size is controlled by (in turn controlled by perplexity parameter) σi p(x|μ, σ) = 1 2πσ2 exp( −(x −μ)2 2σ2 ) For reference, note that the normal distribution is deﬁned as Denominator makes sure that similarity is independent of the point's density where the conditional probability, pj|i, that xi would pick xj as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at xi pij = pi|j + pj|i 2n Based on probability of selecting neighboring points (this is a modiﬁcation to make the entropy [later slides] symmetric) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 19 qij = (1 + ||zi −zj||2)−1 ∑k≠i (1 + ||zi −zk||2)−1 in t-SNE, modeled with Student's t-distribution to prevent crowding problem • t-distribution has ""fatter"" tails (more scale invariant to points far away) • t-distribution avoids crowding problem • (minor point: is faster for computing the density; no exponential) t-Distributed Stochastic Neighbor Embedding (t-SNE) where zi zj and are the points in the low-dimensional space Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 20 t-Distribution With 1 degree of freedom same as Cauchy distribution Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 21 t-Distributed Stochastic Neighbor Embedding (t-SNE) Idea: Map points near on a manifold to a near position in low-dimensional space 1. Measure euclidean distance in high dim & convert to probability of picking a point as a neighbor (similarity is proportional to probability); use Gaussian distribution for density of each point 2. Same as 1. in low dimensionality but with t distribution (has heavier tails) 3. Minimize the diﬀerence of the conditional probabilities (KL-divergence) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 22 Kullback Leibler divergence Measures diﬀerence between 2 distributions; asymmetric DKL(P||Q) = ∫ ∞ −∞ p(x) log( p(x) q(x) )dx = ∫ ∞ −∞ p(x) log p(x)dx −∫ ∞ −∞ p(x) log q(x)dx Entropy Cross-Entropy H(i; xj) = − n ∑ i=1 p(i|xj)log2 p(i|xj) Remember Entropy from the Decision Tree lecture for discrete distributions? for feature xj and class label i Shannon Entropy: average amount of information produced by a stochastic source of data Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 23 conditional similarity between points in original space: pj|i = exp( −|xi −xj||2/2σ2 i ) ∑k≠i −exp(||xi −xk||2/2σ2 i ) VAN DER MAATEN AND HINTON a small q j|i to model a large p j|i), but there is only a small cost for using nearby map points to represent widely separated datapoints. This small cost comes from wasting some of the probability mass in the relevant Q distributions. In other words, the SNE cost function focuses on retaining the local structure of the data in the map (for reasonable values of the variance of the Gaussian in the high-dimensional space, σi). The remaining parameter to be selected is the variance σi of the Gaussian that is centered over each high-dimensional datapoint, xi. It is not likely that there is a single value of σi that is optimal for all datapoints in the data set because the density of the data is likely to vary. In dense regions, a smaller value of σi is usually more appropriate than in sparser regions. Any particular value of σi induces a probability distribution, Pi, over all of the other datapoints. This distribution has an entropy which increases as σi increases. SNE performs a binary search for the value of σi that produces a Pi with a ﬁxed perplexity that is speciﬁed by the user.3 The perplexity is deﬁned as Perp(Pi) = 2H(Pi), where H(Pi) is the Shannon entropy of Pi measured in bits H(Pi) = −∑ j p j|i log2 p j|i. The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50. The minimization of the cost function in Equation 2 is performed using a gradient descent method. The gradient has a surprisingly simple form δC δyi = 2∑ j (p j|i −q j|i + pi|j −qi|j)(yi −y j). Ph i ll h di b i d h l f d b f i b Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605. t-Distributed Stochastic Neighbor Embedding (t-SNE) pij replace with in symmetric SNE and t-SNE Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 24 Gradient Descent Optimization t-Distributed Stochastic Neighbor Embedding (t-SNE) ∂C ∂zi = 2∑ j (pj|i −qj|i + pi|j −qi|j)(zi −zj) Cost function C: C = ∑ i KL(Pi||Qi) = ∑ i ∑ j pj|i log pj|i qj|i Regular SNE Gradient w.r.t. z: pij replace in symmetric SNE pj|i with Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 25 Crowding problem t-Distributed Stochastic Neighbor Embedding (t-SNE) 1 2 3 suppose you want to maintain the neighbor-ship of the 2D space in 1D 4 x1 x2 z d d d d d d d 1 2 3 2d 2d 4 3d bad! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 26 Crowding problem t-Distributed Stochastic Neighbor Embedding (t-SNE) 1 2 3 4 x1 x2 z d d d d d d d 1 2 3 2d 2d 4 3d bad! z d d d 1 2 3 2d 4 3d bad! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 27 t-Distributed Stochastic Neighbor Embedding (t-SNE) 1 2 3 4 x1 x2 z d d d d d d d 1 2 3 2d 2d 4 3d bad! z d d d 1 2 3 2d 4 3d bad! z d d 1 2 3 4 d = 0 bad! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 28 t-Distributed Stochastic Neighbor Embedding (t-SNE) 1 2 3 4 x1 x2 z d d d d d d d 1 2 3 2d 2d 4 3d bad! z d d d 1 2 3 2d 4 3d bad! case where distance representation in low dimension is impossible :( z d d 1 2 3 4 d = 0 bad! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 29 Crowding problem! t-Distributed Stochastic Neighbor Embedding (t-SNE) 1 2 3 4 x1 x2 d d d d 2d 1 2 3 4 What would regular SNE do? Squashes all points! Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 30 t-Distributed Stochastic Neighbor Embedding (t-SNE) VAN DER MAATEN AND HINTON High−dimensional distance > Low−dimensional distance > 0 2 4 6 8 10 12 14 16 18 (a) Gradient of SNE. High−dimensional distance > Low−dimensional distance > −4 −2 0 2 4 6 8 10 12 14 (b) Gradient of UNI-SNE. High−dimensional distance > Low−dimensional distance > −1 −0.5 0 0.5 1 (c) Gradient of t-SNE. Figure 1: Gradients of three types of SNE as a function of the pairwise Euclidean distance between two points in the high-dimensional and the pairwise distance between the points in the low-dimensional data representation. selection of the Student t-distribution is that it is closely related to the Gaussian distribution, as the Student t-distribution is an inﬁnite mixture of Gaussians. A computationally convenient property is that it is much faster to evaluate the density of a point under a Student t-distribution than under a Gaussian because it does not involve an exponential, even though the Student t-distribution is equivalent to an inﬁnite mixture of Gaussians with different variances. The gradient of the Kullback-Leibler divergence between P and the Student-t based joint prob- ability distribution Q (computed using Equation 4) is derived in Appendix A, and is given by δC δyi = 4∑ j (pij −qij)(yi −y j) ! 1+∥yi −y j∥2""−1 . (5) Maaten, L. V. D., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605. negative gradient if points are too close in low-dim space to provide some repulsion against crowding ∂C ∂zi = 4∑ j (pij −qij)(zi −zj)(1 + ||zi −zj||2) −1 t-SNE Gradient w.r.t. z: pij = pi|j + pj|i 2n where Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 31 t-Distributed Stochastic Neighbor Embedding (t-SNE) • Great for visualizing datasets in 2D • Need to analyze multiple perplexity values (tuning parameter related to standard deviation of the Gaussian, to balance local and global attention) • Not deterministic, the cost function for t-SNE is not convex • More hyperparameters (learning rate epsilon) Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 32 t-Distributed Stochastic Neighbor Embedding (t-SNE) Source: https://distill.pub/2016/misread-tsne/ Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 33 Table 1: A list of commonly used f-divergences (along with their generating function) and their corresponsing t-SNE objective (which we refer to as ft-SNE). The last column describes what kind of distance relationship gets emphasized by diﬀerent choices of f-divergence. Df(PkQ) f(t) ft-SNE objective Emphasis Kullback-Leibler (KL) t log t P pij ⇣ log pij qij ⌘ Local Chi-square (X 2 or CH) (t −1)2 P (pij−qij)2 qij Local Reverse-KL (RKL) −log t P qij ⇣ log qij pij ⌘ Global Jensen-Shannon (JS) (t + 1) log 2 (t+1) + t log t 1 2(KL(pijk pij+qij 2 ) + KL(qij| pij+qij 2 )) Both Hellinger distance (HL) ( p t −1)2 P(ppij −pqij)2 Both precision and recall, and we show that this can be achieved by minimizing f-divergences other than the KL-divergence. We prescribe that data scientists create and explore low-dimensional visualizations of their data corre- sponding to several diﬀerent f-divergences, each of which is geared toward diﬀerent types of structure. To this end, we provide eﬃcient code for ﬁnding t-SNE embeddings based on ﬁve diﬀerent f-divergences1. Users can even provide their own speciﬁc instantiation of an f-divergence, if needed. Our code can optimize either the Im DJ, Verma N, Branson K. Stochastic Neighbor Embedding under f-divergences. arXiv preprint arXiv:1811.01247. 2018 Nov 3. t-SNE embeddings based on ﬁve diﬀerent f-divergences f-Divergences In probability theory f-divergence is a function Df(P||Q) for measuring the diﬀerence between 2 probability distributions P Q and Csiszár, I. (1963). ""Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis der Ergodizitat von Markoffschen Ketten"". Magyar. Tud. Akad. Mat. Kutato Int. Kozl. 8: 85–108. Morimoto, T. (1963). ""Markov processes and the H-theorem"". J. Phys. Soc. Jpn. 18 (3): 328–331 Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 34 Uniform Manifold Approximation and Projection (UMAP) McInnes, L., & Healy, J. (2018). Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426. UMAP T-SNE Compared to t-SNE, UMAP seems to be • faster • deterministic • better at preserving clusters Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 35 Reading Assignments • Python Machine Learning, 2nd Edition. Chapter 5: Compressing Data via Dimensionality Reduction • Scikit-learn doc 2.2. Manifold learning: https://scikit-learn.org/stable/modules/manifold.html Sebastian Raschka STAT 479: Machine Learning FS 2018 ! 36 Code Examples https://github.com/rasbt/stat479-machine-learning-fs18/blob/master/ 14_feat-extract/14_feat-extract_code.ipynb "
388,"Introduction to NLP Ruihong Huang Texas A&M University Some slides adapted from slides by Dan Jurafsky, Luke Zettlemoyer, Ellen Riloff • ""An Aggie does not lie, cheat, or steal or tolerate those who do."" For additional information, please visit: http://aggiehonor.tamu.edu. • Upon accepting admission to Texas A&M University, a student immediately assumes a commitment to uphold the Honor Code, to accept responsibility for learning, and to follow the philosophy and rules of the Honor System. Students will be required to state their commitment on examinations, research papers, and other academic work. Ignorance of the rules does not exclude any member of the TAMU community from the requirements or the processes of the Honor System. • The Americans with Disabilities Act (ADA) is a federal anti-discrimination statute that provides comprehensive civil rights protection for persons with disabilities. Among other things, this legislation requires that all students with disabilities be guaranteed a learning environment that provides for reasonable accommodation of their disabilities. If you believe you have a disability requiring an accommodation, please contact Disability Services, currently located in the Disability Services building at the Student Services at White Creek complex on west campus or call 979-845-1637. For additional information, visit http://disability.tamu.edu. • Piazza: CSCE 489-508, NLP • https://piazza.com/class#fall2017/csce489508 • course page: • http://faculty.cse.tamu.edu/huangrh/Fall17/Fall1 7_nlp_foundation_technique.html • Class participation: 10% • Four Programming Assignments: 40% • The Final Project: 25% (abstract: 5%, presentation+report+code+data: 20%) • Annotation assignment: 5% • Final exam: 20% • Late Policy: 20% reduction per day. Including programming assignments, annotation assignment, and the final project. Programming Assignments • Code: has to be runnable • Report: how to run, results and analysis, remaining issues, known bugs. The Final Project • Due by mid semester (10/12, before the class starts): 1-page abstract • By the end of the semester: submit code data and a report, and a class presentation. • Report: 8 pages maximum, describe the problem, approaches and evaluation results. The final Project • Solving a mini core research problem you have identified by reading recent research papers from top NLP conferences. • Developing a nice NLP application system. Basic Recipe of Forming a Project • Choose a Topic and do a quick survey • Prepare data • Think about evaluation methods • Start to work on it Core research problems • Semantics, word sense disambiguation • Coreference resolution, discourse, pragmatics • Consider to participate in a SemEval task (http://alt.qcri.org/semeval2018/index.php?id=ta sks) Applications • Question-Answering • Text Summarization • Dialogue systems • Sentiment Analysis • Machine Translation • Interdisciplinary applications…… What is NLP? What is NLP? § Fundamental goal: deep understand of broad language § Not just string processing or keyword matching § End systems that we want to build: § Simple: spelling correction, text categorization… § Complex: speech recognition, machine translation, information extraction, sentiment analysis, question answering… § Unknown: human-level comprehension (is this just NLP?) Question Answering: Jeopardy! US Cities: Its largest airport is named for a World War II hero; its second largest, for a World War II battle. Jeopardy! World Champion Information Extraction Subject: curriculum meeting Date: January 15, 2012 To: Dan Jurafsky Hi Dan, we’ve now scheduled the curriculum meeting. It will be in Gates 159 tomorrow from 10:00- 11:30. -Chris 15 Create new Calendar entry Event: Curriculum mtg Date: Jan-16-2012 Start: 10:00am End: 11:30am Where: Gates 159 Google Knowledge Graph Knowledge Graph: “things not strings” Text Summarization Summarization § Condensing documents § Single or multiple docs § Extractive or synthetic § Aggregative or representative § Very context- dependent! § An example of analysis with generation Human-machine Dialogs Human-Machine Interactions Machine Translation • Fully automatic 19 • Helping human translators Enter Source Text: Translation from Stanford’s Phrasal: 这不过是一个时间的问题. This is only a matter of time. Inter-Disciplinary Computer Science: artificial intelligence, machine learning Linguistics: computational linguistics Psychology: cognitive psychology, psycholinguistics Statistics: probabilistic methods, information theory Interactions with Linguists (History) • 70s and 80s: more linguistic focus -deeper models, toy domains, rule-based systems • 90s: empirical revolution -robust corpus-based methods, empirical evaluation • 2000s: richer linguistic representations used in statistical approaches Outline of Words: Text classification of Words: language modeling, parts of speech tagging of Words: syntactic parsing, dependency parsing : thesaurus, distributional, distributed , coreference, pragmatics Language Technology Coreference resolution Question answering (QA) Part-of-speech (POS) tagging Word sense disambiguation (WSD) Paraphrase Named entity recognition (NER) Parsing Summarization Information extraction (IE) Machine translation (MT) Dialog Sentiment analysis mostly solved making good progress still really hard Spam detection Let’s go to Agra! Buy V1AGRA … ✓ ✗ Colorless green ideas sleep furiously. ADJ ADJ NOUN VERB ADV Einstein met with UN officials in Princeton PERSON ORG LOC You’re invited to our dinner party, Friday May 27 at 8:30 Party May 27 add Best roast chicken in San Francisco! The waiter ignored us for 20 minutes. Carter told Mubarak he shouldn’t run again. I need new batteries for my mouse. The 13th Shanghai International Film Festival… 第13届上海国际电影节开幕… The Dow Jones is up Housing prices rose Economy is good Q. How effective is ibuprofen in reducing fever in patients with acute febrile illness? I can see Alcatraz from the window! XYZ acquired ABC yesterday ABC has been taken over by XYZ Where is Citizen Kane playing in SF? Castro Theatre at 7:30. Do you want a ticket? The S&P500 jumped •Ambiguity !! Ambiguities inherent in Language • Language is succinct and expressive. • Human resolve ambiguities naturally. Syntax: structural ambiguity Time flies like an arrow. Metaphor: Time/NOUN flies/VERB like/PREP an/ART arrow/NOUN New Fly Species: Time/NOUN flies/NOUN like/VERB an/ART arrow/NOUN Stopwatch Imperative: Time/VERB flies/NOUN like/PREP an/ART arrow/NOUN Syntax: structural ambiguity (attachment) • I saw the Grand Canyon flying to New York. • I watered the plant with yellow leaves. • I saw the man on the hill with the telescope. But syntax doesn’t tell us much about meaning… • Colorless green ideas sleep furiously. [Chomsky] • plastic cat food can cover Semantics: Lexical Ambiguity • I walked to the bank ... of the river. to get money. • The bug in the room ... was planted by spies. flew out the window. • I work for John Hancock ... and he is a good boss. which is a good company. •Discourse, Pragmatics Discourse: coreference President John F. Kennedy was assassinated. The president was shot yesterday. Relatives said that John was a good father. JFK was the youngest president in history. His family will bury him tomorrow. Friends of the Massachusetts native will hold a candlelight service in Mr. Kennedy’s home town. A Short Story Pragmatics Rules of Conversation • Can you tell me what time it is? • Could I please have the salt? Speech Acts • I bet you $50 that the Jazz will win tonight. • Will you marry me? NLP: a branch of AI •Lack of world knowledge •inferences World Knowledge, Inferences John went to the diner. He ordered a steak. He left a tip and went home. John wanted to commit suicide. He got a rope. •Sparsity!!! Zipf’s Law • the frequency of any word is inversely proportional to its rank：f = K / r • fat-tail, most words occur only a couple of times • high lexical diversity -> data sparseness Goals of the class • Key tasks, algorithms • Essentially skills to build your system • (Hopefully) see problems, holes, gaps, start research "
389,"Basic Text Processing Regular Expressions Word Tokenization Word Normalization Sentence Segmentation Many slides adapted from slides by Dan Jurafsky Basic Text Processing Regular Expressions Regular expressions • A formal language for specifying text strings • How can we search for any of these? • woodchuck • woodchucks • Woodchuck • Woodchucks Regular Expressions: Disjunctions • Letters inside square brackets [] • Ranges [A-Z] Pattern Matches [wW]oodchuck Woodchuck, woodchuck [1234567890] Any digit Pattern Matches the First Match in an example [A-Z] An upper case letter Drenched Blossoms [a-z] A lower case letter my beans were impatient [0-9] A single digit Chapter 1: Down the Rabbit Hole Regular Expressions: Negation in Disjunction • Negations [^Ss] • Carat means negation only when ﬁrst in [] Pattern Matches [^A-Z] Not an upper case letter Oyfn pripetchik [^Ss] Neither ‘S’ nor ‘s’ I have no exquisite reason” [^e^] Neither e nor ^ Look here a^b The pattern a carat b Look up a^b now Regular Expressions: More Disjunction • Woodchucks is another name for groundhog! • The pipe | for disjunction Pattern Matches groundhog|woodchuck yours|mine yours mine a|b|c|ab abc [gG]roundhog|[Ww]oodchuck Photo D. Fletcher Regular Expressions: ? * + . Stephen C Kleene Pattern Matches colou?r 0 or 1 of previous char color colour oo*h! 0 or more of previous char oh! ooh! oooh! ooooh! o+h! 1 or more of previous char oh! ooh! oooh! ooooh! baa+ baa baaa baaaa baaaaa beg.n any char begin begun begun beg3n Kleene *, Kleene + Regular Expressions: Anchors ^ $ Pattern Matches ^[A-Z] Palo Alto ^[^A-Za-z] 1 “Hello” \.$ The end. .$ The end? The end! Example • Find me all instances of the word “the” in a text. the Misses capitalized examples [tT]he Incorrectly returns other or theology [^a-zA-Z][tT]he[^a-zA-Z] Errors • The process we just went through was based on ﬁxing two kinds of errors • Matching strings that we should not have matched (there, then, other) • False positives (Type I) • Not matching things that we should have matched (The) • False negatives (Type II) Errors cont. • In NLP we are always dealing with these kinds of errors. • Reducing the error rate for an application often involves two antagonistic efforts: • Increasing accuracy or precision (minimizing false positives) • Increasing coverage or recall (minimizing false negatives). Summary • Regular expressions play a surprisingly large role • Sophisticated sequences of regular expressions are often the ﬁrst model for any text processing task • For many hard tasks, we use machine learning classiﬁers • But regular expressions are used as features in the classiﬁers • Can be very useful in capturing generalizations 12 Basic Text Processing Regular Expressions Basic Text Processing Word tokenization Text Normalization • Every NLP task needs to do text normalization: 1.Segmenting/tokenizing words in running text 2.Normalizing word formats 3.Segmenting sentences in running text How many words? • I do uh main- mainly business data processing • Fragments, ﬁlled pauses • Seuss’s cat in the hat is different from other cats! • Lemma: same stem, part of speech, rough word sense • cat and cats = same lemma • Wordform: the full inﬂected surface form • cat and cats = different wordforms How many words? they lay back on the San Francisco grass and looked at the stars and their • Type: an element of the vocabulary. • Token: an instance of that type in running text. • How many? • 15 tokens (or 14) • 13 types (or 12) (or 11?) How many words? N = number of tokens V = vocabulary = set of types |V| is the size of the vocabulary Tokens = N Types = |V| Switchboard phone conversations 2.4 million 20 thousand Shakespeare 884,000 31 thousand Google N-grams 1 trillion 13 million Church and Gale (1990): |V| > O(N½) Simple Tokenization in UNIX • (Inspired by Ken Church’s UNIX for Poets.) • Given a text ﬁle, output the word tokens and their frequencies tr -sc ’A-Za-z’ ’\n’ < shakes.txt | sort | uniq –c 1945 A 72 AARON 19 ABBESS 5 ABBOT ... ... 25 Aaron 6 Abate 1 Abates 5 Abbess 6 Abbey 3 Abbot .... … Change all non-alpha to newlines Sort in alphabetical order Merge and count each type Will likes to eat. Will likes to babble. 1 babble 1 eat 2 likes 2 to 2 Will tr: translate, -s: squeeze, -c: complement *Assignment for you* The ﬁrst step: tokenizing tr -sc ’A-Za-z’ ’\n’ < shakes.txt | head (head: will print the first lines (10 by default) of its input. head -n NUM input) THE SONNETS by William Shakespeare From fairest creatures *Assignment for you* The second step: sorting tr -sc ’A-Za-z’ ’\n’ < shakes.txt | sort | head A A A A A A A A A ... *Assignment for you* More counting • Merging upper and lower case tr ‘A-Z’ ‘a-z’ < shakes.txt | tr –sc ‘A-Za-z’ ‘\n’ | sort | uniq –c • Sorting the counts (-n: numerical value, -k: column, -r: reverse) tr ‘A-Z’ ‘a-z’ < shakes.txt | tr –sc ‘A-Za-z’ ‘\n’ | sort | uniq –c | sort –n –r 23243 the 22225 i 18618 and 16339 to 15687 of 12780 a 12163 you 10839 my 10005 in 8954 d What happened here? *Assignment for you* Issues in Tokenization • Finland’s capital → Finland Finlands Finland’s ? • what’re, I’m, isn’t → What are, I am, is not • Hewlett-Packard → Hewlett Packard ? • state-of-the-art → state of the art ? • Lowercase → lower-case lowercase lower case ? • San Francisco → one token or two? • m.p.h., PhD. → ?? Tokenization: language issues • French • L'ensemble → one token or two? • L ? L’ ? Le ? • Want l’ensemble to match with un ensemble • German noun compounds are not segmented • Lebensversicherungsgesellschaftsangestellter • ‘life insurance company employee’ • German information retrieval needs compound splitter Tokenization: language issues • Chinese and Japanese no spaces between words: • 莎拉波娃现在居住在美国东南部的佛罗里达。 • 莎拉波娃 现在 居住 在 美国 东南部 的 佛罗里达 • Sharapova now lives in US southeastern Florida Basic Text Processing Word tokenization Basic Text Processing Word Normalization and Stemming Normalization • Need to “normalize” terms • Information Retrieval: indexed text & query terms must have same form. • We want to match U.S.A. and USA • We implicitly deﬁne equivalence classes of terms • e.g., deleting periods in a term • Alternative: asymmetric expansion: • Enter: window Search: window, windows • Enter: windows Search: Windows, windows, window • Enter: Windows Search: Windows Case folding • Applications like IR: reduce all letters to lower case • Since users tend to use lower case • Possible exception: upper case in mid-sentence? • e.g., General Motors • Fed vs. fed • SAIL vs. sail • For sentiment analysis, MT, Information extraction • Case is helpful (US versus us is important) Lemmatization • Reduce inﬂections or variant forms to base form • am, are, is → be • car, cars, car's, cars' → car • the boy's cars are different colors → the boy car be different color • Lemmatization: have to ﬁnd correct dictionary headword form Context dependent. for instance: in our last meeting (noun, meeting). We’re meeting (verb, meet) tomorrow. Morphology • Morphemes: • The small meaningful units that make up words • Stems: The core meaning-bearing units • Afﬁxes: Bits and pieces that adhere to stems • Often with grammatical functions Stemming • Reduce terms to their stems in information retrieval • Stemming is crude chopping of afﬁxes • language dependent • e.g., automate(s), automatic, automation all reduced to automat. for example compressed and compression are both accepted as equivalent to compress. for exampl compress and compress ar both accept as equival to compress context independent Porter’s algorithm The most common English stemmer Step 1a sses → ss caresses → caress ies → i ponies → poni ss → ss caress → caress s → ø cats → cat Step 1b (*v*)ing → ø walking → walk sing → sing (*v*)ed → ø plastered → plaster … Step 2 (for long stems) ational→ ate relational→ relate izer→ ize digitizer → digitize ator→ ate operator → operate … Step 3 (for longer stems) al → ø revival → reviv able → ø adjustable → adjust ate → ø activate → activ … https://tartarus.org/martin/PorterStemmer/ ﬁxed rules put in groups, applied in order. Viewing morphology in a corpus Why only strip –ing if there is a vowel? (*v*)ing → ø walking → walk sing → sing 34 Viewing morphology in a corpus Why only strip –ing if there is a vowel? (*v*)ing → ø walking → walk sing → sing 35 tr -sc 'A-Za-z' '\n' < shakes.txt | grep ’ing$' | sort | uniq -c | sort –nr tr -sc 'A-Za-z' '\n' < shakes.txt | grep '[aeiou].*ing$' | sort | uniq -c | sort –nr 548 being 541 nothing 152 something 145 coming 130 morning 122 having 120 living 117 loving 116 Being 102 going 1312 King 548 being 541 nothing 388 king 375 bring 358 thing 307 ring 152 something 145 coming 130 morning Basic Text Processing Word Normalization and Stemming Basic Text Processing Sentence Segmentation and Decision Trees Sentence Segmentation • !, ? are relatively unambiguous • Period “.” is quite ambiguous • Sentence boundary • Abbreviations like Inc. or Dr. • Numbers like .02% or 4.3 • Build a binary classiﬁer • Looks at a “.” • Decides EndOfSentence/NotEndOfSentence • Classiﬁers: hand-written rules, regular expressions, or machine- learning Determining if a word is end-of- sentence: a Decision Tree More sophisticated decision tree features • Case of word with “.”: Upper, Lower, Cap, Number • Case of word after “.”: Upper, Lower, Cap, Number • Numeric features • Length of word with “.” • Probability(word with “.” occurs at end-of-s) • Probability(word after “.” occurs at beginning-of-s) Implementing Decision Trees • A decision tree is just an if-then-else statement • The interesting research is choosing the features • Setting up the structure is often too hard to do by hand • Hand-building only possible for very simple features, domains • For numeric features, it’s too hard to pick each threshold • Instead, structure usually learned by machine learning from a training corpus Decision Trees and other classiﬁers • We can think of the questions in a decision tree • As features that could be exploited by any kind of classiﬁer • Logistic regression • SVM • Neural Nets • etc. Sentence Splitters • Stanford coreNLP: (deterministic) • http://stanfordnlp.github.io/CoreNLP/ • UIUC sentence splitter: (deterministic) • https://cogcomp.cs.illinois.edu/page/tools_view/2 43 Basic Text Processing Sentence Segmentation and Decision Trees "
39,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal Language+Vision; Guest Research Talks by Ramakanth, Hao Language+Vision Major Language+Vision Tasks ! Image Captioning ! Referring Expressions ! Image/Visual Question Answering ! Visual Dialog ! Video Captioning Brief Task Definitions and Example Papers/Models Image Captioning CNN LSTM LSTM LSTM LSTM a dog is jumping LSTM <EOS> <BOS> Show, Attend, and Tell esentation corresponding to a part of a1, . . . , aL} , ai 2 RD correspondence between the feature of the 2-D image, we extract features lutional layer unlike previous work fully connected layer. This allows the focus on certain parts of an image by all the feature vectors. ONG SHORT-TERM MEMORY short-term memory (LSTM) net- Schmidhuber, 1997) that produces a g one word at every time step condi- vector, the previous hidden state and ated words. Our implementation of g. 2, closely follows the one used in : Uiht−1 + Ziˆ zt + bi), Ufht−1 + Zfˆ zt + bf), h(WcEyt−1 + Ucht−1 + Zcˆ zt + bc), Uoht−1 + Zoˆ zt + bo), tic sigmoid activation. In simple terms, the context vector ˆ zt is a dynamic rep- resentation of the relevant part of the image input at time t. We deﬁne a mechanism φ that computes ˆ zt from the annotation vectors ai, i = 1, . . . , L corresponding to the features extracted at different image locations. For each location i, the mechanism generates a positive weight ↵i which can be interpreted either as the probability that loca- tion i is the right place to focus for producing the next word (stochastic attention mechanism), or as the relative impor- tance to give to location i in blending the ai’s together (de- terministic attention mechanism). The weight ↵i of each annotation vector ai is computed by an attention model fatt for which we use a multilayer perceptron conditioned on the previous hidden state ht−1. To emphasize, we note that the hidden state varies as the output RNN advances in its output sequence: “where” the network looks next depends on the sequence of words that has already been generated. eti =fatt(ai, ht−1) ↵ti = exp(eti) PL k=1 exp(etk) . Once the weights (which sum to one) are computed, the context vector ˆ zt is computed by A9en;on: [Xu et al., 2015] Show, Attend, and Tell Neural Image Caption Generation with Visual Attention Figure 4. Examples of attending to the correct object (white indicates the attended regions, underlines indicated the corresponding word) research. We report BLEU4 from 1 to 4 without a brevity penalty. There has been, however, criticism of BLEU, so we report another common metric METEOR (Denkowski & Lavie, 2014) and compare whenever possible. work (Karpathy & Li, 2014). We note, however, that the differences in splits do not make a substantial difference in overall performance. 5.3. Quantitative Analysis [Xu et al., 2015] Visual Referring Expressions Figure 1: Joint generation examples using our full model with “+rerank” on three datasets. Each sentence shows the g expression for one of the depicted objects (color coded to indicate correspondence). Joint Comprehension+Generation Model Man in the middle wearing yellow MLP MLP Concat LSTM Embedding Loss Generation loss Reward Loss LSTM Speaker Listener Sampling Reinforcer L2-Normalization L2-Normalization Figure 3: Framework: The Speaker is a CNN-LSTM model, which generates a referring expression for the target object. The Listener is a joint-embedding model learned to minimize the distance between paired object and expression representations. In addition, a Reinforcer module helps improve the speaker by sampling more discriminative (less ambiguous) expressions for training. The model is jointly trained with 3 loss functions – generation loss, embedding loss, and reward loss, thereby [Yu et al., 2017] Joint Comprehension+Generation Model [Yu et al., 2017] Figure 1: Joint generation examples using our full model with “+rerank” on three datasets. Each sentence shows the generated expression for one of the depicted objects (color coded to indicate correspondence). Figure 2: Example comprehension results using our full model on three datasets. Green box shows the ground-truth region and blue box shows our correct comprehension based on the detected regions. VQA: Visual Question Answering q g , p g g g , the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas ge, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a iled understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA le to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can ed in a multiple-choice format. We provide a dataset containing ⇠0.25M images, ⇠0.76M questions, and ⇠10M answers alqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared n performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa). F UCTION ssing a renewed excitement in multi-discipline ligence (AI) research problems. In particular, mage and video captioning that combines Com- CV), Natural Language Processing (NLP), and epresentation & Reasoning (KR) has dramati- d in the past year [16], [9], [12], [38], [26], rt of this excitement stems from a belief that e tasks like image captioning are a step towards owever, the current state of the art demonstrates scene-level understanding of an image paired gram statistics sufﬁces to generate reasonable s, which suggests image captioning may not be te” as desired. or a compelling “AI-complete” task? We believe o spawn the next generation of AI algorithms, an uld (i) require multi-modal knowledge beyond a main (such as CV) and (ii) have a well-deﬁned valuation metric to track progress. For some image captioning, automatic evaluation is still open research problem [51] [13] [22] Does it appear to be rainy? Does this person have 20/20 vision? Is this person expecting company? What is just under the tree? How many slices of pizza are there? Is this a vegetarian pizza? What color are her eyes? What is the mustache made of? Fig. 1: Examples of free-form, open-ended questions collected for images via Amazon Mechanical Turk. Note that commonsense knowledge is needed along with a visual understanding of the scene to answer many questions. [Agrawal et al., 2015] Demo • http://vqa.cloudcv.org/ [Agrawal et al., 2015] Simple VQA Baseline [Agrawal et al., 2015] 9 Convolution Layer + Non-Linearity Pooling Layer Convolution Layer + Non-Linearity Pooling Layer Fully-Connected MLP 4096 output units from last hidden layer (VGGNet, Normalized) ͞,ŽǁŵĂŶǇ horses are in this ŝŵĂŐĞ͍͟ 2×2×512 LSTM Fully-Connected 1024 1024 Point-wise multiplicationFully-Connected 1000 Softmax ͞Ϯ͟ 1024 Fully-Connected 1000 Fig. 8: Our best performing model (deeper LSTM Q + norm I). This model uses a two layer LSTM to encode the questions and the last hidden layer of VGGNet [46] to encode the images. The image features are then `2 normalized. Both the question and image features are transformed to a common space and fused via element-wise multiplication, which is then passed through a fully connected layer followed by a softmax layer to obtain a distribution over answers. Open-Ended Multiple-Choice To gain further insights into these results, we computed Hierarchical Co-Attention Model [Lu et al., 2016] Ques%on:)What)color)on)the)))))))))) )))))))))))))))))))stop)light)is)lit)up))?)) …) …) color)stop)light)lit) co6a7en%on) color) …) stop)) light)) …)) What) color) …) the) stop) light)) light)) …)) What) color) What)color)on)the)stop)light)is)lit)up) …) …) the)stop)light) …) …) stop)) Image) Answer:)green) Figure 1: Flowchart of our proposed hierarchical co-attention model. Given a question, we extract its word level, phrase level and question level embeddings. At each level, we apply co-attention on both the image and question. The ﬁnal answer prediction is based on all the co-attended image and question features. Speciﬁcally, we convolve word representations with temporal ﬁlters of varying support, and then combine the various n gram responses by pooling them into a single phrase level representation At Hierarchical Co-Attention Model [Lu et al., 2016] (b)$$ Image A A A Ques+on 0"" Q V (a)$ Image Ques+on x x Q V C x x WvV WqQ aq av 1$ 2$ 3$ ˆ q ˆ q ˆ s ˆ v ˆ v Figure 2: (a) Parallel co-attention mechanism; (b) Alternating co-attention mechanism. 3.3 Co-Attention We propose two co-attention mechanisms that differ in the order in which image and question i d Th ﬁ h i hi h ll ll l i Hierarchical Co-Attention Model [Lu et al., 2016] Q: what is the man holding a snowboard on top of a snow covered? A: mountain what is the man holding a snowboard on top of a snow covered what is the man holding a snowboard on top of a snow covered ? what is the man holding a snowboard on top of a snow covered ? Q: what is the color of the bird? A: white what is the color of the bird ? what is the color of the bird ? what is the color of the bird ? Q: how many snowboarders in formation in the snow, four is sitting? A: 5 how many snowboarders in formation in the snow , four is sitting ? how many snowboarders in formation in the snow , four is sitting ? how many snowboarders in formation in the snow , four is sitting ? Figure 4: Visualization of image and question co-attention maps on the COCO-QA dataset. From left to right: original image and question pairs, word level co-attention maps, phrase level co-attention maps and question level co-attention maps. For visualization, both image and question attentions are scaled (from red:high to blue:low). Best viewed in color. Multimodal Compact Bilinear (MCB) Model titute for Informatics, Saarbr¨ ucken, Germany ion with arge lan- cessfully asks such combin- ach other. include ll as con- represen- methods CNN WE, LSTM What are all the people doing? Ȳ Ȳ FFT FFT-1 Convolution ۨ FFT “flying kites” Classifier Multimodal Compact Bilinear Count Sketch Signed Sqrt L2 Norm Figure 1: Multimodal Compact Bilinear Pooling for visual question answering. [Fukui et al., 2016] MCB Model with Attention 1 x 14 x 14 512 x 14 x 14 CNN (ResNet152) 16k x14x14 2048x14x14 2048x14x14 Conv, Relu Conv “Carrot” 16k 3000 2048 2048 WE, LSTM Softmax Weighted Sum Tile 2048 What is the woman feeding the giraffe? Multimodal Compact Bilinear Multimodal Compact Bilinear FC Softmax Figure 3: Our architecture for VQA: Multimodal Compact Bilinear (MCB) with Attention. Conv implies convolutional layers and FC implies fully connected layers. For details see Sec. 3.2. where ⇤is the convolution operator. Additionally, the l ti th t t th t l ti i th The two vectors are then passed through MCB. Th MCB i f ll d b l t i i d [Fukui et al., 2016] Results Test-dev Test-standard Open Ended MC Open Ended MC Y/N No. Other All All Y/N No. Other All All MCB 81.2 35.1 49.3 60.8 65.4 - - - - - MCB + Genome 81.7 36.6 51.5 62.3 66.4 - - - - - MCB + Att. 82.2 37.7 54.8 64.2 68.6 - - - - - MCB + Att. + GloVe 82.5 37.6 55.6 64.7 69.1 - - - - - MCB + Att. + Genome 81.7 38.2 57.0 65.1 69.5 - - - - - MCB + Att. + GloVe + Genome 82.3 37.2 57.4 65.4 69.9 - - - - - Ensemble of 7 Att. models 83.4 39.8 58.5 66.7 70.2 83.2 39.5 58.0 66.5 70.1 Naver Labs (challenge 2nd) 83.5 39.8 54.8 64.9 69.4 83.3 38.7 54.6 64.8 69.3 HieCoAtt (Lu et al., 2016) 79.7 38.7 51.7 61.8 65.8 - - - 62.1 66.1 DMN+ (Xiong et al., 2016) 80.5 36.8 48.3 60.3 - - - - 60.4 - FDA (Ilievski et al., 2016) 81.1 36.2 45.8 59.2 - - - - 59.5 - D-NMN (Andreas et al., 2016a) 81.1 38.6 45.5 59.4 - - - - 59.4 - AMA (Wu et al., 2016) 81.0 38.4 45.2 59.2 - 81.1 37.1 45.8 59.4 - SAN (Yang et al., 2015) 79.3 36.6 46.1 58.7 - - - - 58.9 - NMN (Andreas et al., 2016b) 81.2 38.0 44.0 58.6 - 81.2 37.7 44.0 58.7 - AYN (Malinowski et al., 2016) 78.4 36.4 46.3 58.4 - 78.2 36.3 46.3 58.4 - SMem (Xu and Saenko, 2016) 80.9 37.3 43.1 58.0 - 80.9 37.5 43.5 58.2 - VQA team (Antol et al., 2015) 80.5 36.8 43.1 57.8 62.7 80.6 36.5 43.7 58.2 63.1 DPPnet (Noh et al., 2015) 80.7 37.2 41.7 57.2 - 80.3 36.9 42.2 57.4 - iBOWIMG (Zhou et al., 2015) 76.5 35.0 42.6 55.7 - 76.8 35.0 42.6 55.9 62.0 Table 4: Open-ended and multiple-choice (MC) results on VQA test set (trained on train+val set) compared [Fukui et al., 2016] Making the V in the VQA matter ! Khot Douglas Summers Stay Dhruv Batra Devi Parikh Army Research Laboratory 3Georgia Institute of Technology 2douglas.a.summers-stay.civ@mail.mil 3{dbatra, parikh}@gatech.edu vision and language challenging research ications they enable. orld and bias in our for learning than vi- at ignore visual infor- their capability. ge priors for the task nd make vision (the V nce the popular VQA ary images such that et is associated with air of similar images o the question Our Who is wearing glasses? Where is the child sitting? Is the umbrella upside down? How many children are in the bed? woman man arms fridge no yes 1 2 Figure 1: Examples from our balanced VQA dataset. 1. Introduction [Goyal et al., 2017] Results formation from swer free-form curately. As ex- ced dataset, the ly worse on the d dataset, again l VQA dataset, mination) in our age bias, visual ed dataset since ch other in im- ith different an- l, VQA models these images. llows us to an- n unique ways. can count the mentary images Table 2: Performance of VQA models when trained on VQA v2.0 train+val and tested on VQA v2.0 test-standard dataset. Approach Ans Type UU UB BhalfB BB MCB [9] Yes/No 81.20 70.40 74.89 77.37 Number 34.80 31.61 34.69 36.66 Other 51.19 47.90 47.43 51.23 All 60.36 54.22 56.08 59.14 HieCoAtt [25] Yes/No 79.99 67.62 70.93 71.80 Number 34.83 32.12 34.07 36.53 Other 45.55 41.96 42.11 46.25 All 57.09 50.31 51.88 54.57 Table 3: Accuracy breakdown over answer types achieved by MCB [9] and HieCoAtt [25] models when trained/tested on unbalanced/balanced VQA datasets UB stands for [Goyal et al., 2017] Visual Dialog Devi Parikh , Dhruv Batra e of Technology, 2Carnegie Mellon University, 3UC Berkeley, 4Virginia Tech ikh, dbatra}@gatech.edu 2{skottur, khushig, moura}@andrew.cmu.edu 3avisingh@cs.berkeley.edu 4deshraj@vt.edu visualdialog.org bstract isual Dialog, which requires an gful dialog with humans in natu- ge about visual content. Speciﬁ- log history, and a question about o ground the question in image, and answer the question accu- entangled enough from a speciﬁc serve as a general test of ma- eing grounded in vision enough ion of individual responses and develop a novel two-person chat curate a large-scale Visual Di- Dial v0.9 has been released and question-answer pairs on ⇠120k total of ⇠1.2M dialog question- ural encoder decoder models for Figure 1: We introduce a new AI task – Visual Dialog, where an AI agent must hold a dialog with a human about visual content. We [Das et al., 2017] Demo • http://visualchatbot.cloudcv.org/ [Das et al., 2017] Visual Dialog vs VQA Figure 2: Differences between image captioning, Visual Question Answering (VQA) and Visual Dialog. Two (partial) dialogs are shown from our VisDial dataset, which is curated from a live chat b A M h i l T k k (S 3) tion to a relev co-reference r fer to?), ‘Is th machine to ha age were we be consistent in wheelchair male and one ing speciﬁed s the problem a Why do we ta (non-visual) d following two [Das et al., 2017] Results ner product of question vector with each history o get scores over previous rounds, which are fed to ax to get attention-over-history probabilities. Con- mbination of history vectors using these attention lities gives us the ‘context vector’, which is passed an fc-layer and added to the question vectorto con- e MN encoding. In the language of Memory Net- ], this is a ‘1-hop’ encoding. ‘[encoder]-[input]-[decoder]’ convention to refer input combinations. For example, ‘LF-QI-D’ has sion encoder with question+image inputs (no his- d a discriminative decoder. Implementation details models can be found in the supplement. riments sDial v0.9 contains 83k dialogs on COCO-train on COCO-val images. We split the 83k into 80k ng, 3k for validation, and use the 40k as test. processing, hyperparameters and training details ded in the supplement. s We compare to a number of baselines: Answer nswer options to a test question are encoded with M and scored by a linear classiﬁer. This captures by frequency of answers in our training set with- ving to exact string matching. NN-Q: Given a test Model MRR R@1 R@5 R@10 Mean Baseline 8 > < > : Answer prior 0.3735 23.55 48.52 53.23 26.50 NN-Q 0.4570 35.93 54.07 60.26 18.93 NN-QI 0.4274 33.13 50.83 58.69 19.62 Generative 8 > > > > > > > > > > > < > > > > > > > > > > > : LF-Q-G 0.5048 39.78 60.58 66.33 17.89 LF-QH-G 0.5055 39.73 60.86 66.68 17.78 LF-QI-G 0.5204 42.04 61.65 67.66 16.84 LF-QIH-G 0.5199 41.83 61.78 67.59 17.07 HRE-QH-G 0.5102 40.15 61.59 67.36 17.47 HRE-QIH-G 0.5237 42.29 62.18 67.92 17.07 HREA-QIH-G 0.5242 42.28 62.33 68.17 16.79 MN-QH-G 0.5115 40.42 61.57 67.44 17.74 MN-QIH-G 0.5259 42.29 62.85 68.88 17.06 Discriminative 8 > > > > > > > > > > > < > > > > > > > > > > > : LF-Q-D 0.5508 41.24 70.45 79.83 7.08 LF-QH-D 0.5578 41.75 71.45 80.94 6.74 LF-QI-D 0.5759 43.33 74.27 83.68 5.87 LF-QIH-D 0.5807 43.82 74.68 84.07 5.78 HRE-QH-D 0.5695 42.70 73.25 82.97 6.11 HRE-QIH-D 0.5846 44.67 74.50 84.22 5.72 HREA-QIH-D 0.5868 44.82 74.81 84.36 5.66 MN-QH-D 0.5849 44.03 75.26 84.49 5.68 MN-QIH-D 0.5965 45.55 76.22 85.37 5.46 VQA % SAN1-QI-D 0.5764 43.44 74.26 83.72 5.88 HieCoAtt-QI-D 0.5788 43.51 74.49 83.96 5.84 Table 1: Performance of methods on VisDial v0.9, measured by mean reciprocal rank (MRR), recall@k and mean rank. Higher is [Das et al., 2017] Video Captioning Early Video Captioning sriptions of still attention, with r and Gaizauskas et al. (2010), and ers. Propelled by groups released past year (Don- ; Karpathy et al., l., 2014; Vinyals 4). ent nets (RNNs), strong results for Sh t T ZI<[d]]YQ[O ,QSXW9LGHR&RQYROXWLRQDO1HW5HFXUUHQW1HW2XWSXW /0! /0! /0! /0! /0! /0!  D]s Qh dY<sQ[O O]YN Ý$/Þ /0! /0! /0! /0! /0! /0! """" """" """" """" """"   Figure 2: The structure of our video description network. We extract fc7 features for each frame, mean pool the features across the entire video and input this at every [Venugopalan et al., 2014] S2VT /670 /670 /670 /670 /670 /670 /670 /670 /670 /670 /670 WLPH SDG! SDG! SDG! %26! PDQ /670 /670 /670 /670 /670 /670 /670 LV WDONLQJ (26! SDG! SDG! SDG! SDG! SDG! SDG! $ (QFRGLQJVWDJH 'HFRGLQJVWDJH Figure 2. We propose a stack of two LSTMs that learn a representation of a sequence of frames in order to decode it into a sentence that describes the event in the video. The top LSTM layer (colored red) models visual feature inputs. The second LSTM layer (colored green) models language given the text input and the hidden representation of the video sequence. We use <BOS> to indicate begin-of-sentence and <EOS> for the end-of-sentence tag. Zeros are used as a <pad> when there is no input at the time step. loss is propagated back in time, the LSTM learns to gener- ate an appropriate hidden state representation (hn) of the region. It is then processed by the CNN. We remove the original last fully-connected classiﬁcation layer and learn a [Venugopalan et al., 2015] Hierarchical Encoder C C C C C C C C C (a) Stacked LSTM video encoder C C C C C C C C C (b) Hierarchical Recurrent Neural Encoder Figure 3: A comparison between stacked LSTM and the proposed Hierarchical Recurrent Neural Encoder. This ﬁgure takes a two layer hierarchy as an example to showcase. The red line in each subﬁgure shows one of the paths from the visual appearance input at t = 1 to the output video vector representation. There are 10 time steps in stacked LSTM and only 6 time steps in our model. our HRNE model. We next introduce the attention mecha- nism part. The core of the soft attention mechanism is that instead 3.3. Video Captioning Our HRNE can be applied to several video processing[Pan et al., 2016] M-to-M Multi-Task for Video Captioning UNSUPERVISED VIDEO PREDICTION VIDEO CAPTIONING ENTAILMENT GENERATION Video Encoder Language Encoder Video Decoder Language Decoder LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM [Pasunuru and Bansal, 2017a] Reinforced Video Captioning w/ Entailment Ent CIDEr LSTM LSTM LSTM LSTM LSTM ... ... ... ... CIDEnt Reward XENT RL [Pasunuru and Bansal, 2017b] Guest Research Talks by Ramakanth Pasunuru: 1) Multi-Task Video Captioning with Video and Entailment Generation (ACL 2017) 2) Reinforced Video Captioning with Entailment Rewards (EMNLP 2017) (45 mins) Guest Research Talks by Hao Tan: 1) A Joint Speaker-Listener-Reinforcer Model for Referring Expressions (CVPR 2017) 2) Source-Target Inference Models for Spatial Instruction Understanding (AAAI 2018) (45 mins) "
390,"Text Classification and Naïve Bayes The Task of Text Classification Many slides are adapted from slides by Dan Jurafsky Is this spam? Who wrote which Federalist papers? • 1787-8: anonymous essays try to convince New York to ratify U.S Constitution: Jay, Madison, Hamilton. • Authorship of 12 of the letters in dispute • 1963: solved by Mosteller and Wallace using Bayesian methods James Madison Alexander Hamilton Male or female author? 1. By 1925 present-day Vietnam was divided into three parts under French colonial rule. The southern region embracing Saigon and the Mekong delta was the colony of Cochin- China; the central area with its imperial capital at Hue was the protectorate of Annam… 2. Clara never failed to be astonished by the extraordinary felicity of her own name. She found it hard to trust herself to the mercy of fate, which had managed over the years to convert her greatest shame into one of her greatest assets… S. Argamon, M. Koppel, J. Fine, A. R. Shimoni, 2003. “Gender, Genre, and Writing Style in Formal Written Texts,” Text, volume 23, number 3, pp. 321–346 Positive or negative movie review? • unbelievably disappointing • Full of zany characters and richly applied satire, and some great plot twists • this is the greatest screwball comedy ever filmed • It was pathetic. The worst part about it was the boxing scenes. 5 What is the subject of this article? • Antogonists and Inhibitors • Blood Supply • Chemistry • Drug Therapy • Embryology • Epidemiology • … 6 MeSH Subject Category Hierarchy ? MEDLINE Article Text Classification • Assigning subject categories, topics, or genres • Spam detection • Authorship identification • Age/gender identification • Language Identification • Sentiment analysis • … Text Classification: definition • Input: – a document d – a fixed set of classes C = {c1, c2,…, cJ} • Output: a predicted class c Î C Classification Methods: Hand-coded rules • Rules based on combinations of words or other features – spam: black-list-address OR (“dollars” AND“have been selected”) • Accuracy can be high – If rules carefully refined by expert • But building and maintaining these rules is expensive Classification Methods: Supervised Machine Learning • Input: – a document d – a fixed set of classes C = {c1, c2,…, cJ} – A training set of m hand-labeled documents (d1,c1),....,(dm,cm) • Output: – a learned classifier γ:d à c 10 Classification Methods: Supervised Machine Learning • Any kind of classifier – Naïve Bayes – Logistic regression, maxent – Support-vector machines – k-Nearest Neighbors – … Text Classification and Naïve Bayes The Task of Text Classification Text Classification and Naïve Bayes Formalizing the Naïve Bayes Classifier Naïve Bayes Intuition • Simple (“naïve”) classification method based on Bayes rule • Relies on very simple representation of document – Bag of words Bayes’ Rule Applied to Documents and Classes •For a document d and a class c P(c | d) = P(d | c)P(c) P(d) Naïve Bayes Classifier (I) cMAP = argmax c∈C P(c | d) = argmax c∈C P(d | c)P(c) P(d) = argmax c∈C P(d | c)P(c) MAP is “maximum a posteriori” = most likely class Bayes Rule Dropping the denominator Naïve Bayes Classifier (II) cMAP = argmax c∈C P(d | c)P(c) Document d represented as features x1..xn = argmax c∈C P(x1, x2,…, xn | c)P(c) Naïve Bayes Classifier (IV) How often does this class occur? cMAP = argmax c∈C P(x1, x2,…, xn | c)P(c) O(|X|n•|C|) parameters We can just count the relative frequencies in a corpus Could only be estimated if a very, very large number of training examples was available. Multinomial Naïve Bayes Independence Assumptions • Bag of Words assumption: Assume position doesn’t matter • Conditional Independence: Assume the feature probabilities P(xi|cj) are independent given the class c. P(x1, x2,…, xn | c) P(x1,…, xn | c) = P(x1 | c)•P(x2 | c)•P(x3 | c)•...•P(xn | c) The bag of words representation I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun… It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet. γ ( )=c The bag of words representation γ ( )=c great 2 love 2 recommend 1 laugh 1 happy 1 ... ... Planning GUI Garbage Collection Machine Learning NLP parser tag training translation language... learning training algorithm shrinkage network... garbage collection memory optimization region... Test document parser language label translation … Bag of words for document classification ... planning temporal reasoning plan language... ? Applying Multinomial Naive Bayes Classifiers to Text Classification cNB = argmax cj∈C P(cj) P(xi | cj) i∈positions ∏ positions ¬ all word positions in test document Text Classification and Naïve Bayes Formalizing the Naïve Bayes Classifier Text Classification and Naïve Bayes Naïve Bayes: Learning Learning the Multinomial Naïve Bayes Model • First attempt: maximum likelihood estimates – simply use the frequencies in the data Sec.13.3 ˆ P(wi | cj) = count(wi,cj) count(w,cj) w∈V ∑ ˆ P(cj) = doccount(C = cj) Ndoc Parameter estimation • Create mega-document for topic j by concatenating all docs in this topic – Use frequency of w in mega-document fraction of times word wi appears among all words in documents of topic cj ˆ P(wi | cj) = count(wi,cj) count(w,cj) w∈V ∑ Problem with Maximum Likelihood • What if we have seen no training documents with the word fantastic and classified in the topic positive (thumbs-up)? • Zero probabilities cannot be conditioned away, no matter the other evidence! ˆ P(""fantastic"" positive) = count(""fantastic"", positive) count(w,positive w∈V ∑ ) = 0 cMAP = argmaxc ˆ P(c) ˆ P(xi | c) i ∏ Sec.13.3 Laplace (add-1) smoothing: unknown words ˆ P(wu | c) = count(wu,c)+1 count(w,c w∈V ∑ ) # $ % % & ' ( ( + V +1 Add one extra word to the vocabulary, the “unknown word” wu = 1 count(w,c w∈V ∑ ) # $ % % & ' ( ( + V +1 Underflow Prevention: log space • Multiplying lots of probabilities can result in floating-point underflow. • Since log(xy) = log(x) + log(y) – Better to sum logs of probabilities instead of multiplying probabilities. • Class with highest un-normalized log probability score is still most probable. • Model is now just max of sum of weights cNB = argmax cj∈C logP(cj)+ logP(xi | cj) i∈positions ∑ Text Classification and Naïve Bayes Naïve Bayes: Learning Text Classification and Naïve Bayes Multinomial Naïve Bayes: A Worked Example Choosing a class: P(c|d5) P(j|d5) 1/4 * (2/10)3 * 2/10 * 2/10 ≈ 0.00008 Doc Words Class Training 1 Chinese Beijing Chinese c 2 Chinese Chinese Shanghai c 3 Chinese Macao c 4 Tokyo Japan Chinese j Test 5 Chinese Chinese Chinese Tokyo Japan ? 33 Conditional Probabilities: P(Chinese|c) = P(Tokyo|c) = P(Japan|c) = P(Chinese|j) = P(Tokyo|j) = P(Japan|j) = Priors: P(c)= P(j)= 3 4 1 4 ˆ P(w | c) = count(w,c)+1 count(c)+ |V | ˆ P(c) = Nc N (5+1) / (8+7) = 6/15 (0+1) / (8+7) = 1/15 (1+1) / (3+7) = 2/10 (0+1) / (8+7) = 1/15 (1+1) / (3+7) = 2/10 (1+1) / (3+7) = 2/10 3/4 * (6/15)3 * 1/15 * 1/15 ≈ 0.0002 µ µ +1 Summary: Naive Bayes is Not So Naive • Robust to Irrelevant Features Irrelevant Features cancel each other without affecting results • Very good in domains with many equally important features Decision Trees suffer from fragmentation in such cases – especially if little data • Optimal if the independence assumptions hold: If assumed independence is correct, then it is the Bayes Optimal Classifier for problem • A good dependable baseline for text classification – But we will see other classifiers that give better accuracy Text Classification and Naïve Bayes Multinomial Naïve Bayes: A Worked Example Text Classification and Naïve Bayes Text Classification: Evaluation The 2-by-2 contingency table correct not correct selected tp fp not selected fn tn Precision and recall • Precision: % of selected items that are correct Recall: % of correct items that are selected correct not correct selected tp fp not selected fn tn A combined measure: F • A combined measure that assesses the P/R tradeoff is F measure (weighted harmonic mean): • People usually use balanced F1 measure – i.e., with b = 1 (that is, a = ½): F = 2PR/(P+R) R P PR R P F + + = − + = 2 2 ) 1 ( 1 ) 1 ( 1 1 β β α α Confusion matrix c • For each pair of classes <c1,c2> how many documents from c1 were incorrectly assigned to c2? – c3,2: 90 wheat documents incorrectly assigned to poultry 40 Docs in test set Assigned UK Assigned poultry Assigned wheat Assigned coffee Assigned interest Assigned trade True UK 95 1 13 0 1 0 True poultry 0 1 0 0 0 0 True wheat 10 90 0 1 0 0 True coffee 0 0 0 34 3 7 True interest - 1 2 13 26 5 True trade 0 0 2 14 5 10 Per class evaluation measures Recall: Fraction of docs in class i classified correctly: Precision: Fraction of docs assigned class i that are actually about class i: Accuracy: (1 - error rate) Fraction of docs classified correctly: 41 cii i ∑ cij i ∑ j ∑ cii cji j ∑ cii cij j ∑ Sec. 15.2.4 Micro- vs. Macro-Averaging – If we have more than one class, how do we combine multiple performance measures into one quantity? • Macroaveraging: Compute performance for each class, then average. Average on classes • Microaveraging: Collect decisions for each instance from all classes, compute contingency table, evaluate. Average on instances 42 Sec. 15.2.4 Micro- vs. Macro-Averaging: Example Truth: yes Truth: no Classifier: yes 10 10 Classifier: no 10 970 Truth: yes Truth: no Classifier: yes 90 10 Classifier: no 10 890 Truth: yes Truth: no Classifier: yes 100 20 Classifier: no 20 1860 43 Class 1 Class 2 Micro Ave. Table Sec. 15.2.4 • Macroaveraged precision: (0.5 + 0.9)/2 = 0.7 • Microaveraged precision: 100/120 = .83 • Microaveraged score is dominated by score on common classes Development Test Sets and Cross- validation • Metric: P/R/F1 or Accuracy • Unseen test set – avoid overfitting (‘tuning to the test set’) – more conservative estimate of performance – Cross-validation over multiple splits • Handle sampling errors from different datasets – Pool results over each split – Compute pooled dev set performance Training set Development Test Set Test Set Test Set Training Set Training Set Dev Test Training Set Dev Test Dev Test Text Classification and Naïve Bayes Text Classification: Evaluation "
391,"Discriminative Estimation (Maxent models and perceptron) Generative vs. Discriminative models Many slides are adapted from slides by Christopher Manning and perceptron slides by Alan Ritter Introduction • So far we’ve looked at “generative models” • Naive Bayes • But there is now much use of conditional or discriminative probabilistic models in NLP, Speech, IR (and ML generally) • Because: • They give high accuracy performance • They make it easy to incorporate lots of linguistically important features • They allow automatic building of language independent, retargetable NLP modules Joint vs. Conditional Models • We have some data {(d, c)} of paired observations d and hidden classes c. • Joint (generative) models place probabilities over both observed data and the hidden stuff (gene- rate the observed data from hidden stuff): • All the classic StatNLP models: • n-gram models, Naive Bayes classifiers, hidden Markov models, probabilistic context-free grammars, IBM machine translation alignment models P(c,d) Joint vs. Conditional Models • Discriminative (conditional) models take the data as given, and put a probability over hidden structure given the data: • Logistic regression, conditional loglinear or maximum entropy models, conditional random fields • Also, SVMs, (averaged) perceptron, etc. are discriminative classifiers (but not directly probabilistic) P(c|d) Bayes Net/Graphical Models • Bayes net diagrams draw circles for random variables, and lines for direct dependencies • Some variables are observed; some are hidden • Each node is a little classifier (conditional probability table) based on incoming arcs c d1 d 2 d 3 Naive Bayes c d1 d2 d3 Generative Logistic Regression Discriminative Conditional vs. Joint Likelihood • A joint model gives probabilities P(d,c) and tries to maximize this joint likelihood. • It turns out to be trivial to choose weights: just relative frequencies. • A conditional model gives probabilities P(c|d). It takes the data as given and models only the conditional probability of the class. • We seek to maximize conditional likelihood. • Harder to do (as we’ll see…) • More closely related to classification error. Maxent Models and Discriminative Estimation Generative vs. Discriminative models Discriminative Model Features Making features from text for discriminative NLP models Features • In these slides and most maxent work: features f are elementary pieces of evidence that link aspects of what we observe d with a category c that we want to predict • A feature is a function with a bounded real value: f: C ´ D → ℝ A Belief: to create a data partition Features • In NLP uses, usually a feature specifies 1. an indicator function – a yes/no boolean matching function – of properties of the input and 2. a particular class fi(c, d) º [Φ(d) Ù c = cj] [Value is 0 or 1] • Each feature picks out a data subset and suggests a label for it Example features • f1(c, d) º [c = LOCATION Ù w-1 = “in” Ù isCapitalized(w)] • f2(c, d) º [c = LOCATION Ù hasAccentedLatinChar(w)] • f3(c, d) º [c = DRUG Ù ends(w, “c”)] • Models will assign to each feature a weight: • A positive weight votes that this configuration is likely correct • A negative weight votes that this configuration is likely incorrect LOCATION in Québec PERSON saw Sue DRUG taking Zantac LOCATION in Arcadia Feature-Based Models • The decision about a data point is based only on the features active at that point. BUSINESS: Stocks hit a yearly low … Data Features {…, stocks, hit, a, yearly, low, …} Label: BUSINESS Text Categorization … to restructure bank:MONEY debt. Data Features {…, w-1=restructure, w+1=debt, …} Label: MONEY Word-Sense Disambiguation DT JJ NN … The previous fall … Data Features {w=fall, t-1=JJ w- 1=previous} Label: NN POS Tagging Example: Text Categorization (Zhang and Oles 2001) • Features are presence of each word in a document and the document class (they do feature selection to use reliable indicator words) • Tests on classic Reuters data set (and others) • Naïve Bayes: 77.0% F1 • Linear regression: 86.0% • Logistic regression: 86.4% • Support vector machine: 86.5% • Paper emphasizes the importance of regularization (smoothing) for successful use of discriminative methods (not used in much early NLP/IR work) Other Maxent Classifier Examples • You can use a maxent classifier whenever you want to assign data points to one of a number of classes: • Sentence boundary detection (Mikheev 2000) • Is a period end of sentence or abbreviation? • Sentiment analysis (Pang and Lee 2002) • Word unigrams, bigrams, POS counts, … • PP attachment (Ratnaparkhi 1998) • Attach to verb or noun? Features of head noun, preposition, etc. • Parsing decisions in general (Ratnaparkhi 1997; Johnson et al. 1999, etc.) Discriminative Model Features Making features from text for discriminative NLP models Feature-based Linear Classifiers How to put features into a classifier 16 Feature-Based Linear Classifiers • Linear classifiers at classification time: • Linear function from feature sets {fi} to classes {c}. • Assign a weight li to each feature fi. • We consider each class for an observed datum d • For a pair (c,d), features vote with their weights: • vote(c) = Slifi(c,d) • Choose the class c which maximizes Slifi(c,d) LOCATION in Québec DRUG in Québec PERSON in Québec Feature-Based Linear Classifiers • Linear classifiers at classification time: • Linear function from feature sets {fi} to classes {c}. • Assign a weight li to each feature fi. • We consider each class for an observed datum d • For a pair (c,d), features vote with their weights: • vote(c) = Slifi(c,d) • Choose the class c which maximizes Slifi(c,d) = LOCATION 1.8 –0.6 0.3 LOCATION in Québec DRUG in Québec PERSON in Québec Feature-Based Linear Classifiers There are many ways to chose weights for features With different loss functions as the optimization goal • Perceptron: find a currently misclassified example, and nudge weights in the direction of its correct classification • Margin-based methods (Support Vector Machines) Feature-Based Linear Classifiers • Exponential (log-linear, maxent, logistic, Gibbs) models: • Make a probabilistic model from the linear combination Slifi(c,d) • P(LOCATION|in Québec) = e1.8e–0.6/(e1.8e–0.6 + e0.3 + e0) = 0.586 • P(DRUG|in Québec) = e0.3 /(e1.8e–0.6 + e0.3 + e0) = 0.238 • P(PERSON|in Québec) = e0 /(e1.8e–0.6 + e0.3 + e0) = 0.176 • The weights are the parameters of the probability model, combined via a “soft max” function ∑ ∑ ' ) , ' ( exp c i i i d c f λ = ) , | ( λ d c P ∑ i i i d c f ) , ( exp λ Makes votes positive Normalizes votes Aside: logistic regression • Maxent models in NLP are essentially the same as multiclass logistic regression models in statistics (or machine learning) • The key role of feature functions in NLP and in this presentation • The features are more general, with f also being a function of the class 21 Quiz Question • Assuming exactly the same set up (3 class decision: LOCATION, PERSON, or DRUG; 3 features as before, maxent), what are: • P(PERSON | by Goéric) = • P(LOCATION | by Goéric) = • P(DRUG | by Goéric) = • 1.8 f1(c, d) º [c = LOCATION Ù w-1 = “in” Ù isCapitalized(w)] • -0.6 f2(c, d) º [c = LOCATION Ù hasAccentedLatinChar(w)] • 0.3 f3(c, d) º [c = DRUG Ù ends(w, “c”)] ∑ ∑ ' ) , ' ( exp c i i i d c f λ = ) , | ( λ d c P ∑ i i i d c f ) , ( exp λ PERSON by Goéric LOCATION by Goéric DRUG by Goéric Feature-based Linear Classifiers How to put features into a classifier 23 Building a Maxent Model The nuts and bolts Building a Maxent Model • We define features (indicator functions) over data points • Features represent sets of data points which are distinctive enough to deserve model parameters. • Words, but also “word contains number”, “word ends with ing”, etc. • We will simply encode each Φ feature as a unique String (index) • A datum will give rise to a set of Strings: the active Φ features • Each feature fi(c, d) º [Φ(d) Ù c = cj] gets a real number weight • We concentrate on Φ features but the math uses i indices of fi Building a Maxent Model • Features are often added during model development to target errors • Often, the easiest thing to think of are features that mark bad combinations • Then, for any given feature weights, we want to be able to calculate: • Data conditional likelihood • Derivative of the likelihood wrt each feature weight • Uses expectations of each feature according to the model • We can then find the optimum feature weights (discussed later). Building a Maxent Model The nuts and bolts Naive Bayes vs. Maxent models Generative vs. Discriminative models: The problem of overcounting evidence Text classification: Asia or Europe NB FACTORS: • P(A) = P(E) = • P(M|A) = • P(M|E) = • P(H|A) = P(K|A) = • P(H|E) = PK|E) = Europe Asia Class H K NB Model PREDICTIONS: • P(A,H,K,M) = • P(E,H,K,M) = • P(A|H,K,M) = • P(E|H,K,M) = Training Data M Monaco Monaco Monaco Monaco Hong Kong Hong Kong Monaco Monaco Hong Kong Hong Kong Monaco Monaco Naive Bayes vs. Maxent Models • Naive Bayes models multi-count correlated evidence • Each feature is multiplied in, even when you have multiple features telling you the same thing • Maximum Entropy models (pretty much) solve this problem • As we will see, this is done by weighting features so that model expectations match the observed (empirical) expectations Naive Bayes vs. Maxent models Generative vs. Discriminative models: The problem of overcounting evidence Maxent Models and Discriminative Estimation Maximizing the likelihood Feature Expectations • We will crucially make use of two expectations • actual or predicted counts of a feature firing: • Empirical count (expectation) of a feature: • Model expectation of a feature: ∑ ∈ = ) , ( observed ) , ( ) , ( ) ( empirical D C d c i i d c f f E ∑ ∈ = ) , ( ) , ( ) , ( ) , ( ) ( D C d c i i d c f d c P f E Goal: well fit the data Exponential Model Likelihood • Maximum (Conditional) Likelihood Models : • Given a model form, choose values of parameters to maximize the (conditional) likelihood of the data. ∑ ∑ ∈ ∈ = = ) , ( ) , ( ) , ( ) , ( log ) , | ( log ) , | ( log D C d c D C d c d c P D C P λ λ ∑ ∑ ' ) , ' ( exp c i i i d c f λ ∑ i i i d c f ) , ( exp λ The Likelihood Value • The (log) conditional likelihood of iid data (C,D) according to maxent model is a function of the data and the parameters l: • If there aren’t many values of c, it’s easy to calculate: ∑ ∏ ∈ ∈ = = ) , ( ) , ( ) , ( ) , ( ) , | ( log ) , | ( log ) , | ( log D C d c D C d c d c P d c P D C P λ λ λ ∑ ∈ = ) , ( ) , ( log ) , | ( log D C d c D C P λ ∑ ∑ ' ) , ' ( exp c i i i d c f λ ∑ i i i d c f ) , ( exp λ The Likelihood Value • We can separate this into two components: • The derivative is the difference between the derivatives of each component ∑ ∑ ∑ ∈ ) , ( ) , ( ' ) , ' ( exp log D C d c c i i i d c f λ ∑ ∑ ∈ ) , ( ) , ( ) , ( exp log D C d c i i i d c f λ − = ) , | ( log λ D C P ) (λ N ) (λ M = ) , | ( log λ D C P − The Derivative I: Numerator Derivative of the numerator is: the empirical count(fi, c) i D C d c i i i d c f λ λ ∂ ∂ = ∑∑ ∈ ) , ( ) , ( ) , ( ∑ ∑ ∈ ∂ ∂ = ) , ( ) , ( ) , ( D C d c i i i i d c f λ λ ∑ ∈ = ) , ( ) , ( ) , ( D C d c i d c f i D C d c i i ci i d c f N λ λ λ λ ∂ ∂ = ∂ ∂ ∑ ∑ ∈ ) , ( ) , ( ) , ( exp log ) ( The Derivative II: Denominator i D C d c c i i i i d c f M λ λ λ λ ∂ ∂ = ∂ ∂ ∑ ∑ ∑ ∈ ) , ( ) , ( ' ) , ' ( exp log ) ( ∑ ∑ ∑ ∑ ∑ ∈ ∂ ∂ = ) , ( ) , ( ' ' ' ) , ' ( exp ) , ' ' ( exp 1 D C d c i c i i i c i i i d c f d c f λ λ λ ∑ ∑ ∑ ∑ ∑ ∑ ∈ ∂ ∂ = ) , ( ) , ( ' ' ' ) , ' ( 1 ) , ' ( exp ) , ' ' ( exp 1 D C d c c i i i i i i i c i i i d c f d c f d c f λ λ λ λ i i i i D C d c c c i i i i i i d c f d c f d c f λ λ λ λ ∂ ∂ = ∑ ∑ ∑∑ ∑ ∑ ∈ ) , ' ( ) , ' ' ( exp ) , ' ( exp ) , ( ) , ( ' ' ' ∑ ∑ ∈ = ) , ( ) , ( ' ) , ' ( ) , | ' ( D C d c i c d c f d c P λ = predicted count(fi, l) The Derivative III • The optimum parameters are the ones for which each feature’s predicted expectation equals its empirical expectation. The optimum distribution is: • Always unique (but parameters may not be unique) • Always exists (if feature counts are from actual data). • These models are also called maximum entropy models because we find the model having maximum entropy and satisfying the constraints: = ∂ ∂ i D C P λ λ) , | ( log ) , ( count actual C fi ) , ( count predicted λ i f − j f E f E j p j p ∀ = ), ( ) ( ~ Finding the optimal parameters • We want to choose parameters λ1, λ2, λ3, … that maximize the conditional log-likelihood of the training data • To be able to do that, we’ve worked out how to calculate the function value and its partial derivatives (its gradient) ) | ( log ) ( 1 i n i i d c P D CLogLik ∑ = = A likelihood surface Finding the optimal parameters • Use your favorite numerical optimization package…. • Commonly, you minimize the negative of CLogLik 1. Gradient descent (GD); Stochastic gradient descent (SGD) 2. Iterative proportional fitting methods: Generalized Iterative Scaling (GIS) and Improved Iterative Scaling (IIS) 3. Conjugate gradient (CG), perhaps with preconditioning 4. Quasi-Newton methods – limited memory variable metric (LMVM) methods, in particular, L-BFGS Gradient Descent (GD) 43 Maxent Models and Discriminative Estimation Maximizing the likelihood Feature Sparsity Regularization Combating overfitting Smoothing: Issues of Scale • Lots of features: • NLP maxent models can have well over a million features. • Even storing a single array of parameter values can have a substantial memory cost. • Lots of sparsity: • Overfitting very easy – we need smoothing! • Many features seen in training will never occur again at test time. • Optimization problems: • Feature weights can be infinite, and iterative solvers can take a long time to get to those infinities. Smoothing/Priors/ Regularization Standard vs. Regularized Updates 48 Feature Sparsity Regularization Combating overfitting Batch vs. Online Learning GD vs. SGD Stochastic Gradient Decent (SGD) 51 Batch vs. Online learning: Batch vs. Online Learning GD vs. SGD Perceptron Another Online Learning algorithem Perceptron Algorithm 54 MaxEnt v.s Perceptron • Perceptron doesn’t always make updates • Probabilities v.s scores 55 Regularization in the Perceptron Algorithm • No gradient computed, so can’t directly include a regularizer in an object function. • Instead run different numbers of iterations • Use parameter averaging, for instance, average of all parameters after seeing each data point 56 "
392,"Introduction to N-grams Language Modeling Many Slides are adapted from slides by Dan Jurafsky Probabilistic Language Models • Today’s goal: assign a probability to a sentence • Machine Translation: • P(high winds tonite) > P(large winds tonite) • Spell Correction • The office is about fifteen minuets from my house • P(about fifteen minutes from) > P(about fifteen minuets from) • Speech Recognition • P(I saw a van) >> P(eyes awe of an) • + Summarization, question-answering, etc., etc.!! Why? Probabilistic Language Modeling • Goal: compute the probability of a sentence or sequence of words: P(W) = P(w1,w2,w3,w4,w5…wn) • Related task: probability of an upcoming word: P(w5|w1,w2,w3,w4) • A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model. • Better: the grammar But language model or LM is standard How to compute P(W) • How to compute this joint probability: • P(its, water, is, so, transparent, that) • Intuition: let’s rely on the Chain Rule of Probability Reminder: The Chain Rule • With 4 variables: P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) • The Chain Rule in General P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1) The Chain Rule applied to compute joint probability of words in sentence P(“its water is so transparent”) = P(its) × P(water|its) × P(is|its water) × P(so|its water is) × P(transparent|its water is so) P(w1w2…wn) = P(wi | w1w2…wi−1) i ∏ How to estimate these probabilities • Could we just count and divide? • No! Too many possible sentences! • We’ll never see enough data for estimating these P(the |its water is so transparent that) = Count(its water is so transparent that the) Count(its water is so transparent that) Markov Assumption • In other words, we approximate each component in the product P(w1w2…wn) ≈ P(wi | wi−k…wi−1) i ∏ P(wi | w1w2…wi−1) ≈P(wi | wi−k…wi−1) Markov Assumption • Simplifying assumption: • Or maybe P(the |its water is so transparent that) ≈P(the |that) P(the |its water is so transparent that) ≈P(the |transparent that) Andrei Markov Simplest case: Unigram model fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass thrift, did, eighty, said, hard, 'm, july, bullish that, or, limited, the Some automatically generated sentences from a unigram model € P(w1w2…wn) ≈ P(wi) i ∏ Condition on the previous word: Bigram model texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, fifty, five, yen outside, new, car, parking, lot, of, the, agreement, reached this, would, be, a, record, november P(wi | w1w2…wi−1) ≈P(wi | wi−1) N-gram models • We can extend to trigrams, 4-grams, 5-grams • In general this is an insufficient model of language • because language has long-distance dependencies: “The computer which I had just put into the machine room on the fifth floor crashed.” • But we can often get away with N-gram models Still, Most words depend on their previous few words Introduction to N-grams Language Modeling Estimating N-gram Probabilities Language Modeling Estimating bigram probabilities • The Maximum Likelihood Estimate € P(wi | wi−1) = count(wi−1,wi) count(wi−1) P(wi | wi−1) = c(wi−1,wi) c(wi−1) An example <s> I am Sam </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s> P(wi | wi−1) = c(wi−1,wi) c(wi−1) More examples: Berkeley Restaurant Project sentences • can you tell me about any good cantonese restaurants close by • mid priced thai food is what i’m looking for • tell me about chez panisse • can you give me a listing of the kinds of food that are available • i’m looking for a good place to eat breakfast • when is caffe venezia open during the day Raw bigram counts • Out of 9222 sentences Raw bigram probabilities • Normalize by unigrams: • Result: Bigram estimates of sentence probabilities P(<s> I want english food </s>) = P(I|<s>) × P(want|I) × P(english|want) × P(food|english) × P(</s>|food) = .000031 What kinds of knowledge? • P(english|want) = .0011 • P(chinese|want) = .0065 • P(to|want) = .66 • P(eat | to) = .28 • P(food | to) = 0 • P(want | spend) = 0 • P (i | <s>) = .25 Practical Issues • We do everything in log space • Avoid underflow • (also adding is faster than multiplying) log(p1 × p2 × p3 × p4) = log p1 + log p2 + log p3 + log p4 Language Modeling Toolkits • SRILM • http://www.speech.sri.com/projects/srilm/ Google N-Gram Release, August 2006 … http://ngrams.googlelabs.com/ Estimating N-gram Probabilities Language Modeling Evaluation and Perplexity Language Modeling Evaluation: How good is our model? • Does our language model prefer good sentences to bad ones? • Assign higher probability to “real” or “frequently observed” sentences • Than “ungrammatical” or “rarely observed” sentences? • We train parameters of our model on a training set. • We test the model’s performance on data we haven’t seen. • A test set is an unseen dataset that is different from our training set, totally unused. • An evaluation metric tells us how well our model does on the test set. Extrinsic evaluation of N-gram models • Best evaluation for comparing models A and B • Put each model in a task • spelling corrector, speech recognizer, MT system • Run the task, get an accuracy for A and for B • How many misspelled words corrected properly • How many words translated correctly • Compare accuracy for A and B Difficulty of extrinsic (in-vivo) evaluation of N-gram models • Extrinsic evaluation • Time-consuming; can take days or weeks • So • Sometimes use intrinsic evaluation: perplexity Intuition of Perplexity • The Shannon Game: • How well can we predict the next word? • Unigrams are terrible at this game. (Why?) • A better model of a text • is one which assigns a higher probability to the word that actually occurs I always order pizza with cheese and ____ The 33rd President of the US was ____ I saw a ____ mushrooms 0.1 pepperoni 0.1 anchovies 0.01 …. fried rice 0.0001 …. and 1e-100 Perplexity Perplexity is the inverse probability of the test set, normalized by the number of words: Chain rule: For bigrams: Minimizing perplexity is the same as maximizing probability The best language model is one that best predicts an unseen test set • Gives the highest P(sentence) PP(W) = P(w1w2...wN ) −1 N = 1 P(w1w2...wN ) N Lower perplexity = better model • Training 38 million words, test 1.5 million words, WSJ N-gram Order Unigram Bigram Trigram Perplexity 962 170 109 Evaluation and Perplexity Language Modeling Generalization and zeros Language Modeling The Shannon Visualization Method • Choose a random bigram (<s>, w) according to its probability • Now choose a random bigram (w, x) according to its probability • And so on until we choose </s> • Then string the words together <s> I I want want to to eat eat Chinese Chinese food food </s> I want to eat Chinese food Approximating Shakespeare Shakespeare as corpus • N=884,647 tokens, V=29,066 • Shakespeare produced 300,000 bigram types out of V2= 844 million possible bigrams. • So 99.96% of the possible bigrams were never seen (have zero entries in the table) • Quadrigrams worse: What's coming out looks like Shakespeare because it is Shakespeare The wall street journal is not shakespeare (no offense) The perils of overfitting • N-grams only work well for word prediction if the test corpus looks like the training corpus • In real life, it often doesn’t • We need to train robust models that generalize! • One kind of generalization: Zeros! • Things that don’t ever occur in the training set • But occur in the test set Zeros • Training set: … denied the allegations … denied the reports … denied the claims … denied the request P(“offer” | denied the) = 0 • Test set … denied the offer … denied the loan Zero probability bigrams • Bigrams with zero probability • mean that we will assign 0 probability to the test set! • And hence we cannot compute perplexity (can’t divide by 0)! Generalization and zeros Language Modeling Smoothing: Add-one (Laplace) smoothing Language Modeling Add-one estimation • Also called Laplace smoothing • Pretend we saw each word one more time than we did • Just add one to all the counts! • MLE estimate: • Add-1 estimate: P MLE(wi | wi−1) = c(wi−1,wi) c(wi−1) P Add−1(wi | wi−1) = c(wi−1,wi)+1 c(wi−1)+V +1 Berkeley Restaurant Corpus: Laplace smoothed bigram counts Laplace-smoothed bigrams +1 Reconstituted counts +1 Compare with raw bigram counts Add-1 estimation is a blunt instrument • So add-1 isn’t used for N-grams: • We’ll see better methods • But add-1 is used to smooth other NLP models • For text classification • In domains where the number of zeros isn’t so huge. Smoothing: Add-one (Laplace) smoothing Language Modeling Interpolation, Backoff Language Modeling Backoff and Interpolation • Sometimes it helps to use less context • Condition on less context for contexts you haven’t learned much about • Backoff: • use trigram if you have good evidence, • otherwise bigram, otherwise unigram • Interpolation: • mix unigram, bigram, trigram • Interpolation works better Linear Interpolation • Simple interpolation • Lambdas conditional on context: N-gram Smoothing Summary • Add-1 smoothing: • OK for text categorization, not for language modeling • The most commonly used method: • Extended Interpolated Kneser-Ney 54 Interpolation, Backoff Language Modeling Language Modeling Advanced: Kneser-Ney Smoothing Advanced smoothing algorithms • Intuition used by many smoothing algorithms • Good-Turing • Kneser-Ney • Use the count of things we’ve seen • to help estimate the count of things we’ve never seen • Better estimate for probabilities of lower-order unigrams! • Shannon game: I can’t see without my reading___________? • “Francisco” is more common than “glasses” • … but “Francisco” always follows “San” • Instead of P(w): “How likely is w” • Pcontinuation(w): “How likely is w to appear as a novel continuation? • For each word, count the number of unique bigram types it completes • Every bigram type was a novel continuation the first time it was seen Francisco Kneser-Ney Smoothing I (smart backoff) glasses P CONTINUATION(w)∝ {wi−1 :c(wi−1,w) > 0} Kneser-Ney Smoothing II • How many times does w appear as a novel continuation: • Normalized by the total number of word bigram types P CONTINUATION(w) = {wi−1 :c(wi−1,w) > 0} {(wj−1,wj):c(wj−1,wj) > 0} P CONTINUATION(w)∝ {wi−1 :c(wi−1,w) > 0} {(wj−1,wj):c(wj−1,wj) > 0} Kneser-Ney Smoothing III 60 P KN(wi | wi−1) = max(c(wi−1,wi)−d,0) c(wi−1) + λ(wi−1)P CONTINUATION(wi) λ(wi−1) = d c(wi−1) {w :c(wi−1,w) > 0} λ is a normalizing constant; the probability mass we’ve discounted the normalized discount The number of word types that can follow wi-1 = # of word types we discounted = # of times we applied normalized discount Language Modeling Advanced: Kneser-Ney Smoothing "
393,"Part-of-speech tagging A simple but useful form of linguistic analysis Many slides adapted from slides by Chris Manning Parts of Speech • Perhaps starting with Aristotle in the West (384–322 BCE), there was the idea of having parts of speech • a.k.a lexical categories, word classes, “tags”, POS • It comes from Dionysius Thrax of Alexandria (c. 100 BCE) the idea that is still with us that there are 8 parts of speech • But actually his 8 aren’t exactly the ones we are taught today • Thrax: noun, verb, article, adverb, preposition, conjunction, participle, pronoun • School grammar: noun, verb, adjective, adverb, preposition, conjunction, pronoun, interjection Open class (lexical) words Closed class (functional) Nouns Verbs Proper Common Modals Main Adjectives Adverbs Prepositions Particles Determiners Conjunctions Pronouns … more … more IBM Italy cat / cats snow see registered can had old older oldest slowly to with off up the some and or he its Numbers 122,312 one Interjections Ow Eh POS Tagging • Words often have more than one POS: back • The back door = JJ • On my back = NN • Win the voters back = RB • Promised to back the bill = VB • The POS tagging problem is to determine the POS tag for a particular instance of a word. POS Tagging • Input: Plays well with others • Ambiguity: NNS/VBZ UH/JJ/NN/RB IN NNS • Output: Plays/VBZ well/RB with/IN others/NNS • Uses: • Text-to-speech (how do we pronounce “lead”?) • Can write regexps like (Det) Adj* N+ over the output for phrases, etc. • As input to or to speed up a full parser • If you know the tag, you can back off to it in other tasks Penn Treebank POS tags https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html POS tagging performance • How many tags are correct? (Tag accuracy) • About 97% currently • But baseline is already 90% • Baseline is performance of stupidest possible method • Tag every word with its most frequent tag • Tag unknown words as nouns • Partly easy because • Many words are unambiguous • You get points for them (the, a, etc.) and for punctuation marks! How difficult is POS tagging? • About 11% of the word types in the Brown corpus are ambiguous with regard to part of speech • But they tend to be very common words. E.g., that • I know that he is honest = IN • Yes, that play was nice = DT • You can’t go that far = RB • 40% of the word tokens are ambiguous Prepsition or Subordinating conjunction Deciding on the correct part of speech can be difficult even for people • Mrs/NNP Shaefer/NNP never/RB got/VBD around/RP to/TO joining/VBG • All/DT we/PRP gotta/VBN do/VB is/VBZ go/VB around/IN the/DT corner/NN • Chateau/NNP Petrus/NNP costs/VBZ around/RB 250/CD particle Part-of-speech tagging A simple but useful form of linguistic analysis Part-of-speech tagging revisited A simple but useful form of linguistic analysis Sources of information • What are the main sources of information for POS tagging? • Knowledge of neighboring words • Bill saw that man yesterday • NNP NN DT NN NN • VB VB(D) IN VB NN • Knowledge of word probabilities • man is rarely used as a verb…. • The latter proves the most useful, but the former also helps More and Better Features è Feature- based tagger • Can do surprisingly well just looking at a word by itself: • Word the: the ® DT • Lowercased word Importantly: importantly ® RB • Prefixes unfathomable: un- ® JJ • Suffixes Importantly: -ly ® RB • Capitalization Meridian: CAP ® NNP • Word shapes 35-year: d-x ® JJ • Then build a maxent (or whatever) model to predict tag • Maxent P(t|w): 93.7% overall / 82.6% unknown How to improve supervised results? • Build better features! • We could fix this with a feature that looked at the next word • We could fix this by linking capitalized words to their lowercase versions PRP VBD IN RB IN PRP VBD . They left as soon as he arrived . NNP NNS VBD VBN . Intrinsic flaws remained undetected . RB JJ Tagging Without Sequence Information t0 w0 Baseline t0 w0 w-1 w1 Three Words Model Features Token Unknown Baseline 56,805 93.69% 82.61% 3Words 239,767 96.57% 86.78% Using words only in a straight classifier works as well as a basic (HMM or discriminative) sequence model!! Overview: POS Tagging Accuracies • Rough accuracies: • Most freq tag: ~90% / ~50% • Maxent P(t|w): 93.7% / 82.6% • Trigram HMM: ~95% / ~55% • MEMM tagger: 96.9% / 86.9% • Upper bound: ~98% (human agreement) Most errors on unknown words Summary of POS Tagging One profits from models for specifying dependence on overlapping features of the observation such as spelling, suffix analysis, etc. An MEMM allows integration of rich features of the observations and considers dependence with the previous word’s tag, but can suffer strongly from assuming independence from following observations; this effect can be relieved by adding dependence on following words. This additional power (of the CRF, Structured Perceptron models) has been shown to result in improvements in accuracy The higher accuracy of discriminative models comes at the price of much slower training Part-of-speech tagging revisited A simple but useful form of linguistic analysis "
394,"Sequence Models • Hidden Markov Models (HMM) • MaxEnt Markov Models (MEMM) Many slides from Michael Collins Overview I The Tagging Problem I Generative models, and the noisy-channel model, for supervised learning I Hidden Markov Model (HMM) taggers I Basic deﬁnitions I Parameter estimation I The Viterbi algorithm and HMMs Part-of-Speech Tagging INPUT: Proﬁts soared at Boeing Co., easily topping forecasts on Wall Street, as their CEO Alan Mulally announced ﬁrst quarter results. OUTPUT: Proﬁts/N soared/V at/P Boeing/N Co./N ,/, easily/ADV topping/V forecasts/N on/P Wall/N Street/N ,/, as/P their/POSS CEO/N Alan/N Mulally/N announced/V ﬁrst/ADJ quarter/N results/N ./. N = Noun V = Verb P = Preposition Adv = Adverb Adj = Adjective . . . Named Entity Recognition INPUT: Proﬁts soared at Boeing Co., easily topping forecasts on Wall Street, as their CEO Alan Mulally announced ﬁrst quarter results. OUTPUT: Proﬁts soared at [Company Boeing Co.], easily topping forecasts on [Location Wall Street], as their CEO [Person Alan Mulally] announced ﬁrst quarter results. Named Entity Extraction as Tagging INPUT: Proﬁts soared at Boeing Co., easily topping forecasts on Wall Street, as their CEO Alan Mulally announced ﬁrst quarter results. OUTPUT: Proﬁts/NA soared/NA at/NA Boeing/SC Co./CC ,/NA easily/NA topping/NA forecasts/NA on/NA Wall/SL Street/CL ,/NA as/NA their/NA CEO/NA Alan/SP Mulally/CP announced/NA ﬁrst/NA quarter/NA results/NA ./NA NA = No entity SC = Start Company CC = Continue Company SL = Start Location CL = Continue Location . . . Our Goal Training set: 1 Pierre/NNP Vinken/NNP ,/, 61/CD years/NNS old/JJ ,/, will/MD join/VB the/DT board/NN as/IN a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD ./. 2 Mr./NNP Vinken/NNP is/VBZ chairman/NN of/IN Elsevier/NNP N.V./NNP ,/, the/DT Dutch/NNP publishing/VBG group/NN ./. 3 Rudolph/NNP Agnew/NNP ,/, 55/CD years/NNS old/JJ and/CC chairman/NN of/IN Consolidated/NNP Gold/NNP Fields/NNP PLC/NNP ,/, was/VBD named/VBN a/DT nonexecutive/JJ director/NN of/IN this/DT British/JJ industrial/JJ conglomerate/NN ./. . . . 38,219 It/PRP is/VBZ also/RB pulling/VBG 20/CD people/NNS out/IN of/IN Puerto/NNP Rico/NNP ,/, who/WP were/VBD helping/VBG Huricane/NNP Hugo/NNP victims/NNS ,/, and/CC sending/VBG them/PRP to/TO San/NNP Francisco/NNP instead/RB ./. I From the training set, induce a function/algorithm that maps new sentences to their tag sequences. Two Types of Constraints Inﬂuential/JJ members/NNS of/IN the/DT House/NNP Ways/NNP and/CC Means/NNP Committee/NNP introduced/VBD legislation/NN that/WDT would/MD restrict/VB how/WRB the/DT new/JJ savings-and-loan/NN bailout/NN agency/NN can/MD raise/VB capital/NN ./. I “Local”: e.g., can is more likely to be a modal verb MD rather than a noun NN I “Contextual”: e.g., a noun is much more likely than a verb to follow a determiner I Sometimes these preferences are in conﬂict: The trash can is in the garage Overview I The Tagging Problem I Generative models, and the noisy-channel model, for supervised learning I Hidden Markov Model (HMM) taggers I Basic deﬁnitions I Parameter estimation I The Viterbi algorithm Supervised Learning Problems I We have training examples x(i), y(i) for i = 1 . . . m. Each x(i) is an input, each y(i) is a label. I Task is to learn a function f mapping inputs x to labels f(x) Supervised Learning Problems I We have training examples x(i), y(i) for i = 1 . . . m. Each x(i) is an input, each y(i) is a label. I Task is to learn a function f mapping inputs x to labels f(x) I Conditional models: I Learn a distribution p(y|x) from training examples I For any test input x, deﬁne f(x) = arg maxy p(y|x) Generative Models I We have training examples x(i), y(i) for i = 1 . . . m. Task is to learn a function f mapping inputs x to labels f(x). Generative Models I We have training examples x(i), y(i) for i = 1 . . . m. Task is to learn a function f mapping inputs x to labels f(x). I Generative models: I Learn a distribution p(x, y) from training examples I Often we have p(x, y) = p(y)p(x|y) Generative Models I We have training examples x(i), y(i) for i = 1 . . . m. Task is to learn a function f mapping inputs x to labels f(x). I Generative models: I Learn a distribution p(x, y) from training examples I Often we have p(x, y) = p(y)p(x|y) I Note: we then have p(y|x) = p(y)p(x|y) p(x) where p(x) = P y p(y)p(x|y) Decoding with Generative Models I We have training examples x(i), y(i) for i = 1 . . . m. Task is to learn a function f mapping inputs x to labels f(x). I Generative models: I Learn a distribution p(x, y) from training examples I Often we have p(x, y) = p(y)p(x|y) I Output from the model: f(x) = arg max y p(y|x) = arg max y p(y)p(x|y) p(x) = arg max y p(y)p(x|y) Overview I The Tagging Problem I Generative models, and the noisy-channel model, for supervised learning I Hidden Markov Model (HMM) taggers I Basic deﬁnitions I Parameter estimation I The Viterbi algorithm Hidden Markov Models I We have an input sentence x = x1, x2, . . . , xn (xi is the i’th word in the sentence) I We have a tag sequence y = y1, y2, . . . , yn (yi is the i’th tag in the sentence) I We’ll use an HMM to deﬁne p(x1, x2, . . . , xn, y1, y2, . . . , yn) for any sentence x1 . . . xn and tag sequence y1 . . . yn of the same length. I Then the most likely tag sequence for x is arg max y1...yn p(x1 . . . xn, y1, y2, . . . , yn) Trigram Hidden Markov Models (Trigram HMMs) For any sentence x1 . . . xn where xi 2 V for i = 1 . . . n, and any tag sequence y1 . . . yn+1 where yi 2 S for i = 1 . . . n, and yn+1 = STOP, the joint probability of the sentence and tag sequence is p(x1 . . . xn, y1 . . . yn+1) = n+1 Y i=1 q(yi|yi−2, yi−1) n Y i=1 e(xi|yi) where we have assumed that x0 = x−1 = *. Parameters of the model: I q(s|u, v) for any s 2 S [ {STOP}, u, v 2 S [ {*} I e(x|s) for any s 2 S, x 2 V y_0 = y_-1 = *. An Example If we have n = 3, x1 . . . x3 equal to the sentence the dog laughs, and y1 . . . y4 equal to the tag sequence D N V STOP, then p(x1 . . . xn, y1 . . . yn+1) = q(D|⇤, ⇤) ⇥q(N|⇤, D) ⇥q(V|D, N) ⇥q(STOP|N, V) ⇥e(the|D) ⇥e(dog|N) ⇥e(laughs|V) I STOP is a special tag that terminates the sequence I We take y0 = y−1 = *, where * is a special “padding” symbol Why the Name? p(x1 . . . xn, y1 . . . yn) = q(STOP|yn−1, yn) n Y j=1 q(yj | yj−2, yj−1) | {z } Markov Chain ⇥ n Y j=1 e(xj | yj) | {z } xj’s are observed Overview I The Tagging Problem I Generative models, and the noisy-channel model, for supervised learning I Hidden Markov Model (HMM) taggers I Basic deﬁnitions I Parameter estimation I The Viterbi algorithm Smoothed Estimation q(Vt | DT, JJ) = λ1 ⇥Count(Dt, JJ, Vt) Count(Dt, JJ) +λ2 ⇥Count(JJ, Vt) Count(JJ) +λ3 ⇥Count(Vt) Count() λ1 + λ2 + λ3 = 1, and for all i, λi ≥0 e(base | Vt) = Count(Vt, base) Count(Vt) Dealing with Low-Frequency Words: An Example Proﬁts soared at Boeing Co. , easily topping forecasts on Wall Street , as their CEO Alan Mulally announced ﬁrst quarter results . Dealing with Low-Frequency Words A common method is as follows: I Step 1: Split vocabulary into two sets Frequent words = words occurring ≥5 times in training Low frequency words = all other words I Step 2: Map low frequency words into a small, ﬁnite set, depending on preﬁxes, suﬃxes etc. Dealing with Low-Frequency Words: An Example [Bikel et. al 1999] (named-entity recognition) Word class Example Intuition twoDigitNum 90 Two digit year fourDigitNum 1990 Four digit year containsDigitAndAlpha A8956-67 Product code containsDigitAndDash 09-96 Date containsDigitAndSlash 11/9/89 Date containsDigitAndComma 23,000.00 Monetary amount containsDigitAndPeriod 1.00 Monetary amount, percentage othernum 456789 Other number allCaps BBN Organization capPeriod M. Person name initial ﬁrstWord ﬁrst word of sentence no useful capitalization information initCap Sally Capitalized word lowercase can Uncapitalized word other , Punctuation marks, all other words Dealing with Low-Frequency Words: An Example Proﬁts/NA soared/NA at/NA Boeing/SC Co./CC ,/NA easily/NA topping/NA forecasts/NA on/NA Wall/SL Street/CL ,/NA as/NA their/NA CEO/NA Alan/SP Mulally/CP announced/NA ﬁrst/NA quarter/NA results/NA ./NA + firstword/NA soared/NA at/NA initCap/SC Co./CC ,/NA easily/NA lowercase/NA forecasts/NA on/NA initCap/SL Street/CL ,/NA as/NA their/NA CEO/NA Alan/SP initCap/CP announced/NA ﬁrst/NA quarter/NA results/NA ./NA NA = No entity SC = Start Company CC = Continue Company SL = Start Location CL = Continue Location . . . •Inference and the Viterbi Algorithm The Viterbi Algorithm Problem: for an input x1 . . . xn, ﬁnd arg max y1...yn+1 p(x1 . . . xn, y1 . . . yn+1) where the arg max is taken over all sequences y1 . . . yn+1 such that yi 2 S for i = 1 . . . n, and yn+1 = STOP. We assume that p again takes the form p(x1 . . . xn, y1 . . . yn+1) = n+1 Y i=1 q(yi|yi−2, yi−1) n Y i=1 e(xi|yi) Recall that we have assumed in this deﬁnition that y0 = y−1 = *, and yn+1 = STOP. Brute Force Search is Hopelessly Ineﬃcient Problem: for an input x1 . . . xn, ﬁnd arg max y1...yn+1 p(x1 . . . xn, y1 . . . yn+1) where the arg max is taken over all sequences y1 . . . yn+1 such that yi 2 S for i = 1 . . . n, and yn+1 = STOP. The Viterbi Algorithm I Deﬁne n to be the length of the sentence I Deﬁne Sk for k = −1 . . . n to be the set of possible tags at position k: S−1 = S0 = {⇤} Sk = S for k 2 {1 . . . n} I Deﬁne r(y−1, y0, y1, . . . , yk) = k Y i=1 q(yi|yi−2, yi−1) k Y i=1 e(xi|yi) I Deﬁne a dynamic programming table ⇡(k, u, v) = maximum probability of a tag sequence ending in tags u, v at position k that is, ⇡(k, u, v) = maxhy−1,y0,y1,...,yki:yk−1=u,yk=v r(y−1, y0, y1 . . . yk) A Recursive Deﬁnition Base case: ⇡(0, *, *) = 1 Recursive deﬁnition: For any k 2 {1 . . . n}, for any u 2 Sk−1 and v 2 Sk: ⇡(k, u, v) = max w2Sk−2 (⇡(k −1, w, u) ⇥q(v|w, u) ⇥e(xk|v)) The Viterbi Algorithm Input: a sentence x1 . . . xn, parameters q(s|u, v) and e(x|s). Initialization: Set ⇡(0, *, *) = 1 Deﬁnition: S−1 = S0 = {⇤}, Sk = S for k 2 {1 . . . n} Algorithm: I For k = 1 . . . n, I For u 2 Sk−1, v 2 Sk, ⇡(k, u, v) = max w2Sk−2 (⇡(k −1, w, u) ⇥q(v|w, u) ⇥e(xk|v)) I Return maxu2Sn−1,v2Sn (⇡(n, u, v) ⇥q(STOP|u, v)) The Viterbi Algorithm with Backpointers Input: a sentence x1 . . . xn, parameters q(s|u, v) and e(x|s). Initialization: Set ⇡(0, *, *) = 1 Deﬁnition: S−1 = S0 = {⇤}, Sk = S for k 2 {1 . . . n} Algorithm: I For k = 1 . . . n, I For u 2 Sk−1, v 2 Sk, ⇡(k, u, v) = max w2Sk−2 (⇡(k −1, w, u) ⇥q(v|w, u) ⇥e(xk|v)) bp(k, u, v) = arg max w2Sk−2 (⇡(k −1, w, u) ⇥q(v|w, u) ⇥e(xk|v)) I Set (yn−1, yn) = arg max(u,v) (⇡(n, u, v) ⇥q(STOP|u, v)) I For k = (n −2) . . . 1, yk = bp(k + 2, yk+1, yk+2) I Return the tag sequence y1 . . . yn The Viterbi Algorithm: Running Time I O(n|S|3) time to calculate q(s|u, v) ⇥e(xk|s) for all k, s, u, v. I n|S|2 entries in ⇡to be ﬁlled in. I O(|S|) time to ﬁll in one entry I ) O(n|S|3) time in total A Simple Bi-gram Example: (X, Y): P(X/Y), POS tags for “bears fish” ? • noun * .80 bears noun .02 • Verb * .10 bears verb .02 • STOP noun .50 fish verb .07 • STOP verb .50 fish noun .08 • noun verb .77 • verb noun .65 • noun noun .0001 • nerb verb .0001 Answer • bears: noun • fish: verb The Viterbi Algorithm Input: a sentence x1 . . . xn, parameters q(s|u, v) and e(x|s). Initialization: Set ⇡(0, *, *) = 1 Deﬁnition: S−1 = S0 = {⇤}, Sk = S for k 2 {1 . . . n} Algorithm: I For k = 1 . . . n, I For u 2 Sk−1, v 2 Sk, ⇡(k, u, v) = max w2Sk−2 (⇡(k −1, w, u) ⇥q(v|w, u) ⇥e(xk|v)) I Return maxu2Sn−1,v2Sn (⇡(n, u, v) ⇥q(STOP|u, v)) Forward Sum Sum Pros and Cons I Hidden markov model taggers are very simple to train (just need to compile counts from the training corpus) I Perform relatively well (over 90% performance on named entity recognition) I Main diﬃculty is modeling e(word | tag) can be very diﬃcult if “words” are complex If you already have a labeled training set. Use forward-backward algorithms in the unsupervised setting. •MaxEnt Markov Models (MEMMs) Log-Linear Models for Tagging I We have an input sentence w[1:n] = w1, w2, . . . , wn (wi is the i’th word in the sentence) I We have a tag sequence t[1:n] = t1, t2, . . . , tn (ti is the i’th tag in the sentence) I We’ll use an log-linear model to deﬁne p(t1, t2, . . . , tn|w1, w2, . . . , wn) for any sentence w[1:n] and tag sequence t[1:n] of the same length. (Note: contrast with HMM that deﬁnes p(t1 . . . tn, w1 . . . wn)) I Then the most likely tag sequence for w[1:n] is t⇤ [1:n] = argmaxt[1:n]p(t[1:n]|w[1:n]) How to model p(t[1:n]|w[1:n])? A Trigram Log-Linear Tagger: p(t[1:n]|w[1:n]) = Qn j=1 p(tj | w1 . . . wn, t1 . . . tj−1) Chain rule = Qn j=1 p(tj | w1, . . . , wn, tj−2, tj−1) Independence assumptions I We take t0 = t−1 = * I Independence assumption: each tag only depends on previous two tags p(tj|w1, . . . , wn, t1, . . . , tj−1) = p(tj|w1, . . . , wn, tj−2, tj−1) An Example Hispaniola/NNP quickly/RB became/VB an/DT important/JJ base/?? from which Spain expanded its empire into the rest of the Western Hemisphere . • There are many possible tags in the position ?? Y = {NN, NNS, Vt, Vi, IN, DT, . . . } Representation: Histories I A history is a 4-tuple ht−2, t−1, w[1:n], ii I t−2, t−1 are the previous two tags. I w[1:n] are the n words in the input sentence. I i is the index of the word being tagged I X is the set of all possible histories Hispaniola/NNP quickly/RB became/VB an/DT important/JJ base/?? from which Spain expanded its empire into the rest of the Western Hemisphere . I t−2, t−1 = DT, JJ I w[1:n] = hHispaniola, quickly, became, . . . , Hemisphere, .i I i = 6 Recap: Feature Vector Representations in Log-Linear Models I We have some input domain X, and a ﬁnite label set Y. Aim is to provide a conditional probability p(y | x) for any x 2 X and y 2 Y. I A feature is a function f : X ⇥Y ! R (Often binary features or indicator functions f : X ⇥Y ! {0, 1}). I Say we have m features fk for k = 1 . . . m ) A feature vector f(x, y) 2 Rm for any x 2 X and y 2 Y. An Example (continued) I X is the set of all possible histories of form ht−2, t−1, w[1:n], ii I Y = {NN, NNS, Vt, Vi, IN, DT, . . . } I We have m features fk : X ⇥Y ! R for k = 1 . . . m For example: f1(h, t) = ⇢1 if current word wi is base and t = Vt 0 otherwise f2(h, t) = ⇢1 if current word wi ends in ing and t = VBG 0 otherwise . . . f1(hJJ, DT, h Hispaniola, . . . i, 6i, Vt) = 1 f2(hJJ, DT, h Hispaniola, . . . i, 6i, Vt) = 0 . . . Training the Log-Linear Model I To train a log-linear model, we need a training set (xi, yi) for i = 1 . . . n. Then search for v⇤= argmaxv 0 B B B B @ X i log p(yi|xi; v) | {z } Log−Likelihood −λ 2 X k v2 k | {z } Regularizer 1 C C C C A (see last lecture on log-linear models) I Training set is simply all history/tag pairs seen in the training data The Viterbi Algorithm Problem: for an input w1 . . . wn, ﬁnd arg max t1...tn p(t1 . . . tn | w1 . . . wn) We assume that p takes the form p(t1 . . . tn | w1 . . . wn) = n Y i=1 q(ti|ti−2, ti−1, w[1:n], i) (In our case q(ti|ti−2, ti−1, w[1:n], i) is the estimate from a log-linear model.) The Viterbi Algorithm I Deﬁne n to be the length of the sentence I Deﬁne r(t1 . . . tk) = k Y i=1 q(ti|ti−2, ti−1, w[1:n], i) I Deﬁne a dynamic programming table ⇡(k, u, v) = maximum probability of a tag sequence ending in tags u, v at position k that is, ⇡(k, u, v) = max ht1,...,tk−2i r(t1 . . . tk−2, u, v) A Recursive Deﬁnition Base case: ⇡(0, *, *) = 1 Recursive deﬁnition: For any k 2 {1 . . . n}, for any u 2 Sk−1 and v 2 Sk: ⇡(k, u, v) = max t2Sk−2 ! ⇡(k −1, t, u) ⇥q(v|t, u, w[1:n], k) "" where Sk is the set of possible tags at position k The Viterbi Algorithm with Backpointers Input: a sentence w1 . . . wn, log-linear model that provides q(v|t, u, w[1:n], i) for any tag-trigram t, u, v, for any i 2 {1 . . . n} Initialization: Set ⇡(0, *, *) = 1. Algorithm: I For k = 1 . . . n, I For u 2 Sk−1, v 2 Sk, ⇡(k, u, v) = max t2Sk−2 ! ⇡(k −1, t, u) ⇥q(v|t, u, w[1:n], k) "" bp(k, u, v) = arg max t2Sk−2 ! ⇡(k −1, t, u) ⇥q(v|t, u, w[1:n], k) "" I Set (tn−1, tn) = arg max(u,v) ⇡(n, u, v) I For k = (n −2) . . . 1, tk = bp(k + 2, tk+1, tk+2) I Return the tag sequence t1 . . . tn Summary I Key ideas in log-linear taggers: I Decompose p(t1 . . . tn|w1 . . . wn) = n Y i=1 p(ti|ti−2, ti−1, w1 . . . wn) I Estimate p(ti|ti−2, ti−1, w1 . . . wn) using a log-linear model I For a test sentence w1 . . . wn, use the Viterbi algorithm to ﬁnd arg max t1...tn n Y i=1 p(ti|ti−2, ti−1, w1 . . . wn) ! I Key advantage over HMM taggers: ﬂexibility in the features they can use "
395,"Context Free Grammars Many slides from Michael Collins Overview I An introduction to the parsing problem I Context free grammars I A brief(!) sketch of the syntax of English I Examples of ambiguous structures Parsing (Syntactic Structure) INPUT: Boeing is located in Seattle. OUTPUT: S NP N Boeing VP V is VP V located PP P in NP N Seattle Syntactic Formalisms I Work in formal syntax goes back to Chomsky’s PhD thesis in the 1950s I Examples of current formalisms: minimalism, lexical functional grammar (LFG), head-driven phrase-structure grammar (HPSG), tree adjoining grammars (TAG), categorial grammars Data for Parsing Experiments I Penn WSJ Treebank = 50,000 sentences with associated trees I Usual set-up: 40,000 training sentences, 2400 test sentences An example tree: Canadian NNP Utilities NNPS NP had VBD 1988 CD revenue NN NP of IN C$ $ 1.16 CD billion CD , PUNC, QP NP PP NP mainly RB ADVP from IN its PRP$ natural JJ gas NN and CC electric JJ utility NN businesses NNS NP in IN Alberta NNP , PUNC, NP where WRB WHADVP the DT company NN NP serves VBZ about RB 800,000 CD QP customers NNS . PUNC. NP VP S SBAR NP PP NP PP VP S TOP The Information Conveyed by Parse Trees (1) Part of speech for each word (N = noun, V = verb, DT = determiner) S NP DT the N burglar VP V robbed NP DT the N apartment The Information Conveyed by Parse Trees (continued) (2) Phrases S NP DT the N burglar VP V robbed NP DT the N apartment Noun Phrases (NP): “the burglar”, “the apartment” Verb Phrases (VP): “robbed the apartment” Sentences (S): “the burglar robbed the apartment” The Information Conveyed by Parse Trees (continued) (3) Useful Relationships S NP subject VP V verb S NP DT the N burglar VP V robbed NP DT the N apartment ) “the burglar” is the subject of “robbed” An Example Application: Machine Translation I English word order is subject – verb – object I Japanese word order is subject – object – verb English: IBM bought Lotus Japanese: IBM Lotus bought English: Sources said that IBM bought Lotus yesterday Japanese: Sources yesterday IBM Lotus bought that said S NP-A Sources VP , SBAR-A , S NP yesterday NP-A IBM VP , NP-A Lotus VB bought COMP that VB said Overview I An introduction to the parsing problem I Context free grammars I A brief(!) sketch of the syntax of English I Examples of ambiguous structures Context-Free Grammars Hopcroft and Ullman, 1979 A context free grammar G = (N, ⌃, R, S) where: I N is a set of non-terminal symbols I ⌃is a set of terminal symbols I R is a set of rules of the form X ! Y1Y2 . . . Yn for n ≥0, X 2 N, Yi 2 (N [ ⌃) I S 2 N is a distinguished start symbol A Context-Free Grammar for English N = {S, NP, VP, PP, DT, Vi, Vt, NN, IN} S = S ⌃= {sleeps, saw, man, woman, telescope, the, with, in} R = S ! NP VP VP ! Vi VP ! Vt NP VP ! VP PP NP ! DT NN NP ! NP PP PP ! IN NP Vi ! sleeps Vt ! saw NN ! man NN ! woman NN ! telescope DT ! the IN ! with IN ! in Note: S=sentence, VP=verb phrase, NP=noun phrase, PP=prepositional phrase, DT=determiner, Vi=intransitive verb, Vt=transitive verb, NN=noun, IN=preposition Left-Most Derivations A left-most derivation is a sequence of strings s1 . . . sn, where I s1 = S, the start symbol I sn 2 ⌃⇤, i.e. sn is made up of terminal symbols only I Each si for i = 2 . . . n is derived from si−1 by picking the left-most non-terminal X in si−1 and replacing it by some β where X ! β is a rule in R For example: [S], [NP VP], [D N VP], [the N VP], [the man VP], [the man Vi], [the man sleeps] Representation of a derivation as a tree: S NP D the N man VP Vi sleeps An Example DERIVATION RULES USED S An Example DERIVATION RULES USED S S ! NP VP NP VP An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP DT ! the the N VP An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP DT ! the the N VP N ! dog the dog VP An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP DT ! the the N VP N ! dog the dog VP VP ! VB the dog VB An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP DT ! the the N VP N ! dog the dog VP VP ! VB the dog VB VB ! laughs the dog laughs An Example DERIVATION RULES USED S S ! NP VP NP VP NP ! DT N DT N VP DT ! the the N VP N ! dog the dog VP VP ! VB the dog VB VB ! laughs the dog laughs S NP DT the N dog VP VB laughs Properties of CFGs I A CFG deﬁnes a set of possible derivations I A string s 2 ⌃⇤is in the language deﬁned by the CFG if there is at least one derivation that yields s I Each string in the language generated by the CFG may have more than one derivation (“ambiguity”) An Example of Ambiguity S NP he VP VP VB drove PP IN down NP DT the NN street PP IN in NP DT the NN car An Example of Ambiguity (continued) S NP he VP VB drove PP IN down NP NP DT the NN street PP IN in NP DT the NN car The Problem with Parsing: Ambiguity INPUT: She announced a program to promote safety in trucks and vans + POSSIBLE OUTPUTS: S NP She VP announced NP NP a program VP to promote NP safety PP in NP trucks and vans S NP She VP announced NP NP NP a program VP to promote NP safety PP in NP trucks and NP vans S NP She VP announced NP NP a program VP to promote NP NP safety PP in NP trucks and NP vans S NP She VP announced NP NP a program VP to promote NP safety PP in NP trucks and vans S NP She VP announced NP NP NP a program VP to promote NP safety PP in NP trucks and NP vans S NP She VP announced NP NP NP a program VP to promote NP safety PP in NP trucks and vans And there are more... Overview I An introduction to the parsing problem I Context free grammars I A brief(!) sketch of the syntax of English I Examples of ambiguous structures 2/8/13 9:36 AM A Comprehensive Grammar of the English Language: Randolph Quirk, Si…um, Geoffrey Leech, Jan Svartvik: 9780582517349: Amazon.com: Books Page 1 of 1 http://www.amazon.com/gp/product/images/0582517346/ref=dp_image_0?ie=UTF8&n=283155&s=books A Comprehensive Grammar of the English Language Close Window Product Details (from Amazon) Hardcover: 1779 pages Publisher: Longman; 2nd Revised edition Language: English ISBN-10: 0582517346 ISBN-13: 978-0582517349 Product Dimensions: 8.4 x 2.4 x 10 inches Shipping Weight: 4.6 pounds A Brief Overview of English Syntax Parts of Speech (tags from the Brown corpus): I Nouns NN = singular noun e.g., man, dog, park NNS = plural noun e.g., telescopes, houses, buildings NNP = proper noun e.g., Smith, Gates, IBM I Determiners DT = determiner e.g., the, a, some, every I Adjectives JJ = adjective e.g., red, green, large, idealistic A Fragment of a Noun Phrase Grammar ¯ N ) NN ¯ N ) NN ¯ N ¯ N ) JJ ¯ N ¯ N ) ¯ N ¯ N NP ) DT ¯ N NN ) box NN ) car NN ) mechanic NN ) pigeon DT ) the DT ) a JJ ) fast JJ ) metal JJ ) idealistic JJ ) clay Prepositions, and Prepositional Phrases I Prepositions IN = preposition e.g., of, in, out, beside, as An Extended Grammar ¯ N ) NN ¯ N ) NN ¯ N ¯ N ) JJ ¯ N ¯ N ) ¯ N ¯ N NP ) DT ¯ N PP ) IN NP ¯ N ) ¯ N PP NN ) box NN ) car NN ) mechanic NN ) pigeon DT ) the DT ) a JJ ) fast JJ ) metal JJ ) idealistic JJ ) clay IN ) in IN ) under IN ) of IN ) on IN ) with IN ) as Generates: in a box, under the box, the fast car mechanic under the pigeon in the box, . . . Verbs, Verb Phrases, and Sentences I Basic Verb Types Vi = Intransitive verb e.g., sleeps, walks, laughs Vt = Transitive verb e.g., sees, saw, likes Vd = Ditransitive verb e.g., gave I Basic VP Rules VP ! Vi VP ! Vt NP VP ! Vd NP NP I Basic S Rule S ! NP VP Examples of VP: sleeps, walks, likes the mechanic, gave the mechanic the fast car Examples of S: the man sleeps, the dog walks, the dog gave the mechanic the fast car PPs Modifying Verb Phrases A new rule: VP ! VP PP New examples of VP: sleeps in the car, walks like the mechanic, gave the mechanic the fast car on Tuesday, . . . Complementizers, and SBARs I Complementizers COMP = complementizer e.g., that I SBAR SBAR ! COMP S Examples: that the man sleeps, that the mechanic saw the dog . . . Subordinate clause More Verbs I New Verb Types V[5] e.g., said, reported V[6] e.g., told, informed V[7] e.g., bet I New VP Rules VP ! V[5] SBAR VP ! V[6] NP SBAR VP ! V[7] NP NP SBAR Examples of New VPs: said that the man sleeps told the dog that the mechanic likes the pigeon bet the pigeon $50 that the mechanic owns a fast car Coordination I A New Part-of-Speech: CC = Coordinator e.g., and, or, but I New Rules NP ! NP CC NP ¯ N ! ¯ N CC ¯ N VP ! VP CC VP S ! S CC S SBAR ! SBAR CC SBAR We’ve Only Scratched the Surface... I Agreement The dogs laugh vs. The dog laughs I Wh-movement The dog that the cat liked I Active vs. passive The dog saw the cat vs. The cat was seen by the dog I If you’re interested in reading more: Syntactic Theory: A Formal Introduction, 2nd Edition. Ivan A. Sag, Thomas Wasow, and Emily M. Bender. Long-­‐distance dependency Overview I An introduction to the parsing problem I Context free grammars I A brief(!) sketch of the syntax of English I Examples of ambiguous structures Sources of Ambiguity I Part-of-Speech ambiguity NN ! duck Vi ! duck VP VP Vt saw NP PRP her NN duck PP IN with NP the telescope VP VP V saw S NP her VP Vi duck PP IN with NP the telescope S NP I VP VP Vi drove PP IN down NP DT the NN road PP IN in NP DT the NN car S NP I VP Vi drove PP IN down NP NP DT the NN road PP IN in NP DT the NN car Two analyses for: John was believed to have been shot by Bill With the same set of grammar rules Sources of Ambiguity: Noun Premodiﬁers I Noun premodiﬁers: NP D the ¯ N JJ fast ¯ N NN car ¯ N NN mechanic NP D the ¯ N ¯ N JJ fast ¯ N NN car ¯ N NN mechanic "
396,"Probabilistic Context Free Grammars Many slides from Michael Collins and Chris Manning Overview I Probabilistic Context-Free Grammars (PCFGs) I The CKY Algorithm for parsing with PCFGs A Probabilistic Context-Free Grammar (PCFG) S ) NP VP 1.0 VP ) Vi 0.4 VP ) Vt NP 0.4 VP ) VP PP 0.2 NP ) DT NN 0.3 NP ) NP PP 0.7 PP ) P NP 1.0 Vi ) sleeps 1.0 Vt ) saw 1.0 NN ) man 0.7 NN ) woman 0.2 NN ) telescope 0.1 DT ) the 1.0 IN ) with 0.5 IN ) in 0.5 I Probability of a tree t with rules ↵1 ! β1, ↵2 ! β2, . . . , ↵n ! βn is p(t) = Qn i=1 q(↵i ! βi) where q(↵! β) is the probability for rule ↵! β. DERIVATION RULES USED PROBABILITY S S ! NP VP 1.0 NP VP NP ! DT NN 0.3 DT NN VP DT ! the 1.0 the NN VP NN ! dog 0.1 the dog VP VP ! Vi 0.4 the dog Vi Vi ! laughs 0.5 the dog laughs Properties of PCFGs I Assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG Properties of PCFGs I Assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG I Say we have a sentence s, set of derivations for that sentence is T (s). Then a PCFG assigns a probability p(t) to each member of T (s). i.e., we now have a ranking in order of probability. Properties of PCFGs I Assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG I Say we have a sentence s, set of derivations for that sentence is T (s). Then a PCFG assigns a probability p(t) to each member of T (s). i.e., we now have a ranking in order of probability. I The most likely parse tree for a sentence s is arg max t2T (s) p(t) Data for Parsing Experiments: Treebanks I Penn WSJ Treebank = 50,000 sentences with associated trees I Usual set-up: 40,000 training sentences, 2400 test sentences An example tree: Canadian NNP Utilities NNPS NP had VBD 1988 CD revenue NN NP of IN C$ $ 1.16 CD billion CD , PUNC, QP NP PP NP mainly RB ADVP from IN its PRP$ natural JJ gas NN and CC electric JJ utility NN businesses NNS NP in IN Alberta NNP , PUNC, NP where WRB WHADVP the DT company NN NP serves VBZ about RB 800,000 CD QP customers NNS . PUNC. NP VP S SBAR NP PP NP PP VP S TOP Deriving a PCFG from a Treebank I Given a set of example trees (a treebank), the underlying CFG can simply be all rules seen in the corpus I Maximum Likelihood estimates: qML(↵! β) = Count(↵! β) Count(↵) where the counts are taken from a training set of example trees. I If the training data is generated by a PCFG, then as the training data size goes to inﬁnity, the maximum-likelihood PCFG will converge to the same distribution as the “true” PCFG. Parsing with a PCFG I Given a PCFG and a sentence s, deﬁne T (s) to be the set of trees with s as the yield. I Given a PCFG and a sentence s, how do we ﬁnd arg max t2T (s) p(t) Chomsky Normal Form A context free grammar G = (N, ⌃, R, S) in Chomsky Normal Form is as follows I N is a set of non-terminal symbols I ⌃is a set of terminal symbols I R is a set of rules which take one of two forms: I X ! Y1Y2 for X 2 N, and Y1, Y2 2 N I X ! Y for X 2 N, and Y 2 ⌃ I S 2 N is a distinguished start symbol A Dynamic Programming Algorithm I Given a PCFG and a sentence s, how do we ﬁnd max t2T (s) p(t) I Notation: n = number of words in the sentence wi = i’th word in the sentence N = the set of non-terminals in the grammar S = the start symbol in the grammar I Deﬁne a dynamic programming table ⇡[i, j, X] = maximum probability of a constituent with non-terminal X spanning words i . . . j inclusive I Our goal is to calculate maxt2T (s) p(t) = ⇡[1, n, S] A Dynamic Programming Algorithm I Base case deﬁnition: for all i = 1 . . . n, for X 2 N ⇡[i, i, X] = q(X ! wi) (note: deﬁne q(X ! wi) = 0 if X ! wi is not in the grammar) I Recursive deﬁnition: for all i = 1 . . . n, j = (i + 1) . . . n, X 2 N, ⇡(i, j, X) = max X!Y Z2R, s2{i...(j−1)} (q(X ! Y Z) ⇥⇡(i, s, Y ) ⇥⇡(s + 1, j, Z)) The Full Dynamic Programming Algorithm Input: a sentence s = x1 . . . xn, a PCFG G = (N, ⌃, S, R, q). Initialization: For all i 2 {1 . . . n}, for all X 2 N, ⇡(i, i, X) = ⇢q(X ! xi) if X ! xi 2 R 0 otherwise Algorithm: I For l = 1 . . . (n −1) I For i = 1 . . . (n −l) I Set j = i + l I For all X 2 N, calculate ⇡(i, j, X) = max X!Y Z2R, s2{i...(j−1)} (q(X ! Y Z) ⇥⇡(i, s, Y ) ⇥⇡(s + 1, j, Z)) and bp(i, j, X) = arg max X!Y Z2R, s2{i...(j−1)} (q(X ! Y Z) ⇥⇡(i, s, Y ) ⇥⇡(s + 1, j, Z)) What’s the run time Complexity? ROOT S NP VP N people V NP PP P NP rods with tanks fish N N An example: before binarization… P NP rods N with NP N people tanks fish N VP V NP PP @VP_V ROOT S After binarization… Unary rules: alchemy in the land of treebanks Treebank: empties and unaries ROOT S-HLN NP-SUBJ VP VB -NONE- e Atone PTB Tree ROOT S NP VP VB -NONE- e Atone NoFuncTags ROOT S VP VB Atone NoEmpties ROOT S Atone NoUnaries ROOT VB Atone High Low Extended CKY parsing • Unaries can be incorporated into the algorithm • Messy, but doesn’t increase algorithmic complexity • Empties can be incorporated • Doesn’t increase complexity; essentially like unaries • Binarization is vital • Without binarization, you don’t get parsing cubic in the length of the sentence and in the number of nonterminals in the grammar function CKY(words, grammar) returns [most_probable_parse,prob] score = new double[#(words)+1][#(words)+1][#(nonterms)] back = new Pair[#(words)+1][#(words)+1][#nonterms]] for i=0; i<#(words); i++ for A in nonterms if A -> words[i] in grammar score[i][i+1][A] = P(A -> words[i]) else score[i][i+1][A] = 0 //handle unaries boolean added = true while added added = false for A, B in nonterms if score[i][i+1][B] > 0 && A->B in grammar prob = P(A->B)*score[i][i+1][B] if prob > score[i][i+1][A] score[i][i+1][A] = prob back[i][i+1][A] = B added = true The CKY algorithm (1960/1965) … extended to unaries for span = 2 to #(words) for begin = 0 to #(words)- span end = begin + span for split = begin+1 to end-1 for A,B,C in nonterms prob=score[begin][split][B]*score[split][end][C]*P(A->BC) if prob > score[begin][end][A] score[begin]end][A] = prob back[begin][end][A] = new Triple(split,B,C) //handle unaries boolean added = true while added added = false for A, B in nonterms prob = P(A->B)*score[begin][end][B]; if prob > score[begin][end][A] score[begin][end][A] = prob back[begin][end][A] = B added = true return buildTree(score, back) The CKY algorithm (1960/1965) … extended to unaries CKY Parsing A worked example The grammar: Binary, Unaries, no epsilons, S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 score[0][1] score[1][2] score[2][3] score[3][4] score[0][2] score[1][3] score[2][4] score[0][3] score[1][4] score[0][4] 0 1 2 3 4 1 2 3 4 fish people fish tanks 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 N ® fish 0.2 V ® fish 0.6 N ® people 0.5 V ® people 0.1 N ® fish 0.2 V ® fish 0.6 N ® tanks 0.2 V ® tanks 0.3 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 for i=0; i<#(words); i++ for A in nonterms if A -> words[i] in grammar score[i][i+1][A] = P(A -> words[i]); N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.3 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 // handle unaries boolean added = true while added added = false for A, B in nonterms if score[i][i+1][B] > 0 && A->B in grammar prob = P(A->B)*score[i][i+1][B] if(prob > score[i][i+1][A]) score[i][i+1][A] = prob back[i][i+1][A] = B added = true N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.3 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 NP ® NP NP 0.0049 VP ® V NP 0.105 S ® NP VP 0.00126 NP ® NP NP 0.0049 VP ® V NP 0.007 S ® NP VP 0.0189 NP ® NP NP 0.00196 VP ® V NP 0.042 S ® NP VP 0.00378 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 prob=score[begin][split][B]*score[split][end][C]*P(A->BC) if (prob > score[begin][end][A]) score[begin]end][A] = prob back[begin][end][A] = new Triple(split,B,C) N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.1 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 NP ® NP NP 0.0049 VP ® V NP 0.105 S ® VP 0.0105 NP ® NP NP 0.0049 VP ® V NP 0.007 S ® NP VP 0.0189 NP ® NP NP 0.00196 VP ® V NP 0.042 S ® VP 0.0042 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 //handle unaries boolean added = true while added added = false for A, B in nonterms prob = P(A->B)*score[begin][end][B]; if prob > score[begin][end][A] score[begin][end][A] = prob back[begin][end][A] = B added = true N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.1 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 NP ® NP NP 0.0049 VP ® V NP 0.105 S ® VP 0.0105 NP ® NP NP 0.0049 VP ® V NP 0.007 S ® NP VP 0.0189 NP ® NP NP 0.00196 VP ® V NP 0.042 S ® VP 0.0042 NP ® NP NP 0.0000686 VP ® V NP 0.00147 S ® NP VP 0.000882 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 for split = begin+1 to end-1 for A,B,C in nonterms prob=score[begin][split][B]*score[split][end][C]*P(A->BC) if prob > score[begin][end][A] score[begin]end][A] = prob back[begin][end][A] = new Triple(split,B,C) N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.1 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 NP ® NP NP 0.0049 VP ® V NP 0.105 S ® VP 0.0105 NP ® NP NP 0.0049 VP ® V NP 0.007 S ® NP VP 0.0189 NP ® NP NP 0.00196 VP ® V NP 0.042 S ® VP 0.0042 NP ® NP NP 0.0000686 VP ® V NP 0.00147 S ® NP VP 0.000882 NP ® NP NP 0.0000686 VP ® V NP 0.000098 S ® NP VP 0.01323 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 for split = begin+1 to end-1 for A,B,C in nonterms prob=score[begin][split][B]*score[split][end][C]*P(A->BC) if prob > score[begin][end][A] score[begin]end][A] = prob back[begin][end][A] = new Triple(split,B,C) N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® people 0.5 V ® people 0.1 NP ® N 0.35 VP ® V 0.01 S ® VP 0.001 N ® fish 0.2 V ® fish 0.6 NP ® N 0.14 VP ® V 0.06 S ® VP 0.006 N ® tanks 0.2 V ® tanks 0.1 NP ® N 0.14 VP ® V 0.03 S ® VP 0.003 NP ® NP NP 0.0049 VP ® V NP 0.105 S ® VP 0.0105 NP ® NP NP 0.0049 VP ® V NP 0.007 S ® NP VP 0.0189 NP ® NP NP 0.00196 VP ® V NP 0.042 S ® VP 0.0042 NP ® NP NP 0.0000686 VP ® V NP 0.00147 S ® NP VP 0.000882 NP ® NP NP 0.0000686 VP ® V NP 0.000098 S ® NP VP 0.01323 NP ® NP NP 0.0000009604 VP ® V NP 0.00002058 S ® NP VP 0.00018522 0 1 2 3 4 1 2 3 4 fish people fish tanks S ® NP VP 0.9 S ® VP 0.1 VP ® V NP 0.5 VP ® V 0.1 VP ® V @VP_V 0.3 VP ® V PP 0.1 @VP_V ® NP PP 1.0 NP ® NP NP 0.1 NP ® NP PP 0.2 NP ® N 0.7 PP ® P NP 1.0 N ® people 0.5 N ® fish 0.2 N ® tanks 0.2 N ® rods 0.1 V ® people 0.1 V ® fish 0.6 V ® tanks 0.3 P ® with 1.0 Call buildTree(score, back) to get the best parse for split = begin+1 to end-1 for A,B,C in nonterms prob=score[begin][split][B]*score[split][end][C]*P(A->BC) if prob > score[begin][end][A] score[begin]end][A] = prob back[begin][end][A] = new Triple(split,B,C) Constituency Parser Evaluation Evaluating constituency parsing Evaluating constituency parsing Gold standard brackets: S-(0:11), NP-(0:2), VP-(2:9), VP-(3:9), NP-(4:6), PP-(6-9), NP-(7,9), NP-(9:10) Candidate brackets: S-(0:11), NP-(0:2), VP-(2:10), VP-(3:10), NP-(4:6), PP-(6-10), NP-(7,10) Labeled Precision 3/7 = 42.9% Labeled Recall 3/8 = 37.5% LP/LR F1 40.0% Tagging Accuracy 11/11 = 100.0% Summary I PCFGs augments CFGs by including a probability for each rule in the grammar. I The probability for a parse tree is the product of probabilities for the rules in the tree I To build a PCFG-parsed parser: 1. Learn a PCFG from a treebank 2. Given a test data sentence, use the CKY algorithm to compute the highest probability tree for the sentence under the PCFG How good are PCFGs? • Penn WSJ parsing accuracy: about 73% LP/LR F1 • Robust but not so accurate • Usually admit everything, but with low probability • A PCFG gives some idea of the plausibility of a parse • But not so good because the independence assumptions are too strong • Give a probabilistic language model • But in the simple case it performs worse than a trigram model • The problem seems to be that PCFGs lack the lexicalization of a trigram model "
397,"Dependency Parsing Introduction Many slides are adapted from Chris Manning Dependency syntax postulates that syntactic structure consists of lexical items linked by binary asymmetric relations (“arrows”) called dependencies The arrow connects a head (governor, superior, regent) with a dependent (modifier, inferior, subordinate) Usually, dependencies form a tree (connected, acyclic, single-head) Dependency Grammar and Dependency Structure submitted Bills were Brownback Senator nsubjpass auxpass prep nn immigration conj by cc and ports pobj prep on pobj Republican Kansas pobj prep of appos • A dependency grammar has a notion of a head. Officially, CFGs don’t. • But modern linguistic theory and all modern statistical parsers (Charniak, Collins, Stanford, …) do, via hand-written phrasal “head rules”: • The head of a Noun Phrase is a noun/number/adj/… • The head of a Verb Phrase is a verb/modal/…. • The head rules can be used to extract a dependency parse from a CFG parse • The closure of dependencies give constituency from a dependency tree • But the dependents of a word must be at the same level (i.e., “flat”) – there can be no VP! Relation between phrase structure and dependency structure Methods of Dependency Parsing 1. Dynamic programming (like in the CKY algorithm) You can do it similarly to lexicalized PCFG parsing: an O(n5) algorithm Eisner (1996) gives a clever algorithm that reduces the complexity to O(n3), by producing parse items with heads at the ends rather than in the middle 2. Graph algorithms You create a Maximum Spanning Tree for a sentence McDonald et al.’s (2005) MSTParser scores dependencies independently using a ML classifier (he uses MIRA, for online learning, but it could be MaxEnt) 3. “Deterministic parsing” Greedy choice of attachments guided by machine learning classifiers MaltParser (Nivre et al. 2008) – transition based, shift-reduce What are the sources of information for dependency parsing? 1. Bilexical affinities [issues à the] is plausible 2. Dependency distance mostly with nearby words 3. Intervening material Dependencies rarely span intervening verbs or punctuation 4. Valency of heads How many dependents on which side are usual for a head? ROOT Discussion of the outstanding issues was completed . Dependency Conditioning Preferences • Dependencies from a CFG tree using heads, must be projective • There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. • But dependency theory normally does allow non-projective structures to account for displaced constituents • You can’t easily get the semantics of certain constructions right without these nonprojective dependencies Who did Bill buy the coffee from yesterday ? Projectivity Quiz question! • Consider this sentence: Retail sales drop in April cools afternoon market trading. • Which word are these words a dependent of? 1. sales 2. April 3. afternoon 4. trading Dependency Parsing Introduction Evaluation Evaluation of Dependency Parsing: (labeled) dependency accuracy ROOT She saw the video lecture 0 1 2 3 4 5 Gold 1 2 She nsubj 2 0 saw root 3 5 the det 4 5 video nn 5 2 lecture dobj Parsed 1 2 She nsubj 2 0 saw root 3 4 the det 4 5 video nsubj 5 2 lecture ccomp Acc = # correct deps # of deps UAS = 4 / 5 = 80% LAS = 2 / 5 = 40% Representative performance numbers • The CoNLL-X (2006) shared task provides evaluation numbers for various dependency parsing approaches over 13 languages • Performance varies depending greatly on language/treebank • Here we give a few UAS numbers for English to allow some comparison to constituency parsing Parser UAS% Sagae and Lavie (2006) ensemble of dependency parsers 92.7 Charniak (2000) generative, constituency 92.2 Collins (1999) generative, constituency 91.7 McDonald and Pereira (2005) – MST graph-based dependency 91.5 Yamada and Matsumoto (2003) – transition-based dependency 90.4 Evaluation Dependencies encode relational structure Relation Extraction with Stanford Dependencies Dependency paths identify relations like protein interaction [Erkan et al. EMNLP 07, Fundel et al. 2007] KaiC çnsubj interacts prep_withè SasA KaiC çnsubj interacts prep_withè SasA conj_andè KaiA KaiC çnsubj interacts prep_withè SasA conj_andè KaiB demonstrated results KaiC interacts rythmically nsubj The compl det ccomp that nsubj KaiB KaiA SasA conj_and conj_and advmod prep_with Stanford Dependencies [de Marneffe et al. LREC 2006] • The basic dependency representation is projective • It can be generated by postprocessing headed phrase structure parses (Penn Treebank syntax) • It can also be generated directly by dependency parsers, such as MaltParser jumped boy over the the little prep nsubj det amod pobj fence det Graph modification to facilitate semantic analysis Bell, based in LA, makes and distributes electronic and computer products. makes and nsubj dobj products computer conj cc and electronic amod Bell in prep partmod based pobj LA cc conj distributes Graph modification to facilitate semantic analysis Bell, based in LA, makes and distributes electronic and computer products. makes nsubj dobj products computer conj_and electronic amod Bell prep_in partmod based LA conj_and distributes amod nsubj BioNLP 2009/2011 relation extraction shared tasks [Björne et al. 2009] 0 5 10 15 20 25 30 35 40 45 50 0 1 2 3 4 5 6 7 8 9 10 >10 Dependency distance Linear distance Dependencies encode relational structure Relation Extraction with Stanford Dependencies "
398,"Word Meaning and Similarity Word Senses and Word Relations Slides are adapted from Dan Jurafsky Reminder: lemma and wordform • A lemma or citation form • Same stem, part of speech, rough semantics • A wordform • The “inflected” word as it appears in text Wordform Lemma banks bank sung sing duermes dormir Lemmas have senses • One lemma “bank” can have many meanings: • …a bank can hold the investments in a custodial account… • “…as agriculture burgeons on the east bank the river will shrink even more” • Sense (or word sense) • A discrete representation of an aspect of a word’s meaning. • The lemma bank here has two senses 1 2 Sense 1: Sense 2: Homonymy Homonyms: words that share a form but have unrelated, distinct meanings: • bank1: financial institution, bank2: sloping land • bat1: club for hitting a ball, bat2: nocturnal flying mammal 1. Homographs (bank/bank, bat/bat) 2. Homophones: 1. Write and right 2. Piece and peace Homonymy causes problems for NLP applications • Information retrieval • “bat care” • Machine Translation • bat: murciélago (animal) or bate (for baseball) • Text-to-Speech • bass (stringed instrument) vs. bass (fish) Polysemy • 1. The bank was constructed in 1875 out of local red brick. • 2. I withdrew the money from the bank • Are those the same sense? • Sense 2: “A financial institution” • Sense 1: “The building belonging to a financial institution” • A polysemous word has related meanings • Most non-rare words have multiple meanings • Lots of types of polysemy are systematic • School, university, hospital • All can mean the institution or the building. • A systematic relationship: • Building Organization • Other such kinds of systematic polysemy: Author (Jane Austen wrote Emma) Works of Author (I love Jane Austen) Tree (Plums have beautiful blossoms) Fruit (I ate a preserved plum) Metonymy or Systematic Polysemy: A systematic relationship between senses How do we know when a word has more than one sense? • The “zeugma” test: Two senses of serve? • Which flights serve breakfast? • Does Lufthansa serve Philadelphia? • ?Does Lufthansa serve breakfast and San Jose? • Since this conjunction sounds weird, • we say that these are two different senses of “serve” Synonyms • Word that have the same meaning in some or all contexts. • filbert / hazelnut • couch / sofa • big / large • automobile / car • vomit / throw up • Water / H20 • Two lexemes are synonyms • if they can be substituted for each other in all situations • If so they have the same propositional meaning Synonyms • But there are few (or no) examples of perfect synonymy. • Even if many aspects of meaning are identical • Still may not preserve the acceptability based on notions of politeness, slang, register, genre, etc. • Example: • Water/H20 • Big/large • Brave/courageous Synonymy is a relation between senses rather than words • Consider the words big and large • Are they synonyms? • How big is that plane? • Would I be flying on a large or small plane? • How about here: • Miss Nelson became a kind of big sister to Benjamin. • ?Miss Nelson became a kind of large sister to Benjamin. • Why? • big has a sense that means being older, or grown up • large lacks this sense Antonyms • Senses that are opposites with respect to one feature of meaning • Otherwise, they are very similar! dark/light short/long fast/slow rise/fall hot/cold up/down in/out • More formally: antonyms can • define a binary opposition or be at opposite ends of a scale • long/short, fast/slow • Be reversives: • rise/fall, up/down Hyponymy and Hypernymy • One sense is a hyponym of another if the first sense is more specific, denoting a subclass of the other • car is a hyponym of vehicle • mango is a hyponym of fruit • Conversely hypernym/superordinate (“hyper is super”) • vehicle is a hypernym of car • fruit is a hypernym of mango Superordinate/hyper vehicle fruit furniture Subordinate/hyponym car mango chair Hyponymy more formally • Extensional: • The class denoted by the superordinate extensionally includes the class denoted by the hyponym • Entailment: • A sense A is a hyponym of sense B if being an A entails being a B • Hyponymy is usually transitive • (A hypo B and B hypo C entails A hypo C) • Another name: the IS-A hierarchy • A IS-A B (or A ISA B) • B subsumes A Hyponyms and Instances • WordNet has both classes and instances. • An instance is an individual, a proper noun that is a unique entity • San Francisco is an instance of city • But city is a class • city is a hyponym of municipality...location... 15 Word Meaning and Similarity Word Senses and Word Relations Word Meaning and Similarity WordNet and other Online Thesauri Applications of Thesauri and Ontologies • Information Extraction • Information Retrieval • Question Answering • Bioinformatics and Medical Informatics • Machine Translation WordNet 3.0 • A hierarchically organized lexical database • On-line thesaurus + aspects of a dictionary • Some other languages available or under development • (Arabic, Finnish, German, Portuguese…) Category Unique Strings Noun 117,798 Verb 11,529 Adjective 22,479 Adverb 4,481 Senses of “bass” in Wordnet How is “sense” defined in WordNet? • The synset (synonym set), the set of near-synonyms, instantiates a sense or concept, with a gloss • Example: chump as a noun with the gloss: “a person who is gullible and easy to take advantage of” • This sense of “chump” is shared by 9 words: chump1, fool2, gull1, mark9, patsy1, fall guy1, sucker1, soft touch1, mug2 • Each of these senses have this same gloss • (Not every sense; sense 2 of gull is the aquatic bird) WordNet Hypernym Hierarchy for “bass” WordNet Noun Relations WordNet 3.0 • Where it is: • http://wordnetweb.princeton.edu/perl/webwn • Libraries • Python: WordNet from NLTK • http://www.nltk.org/Home • Java: • JWNL, extJWNL on sourceforge Synset • MeSH (Medical Subject Headings) • 177,000 entry terms that correspond to 26,142 biomedical “headings” • Hemoglobins Entry Terms: Eryhem, Ferrous Hemoglobin, Hemoglobin Definition: The oxygen-carrying proteins of ERYTHROCYTES. They are found in all vertebrates and some invertebrates. The number of globin subunits in the hemoglobin quaternary structure differs between species. Structures range from monomeric to a variety of multimeric arrangements MeSH: Medical Subject Headings thesaurus from the National Library of Medicine The MeSH Hierarchy • a 26 Uses of the MeSH Ontology • Provide synonyms (“entry terms”) • E.g., glucose and dextrose • Provide hypernyms (from the hierarchy) • E.g., glucose ISA monosaccharide • Indexing in MEDLINE/PubMED database • NLM’s bibliographic database: • 20 million journal articles • Each article hand-assigned 10-20 MeSH terms Word Meaning and Similarity WordNet and other Online Thesauri Word Meaning and Similarity Word Similarity: Thesaurus Methods Word Similarity • Synonymy: a binary relation • Two words are either synonymous or not • Similarity (or distance): a looser metric • Two words are more similar if they share more features of meaning • Similarity is properly a relation between senses • The word “bank” is not similar to the word “slope” • Bank1 is similar to fund3 • Bank2 is similar to slope5 • But we’ll compute similarity over both words and senses Why word similarity • Information retrieval • Question answering • Machine translation • Natural language generation • Language modeling • Automatic essay grading • Plagiarism detection • Document clustering Word similarity and word relatedness • We often distinguish word similarity from word relatedness • Similar words: near-synonyms • Related words: can be related any way • car, bicycle: similar • car, gasoline: related, not similar Two classes of similarity algorithms • Thesaurus-based algorithms • Are words “nearby” in hypernym hierarchy? • Do words have similar glosses (definitions)? • Distributional algorithms • Do words have similar distributional contexts? Path based similarity • Two concepts (senses/synsets) are similar if they are near each other in the thesaurus hierarchy • =have a short path between them • concepts have path 1 to themselves Refinements to path-based similarity • pathlen(c1,c2) = 1 + number of edges in the shortest path in the hypernym graph between sense nodes c1 and c2 • ranges from 0 to 1 (identity) • simpath(c1,c2) = • wordsim(w1,w2) = max simpath(c1,c2) c1Îsenses(w1),c2Îsenses(w2) 1 pathlen(c1,c2) Example: path-based similarity simpath(c1,c2) = 1/pathlen(c1,c2) simpath(nickel,coin) = 1/2 = .5 simpath(fund,budget) = 1/2 = .5 simpath(nickel,currency) = 1/4 = .25 simpath(nickel,money) = 1/6 = .17 simpath(coinage,Richter scale) = 1/6 = .17 Problem with basic path-based similarity • Assumes each link represents a uniform distance • But nickel to money seems to us to be closer than nickel to standard • Nodes high in the hierarchy are very abstract • We instead want a metric that • Represents the cost of each edge independently • Words connected only through abstract nodes • are less similar Information content similarity metrics • Let’s define P(c) as: • The probability that a randomly selected word in a corpus is an instance of concept c • Formally: there is a distinct random variable, ranging over words, associated with each concept in the hierarchy • for a given concept, each observed noun is either • a member of that concept with probability P(c) • not a member of that concept with probability 1-P(c) • All words are members of the root node (Entity) • P(root)=1 • The lower a node in hierarchy, the lower its probability Resnik 1995. Using information content to evaluate semantic similarity in a taxonomy. IJCAI Information content similarity • Train by counting in a corpus • Each instance of hill counts toward frequency of natural elevation, geological formation, entity, etc • Let words(c) be the set of all words that are children of node c • words(“geo-formation”) = {hill,ridge,grotto,coast,cave,shore,natural elevation} • words(“natural elevation”) = {hill, ridge} P(c) = count(w) w∈words(c) ∑ N geological-formation shore hill natural elevation coast cave grotto ridge … entity Information content similarity • WordNet hierarchy augmented with probabilities P(c) D. Lin. 1998. An Information-Theoretic Definition of Similarity. ICML 1998 Information content: definitions • Information content: IC(c) = -log P(c) • Most informative subsumer (Lowest common subsumer) LCS(c1,c2) = The most informative (lowest) node in the hierarchy subsuming both c1 and c2 Using information content for similarity: the Resnik method • The similarity between two words is related to their common information • The more two words have in common, the more similar they are • Resnik: measure common information as: • The information content of the most informative (lowest) subsumer (MIS/LCS) of the two nodes • simresnik(c1,c2) = -log P( LCS(c1,c2) ) Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. IJCAI 1995. Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language. JAIR 11, 95-130. Dekang Lin method • Intuition: Similarity between A and B is not just what they have in common • The more differences between A and B, the less similar they are: • Commonality: the more A and B have in common, the more similar they are • Difference: the more differences between A and B, the less similar • Commonality: IC(common(A,B)) • Difference: IC(description(A,B)）-IC(common(A,B) Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. ICML Dekang Lin similarity theorem • The similarity between A and B is measured by the ratio between the amount of information needed to state the commonality of A and B and the information needed to fully describe what A and B are simLin(A, B)∝ IC(common(A, B)) IC(description(A, B)) • Lin (altering Resnik) defines IC(common(A,B)) as 2 x information of the LCS simLin(c1,c2) = 2logP(LCS(c1,c2)) logP(c1)+ logP(c2) Lin similarity function simLin(A, B) = 2logP(LCS(c1,c2)) logP(c1)+ logP(c2) simLin(hill,coast) = 2logP(geological-formation) logP(hill)+ logP(coast) = 2ln0.00176 ln0.0000189 + ln0.0000216 =.59 The (extended) Lesk Algorithm • A thesaurus-based measure that looks at glosses • Two concepts are similar if their glosses contain similar words • Drawing paper: paper that is specially prepared for use in drafting • Decal: the art of transferring designs from specially prepared paper to a wood or glass or metal surface • For each n-word phrase that’s in both glosses • Add a score of n2 • Paper and specially prepared for 1 + 22 = 5 • Compute overlap also for other relations • glosses of hypernyms and hyponyms Summary: thesaurus-based similarity simpath(c1,c2) = 1 pathlen(c1,c2) simresnik(c1,c2) = −logP(LCS(c1,c2)) simlin(c1,c2) = 2logP(LCS(c1,c2)) logP(c1)+ logP(c2) sim jiangconrath(c1,c2) = 1 logP(c1)+ logP(c2)−2logP(LCS(c1,c2)) simeLesk(c1,c2) = overlap(gloss(r(c1)),gloss(q(c2))) r,q∈RELS ∑ Libraries for computing thesaurus-based similarity • NLTK • http://nltk.github.com/api/nltk.corpus.reader.html?highlight=similarity - nltk.corpus.reader.WordNetCorpusReader.res_similarity • WordNet::Similarity • http://wn-similarity.sourceforge.net/ • Web-based interface: • http://marimba.d.umn.edu/cgi-bin/similarity/similarity.cgi 48 Evaluating similarity • Extrinsic (task-based, end-to-end) Evaluation: • Question Answering • Spell Checking • Essay grading • Intrinsic Evaluation: • Correlation between algorithm and human word similarity ratings • Wordsim353: 353 noun pairs rated 0-10. sim(plane,car)=5.77 • Taking TOEFL multiple-choice vocabulary tests • Levied is closest in meaning to: imposed, believed, requested, correlated Word Meaning and Similarity Word Similarity: Thesaurus Methods "
399,"Word Meaning and Similarity Word Similarity: Distributional Similarity (I) Problems with thesaurus-based meaning • We don’t have a thesaurus for every language • Even if we do, they have problems with recall • Many words are missing • Most (if not all) phrases are missing • Some connections between senses are missing • Thesauri work less well for verbs, adjectives • Adjectives and verbs have less structured hyponymy relations Distributional models of meaning • Also called vector-space models of meaning • Offer much higher recall than hand-built thesauri • Although they tend to have lower precision • Zellig Harris (1954): “oculist and eye-doctor … occur in almost the same environments…. If A and B have almost identical environments we say that they are synonyms. • Firth (1957): “You shall know a word by the company it keeps!” 3 • Also called vector-space models of meaning • Offer much higher recall than hand-built thesauri • Although they tend to have lower precision Intuition of distributional word similarity • Nida example: A bottle of tesgüino is on the table Everybody likes tesgüino Tesgüino makes you drunk We make tesgüino out of corn. • From context words humans can guess tesgüino means • an alcoholic beverage like beer • Intuition for algorithm: • Two words are similar if they have similar word contexts. As#You#Like#It Twelfth#Night Julius#Caesar Henry#V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 Reminder: Term-document matrix • Each cell: count of term t in a document d: tft,d: • Each document is a count vector in ℕv: a column below 5 Reminder: Term-document matrix • Two documents are similar if their vectors are similar 6 As#You#Like#It Twelfth#Night Julius#Caesar Henry#V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The words in a term-document matrix • Each word is a count vector in ℕD: a row below 7 As#You#Like#It Twelfth#Night Julius#Caesar Henry#V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The words in a term-document matrix • Two words are similar if their vectors are similar 8 As#You#Like#It Twelfth#Night Julius#Caesar Henry#V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The Term-Context matrix • Instead of using entire documents, use smaller contexts • Paragraph • Window of 10 words • A word is now defined by a vector over counts of context words 9 Sample contexts: 20 words (Brown corpus) • equal amount of sugar, a sliced lemon, a tablespoonful of apricot preserve or jam, a pinch each of clove and nutmeg, • on board for their enjoyment. Cautiously she sampled her first pineapple and another fruit whose taste she likened to that of 10 • of a recursive type well suited to programming on the digital computer. In finding the optimal R-stage policy from that of • substantially affect commerce, for the purpose of gathering data and information necessary for the study authorized in the first section of this Term-context matrix for word similarity • Two words are similar in meaning if their context vectors are similar 11 aardvark computer data pinch result sugar … apricot 0 0 0 1 0 1 pineapple 0 0 0 1 0 1 digital 0 2 1 0 1 0 information 0 1 6 0 4 0 Should we use raw counts? • For the term-document matrix • We used tf-idf instead of raw term counts • For the term-context matrix • Positive Pointwise Mutual Information (PPMI) is common 12 Pointwise Mutual Information • Pointwise mutual information: • Do events x and y co-occur more than if they were independent? • PMI between two words: (Church & Hanks 1989) • Do words x and y co-occur more than if they were independent? • Positive PMI between two words (Niwa & Nitta 1994) • Replace all PMI values less than 0 with zero PMI(X,Y) = log2 P(x,y) P(x)P(y) PMI(word1,word2) = log2 P(word1,word2) P(word1)P(word2) Computing PPMI on a term-context matrix • Matrix F with W rows (words) and C columns (contexts) • fij is # of times wi occurs in context cj 14 pij = fij fij j=1 C ∑ i=1 W ∑ pi* = fij j=1 C ∑ fij j=1 C ∑ i=1 W ∑ p* j = fij i=1 W ∑ fij j=1 C ∑ i=1 W ∑ pmiij = log2 pij pi*p* j ppmiij = pmiij if pmiij > 0 0 otherwise ! "" # $ # p(w=information,c=data) = p(w=information) = p(c=data) = 15 p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 = .32 6/19 11/19 = .58 7/19 = .37 pij = fij fij j=1 C ∑ i=1 W ∑ p(wi) = fij j=1 C ∑ N p(cj) = fij i=1 W ∑ N 16 pmiij = log2 pij pi*p* j • pmi(information,data) = log2 ( p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 PPMI(w,context) computer data pinch result sugar apricot 1 1 2.25 1 2.25 pineapple 1 1 2.25 1 2.25 digital 1.66 0.00 1 0.00 1 information 0.00 0.57 1 0.47 1 .32 / (.37*.58) ) = .57 Weighing PMI • PMI is biased toward infrequent events • Various weighting schemes help alleviate this • See Turney and Pantel (2010) • Add-one smoothing can also help 17 18 Add#2%Smoothed%Count(w,context) computer data pinch result sugar apricot 2 2 3 2 3 pineapple 2 2 3 2 3 digital 4 3 2 3 2 information 3 8 2 6 2 p(w,context),[add02] p(w) computer data pinch result sugar apricot 0.03 0.03 0.05 0.03 0.05 0.20 pineapple 0.03 0.03 0.05 0.03 0.05 0.20 digital 0.07 0.05 0.03 0.05 0.03 0.24 information 0.05 0.14 0.03 0.10 0.03 0.36 p(context) 0.19 0.25 0.17 0.22 0.17 19 PPMI(w,context).[add22] computer data pinch result sugar apricot 0.00 0.00 0.56 0.00 0.56 pineapple 0.00 0.00 0.56 0.00 0.56 digital 0.62 0.00 0.00 0.00 0.00 information 0.00 0.58 0.00 0.37 0.00 PPMI(w,context) computer data pinch result sugar apricot 1 1 2.25 1 2.25 pineapple 1 1 2.25 1 2.25 digital 1.66 0.00 1 0.00 1 information 0.00 0.57 1 0.47 1 Using syntax to define a word’s context • Zellig Harris (1968) • “The meaning of entities, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities” • Two words are similar if they have similar parse contexts • Duty and responsibility (Chris Callison-Burch’s example) Modified by adjectives additional, administrative, assumed, collective, congressional, constitutional … Objects of verbs assert, assign, assume, attend to, avoid, become, breach … Co-occurrence vectors based on syntactic dependencies • The contexts C are different dependency relations • Subject-of- “absorb” • Prepositional-object of “inside” • Counts for the word cell: Dekang Lin, 1998 “Automatic Retrieval and Clustering of Similar Words” PMI applied to dependency relations • “Drink it” more common than “drink wine” • But “wine” is a better “drinkable” thing than “it” Object of “drink” Count PMI it 3 1.3 anything 3 5.2 wine 2 9.3 tea 2 11.8 liquid 2 10.5 Hindle, Don. 1990. Noun Classification from Predicate-Argument Structure. ACL Object of “drink” Count PMI tea 2 11.8 liquid 2 10.5 wine 2 9.3 anything 3 5.2 it 3 1.3 Word Meaning and Similarity Word Similarity: Distributional Similarity (I) Word Meaning and Similarity Word Similarity: Distributional Similarity (II) Reminder: cosine for computing similarity cos( v,  w) =  v •  w  v  w =  v  v •  w  w = viwi i=1 N ∑ vi 2 i=1 N ∑ wi 2 i=1 N ∑ Dot product Unit vectors vi is the PPMI value for word v in context i wi is the PPMI value for word w in context i. Cos(v,w) is the cosine similarity of v and w Sec. 6.3 Cosine as a similarity metric • -1: vectors point in opposite directions • +1: vectors point in same directions • 0: vectors are orthogonal • Raw frequency or PPMI are non- negative, so cosine range 0-1 26 large data computer apricot 1 0 0 digital 0 1 2 information 1 6 1 27 Which pair of words is more similar? cosine(apricot,information) = cosine(digital,information) = cosine(apricot,digital) = cos( v,  w) =  v •  w  v  w =  v  v •  w  w = viwi i=1 N ∑ vi 2 i=1 N ∑ wi 2 i=1 N ∑ 1+ 0 + 0 1+ 0 + 0 1+36 +1 1+36 +1 0 +1+ 4 0 +1+ 4 1+ 0 + 0 0 + 6 + 2 0 + 0 + 0 = 1 38 =.16 = 8 38 5 =.58 = 0 Other possible similarity measures D: KL Divergence Word Meaning and Similarity Word Similarity: Distributional Similarity (II) "
4,"School of Computer Science Probabilistic Graphical Models Exact Inference: Variable Elimination Eric Xing Lecture 4, January 27, 2014 Reading: KF-chap 9 E F H A E F B A C E G A D C E A D C B A A h m g m e m f m b m c m d m E F H E F H E F H A E F A E F A E F B A C B A C B A C E G E G E G A D C E A D C E A D C E A D C A D C B A B A A h m g m e m f m b m c m d m h m g m e m f m b m c m d m © Eric Xing @ CMU, 2005-2014 1 Recap: Defn: A DAG G is a perfect map (P-map) for a distribution P if I(P)I(G). © Eric Xing @ CMU, 2005-2014 2 Question: Is there a BN that is a perfect map for a given MN?  The ""diamond"" MN A B D C © Eric Xing @ CMU, 2005-2014 3 This MN does not have a perfect I-map as BN! Question: Is there a BN that is a perfect map for a given MN? A C | {B,D} B D | {A,C} A C | {B,D} B D | A A C | {B,D} B D A B D C A B D C A B D C © Eric Xing @ CMU, 2005-2014 4 V-structure example A B C Question: Is there an MN that is a perfect I-map to a given BN? © Eric Xing @ CMU, 2005-2014 5 V-structure has no equivalent in MNs! A B (A B | C) A B | C (A B) (A B |C) (A B) A B C A B C A B C Question: Is there an MN that is a perfect I-map to a given BN? © Eric Xing @ CMU, 2005-2014 6  Also called chain graphs  Nodes can be disjointly partitioned into several chain components  An edge within the same chain component must be undirected  An edge between two nodes in different chain components must be directed Chain components: {A}, {B}, {C,D,E},{F,G},{H}, {I} Partially Directed Acyclic Graphs © Eric Xing @ CMU, 2005-2014 7 Investigated the relationship between BNs and MNs  They represent different families of independence assumptions Not mentioned: Chain networks superset of both BNs and MNs Why we care about this:  BN and MN offer different semantics for designer to capture or expression (conditional) independences among variables  Under certain condition BN can be represented as an MN and vice versa  In the future, for certain operation (i.e., inference), we will be using a single representation as the “data structure” for which an algorithm can operate on.  This makes algorithm design, and analysis of the algorithms simpler Summary © Eric Xing @ CMU, 2005-2014 8 Probabilistic Inference and Learning  We now have compact representations of probability distributions: Graphical Models  A GM M describes a unique probability distribution P  Typical tasks:  Task 1: How do we answer queries about PM, e.g., PM(X|Y) ?  We use inference as a name for the process of computing answers to such queries  Task 2: How do we estimate a plausible model M from data D? i. We use learning as a name for the process of obtaining point estimate of M. ii. But for Bayesian, they seek p(M |D), which is actually an inference problem. iii. When not all variables are observable, even computing point estimate of M need to do inference to impute the missing data. © Eric Xing @ CMU, 2005-2014 9   1 1 x x k k , ,x , x P P ) ( ) ( e e   Query 1: Likelihood Most of the queries one may ask involve evidence  Evidence e is an assignment of values to a set E variables in the domain  Without loss of generality E = { Xk+1, …, Xn } Simplest query: compute probability of evidence  this is often referred to as computing the likelihood of e © Eric Xing @ CMU, 2005-2014 10     x x, X P X, P P X, P X P ) ( ) ( ) ( ) ( ) | ( e e e e e    z e z Z Y Y ) | ( ) | ( , P e P Query 2: Conditional Probability Often we are interested in the conditional probability distribution of a variable given the evidence  this is the a posteriori belief in X, given evidence e We usually query a subset Y of all domain variables X={Y,Z} and ""don't care"" about the remaining, Z:  the process of summing out the ""don't care"" variables z is called marginalization, and the resulting P(y|e) is called a marginal prob. © Eric Xing @ CMU, 2005-2014 11 A C B A C B ? ? Applications of a posteriori Belief  Prediction: what is the probability of an outcome given the starting condition  the query node is a descendent of the evidence  Diagnosis: what is the probability of disease/fault given symptoms  the query node an ancestor of the evidence  Learning under partial observation  fill in the unobserved values under an ""EM"" setting (more later)  The directionality of information flow between variables is not restricted by the directionality of the edges in a GM  probabilistic inference can combine evidence form all parts of the network © Eric Xing @ CMU, 2005-2014 12 2 W 3 W 1 W visible nodes (data) V H1 H2 H3 Example: Deep Belief Network Deep Belief Network (DBN) [Hinton et al., 2006]  Generative model with multiple hidden layers  Successful applications  Recognizing handwritten digits  Learning motion capture data  Collaborative filtering © Eric Xing @ CMU, 2005-2014 13 In this query we want to find the most probable joint assignment (MPA) for some variables of interest Such reasoning is usually performed under some given evidence e, and ignoring (the values of) other variables z :  this is the maximum a posteriori configuration of y.      z y y e z y e y e Y ) | , ( max arg ) | ( max arg ) | ( MPA P P Y Y Query 3: Most Probable Assignment © Eric Xing @ CMU, 2005-2014 14 Applications of MPA Classification  find most likely label, given the evidence Explanation  what is the most likely scenario, given the evidence Cautionary note: The MPA of a variable depends on its ""context""---the set of variables been jointly queried Example:  MPA of Y1 ?  MPA of (Y1, Y2) ? y 1 y 2 P(y 1 ,y 2 ) 0 0 0.35 0 1 0.05 1 0 0.3 1 1 0.3 © Eric Xing @ CMU, 2005-2014 15 Thm: Computing P(X = x | e) in a GM is NP-hard Hardness does not mean we cannot solve inference  It implies that we cannot find a general procedure that works efficiently for arbitrary GMs  For particular families of GMs, we can have provably efficient procedures Complexity of Inference © Eric Xing @ CMU, 2005-2014 16 Approaches to inference Exact inference algorithms  The elimination algorithm  Message-passing algorithm (sum-product, belief propagation)  The junction tree algorithms Approximate inference techniques  Stochastic simulation / sampling methods  Markov chain Monte Carlo methods  Variational algorithms © Eric Xing @ CMU, 2005-2014 17 A signal transduction pathway: Query: P(e) By chain decomposition, we get A B C E D     d c b a d c b a d e P c d P b c P a b P a P e) P(a,b,c,d, e P ) | ( ) | ( ) | ( ) | ( ) ( ) ( a naïve summation needs to enumerate over an exponential number of terms What is the likelihood that protein E is active? Marginalization and Elimination © Eric Xing @ CMU, 2005-2014 18 A B C E D      d c b a d c b a a b P a P d e P c d P b c P d e P c d P b c P a b P a P e P ) | ( ) ( ) | ( ) | ( ) | ( ) | ( ) | ( ) | ( ) | ( ) ( ) ( Elimination on Chains Rearranging terms ... © Eric Xing @ CMU, 2005-2014 19 Now we can perform innermost summation This summation ""eliminates"" one variable from our summation argument at a ""local cost"". A B C E D X      d c b d c b a b p d e P c d P b c P a b P a P d e P c d P b c P e P ) ( ) | ( ) | ( ) | ( ) | ( ) ( ) | ( ) | ( ) | ( ) ( Elimination on Chains © Eric Xing @ CMU, 2005-2014 20 A B C E D        d c d c b d c b c p d e P c d P b p b c P d e P c d P b p d e P c d P b c P e P ) ( ) | ( ) | ( ) ( ) | ( ) | ( ) | ( ) ( ) | ( ) | ( ) | ( ) ( X X Elimination in Chains Rearranging and then summing again, we get © Eric Xing @ CMU, 2005-2014 21 Eliminate nodes one by one all the way to the end, we get Complexity:  Each step costs O(|Val(Xi)|*|Val(Xi+1)|) operations: O(kn2)  Compare to naïve evaluation that sums over joint values of n-1 variables O(nk) A B C E D   d d p d e P e P ) ( ) | ( ) ( X X X X Elimination in Chains © Eric Xing @ CMU, 2005-2014 22 Hidden Markov Model p(x, y) = p(x1……xT, y1, ……, yT) = p(y1) p(x1 | y1) p(y2 | y1) p(x2 | y2) … p(yT | yT-1) p(xT | yT) Conditional probability: A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... © Eric Xing @ CMU, 2005-2014 23 Hidden Markov Model Conditional probability: A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... © Eric Xing @ CMU, 2005-2014 24 Rearranging terms ... A B C E D Undirected Chains        d c b a d c b a a b d e c d b c Z d e c d b c a b Z e P ) , ( ) , ( ) , ( ) , ( ) , ( ) , ( ) , ( ) , ( ) (         1 1 © Eric Xing @ CMU, 2005-2014 25 Conditional Random Fields Y1 Y2 Y5 … X1 … Xn © Eric Xing @ CMU, 2005-2014 26 The Sum-Product Operation In general, we can view the task at hand as that of computing the value of an expression of the form: where F is a set of factors We call this task the sum-product inference task.   z F   © Eric Xing @ CMU, 2005-2014 27 General idea:  Write query in the form this suggests an ""elimination order"" of latent variables to be marginalized  Iteratively Move all irrelevant terms outside of innermost sum Perform innermost sum, getting a new term Insert the new term into the product  wrap-up    n x x x i i i pa x P X P 3 2 ) | ( ) , ( 1  e   1 1 1 1 x X X X P ) , ( ) , ( ) | ( e e e   Inference on General GM via Variable Elimination © Eric Xing @ CMU, 2005-2014 28 Outcome of elimination  Let X be some set of variables, let F be a set of factors such that for each F , Scope[] X, let Y X be a set of query variables, and let Z = X−Y be the variable to be eliminated  The result of eliminating the variable Z is a factor  This factor does not necessarily correspond to any probability or conditional probability in this network. (example forthcoming)    z Y F    ) ( © Eric Xing @ CMU, 2005-2014 29 Dealing with evidence Conditioning as a Sum-Product Operation  The evidence potential:  Total evidence potential:  Introducing evidence --- restricted factors:       i i i i i i e E e E e E if 0 if ) , ( 1      e z e E e Y , ) , ( ) , ( F        E e E I i i i e E ) , ( ) , (   © Eric Xing @ CMU, 2005-2014 30 The elimination algorithm Procedure Elimination ( G, // the GM E, // evidence Z, // Set of variables to be eliminated X, // query variable(s) ) 1. Initialize (G) 2. Evidence (E) 3. Sum-Product-Elimination (F, Z, ≺) 4. Normalization (F) © Eric Xing @ CMU, 2005-2014 31 The elimination algorithm Procedure Initialize (G, Z) 1. Let Z1, . . . ,Zk be an ordering of Z such that Zi ≺Zj iff i < j 2. Initialize F with the full the set of factors Procedure Evidence (E) 1. for each iE , F =F (Ei, ei) Procedure Sum-Product-Variable- Elimination (F, Z, ≺) 1. for i = 1, . . . , k F ← Sum-Product-Eliminate-Var(F, Zi) 2. ∗← F  3. return ∗ 4. Normalization (∗) © Eric Xing @ CMU, 2005-2014 32 The elimination algorithm Procedure Normalization (∗) 1. P(X|E)=∗(X)/x∗(X) Procedure Sum-Product-Eliminate-Var ( F, // Set of factors Z // Variable to be eliminated ) 1. F ′ ← {F : Z Scope[]} 2. F ′′ ← F − F ′  ←F ′   ← Z  5. return F ′′ {} Procedure Initialize (G, Z) 1. Let Z1, . . . ,Zk be an ordering of Z such that Zi ≺Zj iff i < j 2. Initialize F with the full the set of factors Procedure Evidence (E) 1. for each iE , F =F (Ei, ei) Procedure Sum-Product-Variable- Elimination (F, Z, ≺) 1. for i = 1, . . . , k F ← Sum-Product-Eliminate-Var(F, Zi) 2. ∗← F  3. return ∗ 4. Normalization (∗) © Eric Xing @ CMU, 2005-2014 33 B A D C E F G H A food web What is the probability that hawks are leaving given that the grass condition is poor? A more complex network © Eric Xing @ CMU, 2005-2014 34  Query: P(A |h)  Need to eliminate: B,C,D,E,F,G,H  Initial factors:  Choose an elimination order: H,G,F,E,D,C,B  Step 1:  Conditioning (fix the evidence node (i.e., h) on its observed value (i.e., )):  This step is isomorphic to a marginalization step: B A D C E F G H ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( f e h P e g P a f P d c e P a d P b c P b P a P ) , | ~ ( ) , ( f e h h p f e mh   h ~    h h h h f e h p f e m ) ~ ( ) , | ( ) , (  B A D C E F G Example: Variable Elimination © Eric Xing @ CMU, 2005-2014 35  Query: P(B |h)  Need to eliminate: B,C,D,E,F,G  Initial factors:  Step 2: Eliminate G  compute B A D C E F G H ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( f e m e g P a f P d c e P a d P b c P b P a P f e h P e g P a f P d c e P a d P b c P b P a P h  1 ) | ( ) (   g g e g p e m B A D C E F ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( f e m a f P d c e P a d P b c P b P a P f e m e m a f P d c e P a d P b c P b P a P h h g   Example: Variable Elimination © Eric Xing @ CMU, 2005-2014 36  Query: P(B |h)  Need to eliminate: B,C,D,E,F  Initial factors:  Step 3: Eliminate F  compute B A D C E F G H Example: Variable Elimination ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( f e m a f P d c e P a d P b c P b P a P f e m e g P a f P d c e P a d P b c P b P a P f e h P e g P a f P d c e P a d P b c P b P a P h h     f h f f e m a f p a e m ) , ( ) | ( ) , ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( e a m d c e P a d P b c P b P a P f  B A D C E © Eric Xing @ CMU, 2005-2014 37 B A D C E  Query: P(B |h)  Need to eliminate: B,C,D,E  Initial factors:  Step 4: Eliminate E  compute B A D C E F G H Example: Variable Elimination ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( e a m d c e P a d P b c P b P a P f e m a f P d c e P a d P b c P b P a P f e m e g P a f P d c e P a d P b c P b P a P f e h P e g P a f P d c e P a d P b c P b P a P f h h      e f e e a m d c e p d c a m ) , ( ) , | ( ) , , ( ) , , ( ) | ( ) | ( ) ( ) ( d c a m a d P b c P b P a P e  B A D C © Eric Xing @ CMU, 2005-2014 38  Query: P(B |h)  Need to eliminate: B,C,D  Initial factors:  Step 5: Eliminate D  compute B A D C E F G H Example: Variable Elimination ) , , ( ) | ( ) | ( ) ( ) ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( d c a m a d P b c P b P a P e a m d c e P a d P b c P b P a P f e m a f P d c e P a d P b c P b P a P f e m e g P a f P d c e P a d P b c P b P a P f e h P e g P a f P d c e P a d P b c P b P a P e f h h       d e d d c a m a d p c a m ) , , ( ) | ( ) , ( ) , ( ) | ( ) ( ) ( c a m d c P b P a P d  B A C © Eric Xing @ CMU, 2005-2014 39  Query: P(B |h)  Need to eliminate: B,C  Initial factors:  Step 6: Eliminate C  compute B A D C E F G H Example: Variable Elimination ) , ( ) | ( ) ( ) ( c a m d c P b P a P d    c d c c a m b c p b a m ) , ( ) | ( ) , ( ) , ( ) | ( ) ( ) ( ) , , ( ) | ( ) | ( ) ( ) ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( c a m d c P b P a P d c a m a d P d c P b P a P e a m d c e P a d P d c P b P a P f e m a f P d c e P a d P d c P b P a P f e m e g P a f P d c e P a d P d c P b P a P f e h P e g P a f P d c e P a d P d c P b P a P d e f h h      B A © Eric Xing @ CMU, 2005-2014 40  Query: P(B |h)  Need to eliminate: B  Initial factors:  Step 7: Eliminate B  compute B A D C E F G H Example: Variable Elimination ) , ( ) ( ) ( ) , ( ) | ( ) ( ) ( ) , , ( ) | ( ) | ( ) ( ) ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( b a m b P a P c a m d c P b P a P d c a m a d P d c P b P a P e a m d c e P a d P d c P b P a P f e m a f P d c e P a d P d c P b P a P f e m e g P a f P d c e P a d P d c P b P a P f e h P e g P a f P d c e P a d P d c P b P a P c d e f h h         b c b b a m b p a m ) , ( ) ( ) ( ) ( ) ( a m a P b  A © Eric Xing @ CMU, 2005-2014 41  Query: P(B |h)  Need to eliminate: B  Initial factors:  Step 8: Wrap-up B A D C E F G H Example: Variable Elimination ) ( ) ( ) , ( ) ( ) ( ) , ( ) | ( ) ( ) ( ) , , ( ) | ( ) | ( ) ( ) ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( a m a P b a m b P a P c a m d c P b P a P d c a m a d P d c P b P a P e a m d c e P a d P d c P b P a P f e m a f P d c e P a d P d c P b P a P f e m e g P a f P d c e P a d P d c P b P a P f e h P e g P a f P d c e P a d P d c P b P a P b c d e f h h        , ) ( ) ( ) ~ , ( a m a p h a p b     a b b a m a p a m a p h a P ) ( ) ( ) ( ) ( ) ~ | (   a b a m a p h p ) ( ) ( ) ~ ( © Eric Xing @ CMU, 2005-2014 42 Suppose in one elimination step we compute This requires  multiplications  For each value for x, y1, …, yk, we do k multiplications  additions  For each value of y1, …, yk , we do |Val(X)| additions Complexity is exponential in number of variables in the intermediate factor Complexity of variable elimination    i Ci X k ) Val( ) Val( Y   i Ci X ) Val( ) Val( Y   x k x k x y y x m y y m ) , , , ( ' ) , , ( 1 1      k i c i k x i x m y y x m 1 1 ) , ( ) , , , ( ' y  © Eric Xing @ CMU, 2005-2014 43  A graph elimination algorithm moralization B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A graph elimination Understanding Variable Elimination © Eric Xing @ CMU, 2005-2014 44 Graph elimination Begin with the undirected GM or moralized BN Graph G(V, E) and elimination ordering I Eliminate next node in the ordering I  Removing the node from the graph  Connecting the remaining neighbors of the nodes The reconstituted graph G'(V, E')  Retain the edges that were created during the elimination procedure  The graph-theoretic property: the factors resulted during variable elimination are captured by recording the elimination clique © Eric Xing @ CMU, 2005-2014 45  A graph elimination algorithm  Intermediate terms correspond to the cliques resulted from elimination moralization B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A graph elimination Understanding Variable Elimination E F H A E F B A C E G A D C E A D C B A A © Eric Xing @ CMU, 2005-2014 46 Elimination Cliques B A D C E F G H B A D C E F G H B A D C E F G B A D C E F G H B A D C B A D C E F B A D C E B A C B A A ) , ( f e mh ) (e mg ) , ( a e m f ) , , ( d c a me ) , ( c a md ) , ( b a mc ) (a mb © Eric Xing @ CMU, 2005-2014 47 Graph elimination and marginalization Induced dependency during marginalization vs. elimination clique  Summation <-> elimination  Intermediate term <-> elimination clique E F H A E F B A C E G A D C E A D C B A A ) ( ) ( ) , ( ) ( ) ( ) , ( ) | ( ) ( ) ( ) , , ( ) | ( ) | ( ) ( ) ( ) , ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( ) , | ( ) | ( ) | ( ) , | ( ) | ( ) | ( ) ( ) ( a m a P b a m b P a P c a m d c P b P a P d c a m a d P d c P b P a P e a m d c e P a d P d c P b P a P f e m a f P d c e P a d P d c P b P a P f e m e g P a f P d c e P a d P d c P b P a P f e h P e g P a f P d c e P a d P d c P b P a P b c d e f h h        © Eric Xing @ CMU, 2005-2014 48 A clique tree E F H A E F B A C E G A D C E A D C B A A h m g m e m f m b m c m d m   e f g e e a m e m d c e p d c a m ) , ( ) ( ) , | ( ) , , ( © Eric Xing @ CMU, 2005-2014 49 Complexity The overall complexity is determined by the number of the largest elimination clique  What is the largest elimination clique? – a pure graph theoretic question  Tree-width k: one less than the smallest achievable value of the cardinality of the largest elimination clique, ranging over all possible elimination ordering  “good” elimination orderings lead to small cliques and hence reduce complexity (what will happen if we eliminate ""e"" first in the above graph?)  Find the best elimination ordering of a graph --- NP-hard Inference is NP-hard  But there often exist ""obvious"" optimal or near-opt elimination ordering © Eric Xing @ CMU, 2005-2014 50 Examples Star Tree © Eric Xing @ CMU, 2005-2014 51 More example: Ising model © Eric Xing @ CMU, 2005-2014 52 Limitation of Procedure Elimination Limitation B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A © Eric Xing @ CMU, 2005-2014 53  Our algorithm so far answers only one query (e.g., on one node), do we need to do a complete elimination for every such query?  Elimination message passing on a clique tree  Messages can be reused E F H A E F B A C E G A D C E A D C B A A h m g m e m f m b m c m d m B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A  From Elimination to Message Passing   e f g e e a m e m d c e p d c a m ) , ( ) ( ) , | ( ) , , ( © Eric Xing @ CMU, 2005-2014 54 E F H A E F B A C E G A D C E A D C B A A c m b m g m e m d m f m h m From Elimination to Message Passing  Our algorithm so far answers only one query (e.g., on one node), do we need to do a complete elimination for every such query?  Elimination message passing on a clique tree  Another query ...  Messages mf and mh are reused, others need to be recomputed © Eric Xing @ CMU, 2005-2014 55 Summary  The simple Eliminate algorithm captures the key algorithmic Operation underlying probabilistic inference: --- That of taking a sum over product of potential functions  What can we say about the overall computational complexity of the algorithm? In particular, how can we control the ""size"" of the summands that appear in the sequence of summation operation.  The computational complexity of the Eliminate algorithm can be reduced to purely graph-theoretic considerations.  This graph interpretation will also provide hints about how to design improved inference algorithm that overcome the limitation of Eliminate. © Eric Xing @ CMU, 2005-2014 56 "
40,"COMP 790.139 (Fall 2017) Natural Language Processing Mohit Bansal Language+Robotics NLP for Actions/Robotics Turn right at the butterﬂy painting, then go to the end of the hall ! Task-based instructions, e.g., navigation, grasping, manipulation, skill learning NLP for Actions/Robotics Cut some onions, and add to broth, stir it ! Task-based instructions, e.g., navigation, grasping, manipulation, skill learning Navigation Instruction Following Learning to interpret natural language navigation instructions from observations. Chen and Mooney. AAAI 2011. David L. Chen and Raymond J. Mooney Department of Computer Science The University of Texas at Austin 1616 Guadalupe, Suite 2.408 Austin, TX 78701, USA cc@cs.utexas.edu and mooney@cs.utexas.edu guage instructions is crit- at interact with humans. ansform natural-language able formal plans. Given system learns by simply igation instructions. The x virtual indoor environ- andmarks. A previously x English navigation in- used for training and test- n to reﬁne inferred plans semantic parser, the sys- correctly interpret a rea- ructions in this corpus. tion al language processing is ions. The ability to parse ed actions is essential for er or a robot. Some recent ral-language instructions by a computer (Branavan s 2009). In particular, we MacMahon, Stankiewicz, aas 2009; Matuszek, Fox, Figure 1: This is an example of a route in our virtual world. The world consists of interconnecting hallways with vary- ing ﬂoor tiles and paintings on the wall (butterﬂy, ﬁsh, or Eiffel Tower.) Letters indicate objects (e.g. ’C’ is a chair) at a location. “Go towards the coat rack and take a left at the coat rack go all the way to the end of the hall and this is 4 ” Navigation Instruction Following Learning to interpret natural language navigation instructions from observations. Chen and Mooney. AAAI 2011. Figure 2: An overview of our system is not always a direct correspondence between ei and ai. Rather, ei corresponds to an unobserved plan pi that when Figure 3: E on the instruct as supervised During testing ej into formal out by the exe While we b sible for creat use existing sy executing navi system are no of the instruc Navigation Instruction Following Weakly supervised learning of semantic parsers for mapping instructions to actions. Artzi and Zettlemoyer. TACL 2013. vised learning with GENLEX Previous Zettlemoyer & Collins, 2005) introduced a n GENLEX(x, z) to map a sentence x and its ng z to a large set of potential lexical entries. entries are generated by rules that consider the form z and guess potential CCG categories. ample, the rule p ! N : λx.p(x) introduces ries commonly used to model certain types of This rule would, for example, introduce the ry N : λx.chair(x) for any logical form z ntains the constant chair. GENLEX uses a set of such rules to generate categories that red with all possible substrings in x, to create set of lexical entries. The complete learning hm then simultaneously selects a small sub- these entries and estimates parameter values Section 8, we will introduce a new way of GENLEX to learn from different signals that, ly, do not require a labeled logical form z. atial Environment Modeling l execute instructions in an environment, see n 2, which has a set of positions. A position ple (x, y, o), where x and y are horizontal and l coordinates, and o 2 O = {0, 90, 180, 270} rientation. A position also includes properties ing the object it contains, its ﬂoor pattern and lpaper. For example, the square at (4, 3) in 2 has four positions, one per orientation. ause instructional language refers to objects X"" y"" 1"" 2"" 3"" 4"" 5"" 1"" 2"" 3"" 4"" 5"" 270$ 90$ 0$ 180$ C"" D"" E"" A"" B"" ⇢ D"" E"" "" (a) chair λx.chair(x) ⇢ A"" B"" "" (b) hall λx.hall(x) E"" (c) the chair ◆x.chair(x) C"" (d) you you ⇢ B"" "" (e) blue hall λx.hall(x) ^ blue(x) ⇢ E"" "" (f) chair in the intersection λx.chair(x) ^ intersect(◆y.junction(y), x) ⇢ A"" B"" E"" "" (g) in front of you λx.in front of(you, x) Figure 2: Schematic diagram of a map environment and example of semantics of spatial phrases. tial relations and object reference, and is executable. Navigation Instruction Following Weakly supervised learning of semantic parsers for mapping instructions to actions. Artzi and Zettlemoyer. TACL 2013. facing the lamp go until you reach a chair AP/NP NP/N N S AP/S NP S\NP/NP NP/N N λx.λa.pre(a, λf.◆x.f(x) λx.lamp(x) λa.move(a) λs.λa.post(a, s) you λx.λy.intersect(x, y) λf.Ax.f(x) λx.chair(x) front(you, x)) > > NP NP ◆x.lamp(x) Ax.chair(x) > > AP S\NP λa.pre(a, front(you, ◆x.lamp(x))) λy.intersect(Ax.chair(x), y) < S/S S λf.λa.f(a) ^ pre(a, front(you, ◆x.lamp(x))) intersect(Ax.chair(x), you) > AP λa.post(a, intersect(Ax.chair(x), you)) S\S λf.λa.f(a) ^ post(a, intersect(Ax.chair(x), you)) < S λa.move(a) ^ post(a, intersect(Ax.chair(x), you)) > S λa.move(a) ^ post(a, intersect(Ax.chair(x), you)) ^ pre(a, front(you, ◆x.lamp(x))) Figure 4: A CCG parse showing adverbial phrases and topicalization. AT : VT ! S s2S DMs,T be the assignment func- tion, which maps variables to domain objects. For each model Ms the domain DMs,ev is a set of action sequences {ha1, ..., ani : n ≥1}. Each ~ a deﬁnes a sequences of states si, as deﬁned in Sec- tion 6.2, and associated models Msi. The key chal- lenge for execution is that modiﬁers of the event will need to be evaluated under different models from visited while executing ~ a. For example, given the predicate post and the action sequence ha1, . . . , ani, update(ha1, . . . , ani, post) = Msn+1, where sn+1 the state of the agent following action an. By con- vention, we place the event variable as the ﬁrst argu- ment in literals that include one. Given a T-type logical expression l and a start- ing state s0 we compute its interpretation IM T (l) Navigation Instruction Following Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. Mei, Bansal, and Walter. AAAI 2016 Hongyuan Mei Mohit Bansal Matthew R. Walter Toyota Technological Institute at Chicago Chicago, IL 60637 {hongyuan,mbansal,mwalter}@ttic.edu Abstract sequence-to-sequence model for direc- that is essential to realizing effective Our alignment-based encoder-decoder rt-term memory recurrent neural net- translates natural language instructions ased upon a representation of the ob- We introduce a multi-level aligner that to focus on sentence “regions” salient state by using multiple abstractions of contrast to existing methods, our model guistic resources (e.g., parsers) or task- e.g., seed lexicons). It is therefore gen- hieves the best results reported to-date e-sentence dataset and competitive re- aining multi-sentence setting. We ana- h a series of ablations that elucidate the rimary components of our model. Introduction o understand and successfully execute ational instructions if they are to work people. For example, someone using wheelchair might direct it to “Take s from the kitchen,” or a soldier may ial vehicle to “Fly down the hallway on the right.” However, interpreting tions (especially in unknown environ- due to their ambiguity and complexity, their interpretation (e.g., which hall- on refer to), long-term dependencies ctions and the actions, differences in given, and the diverse ways in which B Objects Barstool C Chair E Easel H Hatrack L Lamp S Sofa Wall paintings Tower Butterfly Fish Floor patterns Brick Blue Concrete Flower Grass Gravel Wood Yellow L E H C S S E C B H L Place your back against the wall of the “T” intersection. Go forward one segment to the intersection with the blue-tiled hall. This interesction [sic] contains a chair. Turn left. Go forward to the end of the hall. Turn left. Go forward one seg- ment to the intersection with the wooden-ﬂoored hall. This intersection conatains [sic] an easel. Turn right. Go forward two segments to the end of the hall. Turn left. Go forward one segment to the intersection containing the lamp. Turn right. Go forward one segment to the empty corner. Figure 1: An example of a route instruction-path pair in one of the virtual worlds from MacMahon, Stankiewicz, and Kuipers (2006) with colors that indicate ﬂoor patterns and wall paintings, and let- ters that indicate different objects. Our method successfully infers the correct path for this instruction. Navigation Instruction Following Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. Mei, Bansal, and Walter. AAAI 2016 LSTM-RNN MULTI-LEVEL ALIGNER DECODER ENCODER Aligner LSTM-RNN LSTM-RNN LSTM-RNN go forward two segments to the end of the hall E World State Action Sequence Instruction Figure 2: Our encoder-aligner-decoder model with multi-level alignment servable world state (which we treat as a third se- type that we add as an extra connection to every de- tep). Moreover, our decoder also includes alignment s on the portions of the sentence relevant to the cur- ion, a technique that has proven effective in machine ion (Bahdanau, Cho, and Bengio 2014) and machine Mnih et al. 2014; Ba, Mnih, and Kavukcuoglu 2014; l 2015) H lik h d d li tors were asked to give written commands that describ to navigate from one location to another without subs access to the map. Each instruction was then given t eral human followers who were tasked with navigating virtual world without a map, and their paths were rec Many of the raw instructions include spelling and gra ical errors, others are incorrect (e.g., misusing “lef “ i h ”) i l 10% f h i l h Navigation Instruction Following Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. Mei, Bansal, and Walter. AAAI 2016 B Objects Barstool C Chair E Easel H Hatrack L Lamp S Sofa Wall paintings Tower Butterfly Fish Floor patterns Brick Blue Concrete Flower Grass Gravel Wood Yellow L E H C S S E C B H L go stop go go forward two segments to the the end of hall stop go go intersection forward one segment to the containing the lamp go intersection forward one segment to the with the wooden-floored hall go stop go forward to the the end of hall go stop go go go stop go go go go go stop stop go stop Figure 4: Visualization of the alignment between words to actions in a map for a multi-sentence instruction. References Andreas, J., and Klein, D. 2015. Alignment-based composi- Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learn- ing phrase representations using RNN encoder decoder for Navigation Instruction Following Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. Mei, Bansal, and Walter. AAAI 2016 collected We use orrect any al naviga- uctors for nly across nted into equence. l, observ- ption that ironment n that we ﬂoor pat- luded by nation of (forward, sentation g and the ations. model on ns of the previous Results and Analysis In this section, we compare the overall performance of our model on the single- and multi-sentence benchmarks against previous work. We then present an analysis of our model through a series of ablation studies. Table 1: Overall accuracy (state-of-the-art in bold) Method Single-sent Multi-sent Chen and Mooney (2011) 54.40 16.18 Chen (2012) 57.28 19.18 Kim and Mooney (2012) 57.22 20.17 Kim and Mooney (2013) 62.81 26.57 Artzi and Zettlemoyer (2013) 65.28 31.93 Artzi, Das, and Petrov (2014) 64.36 35.44 Andreas and Klein (2015) 59.60 – Our model (vDev) 69.98 26.07 Our model (vTest) 71.05 30.34 Primary Result We ﬁrst investigate the ability to navigate to the intended destination for a given natural language in- struction. Figure 1 illustrates an output example for which our model successfully executes the input natural language i i bl 1 h ll f Navigation Instruction Following Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. Mei, Bansal, and Walter. AAAI 2016 Table 2: Model components ablations Full Model High-level Aligner No Aligner Unidirectional No Encoder Single-sentence 69.98 68.09 68.05 67.44 61.63 Multi-sentence 26.07 24.79 25.04 24.50 16.67 As we can see from Table 1, we surpass state-of-the-art re- ults on the single-sentence route instruction task (for both Dev and vTest settings), despite using no linguistic knowl- dge or resources. Our multi-sentence accuracy, which is working with a really small amount of training data (a few undred paragraph pairs), is competitive with state-of-the- rt and outperforms several previous methods that use addi- onal, specialized resources in the form of semantic parsers, ogical-form lexicons, and re-rankers.5 We note that our model yields good results using only greedy search (beam width of one). For vDev, we achieve 68.05 on single- entence and 23.93 on multi-sentence, while for vTest, we et 70.56 on single-sentence and 27.91 on multi-sentence. Distance Evaluation Our evaluation required that the ac- on sequence reach the exact desired destination. It is of nterest to consider how close the model gets to the destina- on when it is not reached. Table 3 displays the fraction of in which the context vector zt is an unweighted averag (Eqn. 5). As shown in the Table 2, learning the alignmen does improve the accuracy of the resulting action sequence Note that the “No Aligner” model still maintains all connec tions between the instruction and actions, but is just usin non-learned, uniform weights. Bidirectionality Ablation We train an alternative mode that uses only a unidirectional (forward) encoder. As show in Table 2, the bidirectional encoder (“Full Model”) signiﬁ cantly improves accuracy. Encoder Ablation We further evaluate the beneﬁt of en coding the input sentence and consider an alternative mode that directly feeds word vectors as randomly initialized em beddings into the decoder and relies on the alignment mode g y, working with a really small amount of training data (a few hundred paragraph pairs), is competitive with state-of-the- art and outperforms several previous methods that use addi- tional, specialized resources in the form of semantic parsers, logical-form lexicons, and re-rankers.5 We note that our model yields good results using only greedy search (beam width of one). For vDev, we achieve 68.05 on single- sentence and 23.93 on multi-sentence, while for vTest, we get 70.56 on single-sentence and 27.91 on multi-sentence. Distance Evaluation Our evaluation required that the ac- tion sequence reach the exact desired destination. It is of interest to consider how close the model gets to the destina- tion when it is not reached. Table 3 displays the fraction of test results that reach within d nodes of the destination. Of- ten, the method produces action sequences that reach points close to the desired destination. Table 3: Accuracy as a function of distance from destination Distance (d) 0 1 2 3 Single-sentence 71.73 86.62 92.86 95.74 Multi-sentence 26.07 42.88 59.54 72.08 Multi-level Aligner Ablation Unlike most existing meth- ods that align based only on the hidden annotations hj, we adopt a different approach by also including the original in- put word xj (Eqn. 5). As shown in Table 2, the multi-level representation (“Full Model”) signiﬁcantly improves perfor- mance over a standard aligner (“High-level Aligner”). Fig- ure 4 visualizes the alignment of words to actions in the map g tions between the instruc non-learned, uniform weig Bidirectionality Ablation that uses only a unidirecti in Table 2, the bidirection cantly improves accuracy. Encoder Ablation We coding the input sentence that directly feeds word v beddings into the decoder to choose the salient wo with and without the enc is a substantial gain in en context representation. W the RNN’s ability to incor into the word’s represent sequentially (in both dire solve ambiguities, such a “turn right after ...”. Co We presented an end-to proach to mapping natur tions to action plans gi state, using a bidirectional level aligner. We evalu route instruction dataset a new state-of-the-art on sin Navigation + Manipulation Instructions d Thomas Kollar and Steven Dickerson and shis Gopal Banerjee and Seth Teller and Nicholas Roy Science and Artiﬁcial Intelligence Laboratory Massachusetts Institute of Technology Cambridge, MA 02139 nding natural ems that per- mi-structured models with uence of ac- In contrast, Graphs (G3), al model for a g to the com- tic structure. successfully ural language truck.” The collected us- with robot ac- of the model. ng plans from n in a realistic system’s per- successfully the corpus. (a) Robotic forklift Commands from the corpus - Go to the ﬁrst crate on the left and pick it up. - Pick up the pallet of boxes in the middle and place them on the trailer to the left. - Go forward and drop the pallets to the right of the ﬁrst set of tires. - Pick up the tire pallet off the truck and set it down (b) Sample commands Figure 1: A target robotic platform for mobile manipulation and navigation (Teller et al., 2010), and sample commands from the domain, created by untrained human annotators. Our system can successfully follow these commands. arguments or nested clauses. At training time, when using Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, and Roy. AAAI 2011. Navigation + Manipulation Instructions Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, and Roy. AAAI 2011. EV ENT1(r = Put, l = OBJ2(f = the pallet), l2 = PLACE3(r = on, l = OBJ4(f = the truck))) (a) SDC tree λr 1 “Put” γ1 φ1 λf 2 “the pallet” γ2 φ2 λr 3 “on” γ3 φ3 λf 4 “the truck” γ4 φ4 (b) Induced model Figure 2: (a) SDC tree for “Put the pallet on the truck.” (b) Induced graphical model and factorization. • PLACE A place in the world (e.g. “on the truck,” or “next to the tire pallet”). EV ENT1(r = Go l = PAT H2(r = to, l = OBJ3(f = OBJ4(f = the pallet), r = on, l = OBJ5(f = the truck)))) (a) SDC tree λr 1 “Go” γ1 φ1 λr 2 “to γ2 φ2 λf 4 “the pallet” φ4 λr 3 “on” γ4 φ3 λf 5 “the truck” γ5 φ5 (b) Induced model Figure 3: (a) SDC tree for “Go to the pallet on the truck.” (b) A different induced factor graph from Figure 2. Structural differences between the two models are highlighted in gray. • λf i The words of the ﬁgure ﬁeld of the ith SDC. Navigation + Manipulation Instructions ✖✕ ✗✔ Grounding for γ4 ❤ Grounding for γ3 ♠ Grounding for γ2 (a) Object groundings (b) Pick up the pallet Grounding for γ1 (c) Put it on the truck Figure 4: A sequence of the actions that the forklift takes in response to the command, “Put the tire pallet on the truck.” (a) The search grounds objects and places in the world based on their initial positions. (b) The forklift executes the ﬁrst action, picking up the pallet. (c) The forklift puts the pallet on the trailer. of low-scoring examples were due to words that did not ap- pear many times in the corpus. For PLACE SDCs, the system often correctly classiﬁes examples involving the relation “on,” such as “on the trailer.” However, the model often misclassiﬁes PLACE SDCs that involve frame-of-reference. For example, “just to the right of the furthest skid of tires” requires the model to have fea- tures for “furthest” and the principal orientation of the “skid of tires” to reason about which location should be grounded agreement with the statement, “The forklift in the video is executing the above spoken command” on a ﬁve-point Likert scale. We report command-video pairs as correct if the sub- jects agreed or strongly agreed with the statement, and in- correct if they were neutral, disagreed or strongly disagreed. We collected ﬁve annotator judgments for each command- video pair. To validate our evaluation strategy, we conducted the eval- uation using known correct and incorrect command-video Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation. Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, and Roy. AAAI 2011. Navigation + Manipulation Instructions A Natural Language Planner Interface for Mobile Manipulators. Howard, Tellex, and Roy. ICRA 2014. ge Planner Interface for Mobile Manipulators M. Howard1, Stefanie Tellex2 and Nicholas Roy1 es for robot control ctions that reﬂect the his is difﬁcult because of environments, and has demonstrated that cted from the parse ed to identify motions ses. Such approaches al bottlenecks imposed e of possible actions. regions and separate an environment model, resent the meaning of t a new model called h (DCG) to infer the rom natural language n uses these planning ons that resemble the identifying the action ual steps of planning ng enables us to avoid eration and evaluation rimental results from rate improvements in nding without loss of x(t) o1 o2 o3 o4 Fig. 1. An illustration of the robot trajectory x(t) generated from planning constraints that were inferred from the natural language instruction “move near the red box and the blue crate” using the Distributed Correspondence Graph (DCG) model. The dark gray, light gray, and white regions represent the goal states, admissible states, and inadmissible states respectively. The variables o1 ...o4 identify the four objects in the environment model. admissible, inadmissible, and goal regions. If we can infer Navigation + Manipulation Instructions A Natural Language Planner Interface for Mobile Manipulators. Howard, Tellex, and Roy. ICRA 2014. NGUAGE UNDERSTANDING OF OT INSTRUCTIONS f natural language interfaces for robot trajectory (x(t) = ⇥ x(ti) ... x "" tf #⇤ ) that ivities described by an instruction (L) world model (°): gmax t)2¬K p(x(t) |L,°) (1) ion 1 is difﬁcult to compute because nguage, environments, and trajectories. ounding Graph (G3) model [1] is a que for generating robot actions from ructions. This model is a factor graph corpus of labeled examples to ground , locations, and paths. Grounding is the physical meanings to natural language. mmon to associate nouns with objects, cations, and verbs with actions. The or graph follows the parse tree of the ting phrases (L = [l1 ...ln]) to ground- and correspondence variables (F = del the random variables that represent ondence variables are known and the wn. To ﬁnd the values of the grounding osely resemble the command, we search f G that maximize Equation 2. max gn p(F|g1 ...gn,L,°) (2) VP VB PP IN NP DT JJ NN NP CC NP DT JJ NN move near the red box and the blue crate Fig. 2. A parse tree for the sentence “move near the red box and the blue crate”. Part-of-speech tags in the parse tree are from the Penn Treebank [2]. true true true true true move near and the red box the blue crate l1 l2 l3 l4 l5 f1 f2 f3 f4 f5 g1 g2 g3 g4 g5 f1 f2 f3 f4 f5 Fig. 3. The factor graph resulting from the parse tree in Figure 2, used by the G3 algorithm to infer the groundings of the instructions. Each linguistic is grounded to a object, location, or action through a factor that incorporates the grounding of its children. Black boxes, white spheres, and gray sphere are factors, known random variables, and unknown random variables respectively. Navigation + Manipulation Instructions A Natural Language Planner Interface for Mobile Manipulators. Howard, Tellex, and Roy. ICRA 2014. 5 6 nference runtime as a on each n constraints ion space. e is the time that ings. Effectively ntinuum of robot ystem dynamics. used to generate w rapidly without ction. If instead f well-separated ajectory planning ory planner used p q p of speech, including 13 adjectives, 13 nouns, and 29 verbs. The ﬁrst corpus was randomly divided into a training set of 96 examples and a test set of 288 examples. Three examples illustrating the environment model, robot trajectory, and assigned instructions are shown in Figure 7. (a) “go to the blue box” (b) “move towards the green object” (c) “travel to the or- ange object” Fig. 7. Images of labeled trajectories generated by constraint and environment sampling that form the training and test sets for constraint inference evaluation. Each noun phrase in the G3 and Hybrid G3-DCG models uses a single factor to search the 50 objects for the one that is best described by the linguistic constituents. The state-action space for the G3 model consisted of trajectories that satisﬁed each of the eight constraints sets in the current environment. The constraint set for the Hybrid G3 DCG model consisted Configuration/Assembling Instructions Grounding Names 5 Put the block that looks like a taurus symbol ju Understanding Distance 5 move the texaco block 5 block lengths above th Table 5: We performed a subjective error analysis of the results of our Fixed Semantics mod sentences and the frequency of each type of error are reported above from the worst 50 errors Scene Utterance Move the block that is cur- rently located closest to the top left corner to the bottom left of the table, slightly higher than the block in the bottom right corner. Error: 7.29 Block lengths Move the block closest to the top left corner so it is above half a block length to the right of the blocks near the lower left corner of the table. Error: 0.94 Block lengths Table 6: Above are two commands and the worlds they apply to. Below we see the prediction error of our best model. erence, Direction) paradigm, but automatically ex- tracting that semantics is now more difﬁcult and the plicit training signal fo The nature of the lan fers quite dramatically culty. Table 6 shows t best (and worst) on in make use of a referenc 9 Conclusion We showed how hum be attacked within a supports alternative m compared using obj duced a set of simple in-context command/i should serve as strong in this ﬁeld. The datas tant challenges for NL of the language has va on the world in which Natural Language Communication with Robots. Bisk, Yuret, and Marcu. NAACL 2016. Configuration/Assembling Instructions Natural Language Communication with Robots. Bisk, Yuret, and Marcu. NAACL 2016. Encoder W1 Hidden W1 Wn . . . . . . + (x,y,z) World (3x20) Hidden Semantics 2 Semantics 3 * Semantics 1 Hidden Representation Grounding Prediction Figure 2: Our models all follow the above architecture. 1-Hot word vectors (orange) are fed as input to a Feed-Forward or Re- current Neural Network for encoding. A semantic representa- tion is extracted (green), which in conjunction with knowledge of the world (blue) is grounded to predict an action. can be trained independently (Sections 5.2 and 5.3) direction from the ref Move the Adidas b For example, in the si distill three pieces of Source Referen Directi By assuming a gr verted to its location which we shift east b to yield: (x, y + δ, z) nine relative positions NW (x-ẟ, y+ NW (x-ẟ, y SW (x-ẟ, y- First our model pro tence. We present two Configuration/Assembling Instructions Natural Language Communication with Robots. Bisk, Yuret, and Marcu. NAACL 2016. MNIST Patterns with labeled blocks Random Patterns with blank blocks Source Target Source Target Med Mean Med Mean S R D Med Mean Med Mean S R D Human Performance 0.00 0.00 0.21 0.53 100 0.00 0.30 0.37 1.39 93 Oracle – – 0.00 0.45 100 100 100 – – 1.00 1.09 100 100 100 FFN Discrete Predictions 0.00 0.49 1.09 2.17 93 69 63 5.28 5.09 5.51 5.46 9 15 32 Continuous Predictions 0.49 1.00 1.59 2.42 4.25 4.04 3.86 3.93 End-to-End 0.02 0.38 1.14 1.81 3.45 3.52 3.60 3.94 RNN Discrete Predictions 0.00 0.14 0.00 0.98 98 92 78 5.29 5.00 5.51 5.57 10 7 46 Continuous Predictions 0.47 0.64 1.23 1.60 4.16 4.05 3.71 3.87 End-to-End 0.03 0.19 0.53 1.05 3.29 3.47 3.60 3.70 Center Baseline – – 3.46 3.43 100 – – 4.09 4.06 100 Random Baseline 6.37 6.49 6.12 6.21 5 5 11 4.90 4.97 5.51 5.44 10 11 12 Table 4: Model error when trained on only the subset of the data with decorated blocks or blank blocks. Where appropriate S, R, and D are the model’s predictive accuracy at identifying the Source, Reference and Direction. All models are evaluated on the Median and Mean prediction error the source block and its ﬁnal target location. Distances are presented in block-lengths. not told about the high-level goal of drawing a num- ber. Despite this, Table 4 shows human performance is very similar to Oracle performance. Although hu- data, the End-to-End model performs best by learn- ing its own more appropriate representation. Parameters Where appropriate we used 256 unit Configuration/Assembling Instructions Source-Target Inference Models for Spatial Instruction Understanding. Tan and Bansal. AAAI 2018. Department of Computer Science iversity of North Carolina at Chapel Hill haotan, mbansal}@cs.unc.edu uctions for sit- ation have sev- d remote sce- ferred conﬁg- n the challeng- s task involves e target posi- et), where the eferred to via els for the sub- osition regres- l-world repre- dual attention world blocks prediction, we sampling via via supervised -of-the-art on ce block accu- Figure 1: An example of the conﬁguration instruction under- standing task (based on blank-labeled blocks). Our model is able to correctly predict the source block and the target po- sition in this case. Models that can understand the semantics of block se- lection and moving instructions (and the involved referring expressions) have been a topic of study since the 1970s, Configuration/Assembling Instructions Source-Target Inference Models for Spatial Instruction Understanding. Tan and Bansal. AAAI 2018. Figure 2: Our overall model for the assembly instruction understanding task, showing instruction and world representation learning, language-to-block alignment modules, and source and target (expectation vs. sampling) loss functions. beddings. Finally, the block which best aligns with the in- struction is chosen as the source block. We ﬁrst use a standard LSTM-RNN to encode the instruction I into its embedding representation H = {h1, h2, . . . , hm} by the recurrent function: ht = r (ht−1, WWwt) (1) where WW is the word embedding matrix layer. The blocks d d i t th i b ddi i f ll t d this task). P(S = bi|I) / exp(A(ci, H)) (3) LSRC = − X i G(bi) log P(S = bi|I) (4) This loss function is then summed over for all data instances (instructions) and the total loss is minimized to learn all the source-related weights described above. Next, we describe the different attention modules that we Configuration/Assembling Instructions Source-Target Inference Models for Spatial Instruction Understanding. Tan and Bansal. AAAI 2018. Model SOURCE TARGET Accuracy Median Mean Median Mean End-to-End FFN (Bisk, Yuret, and Marcu 2016) 9.0% 3.45 3.52 3.60 3.94 End-to-End RNN (Bisk, Yuret, and Marcu 2016) 10.0% 3.29 3.47 3.60 3.70 Our Expectation Model 56.1% 0.00 2.21 2.78 3.07 Our Sampling Model 56.3% 0.00 2.18 3.12 3.18 Our Expectation Model w/ Ensemble 56.6% 0.00 2.12 2.65 2.91 Our Sampling Model w/ Ensemble 56.8% 0.00 2.11 2.71 2.90 Table 2: Final test results of our ﬁnal sampling and expectation models (w/o and w/ ensemble), compared to the previous state-of-the-art on this dataset. Configuration/Assembling Instructions Source-Target Inference Models for Spatial Instruction Understanding. Tan and Bansal. AAAI 2018. Ou pectat o ode w/ se b e 56.6% Our Sampling Model w/ Ensemble 56.8% Table 2: Final test results of our ﬁnal sampling and expectation models state-of-the-art on this dataset. Positive Examples Figure 3: Analysis: positive and negative output examples showing interes in each pair depict the ground truth movement of the source block to the and target distance in bottom-right of each second image. We also use a (ground truth target position can be inferred directly from the image diffe the cases where our model predicted an incorrect source we represent tha Configuration/Assembling Instructions Source-Target Inference Models for Spatial Instruction Understanding. Tan and Bansal. AAAI 2018. 56.8% 0.00 2.11 2.71 2.90 expectation models (w/o and w/ ensemble), compared to the previous Negative Examples ples showing interesting instruction scenarios. The ﬁrst and second image source block to the target position. We report predicted source accuracy mage. We also use a red cross to represent our predicted target position rom the image difference between the ﬁrst and second image). Also, for ce we represent that wrongly predicted source block by a red circle Recipe Instruction Following Interpreting and Executing Recipes with a Cooking Robot. Bollini, Tellex, Thompson, Roy, Rus. ISER 2012. Interpreting and Executing Recipes with a Cooking Robot 3 Fig. 2 The human interaction with the BakeBot system for recipe execution. First the person pro- vides the plain-text recipe and the measured ingredients. Then BakeBot infers a sequence of baking primitives to execute that correspond to following the recipe. If BakeBot encounters an unsup- ported baking primitive, it asks its human partner for help executing the instruction. The end result is baked cookies. Recipe Instruction Following demonstrate an end-to-end robot cooking system capable of implementing any bak- ing recipe that requires pouring, mixing, and oven operations on premeasured in- gredients provided to the system. Our system is able to follow recipes downloaded from the internet; we demonstrate it by following two different recipes in the real world and by further evaluating its performance on a larger test set in simulation. Fig. 3 Architecture of the BakeBot system. The NL system processes the plain text recipe, pro- ducing a high-level plan which is sent to the robot. For each instruction in the high-level plan, the motion planner assembles a motion plan and executes it on the PR2 robot. Interpreting and Executing Recipes with a Cooking Robot. Bollini, Tellex, Thompson, Roy, Rus. ISER 2012. http://projects.csail.mit.edu/video/research/robo/bakebot_final.mp4 Recipe Instruction Following Interpreting and Executing Recipes with a Cooking Robot. Bollini, Tellex, Thompson, Roy, Rus. ISER 2012. Interpreting and Executing Recipes with a Cooking Robot 9 Recipe Text Afghan Biscuits 200g (7 oz) butter 75g (3 oz) sugar 175g (6 oz) ﬂour 25g (1 oz) cocoa powder 50g cornﬂakes (or crushed weetbix) Soften butter. Add sugar and beat to a cream. Add ﬂour and cocoa. Add cornﬂakes last. Put spoonfuls on a greased oven tray. Bake about 15 minutes at 180◦C (350◦F). Inferred Action Sequence pour(butter,bowl);mix(bowl) pour(sugar,bowl);mix(bowl) pour( flour,bowl); pour(cocoa,bowl) pour(corn flakes,bowl);mix(bowl) scrape() preheat(350);bake(pan,20) Fig. 4 Text from a recipe in our dataset, paired with the inferred action sequence for the robot. formally speciﬁed the speciﬁc ingredients and implements necessary to follow the recipe. This initialization is given to the robot. Next, for each instruction in the recipe text, we annotated the sequence of primitive actions the robot should take in order to follow that instruction. We used 45 recipes from this corpus to train the Recipe Instruction Following Interpreting and Executing Recipes with a Cooking Robot. Bollini, Tellex, Thompson, Roy, Rus. ISER 2012. Interpreting and Executing Recipes with a Cooking Robot 11 Recipe Instruction Following ! Recipes: Tell Me Dave (http://tellmedave.cs.cornell.edu/) Fig. 1. Natural Language Instructions to sequence of instructions for a given new environment. Our approach takes description in natural language and sequences together robotic instructions that are appropriate for a given environment and task. Note that the NL instructions are often ambiguous, and are incomplete, and need to be grounded into the environment. form natural language data and robotic instruction logs, col- lected from several users. The tasks comprise performing several steps in sequence and there are often different ways [41]. Since real environments have uncertainty and non- determinism, Kaelbling and Lozano-P´ erez [22] start with an abstract plan and recursively generate plans as needed Or the Tell Me Dave: Context-Sensitive Grounding of Natural Language to Mobile Manipulation Instructions, Misra, Sung, Lee, and Saxena. RSS 2014. Recipe Instruction Following ! Recipes: Tell Me Dave (http://tellmedave.cs.cornell.edu/) Fig. 4. Robot Experiment. Given the language instruction for making the dessert ‘Affogato’: ‘Take some coffee in a cup. Add icecream of your choice. Finally, add raspberry syrup to the mixture.’, our algorithm outputs a sequence that the PR2 executes to make the dessert. (Please see the video.) Predeﬁned Templates [18] focused on disambiguating spatial relations but was extremely brittle to ambiguity in grounding, therefore giving low performance. Method Instruction-Tree [7] was able to give reasonable re- sults on some sequences. However this approach has problem working with large search tree. Furthermore, the bag-of-word feature do not take into account the environment context, the language might say that keep the cup in microwave but the cup might already be inside the microwave (unless such constraints are hard-coded). This approach thus fails when the language is vague, for example, for the following sentence, heat the water and add ramen., However, our approach takes this vague How well does our model generalize to new environments and tasks? In this test, we wanted to examine how well our model can make use of examples from different environment and tasks. Its not obvious whether the templates learned for one task, such as making affogato will be useful for another task such as making ramen. For studying the effect of a differ- ent task, we performed another experiment in which we trained and tested the model on making ramen task only (instead of training together for {making ramen,making affogato}). We found that because the VEIL library from the making affogato task was not available for training, the performance dropped to 64.9 on the IED metric as compared to 67.3 in Table II. Tell Me Dave: Context-Sensitive Grounding of Natural Language to Mobile Manipulation Instructions, Misra, Sung, Lee, and Saxena. RSS 2014. Recipe Instruction Following ! Recipes: RoboBarista (http://robobarista.cs.cornell.edu/) Fig. 5. Screen-shot of Robobarista, the crowd-sourcing platform running on Chrome browser. We have built Robobarista platform for collecting a large number of crowd demonstrations for teaching the robot. to the cumulative sum, |D(mA, mB)|path⇤(i.e. the length of the optimal warping path), giving the ﬁnal form: Generating a full trajectory from scratch can be difﬁcult for non-experts. Thus, similar to Forbes et al. [17], we provide a trajectory that the system has already seen for Robobarista: Object Part based Transfer of Manipulation Trajectories from Crowd-sourcing in 3D Pointclouds. Sung, Jin, and Saxena. ISRR 2015. Recipe Instruction Following ! RoboBarista: http://robobarista.cs.cornell.edu/ Fig. 10. Examples of transferred trajectories being executed on PR2. On the left, PR2 is able to rotate the ‘knob’ to turn the lamp on. In the third snapshot, using two transferred trajectories, PR2 is able to hold the cup below the ‘nozzle’ and press the ‘lever’ of ‘coffee dispenser’. In the last example, PR2 is frothing milk by pulling down on the lever, and is able to prepare a cup of latte with many transferred trajectories. over 70% recognition accuracy in classifying ﬁve major classes of object parts (‘button’, ‘knob’, ‘handle’, ‘nozzle’, ‘lever’.) However, the Object Part Classiﬁer baseline, based on this classiﬁcation, performed at only 23.3% accuracy for actual trajectory transfer, outperforming chance by merely 12.1%, and signiﬁcantly underperforming our model’s result of 65.1%. This shows that object part labels alone are not sufﬁcient to enable manipulation motion transfer, while our model, which makes use of richer information, does a much better job Robobarista: Object Part based Transfer of Manipulation Trajectories from Crowd-sourcing in 3D Pointclouds. Sung, Jin, and Saxena. ISRR 2015. Navigation Instruction Generation tion ntly nce- we In r in mon- each in- pair aset turn aug- stra- sing 100, ner- oder arly TM Fig. 4. Participants’ ﬁeld of view in the virtual world used for the human navigation experiments. of the quality of the instructions (e.g., instructions that are correct but different in prose will receive a low BLEU score). In an effort to further evaluate the accuracy and usability of our method, we conducted a set of human evaluation experiments in which we asked 42 novice participants on Amazon Mechanical Turk (21 females and 21 males, ages 18–64, all native English speakers) to follow natural language route instructions, randomly chosen from two equal-sized sets Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation. Daniele, Bansal, and Walter. HRI 2017. g afdaniele@ttic.edu p mbansal@cs.unc.edu g mwalter@ttic.edu ct—Modern robotics applications that involve human- eraction require robots to be able to communicate with seamlessly and effectively. Natural language provides a nd efﬁcient medium through which robots can exchange ion with their human partners. Signiﬁcant advancements en made in developing robots capable of interpreting m instructions, but less attention has been devoted to g robots with the ability to generate natural language. ose a navigational guide model that enables robots to natural language instructions that allow humans to a priori unknown environments. We ﬁrst decide which ion to share with the user according to their preferences, policy trained from human demonstrations via inverse ment learning. We then “translate” this information atural language instruction using a neural sequence-to- model that learns to generate free-form instructions tural language corpora. We evaluate our method on mark route instruction dataset and achieve a BLEU 72.18% when compared to human-generated reference ons. We additionally conduct navigation experiments man participants that demonstrate that our method s instructions that people follow as accurately and easily produced by humans. I. INTRODUCTION s are increasingly being used as our partners, working d alongside people, whether it is serving as assistants homes [59], transporting cargo in warehouses [11], students with language learning in the classroom [28], ng as guides in public spaces [23]. In order for d b k h ff i l b Input: map and path C B H E L S B C E H L S Blue Brick Concrete Flower Grass Black Wood Yellow Floor patterns: Tower Butterfly Fish Wall paintings: Barstool Chair Easel Hatrack Lamp Sofa Objects: Output: route instruction “turn to face the grass hallway. walk forward twice. face the easel. move until you see black ﬂoor to your right. face the stool. move to the stool” Fig. 1. An example route instruction that our framework generates for the shown map and path. f it ( “I h i t d t ”) d Navigation Instruction Generation Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation. Daniele, Bansal, and Walter. HRI 2017. MDP Content Selection Sentence Planning Surface Realization Language Model Seq2Seq RNN Fig. 2. Our method generates natural language instructions for a given map and path. A. Compound Action Speciﬁcations In order to bridge the gap between the low-level nature of the input paths and the natural language output, we encode paths using an intermediate logic-based formal language. Speciﬁcally, we use the Compound Action Speciﬁcation (CAS) representation [39], which provides a formal abstraction of navigation commands for hybrid metric-topologic-semantic maps such as ours. The CAS language consists of ﬁve actions (i.e., Travel, Turn, Face, Verify, and Find), each of which is associated with a number of attributes that together deﬁne spe- ciﬁc commands (e g Travel distance Turn direction) We dis of transiti a, and γ correspon state. In t deﬁnes th of the en context fe in orienta and nearb the state which con state spac context fe the space attributes) We see lative rew value of p d difﬁ ""go forward 3 segments passing the bench"" Aligner LSTM-RNN LSTM-RNN LSTM-RNN Travel distance count.3 past type.Object value.Sofa CAS Command Encoder Aligner Decoder Instruction Fig. 3. Our encoder-aligner-decoder model for surface realization. CAS structures Cs. 2) Sentence Planning: Given the set of candidate CAS structures Cs, our method next chooses the attributes values 1) Sequence-to-Sequence Model: We formulate the prob lem of generating natural language route instructions as infer ence over a probabilistic model P(λ1:T |x1:N), where λ1:T = Navigation Instruction Generation (a) Q1: “How do you deﬁne the amount of information provided?” (b) Q2: “How would you evaluate the task in terms of difﬁculty?” (c) Q3: “How conﬁdent are you that you followed the desired path?” (d) Q4: “How many times did you have to backtrack?” (e) Q5: “Who do you think generated the instructions?” Fig. 7. Participants’ survey response statistics. and were rated as providing too little information 15% less frequently than the human-generated baseline (Fig 7(a)) Map and Paths C H B L 2 1 S H L H G S Legend: H B C S L - Hatrack - Barstool - Chair - Sofa - Lamp Fish Eiffel Butterfly 1 S - Initial position - Goal position - Final position G # 2 3 S G (a) (b) Instructions (a) Human “with your back to the wall turn left. walk along the ﬂowers to the hatrack. turn left. walk along the brick two alleys past the lamp. turn left. move along the wooden ﬂoor to the chair. in the next block is a hatrack” Ours “you should have the olive hallway on your right now. walk forward twice. turn left. move until you see wooden ﬂoor to your left. face the bench. move to the bench” (b) Human “head toward the blue ﬂoored hallway. make a right on it. go down till you see the ﬁsh walled areas. make a left in the ﬁsh walled hallway and go to the very end” Ours “turn to face the white hallway. walk forward once. turn right. walk forward twice. turn left. move to the wall” Fig. 8. Examples of paths from the SAIL corpus that ten participants (ﬁve for each map) followed according to instructions generated by humans and by our method. Paths in red are those traversed according to human-generated instructions, while paths in green were executed according to our instructions. Circles with an “S” and “G” denote the start and goal locations, respectively. following the human generated instruction One participant Navigational Instruction Generation as Inverse Reinforcement Learning with Neural Machine Translation. Daniele, Bansal, and Walter. HRI 2017. Verify value.Path side.Right appear.Honeycomb you should have the olive hallway on your right now Turn face value.Sofa side.Right turn so that the bench is on your right Fig. 5. Alignment visualization for two pairs of CAS (left) and natural language instructions (top). Darker colors denote greater attention weights. shows an example of a route instruction generated by our system for a given map and path. A. Aligner Ablation Our model employs an aligner in order to learn to focus on particular CAS tokens that are salient to words in the output instruction. We evaluate the contribution of the aligner by implementing and training an alternative model in which LM-score Candidate 105.00 “so so a straight chair to your left” 27.65 “turn so that the chair is on your left 101.00 “keep going till the blue ﬂor id on yo 11.00 “move until you see blue ﬂoor to you TABLE II LANGUAGE MODEL ABLATION OU Fig. 6. Comparison between the performances ach while following human annotated and machine gene and the generated route instruction (top) fo narios drawn from the SAIL validation set demonstrate that our method learns to ali Language Generation/Dialogue by Robots ! Navigation Dialogue formation-Theoretic Dialog to e Spatial-Semantic Representations emachandra Matthew R. Walter that enables robots to ntation of an environ- ng a guided tour. The athering actions in the the ambiguity over the uage descriptions (e.g., hese questions include ound (e.g., “Are we in distant from the robot oom?”). Our algorithm m that seeks to balance idate questions with a . In this manner, the to ask based upon the nting for the burden on for a joint distribution emantic representation ovided descriptions and d tour. We demonstrate the user, the method he learned map. N deployed in human- The kitchen is down the hallway Is the kitchen in front of me? Yes Fig. 1. A user gives a tour to a robotic wheelchair designed to assist resi- dents in a long-term care facility. (Left) The guide provides an ambiguous description of the kitchen’s location. (Right) When the robot is near one of the likely locations, it asks the guide a question to resolve the ambiguity. I thi ti h h b Information-Theoretic Dialog to Improve Spatial-Semantic Representations. Hemachandra and Walter. IROS 2015. Language Generation/Dialogue by Robots ! Manipulation Dialogue Clarifying Commands with Information-Theoretic Human-Robot Dialog. Deits et al. JHRI 2013. Deits et al., Clarifying Commands with Information-Theoretic Human-Robot Dialog λ1 “Pick up” Command γ1 φ1 λ2 “the pallet.” φ2 γ2 λ3 “Which one?” Question γ3 φ3 Answer λ5 “The one” φ5 γ5 λ6 “near” φ6 λ7 “the truck.” γ6 φ7 (a) Unmerged grounding graphs for three dialog acts. The noun phrases “the pallet,” “one” and “the one near the truck” refer to the same grounding in the external world but initially have separate variables in the grounding graphs. λ1 “Pick up” γ1 φ1 λ2 “the pallet.” φ2 λ3 “Which one?” γ3 φ3 λ5 “The one” φ5 λ6 “near” φ6 λ7 “the truck.” γ6 φ7 (b) The grounding graph after merging γ2, γ3 and γ5 based on linguistic coreference. Figure 2. Grounding graphs for a three-turn dialog, before and after merging based on coreference. The robot merges the three shaded variables. Language Generation/Dialogue by Robots ! Manipulation Dialogue Clarifying Commands with Information-Theoretic Human-Robot Dialog. Deits et al. JHRI 2013. an entropy-based metric and asks it, as described in Section 3.1. We describe and analyze three such metrics for selecting questions in Sections 3.1.1 and 3.1.2. After asking the chosen question and receiving an answer from a human partner, the robot merges grounding graphs that correspond to the original command, question, and answer into a single graphical model. Finally, the system performs inference in the merged graph to ﬁnd a new set of groundings that incorporates information from the answer as well as information from the original command. Figure 3 shows the dataﬂow in the system. Figure 3. System diagram. Grayed-out blocks show components developed in previous work and are therefore not discussed in detail in this paper; black blocks show the question-asking feedback system new to this paper. 3 1 Generating Questions Language Generation/Dialogue by Robots ! Manipulation Dialogue Clarifying Commands with Information-Theoretic Human-Robot Dialog. Deits et al. JHRI 2013. Deits et al., Clarifying Commands with Information-Theoretic Human-Robot Dialog Command: Move your pallet further right. Question: What do the words ‘your pallet’ refer to? Answer: Your pallet refers to the pallet you are currently carrying. Command: Move closer to it. Question: What does the word ‘it’ refer to? Answer: It refers to the empty truck trailer. Command: Take the pallet and place it on the one to the left. Question: What do the words ‘the one’ refer to? Answer: The one refers to the empty trailer. Command: Place the pallet just to the right of the other pallet. Question: What do the words ‘the pallet’ refer to? Answer: The wooden crate that the merchandise sits on top of. Figure 4. Sample commands, questions, and answers from the corpus. Language Generation/Dialogue by Robots Figure 1: An example setup and dialogue. Objects are marked with labels only for the illustration pur- pose. ing may not be ad where only resou human partners. T can engage in a na robots new skills. to learn new skill by (Allen et al., 20 developed to acq guage instruction work only ground widgets on web p In the robotics applied learning b new skills (Cakm allow natural lang has also explored l l t l t Back to the Blocks World: Learning New Actions through Situated Human-Robot Dialogue. She, Yang, Cheng, Jia, Chai, Xi. SigDial 2014. ! Learning New Actions via Dialogue Language Generation/Dialogue by Robots Figure 3: The Mechanical Turk interface for the delivery task. This abridged conversation is from a Turker in training batch 0, when the system had access to only the seed lexicon. Because of this conversation, the agent learned that “calender” and “planner” mean “calendar” during retraining. Figure 4: Left: Average Mechanical Turk survey responses across the four test batches. Right: Mean user turns in Mechanical Turk dialogs where the correct goal was reached. Means in underlined bold differ signiﬁcantly (p < 0.05) from the batch 0 mean. dition to learning misspelling corrections and new referring domain knowledge The sensor readings were converted to Figure 5: Left: Robot platform (Segbot) used in experi- ments. Right: Segbot architecture, implemented using Robot Operating System (ROS). included the directory panels used in the Mechanical Turk experiments pairing names and ofﬁce numbers and showing Table group bold d differe Sur Tas Rob Rob Use Use Goa Nav Del slowly Learning to Interpret Natural Language Commands through Human-Robot Dialog. Thomason, Zhang, Mooney and Stone. IJCAI 2015. "
400,"Vector Semantics Dense Vectors Sparse versus dense vectors • PPMI vectors are • long (length |V|= 20,000 to 50,000) • sparse (most elements are zero) • Alternative: learn vectors which are • short (length 200-1000) • dense (most elements are non-zero) 2 Sparse versus dense vectors • Why dense vectors? • Short vectors may be easier to use as features in machine learning (less weights to tune) • Dense vectors may generalize better than storing explicit counts • They may do better at capturing synonymy: • car and automobile are synonyms; but are represented as distinct dimensions; this fails to capture similarity between a word with car as a neighbor and a word with automobile as a neighbor 3 Three methods for getting short dense vectors • Singular Value Decomposition (SVD) • A special case of this is called LSA – Latent Semantic Analysis • “Neural Language Model”-inspired predictive models • skip-grams and CBOW • Brown clustering 4 Vector Semantics Dense Vectors via SVD Intuition • Approximate an N-dimensional dataset using fewer dimensions • By first rotating the axes into a new space • In which the highest order dimension captures the most variance in the original dataset • And the next dimension captures the next most variance, etc. • Many such (related) methods: • PCA – principle components analysis • Factor Analysis • SVD 6 1 2 3 4 5 6 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 6 PCA dimension 1 PCA dimension 2 Dimensionality reduction Singular Value Decomposition 8 Any rectangular matrix X equals the product of 3 matrices: W: rows corresponding to original but m columns represents a dimension in a new latent space, such that • M column vectors are orthogonal to each other • Columns are ordered by the amount of variance in the dataset each new dimension accounts for S: diagonal m x m matrix of singular values expressing the importance of each dimension. C: columns corresponding to original but m rows corresponding to singular values Singular Value Decomposition here. Suffice it to say that cookbook versions of SVD adequate for small (e.g., 100 × 100) matrices are available in several places (e.g., Mathematica, 1991 ), and a free software version (Berry, 1992) suitable Contexts 3= m x m m x c wxc w xm Figure A1. Schematic diagram of the singular value decomposition Th discu matr rows cont ing s are o of th loss curre See Dum A~ S 9 Landuaer and Dumais 1997 SVD applied to term-document matrix: Latent Semantic Analysis • If instead of keeping all m dimensions, we just keep the top k singular values. Let’s say 300. • The result is a least-squares approximation to the original X • But instead of multiplying, we’ll just make use of W. • Each row of W: • A k-dimensional vector • Representing word W 10 reduce the number of dimensions systematically by, for example, ing those with the smallest effect on the sum-squared error of the a imation simply by deleting those with the smallest singular valu The actual algorithms used to compute SVDs for large sparse m of the sort involved in LSA are rather sophisticated and are not de here. Suffice it to say that cookbook versions of SVD adequ small (e.g., 100 × 100) matrices are available in several place Mathematica, 1991 ), and a free software version (Berry, 1992) s Contexts 3= m x m m x c wxc w xm k / / k / k / k Deerwester et al (1988) Let’s return to PPMI word-word matrices • Can we apply SVD to them? 11 SVD applied to term-term matrix riginal M. Since the ﬁrst dimensions encode the most variance, one way to view e reconstruction is thus as modeling the most important information in the original ataset. VD applied to co-occurrence matrix X: 2 6 6 6 6 6 4 X 3 7 7 7 7 7 5 |V|⇥|V| = 2 6 6 6 6 6 4 W 3 7 7 7 7 7 5 |V|⇥|V| 2 6 6 6 6 6 4 s1 0 0 ... 0 0 s2 0 ... 0 0 0 s3 ... 0 . . . . . . . . . ... . . . 0 0 0 ... sV 3 7 7 7 7 7 5 |V|⇥|V| 2 6 6 6 6 6 4 C 3 7 7 7 7 7 5 |V|⇥|V| aking only the top k dimensions after SVD applied to co-occurrence matrix X: 2 3 2 32 3h i 12 (I’m simplifying here by assuming the matrix has rank |V|) Truncated SVD on term-term matrix 6 4 7 5 |V|⇥|V| 6 4 7 5 |V|⇥|V| 6 4 . . . . . . .. . . 0 0 0 ... sV 7 5 |V|⇥|V| 6 4 7 5 |V|⇥|V| king only the top k dimensions after SVD applied to co-occurrence matrix X: 2 6 6 6 6 6 4 X 3 7 7 7 7 7 5 |V|⇥|V| = 2 6 6 6 6 6 4 W 3 7 7 7 7 7 5 |V|⇥k 2 6 6 6 6 6 4 s1 0 0 ... 0 0 s2 0 ... 0 0 0 s3 ... 0 . . . . . . . . . ... . . . 0 0 0 ... sk 3 7 7 7 7 7 5 k ⇥k h C i k ⇥|V| gure 19.11 SVD factors a matrix X into a product of three matrices, W, S, and C. Taking ﬁrst k dimensions gives a |V|⇥k matrix Wk that has one k-dimensioned row per word that 13 Truncated SVD produces embeddings 14 • Each row of W matrix is a k-dimensional representation of each word w • K might range from 50 to 1000 • Generally we keep the top k dimensions, but some experiments suggest that getting rid of the top 1 dimension or even the top 50 dimensions is helpful (Lapesa and Evert 2014). 4 5 |V|⇥|V| 4 5 |V|⇥|V| Taking only the top k dimensions after SV 2 6 6 6 6 6 4 X 3 7 7 7 7 7 5 |V|⇥|V| = 2 6 6 6 6 6 4 W 3 7 7 7 7 7 5 |V|⇥k 2 6 6 6 6 6 4 Figure 19.11 SVD factors a matrix X into a the ﬁrst k dimensions gives a |V|⇥k matrix W can be used as an embedding. embedding for word i Embeddings versus sparse vectors • Dense SVD embeddings sometimes work better than sparse PPMI matrices at tasks like word similarity • Denoising: low-order dimensions may represent unimportant information • Truncation may help the models generalize better to unseen data. • Having a smaller number of dimensions may make it easier for classifiers to properly weigh the dimensions for the task. • Dense models may do better at capturing higher order co- occurrence. 15 Vector Semantics Embeddings inspired by neural language models: skip-grams and CBOW Prediction-based models: An alternative way to get dense vectors • Skip-gram (Mikolov et al. 2013a) CBOW (Mikolov et al. 2013b) • Learn embeddings as part of the process of word prediction. • Train a neural network to predict neighboring words • Inspired by neural net language models. • In so doing, learn dense embeddings for the words in the training corpus. • Advantages: • Fast, easy to train (much faster than SVD) • Available online in the word2vec package • Including sets of pretrained embeddings! 17 Embeddings capture relational meaning! vector(‘king’) - vector(‘man’) + vector(‘woman’) ≈ vector(‘queen’) vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) ≈vector(‘Rome’) 18 Vector Semantics Brown clustering Brown clustering • An agglomerative clustering algorithm that clusters words based on which words precede or follow them • These word clusters can be turned into a kind of vector • We’ll give a very brief sketch here. 20 Brown clustering algorithm • Each word is initially assigned to its own cluster. • We now consider merging each pair of clusters. Highest quality merge is chosen. • Quality = merges two words that have similar probabilities of preceding and following words • Clustering proceeds until all words are in one big cluster. 21 Brown Clusters as vectors • By tracing the order in which clusters are merged, the model builds a binary tree from bottom to top. • Each word represented by binary string = path from root to leaf • Each intermediate node is a cluster • Chairman is 0010, “months” = 01, and verbs = 1 22 Brown Algorithm 011 president walk run sprint chairman CEO November October 0 1 00 01 0011 0010 001 10 11 000 100 101 010 Brown cluster examples g p g y based on immediately neighboring words, Brown clusters are most commonly used for representing the syntactic properties of words, and hence are commonly used as a feature in parsers. Nonetheless, the clusters do represent some semantic properties as well. Fig. 19.17 shows some examples from a large clustering from Brown et al. (1992). Friday Monday Thursday Wednesday Tuesday Saturday Sunday weekends Sundays Saturdays June March July April January December October November September August pressure temperature permeability density porosity stress velocity viscosity gravity tension anyone someone anybody somebody had hadn’t hath would’ve could’ve should’ve must’ve might’ve asking telling wondering instructing informing kidding reminding bothering thanking deposing mother wife father son husband brother daughter sister boss uncle great big vast sudden mere sheer gigantic lifelong scant colossal down backwards ashore sideways southward northward overboard aloft downwards adrift Figure 19.17 Some sample Brown clusters from a 260,741-word vocabulary trained on 366 million words of running text (Brown et al., 1992). Note the mixed syntactic-semantic nature of the clusters. 23 "
401,"Semantic Role Labeling Introduction Many slides adapted from Dan Jurafsky Semantic Role Labeling Applications ` Question & answer systems Who did what to whom at where? The police officer detained the suspect at the scene of the crime ARG0 ARG2 AM-loc V Agent Theme Predicate Location Can we figure out that these have the same meaning? XYZ corporation bought the stock. They sold the stock to XYZ corporation. The stock was bought by XYZ corporation. The purchase of the stock by XYZ corporation... The stock purchase by XYZ corporation... 3 A Shallow Semantic Representation: Semantic Roles Predicates (bought, sold, purchase) represent an event semantic roles express the abstract role that arguments of a predicate can take in the event 4 buyer agent agent More specific More general Semantic Role Labeling Semantic Roles Getting to semantic roles Neo-Davidsonian event representation: Sasha broke the window Pat opened the door Subjects of break and open: Breaker and Opener Deep roles specific to each event (breaking, opening) Hard to reason about them for NLU applications like QA 6 GOAL The destination of an object of a transfer event Figure 22.1 Some commonly used thematic roles with their deﬁnitions. (22.1) Sasha broke the window. (22.2) Pat opened the door. A neo-Davidsonian event representation of these two sentences w 9e,x,y Breaking(e)^Breaker(e,Sasha) ^BrokenThing(e,y)^Window(y) 9e,x,y Opening(e)^Opener(e,Pat) ^OpenedThing(e,y)^Door(y) In this representation, the roles of the subjects of the verbs break Breaker and Opener respectively. These deep roles are speciﬁc to each deep roles ing events have Breakers, Opening events have Openers, and so on. If we are going to be able to answer questions, perform inferen further kinds of natural language understanding of these events, we’ll a little more about the semantics of these arguments Breakers and Thematic roles • Breaker and Opener have something in common! • Volitional actors • Often animate • Direct causal responsibility for their events • Thematic roles are a way to capture this semantic commonality between Breakers and Eaters. • They are both AGENTS. • The BrokenThing and OpenedThing, are THEMES. • prototypically inanimate objects affected in some way by the action 7 Thematic roles • One of the oldest linguistic models • Indian grammarian Panini between the 7th and 4th centuries BCE • Modern formulation from Fillmore (1966,1968), Gruber (1965) • Fillmore influenced by Lucien Tesnière’s (1959) Éléments de Syntaxe Structurale, the book that introduced dependency grammar • Fillmore first referred to roles as actants (Fillmore, 1966) but switched to the term case 8 Thematic roles • A typical set: 9 R 22 • SEMANTIC ROLE LABELING Thematic Role Deﬁnition AGENT The volitional causer of an event EXPERIENCER The experiencer of an event FORCE The non-volitional causer of the event THEME The participant most directly affected by an event RESULT The end product of an event CONTENT The proposition or content of a propositional event INSTRUMENT An instrument used in an event BENEFICIARY The beneﬁciary of an event SOURCE The origin of the object of a transfer event GOAL The destination of an object of a transfer event Figure 22.1 Some commonly used thematic roles with their deﬁnitions. (22.1) Sasha broke the window. (22.2) Pat opened the door. 22.2 • DIATHESIS ALTERNATIONS 3 Thematic Role Example AGENT The waiter spilled the soup. EXPERIENCER John has a headache. FORCE The wind blows debris from the mall into our yards. THEME Only after Benjamin Franklin broke the ice... RESULT The city built a regulation-size baseball diamond... CONTENT Mona asked “You met Mary Ann at a supermarket?” INSTRUMENT He poached catﬁsh, stunning them with a shocking device... BENEFICIARY Whenever Ann Callahan makes hotel reservations for her boss... SOURCE I ﬂew in from Boston. GOAL I drove to Portland. Figure 22.2 Some prototypical examples of various thematic roles. 22.2 Diathesis Alternations The main reason computational systems use semantic roles is to act as a shallow Thematic grid, case frame, θ-grid earlier examples, if a document says that Company A acquired Company B, we d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation. Semantic roles thus help generalize over different surface realizations of pred- icate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. Consider these possible realizations of the thematic arguments of the verb break: (22.3) John AGENT broke the window. THEME (22.4) John AGENT broke the window THEME with a rock. INSTRUMENT (22.5) The rock INSTRUMENT broke the window. THEME (22.6) The window THEME broke. (22.7) The window THEME was broken by John. AGENT These examples suggest that break has (at least) the possible arguments AGENT, THEME and INSTRUMENT The set of thematic role arguments taken by a verb is 10 thematic grid, case frame, θ-grid Break: AGENT, THEME, INSTRUMENT. AGENT THEME (22.4) John AGENT broke the window THEME with a rock. INSTRUMENT (22.5) The rock INSTRUMENT broke the window. THEME (22.6) The window THEME broke. (22.7) The window THEME was broken by John. AGENT These examples suggest that break has (at least) the possible a THEME, and INSTRUMENT. The set of thematic role arguments often called the thematic grid, q-grid, or case frame. We can thematic grid case frame (among others) the following possibilities for the realization of t break: AGENT/Subject, THEME/Object AGENT/Subject, THEME/Object, INSTRUMENT/PPwith INSTRUMENT/Subject, THEME/Object THEME/Subject It turns out that many verbs allow their thematic roles to be syntactic positions. For example, verbs like give can realize the arguments in two different ways: Example usages of “break” Some realizations: Diathesis alternations (or verb alternation) Dative alternation: particular semantic classes of verbs, “verbs of future having” (advance, allocate, offer, owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw), etc. Levin (1993): 47 semantic classes (“Levin classes”) for 3100 English verbs and alternations. In online resource VerbNet. 11 MANTIC ROLE LABELING a. Doris AGENT gave the book THEME to Cary. GOAL b. Doris AGENT gave Cary GOAL the book. THEME multiple argument structure realizations (the fact that break can take AGENT, NT, or THEME as subject, and give can realize its THEME and GOAL in ) are called verb alternations or diathesis alternations. The alternation above for give, the dative alternation, seems to occur with particular se- ses of verbs, including “verbs of future having” (advance, allocate, offer, d verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw), Break: AGENT, INSTRUMENT, or THEME as subject Give: THEME and BENEFICIARY in either order BENEFICIARY BENEFICIARY Problems with Thematic Roles Hard to create standard set of roles or formally define them Often roles need to be fragmented to be defined. Levin and Rappaport Hovav (2015): two kinds of INSTRUMENTS intermediary instruments that can appear as subjects The cook opened the jar with the new gadget. The new gadget opened the jar. enabling instruments that cannot Shelly ate the sliced banana with a fork. *The fork ate the sliced banana. 12 Alternatives to thematic roles 1. Fewer roles: generalized semantic roles, defined as prototypes (Dowty 1991) PROTO-AGENT PROTO-PATIENT 2. More roles: Define roles specific to a group of predicates 13 FrameNet PropBank Semantic Role Labeling The Proposition Bank (PropBank) PropBank • Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106 15 PropBank Roles Proto-Agent • Volitional involvement in event or state • Sentience (and/or perception) • Causes an event or change of state in another participant • Movement (relative to position of another participant) Proto-Patient • Undergoes change of state • Causally affected by another participant • Stationary relative to movement of another participant 16 Following Dowty 1991 PropBank Roles • Following Dowty 1991 • Role definitions determined verb by verb, with respect to the other roles • Semantic roles in PropBank are thus verb-sense specific. • Each verb sense has numbered argument: Arg0, Arg1, Arg2,… Arg0: PROTO-AGENT Arg1: PROTO-PATIENT Arg2: usually: benefactive, instrument, attribute, or end state Arg3: usually: start point, benefactive, instrument, or attribute Arg4 the end point (Arg2-Arg5 are not really that consistent, causes a problem for labeling) 17 PropBank Frame Files 18 deﬁnitions. (22.11) agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing Ex1: [Arg0 The group] agreed [Arg1 it wouldn’t make an offer]. Ex2: [ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary] [Arg1 on everything]. (22.12) fall.01 Arg1: Logical subject, patient, thing falling Arg2: Extent, amount fallen Arg3: start point Arg4: end point, end state of arg1 Ex1: [Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million]. Ex2: [Arg1 The average junk bond] fell [Arg2 by 4.2%]. Note that there is no Arg0 role for fall because the normal subject of fall is a (22.11) agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing Ex1: [Arg0 The group] agreed [Arg1 it wouldn’t make an offer]. Ex2: [ArgM-TMP Usually] [Arg0 John] agrees [Arg2 with Mary] [Arg1 on everything]. (22.12) fall.01 Arg1: Logical subject, patient, thing falling Arg2: Extent, amount fallen Arg3: start point Arg4: end point, end state of arg1 Ex1: [Arg1 Sales] fell [Arg4 to $25 million] [Arg3 from $27 million]. Ex2: [Arg1 The average junk bond] fell [Arg2 by 4.2%]. Advantage of a ProbBank Labeling The PropBank semantic roles can be useful in recovering shallow formation about verbal arguments. Consider the verb increase: (22.13) increase.01 “go up incrementally” Arg0: causer of increase Arg1: thing increasing Arg2: amount increased by, EXT, or MNR Arg3: start point Arg4: end point A PropBank semantic role labeling would allow us to infer the c the event structures of the following three examples, that is, that in Fruit Co. is the AGENT and the price of bananas is the THEME, despi surface forms. (22.14) [Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas]. (22 15) [ Th i f b ] i d i [ b B 19 The PropBank semantic roles can be useful in recovering shallow semantic in- formation about verbal arguments. Consider the verb increase: (22.13) increase.01 “go up incrementally” Arg0: causer of increase Arg1: thing increasing Arg2: amount increased by, EXT, or MNR Arg3: start point Arg4: end point A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms. (22.14) [Arg0 Big Fruit Co. ] increased [Arg1 the price of bananas]. (22.15) [Arg1 The price of bananas] was increased again [Arg0 by Big Fruit Co. ] (22.16) [Arg1 The price of bananas] increased [Arg2 5%]. PropBank also has a number of non numbered arguments called ArgMs (ArgM This would allow us to see the commonalities in these 3 sentences: Modifiers or adjuncts of the predicate: Arg-M Arg1 Arg2 PropBank also has a number of non-numbered arguments called ArgMs, (ArgM- TMP, ArgM-LOC, etc) which represent modiﬁcation or adjunct meanings. These are relatively stable across predicates, so aren’t listed with each frame ﬁle. Data labeled with these modiﬁers can be helpful in training systems to detect temporal, location, or directional modiﬁcation across predicates. Some of the ArgM’s include: TMP when? yesterday evening, now LOC where? at the museum, in San Francisco DIR where to/from? down, to Bangkok MNR how? clearly, with much enthusiasm PRP/CAU why? because ... , in response to the ruling REC themselves, each other ADV miscellaneous PRD secondary predication ...ate the meat raw While PropBank focuses on verbs, a related project, NomBank (Meyers et al., 2004) adds annotations to noun predicates. For example the noun agreement in Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as the Arg2. This allows semantic role labelers to assign labels to arguments of both verbal and nominal predicates 20 ArgM- PropBanking a Sentence PropBank - A TreeBanked Sentence Analysts S NP-SBJ VP have VP been VP expecting NP a GM-Jaguar pact NP that SBAR WHNP-1 *T*-1 S NP-SBJ VP would VP give the US car maker NP NP an eventual 30% stake NP the British company NP PP-LOC in (S (NP-SBJ Analysts) (VP have (VP been (VP expecting (NP (NP a GM-Jaguar pact) (SBAR (WHNP-1 that) (S (NP-SBJ *T*-1) (VP would (VP give (NP the U.S. car maker) (NP (NP an eventual (ADJP 30 %) stake) (PP-LOC in (NP the British company)))))))))))) Analysts have been expecting a GM-Jaguar pact that would give the U.S. car maker an eventual 30% stake in the British company. 21 Martha Palmer 2013 A sample parse tree The same parse tree PropBanked The same sentence, PropBanked Analysts have been expecting a GM-Jaguar pact Arg0 Arg1 (S Arg0 (NP-SBJ Analysts) (VP have (VP been (VP expecting Arg1 (NP (NP a GM-Jaguar pact) (SBAR (WHNP-1 that) (S Arg0 (NP-SBJ *T*-1) (VP would (VP give Arg2 (NP the U.S. car maker) Arg1 (NP (NP an eventual (ADJP 30 %) stake) (PP-LOC in (NP the British company)))))))))))) that would give *T*-1 the US car maker an eventual 30% stake in the British company Arg0 Arg2 Arg1 expect(Analysts, GM-J pact) give(GM-J pact, US car maker, 30% stake) 22 Martha Palmer 2013 Annotated PropBank Data • Penn English TreeBank, OntoNotes 5.0. • Total ~2 million words • Penn Chinese TreeBank • Hindi/Urdu PropBank • Arabic PropBank 23 Verb Frames Coverage By Lang Current Count of Senses (lexic Language Final Count E English 10,615* Chinese 24, 642 Arabic 7,015 • Only 111 English adje 2013 Verb Frames Coverage Count of word sense (lexical units) From Martha Palmer 2013 Tutorial Plus nouns and light verbs English Noun and LVC annotation ! Example Noun: Decision ! Roleset: Arg0: decider, Arg1: decision… ! “…[yourARG0] [decisionREL] [to say look I don't want to go through this anymoreARG1]” ! Example within an LVC: Make a decision ! “…[the PresidentARG0] [madeREL-LVB] the [fundamentally correctARGM-ADJ] [decisionREL] [to get on offenseARG1]” 24 Slight from Palmer 2013 Semantic Role Labeling Semantic Role Labeling Algorithm Semantic role labeling (SRL) • The task of finding the semantic roles of each argument of each predicate in a sentence. • FrameNet versus PropBank: 26 22.6 • SEMANTIC ROLE LABELING 9 Recall that the difference between these two models of semantic roles is that FrameNet (22.27) employs many frame-speciﬁc frame elements as roles, while Prop- Bank (22.28) uses a smaller number of numbered argument labels that can be inter- preted as verb-speciﬁc labels, along with the more general ARGM labels. Some examples: (22.27) [You] can’t [blame] [the program] [for being unable to identify it] COGNIZER TARGET EVALUEE REASON (22.28) [The San Francisco Examiner] issued [a special edition] [yesterday] ARG0 TARGET ARG1 ARGM-TMP A simpliﬁed semantic role labeling algorithm is sketched in Fig. 22.4. While there are a large number of algorithms, many of them use some version of the steps in this algorithm. History • Semantic roles as a intermediate semantics, used early in • machine translation (Wilks, 1973) • question-answering (Hendrix et al., 1973) • spoken-language understanding (Nash-Webber, 1975) • dialogue systems (Bobrow et al., 1977) • Early SRL systems Simmons 1973, Marcus 1980: • parser followed by hand-written rules for each verb • dictionaries with verb-specific case frames (Levin 1977) 27 Why Semantic Role Labeling • A useful shallow semantic representation • Improves NLP tasks like: • question answering Shen and Lapata 2007, Surdeanu et al. 2011 • machine translation Liu and Gildea 2010, Lo et al. 2013 28 A simple modern algorithm extra NONE role for non-role constituents. Most standard classiﬁcation algorithms have been used (logistic regression, SVM, etc). Finally, for each test sentence to be labeled, the classiﬁer is run on each relevant constituent. We give more details of the algorithm after we discuss features. function SEMANTICROLELABEL(words) returns labeled tree parse PARSE(words) for each predicate in parse do for each node in parse do featurevector EXTRACTFEATURES(node, predicate, parse) CLASSIFYNODE(node, featurevector, parse) Figure 22.4 A generic semantic-role-labeling algorithm. CLASSIFYNODE is a 1-of-N clas- siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data such as FrameNet or PropBank. 29 How do we decide what is a predicate • If we’re just doing PropBank verbs • Choose all verbs • Possibly removing light verbs (from a list) • If we’re doing FrameNet (verbs, nouns, adjectives) • Choose every word that was labeled as a target in training data 30 Semantic Role Labeling 10 CHAPTER 22 • SEMANTIC ROLE LABELING S NP-SBJ = ARG0 VP DT NNP NNP NNP The San Francisco Examiner VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP issued DT JJ NN IN NP a special edition around NN NP-TMP noon yesterday Figure 22.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line shows the path feature NP""S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner. 31 Features Headword of constituent Examiner Headword POS NNP Voice of the clause Active Subcategorization of pred VP -> VBD NP PP 32 S NP-SBJ = ARG0 VP DT NNP NNP NNP The San Francisco Examiner VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP issued DT JJ NN IN NP a special edition around NN NP-TMP noon yesterday Figure 22.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line shows the path feature NP""S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner. • The headword of the constituent, Examiner. The headword of a constituent can be computed with standard head rules, such as those given in Chapter 11 in Fig. ??. Certain headwords (e.g., pronouns) place strong constraints on the possible semantic roles they are likely to ﬁll. • The headword part of speech of the constituent, NNP. • The path in the parse tree from the constituent to the predicate. This path is marked by the dotted line in Fig. 22.5. Following Gildea and Jurafsky (2000), we can use a simple linear representation of the path, NP""S#VP#VBD. "" and # represent upward and downward movement in the tree, respectively. The path is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate Named Entity type of constit ORGANIZATION First and last words of constit The, Examiner Linear position,clause re: predicate before Path Features Path in the parse tree from the constituent to the predicate 33 10 CHAPTER 22 • SEMANTIC ROLE LABELING S NP-SBJ = ARG0 VP DT NNP NNP NNP The San Francisco Examiner VBD = TARGET NP = ARG1 PP-TMP = ARGM-TMP issued DT JJ NN IN NP a special edition around NN NP-TMP noon yesterday , NNP. o the predicate. This path is Gildea and Jurafsky (2000), path, NP""S#VP#VBD. "" and n the tree, respectively. The f many kinds of grammatical d the predicate. appears, in this case, active nd to have strongly different ti Final feature vector • For “The San Francisco Examiner”, • Arg0, [issued, NP, Examiner, NNP, active, before, VPàVBD NP PP, ORG, The, Examiner, ] • Other features could be used as well • sets of n-grams inside the constituent • other path features • the upward or downward halves • whether particular nodes occur in the path 34 the constituent, Examiner. The headword of a constituent with standard head rules, such as those given in Chapter 11 headwords (e.g., pronouns) place strong constraints on the roles they are likely to ﬁll. rt of speech of the constituent, NNP. arse tree from the constituent to the predicate. This path is ted line in Fig. 22.5. Following Gildea and Jurafsky (2000), le linear representation of the path, NP""S#VP#VBD. "" and d and downward movement in the tree, respectively. The as a compact representation of many kinds of grammatical hips between the constituent and the predicate. lause in which the constituent appears, in this case, active h passive). Passive sentences tend to have strongly different ic roles to surface form than do active ones. position of the constituent with respect to the predicate, ter. ti f th di t th t f t d t th t 3-step version of SRL algorithm 1. Pruning: use simple heuristics to prune unlikely constituents. 2. Identification: a binary classification of each node as an argument to be labeled or a NONE. 3. Classification: a 1-of-N classification of all the constituents that were labeled as arguments by the previous stage 35 Why add Pruning and Identification steps? • Algorithm is looking at one predicate at a time • Very few of the nodes in the tree could possibly be arguments of that one predicate • Imbalance between • positive samples (constituents that are arguments of predicate) • negative samples (constituents that are not arguments of predicate) • Imbalanced data can be hard for many classifiers • So we prune the very unlikely constituents first, and then use a classifier to get rid of the rest. 36 Pruning heuristics – Xue and Palmer (2004) • Add sisters of the predicate, then aunts, then great-aunts, etc • But ignoring anything in a coordination structure 37 and then a binary classiﬁer is trained to further separate the positive samples from the negative samples. The goal of this ﬁltering process is just to decide whether a constituent is an argument or not. Then a multi-class classiﬁer is trained to decide the speciﬁc semantic role for this argument. In the ﬁltering stage, it is generally a good idea to be conservative and err on the side of keeping too many constituents rather than being too aggressive and ﬁltering out true arguments. This can be achieved by lowering the threshold for positive samples, or conversely, raising the threshold for negative samples. (20) S S CC S NP VP and NP VP Strikes and mismanagement VBD VP Premier Ryzhkov VBD PP were VBD warned of tough measures cited A common final stage: joint inference • The algorithm so far classifies everything locally – each decision about a constituent is made independently of all others • But this can’t be right: Lots of global or joint interactions between arguments • Constituents in FrameNet and PropBank must be non-overlapping. • A local system may incorrectly label two overlapping constituents as arguments • PropBank does not allow multiple identical arguments • labeling one constituent ARG0 • Thus should increase the probability of another being ARG1 38 How to do joint inference • Reranking • The first stage SRL system produces multiple possible labels for each constituent • The second stage classifier the best global label for all constituents • Often a classifier that takes all the inputs along with other features (sequences of labels) 39 Semantic Role Labeling Conclusion Semantic Role Labeling • A level of shallow semantics for representing events and their participants • Intermediate between parses and full semantics • Two common architectures, for various languages • FrameNet: frame-specific roles • PropBank: Proto-roles • Current systems extract by • parsing sentence • Finding predicates in the sentence • For each one, classify each parse tree constituent 41 "
402,"Intro to Informa*on Extrac*on -­‐Sen*ment Lexicon Induc*on as an example Many slides adapted from Ellen Riloff and Dan Jurafsky What is Informa*on Extrac*on? • Informa*on extrac*on (IE) is an umbrella term for NLP tasks that involve extrac*ng pieces of informa*on from text and assigning some meaning to the informa*on. • Many IE applica*ons aim to turn unstructured text into a “structured” representa*on. • IE problems typically involve: – iden*fying text snippets to extract – assigning seman*c meaning to en**es or concepts – ﬁnding rela*ons between en**es or concepts IE Applica*ons • Biological Processes (Genomics) • Clinical Medicine • Ques*on Answering / Web Search • Query Expansion / Seman*c Sets • Extrac*ng En*ty Proﬁles • Tracking Events (Violent, Diseases, Business, etc) • Tracking Opinions (Poli*cal, Product Reputa*on, Financial Predic*on, On-­‐line Reviews, etc.) General Techniques • Syntac*c Analysis – Phrase Iden*ﬁca*on – Feature Extrac*on • Seman*c Analysis • Sta*s*cal Measures • Machine Learning – Supervised & Weakly Supervised • Graph Algorithms Named En*ty Recogni*on (NER) Mars One announced Monday that it has picked 1,058 aspiring spaceﬂyers to move on to the next round in its search for the ﬁrst humans to live and die on the Red Planet. The Wall Street Journal reports that Google plans to partner with Toyota to develop Android so`ware for their hybrid cars. NER typically involves extrac*ng and labeling certain types of en**es, such as proper names and dates. Domain-­‐speciﬁc NER IL−2 gene expression and NFkappa B ac*va*on through CD28 requires reac*ve oxygen produc*on by 5lipoxygenase. Biomedical systems must recognize genes and proteins: Adrenal-­‐Sparing surgery is safe and eﬀec*ve, and may become the treatment of choice in pa*ents with hereditary phaeochromocytoma. Clinical medical systems must recognize problems and treatments: Seman*c Class Iden*ﬁca*on Mars One announced Monday that it has picked 1,058 aspiring spaceﬂyers to move on to the next round in its search for the ﬁrst humans to live and die on the Red Planet. The Wall Street Journal reports that Google plans to partner with Toyota to develop Android so`ware for their hybrid cars. Seman*c Lexicon Induc*on • Although some general seman*c dic*onaries exist (e.g., WordNet), domain-­‐speciﬁc applica*ons o`en have specialized vocabulary. • Seman*c Lexicon Induc*on techniques learn lists of words that belong to a seman*c class. Vehicles: car, jeep, helicopter, bike, tricycle, scooter, … Animal: *ger, zebra, wolverine, platypus, echidna, … Symptoms: cough, sneeze, pain, pu/pd, elevated bp, … Products: camera, laptop, iPad, tablet, GPS device, … Domain-­‐speciﬁc Vocabulary A 14yo m/n doxy owned by a reputable breeder is being treated for IBD with pred. doxy pred IBD breeder ANIMAL HUMAN DISEASE DRUG Domain-specific meanings: lab, mix, m/n = ANIMAL Seman*c Taxonomy Induc*on • Ideally, we want seman*c concepts to be organized in a taxonomy, to support generaliza*on but to dis*nguish diﬀerent subtypes. Animal Mammal Feline Lion, Panthera Leo Tiger, Panthera Tigris, Felis Tigris Cougar, Mountain Lion, Puma, Panther, Catamount Canine Wolf, Canis Lupus Coyote, Prairie Wolf, Brush Wolf, American Jackal Dog, Puppy, Canis Lupus Familiaris, Mongrel Challenges in Taxonomy Induc*on • But there are o`en many ways to organize a conceptual space! • Strict hierarchies are rare in real data – graphs/networks are more realis*c than tree structures. • For example, animals could be subcategorized based on: – carnivore vs. herbivore – water-­‐dwelling vs. land-­‐dwelling – wild vs. pets vs. agricultural – physical characteris*cs (e.g., baleen vs. toothed whales) – habitat (e.g., arc*c vs. desert) Rela*on Extrac*on In Salzburg, liile Mozart grew up in a loving middle-­‐ class environment. Birthplace(Mozart, Salzburg) Steve Ballmer is an American businessman who has been serving as the CEO of Microso4 since January 2000 Employed-­‐By(Steve Ballmer, Microso`) CEO(Steve Ballmer, Microso`) Rela*ons for Web Search Paraphrasing • Rela*ons can o`en be expressed with a mul*tude of diﬀerence expressions. • Paraphrasing systems try to explicitly learn phrases that represent the same type of rela*on. • Examples: – X was born in Y – Y is the birthplace of X – X’s birthplace is Y – X’s hometown is Y – X grew up in Y Thema*c/Seman*c Roles John broke the window with a hammer. The hammer broke the window. The window broke. Agent = John Theme = window Instrument = hammer I ate the spaghen with tomato sauce with a fork with a friend. Agent = I Theme = spaghen Co-­‐theme = tomato sauce Instrument = fork Co-­‐Agent = friend Seman*c Role Labeling Protagonist1: Julie Protagonist2: Bob Topic: poli*cs Medium: French Julie argued with Bob about poli*cs in French. She blamed the government for failing to do enough to help. Judge: She Evaluee: the government Reason: failing to do enough to help 17 Event Extraction Goal: extract facts about events from unstructured documents December 29, Pakistan - The U.S. embassy in Islamabad was damaged this morning by a car bomb. Three diplomats were injured in the explosion. Al Qaeda has claimed responsibility for the attack. EVENT Type: bombing Target: U.S. embassy Location: Islamabad, Pakistan Date: December 29 Weapon: car bomb Victim: three diplomats Perpetrator: Al Qaeda Example: extracting information about terrorism events in news articles: Event Extrac*on Document Text New Jersey, February, 26. An outbreak of swine ﬂu has been conﬁrmed in Mercer County, NJ. Five teenage boys appear to have contracted the deadly virus from an unknown source. The CDC is investigating the cases and is taking measures to prevent the spread. . . Event Disease: swine ﬂu Location: Mercer County, NJ Victim: Five teenage boys Date: February 26 Status: conﬁrmed Another example: extrac*ng informa*on about disease outbreak events. Large-­‐Scale IE from the Web • Some researchers have been developing IE systems for large-­‐scale extrac*on of facts and rela*ons from the Web. • These systems exploit the massive amount of text and redundancy available on the Web and use weakly supervised, itera*ve learning to harvest informa*on for automated knowledge base construc*on. • The KnowItAll project at UW and NELL project at CMU are well-­‐known research groups pursuing this work. Opinion Extrac*on Source: <writer> Target: Powershot Aspect: pictures, colors Evalua*on: beau*ful, easy to grip I just bought a Powershot a few days ago. I took some pictures using the camera. Colors are so beau*ful even when ﬂash is used. Also easy to grip since the body has a grip handle. [Kobayashi et al., 2007] Opinion Extrac*on from News [Wilson & Wiebe, 2009] Source: Italian senator Renzo Gubert Target: the Chinese Government Evalua*on: praisedPOSITIVE Italian senator Renzo Gubert praised the Chinese Government’s eﬀorts. African observers generally approved of his victory while Western governments denounced it. Source: African observers Target: his victory Evalua*on: approvedPOSITIVE Source: Western governments Target: it (his victory) Evalua*on: denouncedNEGATIVE Summary • Informa*on extrac*on systems frequently rely on low-­‐ level NLP tools for basic language analysis, o`en in a pipeline architecture. • There are a wide variety of applica*ons for IE, including both broad-­‐coverage and domain-­‐speciﬁc applica*ons. • Some IE tasks are rela*vely well-­‐understood (e.g., named en*ty recogni*on), while others are s*ll quite challenging! • We’ve only scratched the surface of possible IE tasks … nearly endless possibili*es. Turney Algorithm to learn a Sen*ment Lexicon 1. Extract a phrasal lexicon from reviews 2. Learn polarity of each phrase 3. Rate a review by the average polarity of its phrases 24 Turney (2002): Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classiﬁcation of Reviews Extract two-­‐word phrases with adjec*ves First Word Second Word Third Word (not extracted) JJ NN or NNS anything RB, RBR, RBS JJ Not NN nor NNS JJ JJ Not NN nor NNS NN or NNS JJ Not NN nor NNS RB, RBR, or RBS VB, VBD, VBN, VBG anything 25 How to measure polarity of a phrase? • Posi*ve phrases co-­‐occur more with “excellent” • Nega*ve phrases co-­‐occur more with “poor” • But how to measure co-­‐occurrence? 26 Pointwise Mutual Informa*on • Mutual informa?on between 2 random variables X and Y • Pointwise mutual informa?on: – How much more do events x and y co-­‐occur than if they were independent? I(X,Y)= P(x, y) y ∑ x ∑ log2 P(x,y) P(x)P(y) PMI(X,Y)= log2 P(x,y) P(x)P(y) Pointwise Mutual Informa*on • Pointwise mutual informa?on: – How much more do events x and y co-­‐occur than if they were independent? • PMI between two words: – How much more do two words co-­‐occur than if they were independent? PMI(word1,word2)= log2 P(word1,word2) P(word1)P(word2) PMI(X,Y)= log2 P(x,y) P(x)P(y) How to Es*mate Pointwise Mutual Informa*on – Query search engine (Altavista) • P(word) es*mated by hits(word)/N • P(word1,word2) by hits(word1 NEAR word2)/N – (More correctly the bigram denominator should be kN, because there are a total of N consecu*ve bigrams (word1,word2), but kN bigrams that are k words apart, but we just use N on the rest of this slide and the next.) PMI(word1,word2)= log2 1 Nhits(word1 NEAR word2) 1 Nhits(word1) 1 Nhits(word2) Does phrase appear more with “poor” or “excellent”? 30 Polarity(phrase)= PMI(phrase,""excellent"")−PMI(phrase,""poor"") = log2 hits(phrase NEAR ""excellent"")hits(""poor"") hits(phrase NEAR ""poor"")hits(""excellent"") ! "" # $ % & = log2 hits(phrase NEAR ""excellent"") hits(phrase)hits(""excellent"") hits(phrase)hits(""poor"") hits(phrase NEAR ""poor"") = log2 1 N hits(phrase NEAR ""excellent"") 1 N hits(phrase) 1 N hits(""excellent"") −log2 1 N hits(phrase NEAR ""poor"") 1 N hits(phrase) 1 N hits(""poor"") Phrases from a thumbs-­‐up review 31 Phrase POS tags Polarit y online service JJ NN 2.8 online experience JJ NN 2.3 direct deposit JJ NN 1.3 local branch JJ NN 0.42 … low fees JJ NNS 0.33 true service JJ NN -0.73 other bank JJ NN -0.85 inconveniently located JJ NN -1.5 Average 0.32 Phrases from a thumbs-­‐down review 32 Phrase POS tags Polarity direct deposits JJ NNS 5.8 online web JJ NN 1.9 very handy RB JJ 1.4 … virtual monopoly JJ NN -2.0 lesser evil RBR JJ -2.3 other problems JJ NNS -2.8 low funds JJ NNS -6.8 unethical prac*ces JJ NNS -8.5 Average -1.2 Results of Turney algorithm • 410 reviews from Epinions – 170 (41%) nega*ve – 240 (59%) posi*ve • Majority class baseline: 59% • Turney algorithm: 74% • Phrases rather than words • Learns domain-­‐speciﬁc informa*on 33 "
403,"Semantic Class Induction Some slides were adapted from the ones created by Ellen Riloff Motivation • A semantic lexicon assigns semantic categories to words. • Domain-specific vocabulary is often not found in general purpose resources, such as WordNet. • Automatic methods could be used to enhance these resources or create domain-specific lexicons. politician human truck vehicle grenade weapon Syntactic Heuristics for Learning Semantic Labels Conjunctions lions and tigers and bears Lists lions, tigers, bears Appositives the horse, a stallion Predicate Nominals the wolf is a mammal Compound nouns tuna fish Honda Sedan [Riloff & Shepherd 97; Roark & Charniak 98; Phillips & Riloff 02; etc.] [Hearst 92; KnowItAll (U.Washington), Kozareva et al. 2008; etc.] Hyponym patterns dogs such as beagles and boxers dogs, including beagles and boxers Bootstrapping Semantic Lexicons Unannotated Texts Co-occurrence Statistics prospective category words Ex: dog, cat, lion, lizard, snake N best words Ex: terrier, poodle, tiger, frog, iguana Extraction Patterns • Represent syntactic context that often reveals the semantic class of a word. • AutoSlog: each pattern extracts an NP from one of 3 syntactic positions: subject, direct object, pp obj. Some patterns to extract locations: <subject> was inhabited the locality was inhabited… patrolling <direct object> …patrolling Zacamil neighborhood lives in <pp obj> …lives in Argentina <subject> passive-vp <target> was bombed <subject> active-vp <perpetrator> bombed <subject> active-vp dobj <perpetrator> threw dynamite <subject> active-vp infinitive <perpetrator> tried to kill <subject> passive-vp infinitive <perpetrator> was hired to kill <subject> auxiliary dobj <victim> was fatality active-vp <dobj> bombed <target> infinitive <dobj> to kill <victim> active-vp infinitive <dobj> tried to kill <victim> passive-vp infinitive <dobj> was hired to kill <victim> subject auxiliary <dobj> fatality was <victim> passive-vp prep <np> was killed by <perpetrator> active-vp prep <np> exploded in <target> infinitive prep <np> to kill with <weapon> noun prep <np> assassination of <victim> EXTRACTION PATTERN TYPES Mutual Bootstrapping [Riloff & Jones 99] Unannotated Texts Best Extraction Pattern Extractions (Nouns) Ex: dog, cat, lion, lizard, snake Ex: <NP> growled Ex: Rottweiler, terrier, cougar Mutual Bootstrapping Example Best pattern: headquartered in <NP> Extractions: Nicaragua, city, Chapare region, San Miguel Best pattern: downed in <NP> Extractions: Nicaragua, city, Usulutan region, San Miguel, area, Soyapango Best pattern: to occupy <NP> Extractions: Nicaragua, town, this northern area, small country, San Sebastian neighborhood, private property SEEDS: Nicaragua, city, region, town Examples of Learned Patterns Location Patterns (Web) Location Patterns (Terrorism) offices in <np> living in <np> facilities in <np> traveled in <np> operations in <np> become in <np> loans in <np> sought in <np> operates in <np> presidents in <np> locations in <np> parts of <np> producer in <np> to enter <np> states of <np> condemned in <np> seminars in <np> relations between <np> activities in <np> ministers of <np> consulting in <np> part in <np> countries of <np> taken in <np> Bootstrapping Procedure 1. Start from several seed words for a semantic class and an unlabeled text corpus. 2. Score patterns and keep the top N patterns. 3. Score pattern extractions (candidate lexicon words) and select the top M new words as new lexicon words. 4. Increase N by 1 and go back to step 1. Pattern Scoring RlogF (patterni) = Fi Ni * log2 (Fi) Fi is the number of unique category members extracted by patterni Ni is the total number of unique nouns extracted by patterni where: Every extraction pattern is scored and the best patterns are kept. The scoring function is: Selecting Words for the Lexicon Score: the average number of category members extracted by each pattern (while the original algorithm considers all patterns, we’ll only consider patterns selected in the previous iteration) that extracted the candidate word. score (wordi) = Fj Ni S j=1 Ni Fj is the number of unique category members extracted by patternj Ni is the total number of patterns that extract wordi where: S j=1 Ni AvgLog (wordi) = log2 (Fj + 1) Ni More notes • The mutual bootstrapping approach can be extend to learn semantic lexicon in multiple categories (the Basilisk system). • Very often, manual review is still necessary to use the learned dictionaries. • Performance for some categories is beginning to approach levels for which manual review may not be necessary. "
404,"Relation Extraction What is relation extraction? Many slides adapted from Dan Jurafsky Extracting relations from text • Company report: “International Business Machines Corporation (IBM or the company) was incorporated in the State of New York on June 16, 1911, as the Computing-Tabulating-Recording Co. (C-T-R)…” • Extracted Complex Relation: Company-Founding Company IBM Location New York Date June 16, 1911 Original-Name Computing-Tabulating-Recording Co. • But we will focus on the simpler task of extracting relation triples Founding-year(IBM,1911) Founding-location(IBM,New York) Why Relation Extraction? • Create new structured knowledge bases, useful for any app • Augment current knowledge bases • Adding words to WordNet thesaurus, facts to FreeBase or DBPedia • Support question answering • The granddaughter of which actor starred in the movie “E.T.”? (acted-in ?x “E.T.”)(is-a ?y actor)(granddaughter-of ?x ?y) • But which relations should we extract? 3 Automatic Content Extraction (ACE) ARTIFACT GENERAL AFFILIATION ORG AFFILIATION PART- WHOLE PERSON- SOCIAL PHYSICAL Located Near Business Family Lasting Personal Citizen- Resident- Ethnicity- Religion Org-Location- Origin Founder Employment Membership Ownership Student-Alum Investor User-Owner-Inventor- Manufacturer Geographical Subsidiary Sports-Affiliation 17 relations from 2008 “Relation Extraction Task” Automatic Content Extraction (ACE) • Physical-Located PER-GPE He was in Tennessee • Part-Whole-Subsidiary ORG-ORG XYZ, the parent company of ABC • Person-Social-Family PER-PER John’s wife Yoko • Org-AFF-Founder PER-ORG Steve Jobs, co-founder of Apple… • 5 UMLS: Unified Medical Language System • 134 entity types, 54 relations Injury disrupts Physiological Function Bodily Location location-of Biologic Function Anatomical Structure part-of Organism Pharmacologic Substance causes Pathological Function Pharmacologic Substance treats Pathologic Function Databases of Wikipedia Relations 7 Relations extracted from Infobox Stanford state California Stanford motto “Die Luft der Freiheit weht” … Wikipedia Infobox How to build relation extractors 1. Hand-written patterns 2. Supervised machine learning 3. Semi-supervised and unsupervised • Bootstrapping (using seeds) • Distant supervision • Unsupervised learning from the web Relation Extraction What is relation extraction? Relation Extraction Using patterns to extract relations Extracting Richer Relations Using Rules • Intuition: relations often hold between specific entities • located-in (ORGANIZATION, LOCATION) • founded (PERSON, ORGANIZATION) • cures (DRUG, DISEASE) • Start with Named Entity tags to help extract relation! Named Entities aren’t quite enough. Which relations hold between 2 entities? Drug Disease Cure? Prevent? Cause? What relations hold between 2 entities? PERSON ORGANIZATION Founder? Investor? Member? Employee? President? Extracting Richer Relations Using Rules and Named Entities Who holds what office in what organization? PERSON, POSITION of ORG • George Marshall, Secretary of State of the United States PERSON(named|appointed|chose|etc.) PERSON Prep? POSITION • Truman appointed Marshall Secretary of State PERSON [be]? (named|appointed|etc.) Prep? ORG POSITION • George Marshall was named US Secretary of State Hand-built patterns for relations • Plus: • Human patterns tend to be high-precision • Can be tailored to specific domains • Minus • Human patterns are often low-recall • A lot of work to think of all possible patterns! • Don’t want to have to do this for every relation! • We’d like better accuracy Relation Extraction Using patterns to extract relations Relation Extraction Supervised relation extraction Supervised machine learning for relations • Choose a set of relations we’d like to extract • Choose a set of relevant named entities • Find and label data • Choose a representative corpus • Label the named entities in the corpus • Hand-label the relations between these entities • Break into training, development, and test • Train a classifier on the training set 18 How to do classification in supervised relation extraction 1. Find all pairs of named entities (usually in same sentence) 2. Decide if 2 entities are related 3. If yes, classify the relation • Why the extra step? • Faster classification training by eliminating most pairs • Can use distinct feature-sets appropriate for each task. 19 Automated Content Extraction (ACE) ARTIFACT GENERAL AFFILIATION ORG AFFILIATION PART- WHOLE PERSON- SOCIAL PHYSICAL Located Near Business Family Lasting Personal Citizen- Resident- Ethnicity- Religion Org-Location- Origin Founder Employment Membership Ownership Student-Alum Investor User-Owner-Inventor- Manufacturer Geographical Subsidiary Sports-Affiliation 17 sub-relations of 6 relations from 2008 “Relation Extraction Task” Relation Extraction Classify the relation between two entities in a sentence American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said. SUBSIDIARY FAMILY EMPLOYMENT NIL FOUNDER CITIZEN INVENTOR … Word Features for Relation Extraction • Headwords of M1 and M2, and combination Airlines Wagner Airlines-Wagner • Bag of words and bigrams in M1 and M2 {American, Airlines, Tim, Wagner, American Airlines, Tim Wagner} • Words or bigrams in particular positions left and right of M1/M2 M2: -1 spokesman M2: +1 said • Bag of words or bigrams between the two entities {a, AMR, of, immediately, matched, move, spokesman, the, unit} American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said Mention 1 Mention 2 Named Entity Type and Mention Level Features for Relation Extraction • Named-entity types • M1: ORG • M2: PERSON • Concatenation of the two named-entity types • ORG-PERSON • Entity Level of M1 and M2 (NAME, NOMINAL, PRONOUN) • M1: NAME [it or he would be PRONOUN] • M2: NAME [the company would be NOMINAL] American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said Mention 1 Mention 2 Parse Features for Relation Extraction • Base syntactic chunk sequence from one to the other NP NP PP VP NP NP • Constituent path through the tree from one to the other NP é NP é S é S ê NP • Dependency path Airlines matched Wagner said American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said Mention 1 Mention 2 Gazeteer and trigger word features for relation extraction • Trigger list for family: kinship terms • parent, wife, husband, grandparent, etc. [from WordNet] • Gazeteer: • Lists of useful geo or geopolitical words • Country name list • Other sub-entities American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said. Classifiers for supervised methods • Now you can use any classifier you like • MaxEnt • Naïve Bayes • SVM • ... • Train it on the training set, tune on the dev set, test on the test set Evaluation of Supervised Relation Extraction • Compute P/R/F1 for each relation 28 P = # of correctly extracted relations Total # of extracted relations R = # of correctly extracted relations Total # of gold relations F 1 = 2PR P + R Summary: Supervised Relation Extraction + Can get high accuracies with enough hand-labeled training data, if test similar enough to training - Labeling a large training set is expensive - Supervised models are brittle, don’t generalize well to different genres Relation Extraction Supervised relation extraction Relation Extraction Semi-supervised and unsupervised relation extraction Seed-based or bootstrapping approaches to relation extraction • No training set? Maybe you have: • A few seed tuples or • A few high-precision patterns • Can you use those seeds to do something useful? • Bootstrapping: use the seeds to directly learn to populate a relation Relation Bootstrapping (Hearst 1992) • Gather a set of seed pairs that have relation R • Iterate: 1. Find sentences with these pairs 2. Look at the context between or around the pair and generalize the context to create patterns 3. Use the patterns for grep for more pairs Bootstrapping • <Mark Twain, Elmira> Seed tuple • Grep (google) for the environments of the seed tuple “Mark Twain is buried in Elmira, NY.” X is buried in Y “The grave of Mark Twain is in Elmira” The grave of X is in Y “Elmira is Mark Twain’s final resting place” Y is X’s final resting place. • Use those patterns to grep for new tuples • Iterate Dipre: Extract <author,book> pairs • Start with 5 seeds: • Find Instances: The Comedy of Errors, by William Shakespeare, was The Comedy of Errors, by William Shakespeare, is The Comedy of Errors, one of William Shakespeare's earliest attempts The Comedy of Errors, one of William Shakespeare's most • Extract patterns (group by middle, take longest common prefix/suffix) ?x , by ?y , ?x , one of ?y ‘s • Now iterate, finding new seeds that match the pattern Brin, Sergei. 1998. Extracting Patterns and Relations from the World Wide Web. Author Book Isaac Asimov The Robots of Dawn David Brin Startide Rising James Gleick Chaos: Making a New Science Charles Dickens Great Expectations William Shakespeare The Comedy of Errors Distant Supervision • Combine bootstrapping with supervised learning • Instead of 5 seeds, • Use a large database to get huge # of seed examples • Create lots of features from all these examples • Combine in a supervised classifier Snow, Jurafsky, Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. NIPS 17 Fei Wu and Daniel S. Weld. 2007. Autonomously Semantifying Wikipeida. CIKM 2007 Mintz, Bills, Snow, Jurafsky. 2009. Distant supervision for relation extraction without labeled data. ACL09 Distant supervision paradigm • Like supervised classification: • Uses a classifier with lots of features • Supervised by detailed hand-created knowledge • Doesn’t require iteratively expanding patterns • Like unsupervised classification: • Uses very large amounts of unlabeled data • Not sensitive to genre issues in training corpus Distantly supervised learning of relation extraction patterns For each relation For each tuple in big database Find sentences in large corpus with both entities Extract frequent features (parse, words, etc) Train supervised classifier using thousands of features 4 1 2 3 5 PER was born in LOC PER, born (XXXX), LOC PER’s birthplace in LOC <Edwin Hubble, Marshfield> <Albert Einstein, Ulm> Born-In Hubble was born in Marshfield Einstein, born (1879), Ulm Hubble’s birthplace in Marshfield P(born-in | f1,f2,f3,…,f70000) Unsupervised relation extraction • Open Information Extraction: • extract relations from the web with no training data, no list of relations 1. Use parsed data to train a “trustworthy tuple” classifier 2. Single-pass extract all relations between NPs, keep if trustworthy 3. Assessor ranks relations based on text redundancy (FCI, specializes in, software development) (Tesla, invented, coil transformer) 39 M. Banko, M. Cararella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the web. IJCAI Evaluation of Semi-supervised and Unsupervised Relation Extraction • Since it extracts totally new relations from the web • There is no gold set of correct instances of relations! • Can’t compute precision (don’t know which ones are correct) • Can’t compute recall (don’t know which ones were missed) • Instead, we can approximate precision (only) • Draw a random sample of relations from output, check precision manually • Can also compute precision at different levels of recall. • Precision for top 1000 new relations, top 10,000 new relations, top 100,000 • In each case taking a random sample of that set • But no way to evaluate recall 40 ˆ P = # of correctly extracted relations in the sample Total # of extracted relations in the sample Relation Extraction Semi-supervised and unsupervised relation extraction "
405,"Discourse, Pragmatics, Coreference Resolution Many slides are adapted from Roger Levy, Chris Manning, Vicent Ng, Heeyoung Lee, Altaf Rahman A pragmatic issue • Just how are pronouns interpreted (resolved) in a discourse? What%is%Coreference%Resolu2on%?% – Iden2fy%all%noun%phrases%(men$ons)%that%refer%to% the%same%real%world%en2ty% Barack%Obama%nominated%Hillary%Rodham%Clinton%as%his% secretary%of%state%on%Monday.%He%chose%her%because%she% had%foreign%aﬀairs%experience%as%a%former%First%Lady.% 2% What%is%Coreference%Resolu2on%?% – Iden2fy%all%noun%phrases%(men$ons)%that%refer%to% the%same%real%world%en2ty% Barack%Obama%nominated%Hillary%Rodham%Clinton%as%his% secretary%of%state%on%Monday.%He%chose%her%because%she% had%foreign%aﬀairs%experience%as%a%former%First%Lady.% 3% What%is%Coreference%Resolu2on%?% – Iden2fy%all%noun%phrases%(men$ons)%that%refer%to% the%same%real%world%en2ty% Barack%Obama%nominated%Hillary%Rodham%Clinton%as%his% secretary%of%state%on%Monday.%He%chose%her%because%she% had%foreign%aﬀairs%experience%as%a%former%First%Lady.% 4% What%is%Coreference%Resolu2on%?% – Iden2fy%all%noun%phrases%(men$ons)%that%refer%to% the%same%real%world%en2ty% Barack%Obama%nominated%Hillary%Rodham%Clinton%as%his% secretary%of%state%on%Monday.%He%chose%her%because%she% had%foreign%aﬀairs%experience%as%a%former%First%Lady.% 5% What%is%Coreference%Resolu2on%?% – Iden2fy%all%noun%phrases%(men$ons)%that%refer%to% the%same%real%world%en2ty% Barack%Obama%nominated%Hillary%Rodham%Clinton%as%his% secretary%of%state%on%Monday.%He%chose%her%because%she% had%foreign%aﬀairs%experience%as%a%former%First%Lady.% 6% A(couple(of(years(later,(Vanaja(met(Akhila(at(the(local(park.( Akhila’s(son(Prajwal(was(just(two(months(younger(than(her( son(Akash,(and(they(went(to(the(same(school.(For(the(preK school(play,(Prajwal(was(chosen(for(the(lead(role(of(the( naughty(child(Lord(Krishna.(Akash(was(to(be(a(tree.(She( resigned(herself(to(make(Akash(the(best(tree(that(anybody( had(ever(seen.(She(bought(him(a(brown(TKshirt(and(brown( trousers(to(represent(the(tree(trunk.(Then(she(made(a(large( cardboard(cutout(of(a(tree’s(foliage,(with(a(circular(opening( in(the(middle(for(Akash’s(face.(She(attached(red(balls(to(it( to(represent(fruits.(It(truly(was(the(nicest(tree.( From(The(Star(by(Shruthi(Rao,(with(some(shortening.( Reference(Resolution( • Noun(phrases(refer(to(entities(in(the(world,(many( pairs(of(noun(phrases(coKrefer,(some(nested(inside( others( John(Smith,(CFO(of(Prime(Corp.(since(1986,(( saw((his(pay(jump(20%(to($1.3(million(( as(the(57KyearKold(also(became(( the(ﬁnancial(services(co.’s(president.( Kinds(of(Reference( • Referring(expressions( – John%Smith% – President%Smith% – the%president% – the%company’s%new%executive% • Free(variables( – Smith(saw(his%pay%increase( • Bound(variables(( – The(dancer(hurt(herself.( More(interesting( grammatical( constraints,( more(linguistic( theory,(easier(in( practice( “anaphora( resolution”( More(common(in( newswire,(generally( harder(in(practice( Not(all(NPs(are(referring!( • Every%dancer(twisted(her%knee.% • (No%dancer(twisted(her%knee.)( • There(are(three(NPs(in(each(of(these( sentences;(because(the(ﬁrst(one(is(nonK referential,(the(other(two(aren’t(either.(( Two(diﬀerent(things…( • Anaphora( – Text( – World( • (Co)Reference( – Text( – World( Supervised(Machine(Learning( Pronominal(Anaphora(Resolution( • Given%a%pronoun%and%an%en2ty%men2oned%earlier,%classify% whether%the%pronoun%refers%to%that%en2ty%or%not%given%the% surrounding%context%(yes/no)% • Usually%ﬁrst%ﬁlter%out%pleonas2c%pronouns%like%“It%is% raining.”%(perhaps%using%handUwriVen%rules)% • Use%any%classiﬁer,%obtain%posi2ve%examples%from%training%data,% generate%nega2ve%examples%by%pairing%each%pronoun%with% other%(incorrect)%en22es%% • This%is%naturally%thought%of%as%a%binary%classiﬁca2on%(or% ranking)%task% % Mr.%Obama%visited%the%city.%The%president%talked%about%Milwaukee%’s%economy.%He%men2oned%new%jobs.% ? ? ? Features(for(Pronominal(Anaphora( Resolution( • Constraints:( – Number(agreement( • Singular(pronouns((it/he/she/his/her/him)(refer(to(singular( entities(and(plural(pronouns((we/they/us/them)(refer(to( plural(entities( – Person(agreement( • He/she/they(etc.(must(refer(to(a(third(person(entity( – Gender(agreement( • He((John;(she((Mary;(it((car( • Jack(gave(Mary(a(gift.((She(was(excited.( – Certain(syntactic(constraints( • John(bought(himself(a(new(car.([himself((John]( • John(bought(him(a(new(car.([him(can(not(be(John](( ( Features for Pronominal Anaphora Resolution • Preferences:% – Recency:%More%recently%men2oned%en22es%are%more% likely%to%be%referred%to% • John%went%to%a%movie.%Jack%went%as%well.%He%was%not%busy.% – Gramma2cal%Role:%En22es%in%the%subject%posi2on%is% more%likely%to%be%referred%to%than%en22es%in%the%object% posi2on% • John%went%to%a%movie%with%Jack.%He%was%not%busy.%% – Parallelism:%% • John%went%with%Jack%to%a%movie.%Joe%went%with%him%to%a%bar.% % Features for Pronominal Anaphora Resolution • Preferences:% – Verb%Seman2cs:%Certain%verbs%seem%to%bias%whether% the%subsequent%pronouns%should%be%referring%to%their% subjects%or%objects% • John%telephoned%Bill.%He%lost%the%laptop.% • John%cri2cized%Bill.%He%lost%the%laptop.% – %Selec2onal%Restric2ons:%Restric2ons%because%of% seman2cs% • John%parked%his%car%in%the%garage%aber%driving%it%around%for% hours.%% • Encode%all%these%and%maybe%more%as%features% % Machine(learning(models(of(coref( • Start(with(supervised(data( • positive(examples(that(corefer( • negative(examples(that(don’t(corefer( – Note(that(it’s(very(skewed( • The(vast(majority(of(mention(pairs(don’t%corefer( • Usually(learn(some(sort(of(discriminative(model(of(phrases/ clusters(coreferring( – Predict(1(for(coreference,(0(for(not(coreferent( • But(there(is(also(work(that(builds(clusters(of(coreferring( expressions( – E.g.,(generative(models(of(clusters(in((Haghighi(&(Klein(2007)(( Kinds(of(Models( • Mention(Pair(models( – Treat(coreference(chains(as(a( collection(of(pairwise(links( – Make(independent(pairwise(decisions( and(reconcile(them(in(some(way((e.g.( clustering(or(greedy(partitioning)( • Mention(ranking(models( – Explicitly(rank(all(candidate( antecedents(for(a(mention( • EntityKMention(models( – A(cleaner,(but(less(studied,(approach( – Posit(single(underlying(entities( – Each(mention(links(to(a(discourse( entity([Pasula(et(al.(03],([Luo(et(al.(04]( ( Pairwise(Features( [Luo(et(al.(04]( Lee(et(al.((2010):(Stanford( deterministic(coreference( 10/10/10( EMNLP(2010( 24( • Cautious(and(incremental( approach( • Multiple(passes(over(text( • Precision(of(each(pass(is( lesser(than(preceding(ones( • Recall(keeps(increasing(with( each(pass( • Decisions(once(made(cannot( be(modiﬁed(by(later(passes( • RuleKbased((“unsupervised”)( Increasing(Recall( Pass$1$ Pass$2$ Pass$3$ Pass$4$ Increasing(Precision( Approach:(start(with(high(precision( clumpings( E.g.$ % Pepsi%hopes%to%take%Quaker%oats%to%a%whole%new%level.%...%Pepsi% says%it%expects%to%double%Quaker's%snack%food%growth%rate.%...% the%deal%gives%Pepsi%access%to%Quaker%oats’%Gatorade%sport% drink%as%well%as%....%% % % % % %% 10/10/10( EMNLP(2010( 25( E.g.$ % Pepsi%hopes%to%take%Quaker$oats%to%a%whole%new%level.%...%Pepsi% says%it%expects%to%double%Quaker's%snack%food%growth%rate.%...% the%deal%gives%Pepsi%access%to%Quaker$oats’%Gatorade%sport% drink%as%well%as%....%% % % % % %% E.g.$ % Pepsi%hopes%to%take%Quaker$oats%to%a%whole%new%level.%...%Pepsi% says%it%expects%to%double%Quaker's%snack%food%growth%rate.%...% the%deal%gives%Pepsi%access%to%Quaker$oats’%Gatorade%sport% drink%as%well%as%....%% % % Exact(String(Match:(A(high(precision(feature( % %%  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . EntityKmention(model:(Clusters( instead(of(mentions( 10/10/10( EMNLP(2010( 26( m1( m2( m3( m4( m5( m6( m7( Clusters:$ m1 m2 m3 m5 m4 m6 m7 m2((((((m3( m1( (((((( m5( m2((((((m3(((((m6( Detailed(Architecture( 10/10/10( EMNLP(2010( 27( The(system(consists(of(seven(passes((or(sieves):( • Exact(Match( • Precise(Constructs((appositives,(predicate(nominatives,(…)( • Strict(Head(Matching( • Strict(Head(Matching(–(Variant(1( • Strict(Head(Matching(–(Variant(2( • Relaxed(Head(Matching( • Pronouns( Cumulative(performance(of(passes 10/10/10( EMNLP(2010( 31( Graph(showing(the(system’s(B3(Precision,(Recall(and(F1(on(ACE2004-DEV after each additional pass( 0( 10( 20( 30( 40( 50( 60( 70( 80( 90( 100( Pass(1( Pass(2( Pass(3( Pass(4( Pass(5( Pass(6( Pass(7( Precision( Recall( F1( Evaluation( • B3((BKCUBED)(algorithm(for(evaluation( – Precision(&(recall(for(entities%in%a%reference%chain( – Precision:(%(of(elements(in(a(hypothesized(reference( chain(that(are(in(the(true(reference(chain( – Recall:(%(of(elements(in(a(true(reference(chain(that( are(in(the(hypothesized(reference(chain( – Overall(precision(&(recall(are(the((weighted)(average( of(perKchain(precision(&(recall( – Optimizing(chainKchain(pairings(is(a(hard(problem( • In(the(computational(NPKhard(sense( – Greedy(matching(is(done(in(practice(for(evaluation( Evaluation(metrics( • MUC(Score((Vilain(et(al.,(1995)( – Link(based:(Counts(the(number(of(common(links(and( computes(fKmeasure( • CEAF((Luo(2005);(entity(based( • BLANC((Recasens(and(Hovy(2011)(Cluster(RANDKindex( • …( • All(of(them(are(sort(of(evaluating(getting(coreference(links/ clusters(right(and(wrong,(but(the(diﬀerences(can(be( important( – Look(at(it(in(PA3( CoNLL(2011(Shared(task(on(coref( Remarks( • This(simple(deterministic(approach(gives(state(of( the(art(performance!( • Easy(insertion(of(new(features(or(models( • The(idea(of(“easy(ﬁrst”(model(has(also(had(some( popularity(in(other((MLKbased)(NLP(systems( – Easy(ﬁrst(POS(tagging(and(parsing( • It’s(a(ﬂexible(architecture,(not(an(argument(that( ML(is(wrong( • Pronoun(resolution(pass(would(be(easiest(place(to( reinsert(an(ML(model??( "
406,"Event Extraction Slides were adapted from Ellen Riloff Event Information Extraction Extracting role fillers associated with events. Examples Terrorism: perpetrator, victim, target, date, location Management succession: person fired, successor, position, organization, date Disease outbreaks: disease, victim, symptoms, containment measures Alleged guerrilla urban commandos launched two highpower bombs against a car dealership in downtown San Salvador this morning . A police report said that the attack set the building on fire , but did not result any casualties. Sample Text perpetrator target weapon location date damage injury Date 10 January 1990 Location El Salvador: San Salvador (city) Event type bombing Weapon “highpower bombs” Perpetrator individual “guerrilla urban commandos” Perpetrator organization - Physical target “car dealership” Physical target effect some damage Human target - Human target effect no injury or death Filled Terrorism Event Template Disease Outbreak Template Definition Story: <document id> ID: <template id> Date: <date> Event: outbreak Status: confirmed, suspected, or possible Containment: culling, disinfecting, facility closing, inspection, medicine, pesticide, quarantine, vaccine, or other Country: <set fill> Victims: <string list> Disease: <string> Ebola Haemorrhagic Fever In Uganda - Update 5 As of Sat 21 Oct 2000, the Ugandan Ministry of Health has reported 139 cases including 51 deaths. The increase of 17 cases in the last 24 hours reflects the intensified active surveillance. A team from the WHO Collaborating Centre at the US Centers for Disease Control and Prevention (CDC), United States is establishing a field diagnostic laboratory in Gulu district. The last laboratory equipment arrived Sat 20 Oct 2000 and the laboratory is expected to be operational shortly. A WHO information officer from Geneva arrived in Uganda on Wed 18 Oct 2000 and is based in Gulu district. He is working with the Ugandan Ministry of Health as media focal point. Ebola Haemorrhagic Fever In Uganda - Update 6 As of Sun 22 Oct 2000, the Ugandan Ministry of Health has reported 149 cases, including 54 deaths. [This represents an increase of 10 cases and 3 deaths in the last 24 hours. - Mod.CP] Ebola Haemorrhagic Fever In Uganda - Update 5 As of Sat 21 Oct 2000, the Ugandan Ministry of Health has reported 139 cases including 51 deaths. The increase of 17 cases in the last 24 hours reflects the intensified active surveillance. A team from the WHO Collaborating Centre at the US Centers for Disease Control and Prevention (CDC), United States is establishing a field diagnostic laboratory in Gulu district. The last laboratory equipment arrived Sat 20 Oct 2000 and the laboratory is expected to be operational shortly. A WHO information officer from Geneva arrived in Uganda on Wed 18 Oct 2000 and is based in Gulu district. He is working with the Ugandan Ministry of Health as media focal point. Ebola Haemorrhagic Fever In Uganda - Update 6 As of Sun 22 Oct 2000, the Ugandan Ministry of Health has reported 149 cases, including 54 deaths. [This represents an increase of 10 cases and 3 deaths in the last 24 hours. - Mod.CP] Unstructured vs. Semi-structured Text Professor John Skvoretz, U. of South Carolina, Columbia, will present a seminar entitled “Embedded Commitment,” on Thursday, May 4th from 4-5:30 in PH 223D. Unstructured Text Laura Petitte Department of Psychology McGill University Thursday, May 4, 1995 12:00 pm Baker Hall 355 Semi-Structured Text Unstructured text depends 100% on language understanding. Semi-structured text has some structure (layout) that can aid in understanding. Another Semi-Structured Seminar Announcement Name: Dr. Jeffrey D. Hermes Affiliation: Department of AutoImmune Diseases Research & Biophysical Chemistry Merch Research Laboratories Title: “MHC Class II: A Target for Specific Immunomodulation of the Immune Response” Host/e-mail: Robert Murphy Date: Wednesday, May 3, 1995 Time: 3:30 p.m. Place: Mellon Institute Conference Room Sponsor: MERCK RESEARCH LABORATORIES Event Extraction vs. Named Entity Recognition • Named Entity Recognition = identifying types of entities • Event Extraction = identifying role relationships associated with events. Paul Nelson killed John Smith. Paul Nelson was killed by John Smith. IBM purchased Microsoft. IBM was purchased by Microsoft. IBM was purchased on Tuesday by Microsoft. Supervised Learning for IE • In the mid-1990s,researchers began to develop methods to automatically create (learn) IE systems. • Supervised learning requires annotated training data. • Trade-off: annotating texts vs. manual knowledge engineering – weeks vs. months – domain experts vs. computational linguists Patterns/Rules vs. Sequence Tagging Two general approaches to IE: Pattern-based systems use patterns or rules that are applied to text. Sequence tagging models classify individual tokens as to whether or not they should be extracted. IBM fired its CEO. Event: FIRING Agent: IBM Fired: its CEO IBM fired its CEO. John Smith was let go on Monday. Event: FIRING Agent: IBM Fired: John Smith, CEO Date: Monday IBM fired its CEO. Subj VP Dobj Syntactic Analysis Pattern Extraction Coreference Template Creation Text Pattern-based Template-Filling Pipeline Alleged guerrilla urban commandos launched two highpower bombs against a car dealership in downtown San Salvador this morning . A police report said that the attack set the building on fire , but did not result any casualties. Annotating Texts for IE perpetrator target weapon location date damage injury IE as Sequence Tagging • A different approach: build a classifier as a sequence tagging model. • Each document is processed sequentially and each token is labeled as Extraction or Non-Extraction. Ex: B (beginning), I (inside), or O (outside) tags. • Features are usually simple: e.g., words, POS tags, orthography, and a small context window of preceding/following words. Alleged guerrilla urban commandos launched two highpower bombs against a car dealership in downtown San Salvador this morning . perpetrator target weapon location date B I B I I I O O O I I I I I B B B O Sequence Tagging Example The Perils of Manual Text Annotation • Time consuming • Tedious • Deceptively tricky • A new corpus must be annotated for each domain! Weakly Supervised Learning for IE • Idea: can we train an IE system using only unannotated texts? • Yes, if we have “preclassified” texts: – One pile of relevant texts – One pile of irrelevant texts – Manual review of ranked patterns • Much easier than annotating texts! Relevant Irrelevant [The World Trade Center], [an icon] of [New York City], was horrifically attacked on [an otherwise beautiful day] in [September 2001] by [Al Qaeda]. Shallow Parser Extraction Patterns: <subj> was attacked icon of <np> was attacked on <np> was attacked in <np> was attacked by <np> Syntactic Templates AutoSlog-TS [Riloff 96] (Step 1) AutoSlog-TS (Step 2) Relevant Irrelevant Extraction Patterns Freq Prob <subj> was attacked 100 .90 icon of <np> 5 .20 was attacked on <np> 80 .79 was attacked in <np> 85 .87 was attacked by <np> 95 .95 Extraction Patterns: <subj> was attacked icon of <np> was attacked on <np> was attacked in <np> was attacked by <np> Top Terrorism Extraction Patterns 1. <subject> exploded 14. <subject> occurred 2. murder of <np> 15. <subject> was located 3. assassination of <np> 16. took_place on <np> 4. <subject> was killed 17. responsibility for <np> 5. <subject> was kidnapped 18. occurred on <np> 6. attack on <np> 19. was wounded in <np> 7. <subject> was injured 20. destroyed <dobj> 8. exploded in <np> 21. <subject> was murdered 9. death of <np> 22. one of <np> 10. <subject> took_place 23. <subject> kidnapped 11. caused <dobj> 24. exploded on <np> 12. claimed <dobj> 25. <subject> died 13. <subject> was wounded Examples of Learned Disease Patterns outbreak of <np> <subj> spread cases of <np> <subj> was confirmed outbreaks of <np> <subj> was transmitted contracted <dobj> spread of <np> <subj> infected <subj> killed Event Keywords Keywords alone are not as reliable as you might think due to ambiguity, metaphor, and context. The comedian bombed at the club … Parliament exploded in anger about ... Obama was attacked by House Republicans … Secondary Contexts A terrorist arrested by the Salvadoran national police, has been identified as Ruth Esperanza Aguilar Marroquin. Oqueli’s body was found next to the body of Guatemalan politician Gilda Flores. There were seven children, including four of the Vice President’s children, in the home at the time. Linker [Huang & Riloff 2012] Event Extraction Performance System Average (P/R/F) AutoSlog-TS (1996) 45/48/46 GLACIER (2009) 48/57/52 TIER (2011) 51/62/56 Linker (2012) 58/60/59 Challenges for the Future • Contextual Effects The man took the money and fled. robbery kidnapping Abilio Diniz is in the hands of a group presumed to be Chilean terrorists. Challenges for the Future • Inference He was shot. He was shot. His body was found yesterday. He was shot to death. He was riddled with machine gun fire. • Metaphor shot in the arm; shot in the dark killing two birds with one stone "
407,"Deep Learning in NLP Many slides adapted from Richard Socher, Tom Mitchell What’s&Deep&Learning&(DL)?& • Deep&learning&is&a&subﬁeld&of&machine&learning& • Most&machine&learning&methods&work&& well&because&of&human\designed&& representa8ons&and&input&features& • For&example:&features&for&ﬁnding&& named&en88es&like&loca8ons&or&& organiza8on&names&(Finkel,&2010):& • Machine&learning&becomes&just&op8mizing& weights&to&best&make&a&ﬁnal&predic8on& 3.3. APPROACH Feature NER TF Current Word ✓ ✓ Previous Word ✓ ✓ Next Word ✓ ✓ Current Word Character n-gram all length ≤6 Current POS Tag ✓ Surrounding POS Tag Sequence ✓ Current Word Shape ✓ ✓ Surrounding Word Shape Sequence ✓ ✓ Presence of Word in Left Window size 4 size 9 Presence of Word in Right Window size 4 size 9 Table 3.1: Features used by the CRF for the two tasks: named entity recognition (N and template ﬁlling (TF). can go beyond imposing just exact identity conditions). I illustrate this by modeling forms of non-local structure: label consistency in the named entity recognition task template consistency in the template ﬁlling task. One could imagine many ways of deﬁ such models; for simplicity I use the form Machine&Learning&vs&Deep&Learning& Machine Learning in Practice Describing your data with features a computer can understand Learning algorithm Domain&speciﬁc,&requires&Ph.D.& level&talent& Op8mizing&the& weights&on&features& What’s&Deep&Learning&(DL)?& • Representa8on&learning&aUempts&& to&automa8cally&learn&good&& features&or&representa8ons& • Deep&learning&algorithms&aUempt&to& learn&(mul8ple&levels&of)&& representa8on&and&an&output& • From&“raw”&inputs&x&(e.g.&words)& Reasons&for&Exploring&Deep&Learning& • Manually&designed&features&are&o`en&over\speciﬁed,&incomplete& and&take&a&long&8me&to&design&and&validate& • Learned&Features&are&easy&to&adapt,&fast&to&learn& • Deep&learning&provides&a&very&ﬂexible,&(almost?)&universal,& learnable&framework&for&represen>ng&world,&visual&and& linguis8c&informa8on.& • Deep&learning&can&learn&unsupervised&(from&raw&text)&and& supervised&(with&speciﬁc&labels&like&posi8ve/nega8ve)& 3/30/15& Richard&Socher& Lecture&1,&Slide&15& Reasons&for&Exploring&Deep&Learning& • In&2006&deep&learning&techniques&started&outperforming&other& machine&learning&techniques.&Why&now?& • DL&techniques&beneﬁt&more&from&a&lot&of&data& • Faster&machines&and&mul8core&CPU/GPU&help&DL&& • New&models,&algorithms,&ideas&& !&Improved&performance&(ﬁrst&in&speech&and&vision,&then&NLP)& 3/30/15& Richard&Socher& Lecture&1,&Slide&16& Deep&Learning&for&Speech& • The&ﬁrst&breakthrough&results&of& “deep&learning”&on&large&datasets& happened&in&speech&recogni8on& • Context\Dependent&Pre\trained& Deep&Neural&Networks&for&Large& Vocabulary&Speech&Recogni8on&& Dahl&et&al.&(2010)& 3/30/15& Richard&Socher& Lecture&1,&Slide&17& Phonemes/Words& Acous>c&model& Recog& \&WER& RT03S& FSH& Hub5& SWB& Tradi8onal& features& 1\pass& −adapt& 27.4& 23.6& Deep&Learning& 1\pass& −adapt& 18.5& (−33%)& 16.1& (−32%)& Deep&Learning&for&Computer&Vision& • Most&deep&learning&groups& have&(un8l&recently)&largely& focused&on&computer&vision& • Break&through&paper:& ImageNet&Classiﬁca8on&with& Deep&Convolu8onal&Neural& Networks&by&Krizhevsky&et& al.&2012& Richard&Socher& Lecture&1,&Slide&18& 18& Zeiler&and&Fergus&(2013)& Olga Russakovsky* et al. PASCAL ILSVRC birds · · · cats · · · ogs )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  7KHZHLJKWVRIWKLVQHXURQYLVXDOL]HG )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV 9LVXDOL]LQJDUELWUDU\QH 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROX =HLOHU )HUJXV )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV 9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV Neural&word&vectors&A&visualiza>on& 22& Representa>ons&at&NLP&Levels:&Syntax& • Tradi8onal:&Phrases& Discrete&categories&like&NP,&VP& • DL:&& • Every&word&and&every&phrase& is&a&vector& • a&neural&network&combines&& two&vectors&into&one&vector& • Socher&et&al.&2011& 3/30/15& Richard&Socher& Lecture&1,&Slide&23& Machine&Transla>on& • Many&levels&of&transla8on&& have&been&tried&in&the&past:& • Tradi8onal&MT&systems&are&& very&large&complex&systems&& • What&do&you&think&is&the&interlingua&for&the&DL&approach&to& transla8on?& Machine&Transla>on& • Source&sentence&mapped&to&vector,&then&output&sentence& generated.&& • Sequence&to&Sequence&Learning&with&Neural&Networks&by& Sutskever&et&al.&2014& • Very&new&but&could&replace&very&complex&architectures!& 3/30/15& Richard&Socher& Lecture&1,&Slide&29& due to the considerable time lag between the inputs and their corresponding outputs (ﬁg. 1). There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18] who were the ﬁrst to map the entire input sentence to vector, and is related to Cho et al. [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on dif- ferent parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al. [2]. The Connectionist Sequence Classiﬁcation is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs [11]. Figure 1: Our model reads an input sentence “ABC” and produces “WXYZ” as the output sentence. The model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the input sentence in reverse, because doing so introduces many short term dependencies in the data that make the optimization problem much easier. The main result of this work is the following. On the WMT’14 English to French translation task, we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam- search decoder. This is by far the best result achieved by direct translation with large neural net- works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81 BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not covered by these 80k. This result shows that a relatively unoptimized small-vocabulary neural network architecture which has much room for improvement outperforms a phrase based SMT system Neural&Nets&for&the&Win!& 4/8/15' Richard'Socher' 31' • Neural'networks'can'learn'much'more'complex' func,ons'and'nonlinear'decision'boundaries!' Automatic Differentiation • The(gradient(computa2on(can( be(automa;cally'inferred'from( the(symbolic(expression(of(the( fprop.( • Each(node(type(needs(to(know( how(to(compute(its(output(and( how(to(compute(the(gradient( wrt(its(inputs(given(the( gradient(wrt(its(output.( • Easy(and(fast(prototyping( 40( …' …' Review • Deep Learning • Learning Representations of Inputs • Neural Networks • Layers of Logistic Regression • Can represent any nonlinear function (with a large enough network) • Training with backpropagation • Recent breakthroughs in predictive tasks • Speech Recognition • Object Recognition (computer vision) Neural Network Language Models Q: How to model sequences with neural Networks? • Fixed number of inputs. How about just predicting the next word in the input? • Q: what about just predicting the next word? • From the context? • No longer a language model • Word2Vec! Word2vec • Learn continuous word embedding for each word • Each word represented by a vector Using more than one word of context Word2Vec: fast to train • Word2Vec is a fairly simple model, • But Can efficiently train word vectors on really big corpora • This is probably the main advantage of Word2vec over other approaches... • Principal Component Analysis • Recurrent Neural Network Language Models man queen king woman VEC(king) + VEC(woman) - VEC(man) = ? Neural Translation Models (sequence to sequence) Conversation Generation [Szegedy et al., 2014] 6.6% error [Simonyan and Zisserman, 2014] 7.3% error [Krizhevsky, Sutskever, Hinton. 2012] 16.4% error [Zeiler and Fergus, 2013] 11.1% error Image Sentence Datasets Microsoft COCO [Tsung-Yi Lin et al. 2014] mscoco.org currently: ~120K images ~5 sentences each Summary of the approach We wanted to describe images with sentences. 1. Define a single function from input -> output 2. Initialize parts of net from elsewhere if possible 3. Get some data 4. Train with SGD Wow I can’t believe that worked Wow I can’t believe that worked Well, I can kind of see it Summary • Deep learning is a popular area in machine learning recently • Very successful in speech recognition and computer vision • Becoming very popular in NLP these days • Main motivation: • Learn feature representations from data • Alternative to hand-engineered features • Neural networks: • Primary deep learning approach • Layers of logistic regressions – can directly calculate gradients from outputs • Nonlinear decision boundaries "
458,"Natural Language Processing with Deep Learning CS224N/Ling284 Richard Socher Lecture 1: Introduction Lecture Plan 1. What is Natural Language Processing? The nature of human language (15 mins) 2. What is Deep Learning? (15 mins) 3. Course logistics (15 mins) 4. Why is language understanding difficult (10 mins) 5. Intro to the application of Deep Learning to NLP (20 mins) Buffer: 5 mins 1/9/18 2 1. What is Natural Language Processing (NLP)? • Natural language processing is a field at the intersection of • computer science • artificial intelligence • and linguistics. • Goal: for computers to process or “understand” natural language in order to perform tasks that are useful, e.g., • Performing Tasks, like making appointments, buying things • Language translation • Question Answering • Siri, Google Assistant, Facebook M, Cortana … • Fully understanding and representing the meaning of language (or even defining it) is a difficult goal. • Perfect language understanding is AI-complete 1/9/18 3 NLP Levels 1/9/18 4 (A tiny sample of) NLP Applications Applications range from simple to complex: • Spell checking, keyword search, finding synonyms • Extracting information from websites such as • product price, dates, location, people or company names • Classifying: reading level of school texts, positive/negative sentiment of longer documents • Machine translation • Spoken dialog systems • Complex question answering 1/9/18 5 NLP in industry … is taking off • Search (written and spoken) • Online advertisement matching • Automated/assisted translation • Sentiment analysis for marketing or finance/trading • Speech recognition • Chatbots / Dialog agents • Automating customer support • Controlling devices • Ordering goods 1/9/18 6 What’s special about human language? A human language is a system specifically constructed to convey the speaker/writer’s meaning • Not just an environmental signal, it’s a deliberate communication • Using an encoding which little kids can quickly learn (amazingly!) and which changes A human language is mostly a discrete/symbolic/categorical signaling system • rocket = 🚀; violin = 🎻 • Presumably because of greater signaling reliability • Symbols are not just an invention of logic / classical AI! 1/9/18 7 What’s special about human language? The categorical symbols of a language can be encoded as a signal for communication in several ways: • Sound • Gesture • Writing/Images The symbol is invariant across different encodings! CC BY 2.0 David Fulmer 2008 National Library of NZ, no known restrictions 1/9/18 8 What’s special about human language? A human language is a symbolic/categorical signaling system However, a brain encoding appears to be a continuous pattern of activation, and the symbols are transmitted via continuous signals of sound/vision The large vocabulary, symbolic encoding of words creates a problem for machine learning – sparsity! We will explore a continuous encoding pattern of thought lab 1/9/18 9 2. What’s Deep Learning (DL)? • Deep learning is a subfield of machine learning • Most machine learning methods work well because of human-designed representations and input features • For example: features for finding named entities like locations or organization names (Finkel et al., 2010): • Machine learning becomes just optimizing weights to best make a final prediction 3.3. APPROACH Feature NER Current Word ✓ Previous Word ✓ Next Word ✓ Current Word Character n-gram all leng Current POS Tag ✓ Surrounding POS Tag Sequence ✓ Current Word Shape ✓ Surrounding Word Shape Sequence ✓ Presence of Word in Left Window size 4 s Presence of Word in Right Window size 4 s Table 3.1: Features used by the CRF for the two tasks: named en and template ﬁlling (TF). can go beyond imposing just exact identity conditions). I illustrat forms of non-local structure: label consistency in the named enti template consistency in the template ﬁlling task. One could imagine such models; for simplicity I use the form P (y|x) ∝∏θ#(λ,y,x) 1/9/18 10 Machine Learning vs. Deep Learning Machine Learning in Practice Describing your data with features a computer can understand Learning algorithm Domain specific, requires Ph.D. level expertise Optimizing the weights on features 1/9/18 11 What’s Deep Learning (DL)? • In contrast to standard machine learning, • Representation learning attempts to automatically learn good features or representations • Deep learning algorithms attempt to learn (multiple levels of) representations (here: h1,h2,h3) and an output (h4) • From “raw” inputs x (e.g. sound, pixels, characters, or words) 1/9/18 12 On the history of “Deep Learning” • We will focus on different kinds of neural networks • The dominant model family inside deep learning • Only clever terminology for stacked logistic regression units? • Maybe, but interesting modeling principles (end-to-end) and actual connections to neuroscience in some cases. • Recently: Differentiable Programming – becomes clear later • We will not take a historical approach but instead focus on methods which work well on NLP problems now • For a long history of deep learning models (starting ~1960s), see: Deep Learning in Neural Networks: An Overview by Jürgen Schmidhuber 1/9/18 13 Reasons for Exploring Deep Learning • Manually designed features are often over-specified, incomplete and take a long time to design and validate • Learned Features are easy to adapt, fast to learn • Deep learning provides a very flexible, (almost?) universal, learnable framework for representing world, visual and linguistic information. • Deep learning can learn unsupervised (from raw text) and supervised (with specific labels like positive/negative) 1/9/18 14 Reasons for Exploring Deep Learning • In ~2010 deep learning techniques started outperforming other machine learning techniques. Why this decade? • Large amounts of training data favor deep learning • Faster machines and multicore CPU/GPUs favor Deep Learning • New models, algorithms, ideas • Better, more flexible learning of intermediate representations • Effective end-to-end joint system learning • Effective learning methods for using contexts and transferring between tasks • Better regularization and optimization methods à Improved performance (first in speech and vision, then NLP) 1/9/18 15 Deep Learning for Speech • The first breakthrough results of “deep learning” on large datasets happened in speech recognition • Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition Dahl et al. (2010) Phonemes/Words Acoustic model and WER RT03S FSH Hub5 SWB Traditional features 27.4 23.6 Deep Learning 18.5 (−33%) 16.1 (−32%) 1/9/18 16 Deep Learning for Computer Vision First major focus of deep learning groups was computer vision The breakthrough DL paper: ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky, Sutskever, & Hinton, 2012, U. Toronto. 37% error red. Zeiler and Fergus (2013) Olga Russakovsky* et al. ILSVRC · · · · · · )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH  7KHZHLJKWVRIWKLVQHXURQYLVXDOL]HG )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV 9LVXDOL]LQJDU 9LVXDOL]LQJDQG8QGHUV =HLOHU )HUJXV )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE )HL)HL/L $QGUHM.DUSDWK\ /HFWXUH )HE  9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV 9LVXDOL]LQJDUELWUDU\QHXURQVDORQJWKHZD\WRWKHWRS 9LVXDOL]LQJDQG8QGHUVWDQGLQJ&RQYROXWLRQDO1HWZRUNV =HLOHU )HUJXV 1/9/18 17 3. Course logistics in brief • Instructor: Richard Socher • Head TAs: Kevin Clark and Abigail See • TAs: Many wonderful people! • Time: TuTh 4:30–5:50, Nvidia Aud (à video) • Other information: see the class webpage • http://cs224n.stanford.edu/ a.k.a., http://www.stanford.edu/class/cs224n/ • Syllabus, office hours (I will start today, rest start next week), “handouts”, TAs, Piazza • Slides uploaded before each lecture 1/9/18 18 Prerequisites • Proficiency in Python • All class assignments will be in Python. • Python refresh session: 3:00-4:20pm, January 19! • Multivariate Calculus, Linear Algebra (e.g., MATH 51, CME 100) • Basic Probability and Statistics (e.g. CS 109 or other stats course) • Fundamentals of Machine Learning (e.g., from CS229 or CS221) • loss functions • taking simple derivatives • performing optimization with gradient descent.1/9/18 19 A note on your experience :) • This is a hard, advanced, graduate level class • I and all the TAs really care about your success in this class • Give Feedback. Visit refresh sessions. • Come to office hours (early, often and off-cycle) “Best class at Stanford” “Changed my life” “Obvious that instructors care” “Learned a ton” “Hard but worth it” “Terrible class” “Don’t take it” “Instructors don’t care” “Too much work” 1/9/18 20 What do we hope to teach? 1. An understanding of and ability to use the effective modern methods for deep learning • Basics first, then key methods used in NLP: Recurrent networks, attention, etc. 2. Some big picture understanding of human languages and the difficulties in understanding and producing them 3. An understanding of and ability to build systems (in TensorFlow) for some of the major problems in NLP: • Word similarities, parsing, machine translation, entity recognition, question answering, sentence comprehension 1/9/18 21 Grading Policy • 3 Assignments: 15% x 3 = 45% • Midterm Exam: 20% • Final Course Project or PSet4 (1–3 people): 35% • Including for final project doing: project proposal, milestone, interacting with mentor • Final poster session (must be there: 12:15–3:15 ): 2% of the 35% • Late policy • 6 free late days – use as you please • Afterwards, 10% off per day late • Assignments not accepted after 3 late days per assignment • Collaboration policy: Read the website and the Honor Code! Understand allowed ‘collaboration’ and how to document it 1/9/18 22 High Level Plan for Problem Sets • Beginning PSets and final project are hard (in different ways) • PSet 1 is written work and pure python code (numpy etc.) to really understand the basics • Released on January 11 (this Thursday!) • PSet 2 & 3 will be in TensorFlow, a library for putting together neural network models quickly (à special lecture) • Libraries like TensorFlow are becoming standard tools • Also: PyTorch, Theano, Chainer, CNTK, Paddle, MXNet, Keras, Caffe, … 1/9/18 23 High Level Plan for PSet4 and Final Project • You can propose a final project • Requires instructor sign-off • Or we give you one: PSet 4, • Earlier release (after PSet 2, 2 weeks before project proposal), • Improved, easier, a good default for most • Open ended but with an easier start • Can use any language and/or deep learning framework for project but starter code for PSet4 will be in TensorFlow again • We encourage teams of 2 people (and with exceptions 3) • Start finding a partner soon. 1/9/18 24 4. Why is NLP hard? • Complexity in representing, learning and using linguistic/situational/contextual/world/visual knowledge • But interpretation depends on these • Human languages are ambiguous (unlike programming and other formal languages) • E.g. “I made her duck.” 1/9/18 25 Why NLP is difficult: Real newspaper headlines/tweets 1. The Pope’s baby steps on gays 2. Boy paralyzed after tumor fights back to gain black belt 3. Enraged cow injures farmer with axe 4. Juvenile Court to Try Shooting Defendant 1/9/18 26 5. Deep NLP = Deep Learning + NLP Combine ideas and goals of NLP with using representation learning and deep learning methods to solve them Several big improvements in recent years in NLP • Linguistic levels: (speech), words, syntax, semantics • Intermediate tasks/tools: parts-of-speech, entities, parsing • Full applications: sentiment analysis, question answering, dialogue agents, machine translation 1/9/18 27 Word meaning as a neural word vector – visualization 0.286 0.792 −0.177 −0.107 0.109 −0.542 0.349 0.271 0.487 expect = 1/9/18 28 Nearest words to frog: 1. frogs 2. toad 3. litoria 4. leptodactylidae 5. rana 6. lizard 7. eleutherodactylus Word similarities litoria leptodactylidae rana eleutherodactylus http://nlp.stanford.edu/projects/glove/ 1/9/18 29 Representations of NLP Levels: Morphology • Traditional: Words are prefix stem suffix made of morphemes un interest ed • DL: • every morpheme is a vector • a neural network combines two vectors into one vector • Luong et al. 2013 !""!""# #$%&!""'&($%& )*$'( !""#$%&!""'&()*$%& ! !! ""! ! !! ""! !""#$%&!""'&($%& Figure 1: Morphological Recursive Neural work. A vector representation for the word 1/9/18 30 NLP Tools: Parsing for sentence structure • Neural networks can accurately determine the grammatical structure of sentences • This supports interpretation and may help in disambiguation 1/9/18 31 Representations of NLP Levels: Semantics • Traditional: Lambda calculus • Carefully engineered functions • Take as inputs specific other functions • No notion of similarity or fuzziness of language • DL: • Every word and every phrase and every logical expression is a vector • a neural network combines two vectors into one vector • Bowman et al. 2014 Much of the theoretical work on natural lan- guage inference (and some successful imple- mented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that deﬁne rules of in- ference between natural language words, phrases, and sentences without the need of intermediate representations in an artiﬁcial logical language. In our ﬁrst three experiments, we test our mod- els’ ability to learn the foundations of natural lan- guage inference by training them to reproduce the P(@) = 0.8 all reptiles walk vs. some turtles move Softmax classiﬁer Comparison N(T)N layer Composition RN(T)N layers Pre-trained or randomly initialized learned word vectors all reptiles all reptiles walk all reptiles walk some turtles some turtles move some turtles move Figure 1: In our model, two separate tree- structured networks build up vector representa 1/9/18 32 NLP Applications: Sentiment Analysis • Traditional: Treat sentence as a bag-of-words (ignore word order); consult a curated list of ""positive"" and ""negative"" words to determine sentiment of sentence. Need hand-designed features to capture negation! --> Ain’t gonna capture everything • Same deep learning model that could be used for morphology, syntax and logical semantics à RecursiveNN (aka TreeRNNs) 1/9/18 33 Question Answering • Traditional: A lot of feature engineering to capture world and other knowledge, e.g., regular expressions, Berant et al. (2014) • DL: Again, a deep learning architecture can be used! • Facts are stored in vectors a: Light absorption b: Transfer of ions Temporal Q: What is the correct order of events? 57 (9.74%) a: PDGF binds to tyrosine kinases, then cells divide, then wound healing b: Cells divide, then PDGF binds to tyrosine kinases, then wound healing True-False Q: Cdk associates with MPF to become cyclin 121 (20.68%) a: True b: False Table 3: Examples and statistics for each of the three coarse types of questions. Is main verb trigger? Condition Regular Exp. Wh- word subjective? AGENT Wh- word object? THEME Condition Regular Exp. default (ENABLE|SUPER)+ DIRECT (ENABLE|SUPER) PREVENT (ENABLE|SUPER)⇤PREVENT(ENABLE|SUPER)⇤ Yes No Figure 3: Rules for determining the regular expressions for queries concerning two triggers. In each table, the condition column decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root to the Wh- word in the question. In the right table, if the word directly modiﬁes the main trigger, the DIRECT regular expression is chosen. If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regular expression. Otherwise, the default one is chosen. We omit the relation label SAME from the expressions, but allow going through any number of edges labeled by SAME when matching expressions to the structure. that we expand using WordNet. The ﬁnal step in constructing the query is to identify the regular expression for the path con- necting the source and the target. Due to paucity of data, we do not map a question and an answer to arbitrary regular expressions. Instead, we con- struct a small set of regular expressions, and build a rule-based system that selects one. We used the 5.3 Answering Questions We match the query of an answer to the process structure to identify the answer. In case of a match, the corresponding answer is chosen. The matching path can be thought of as a proof for the answer. If neither query matches the graph (or both do) 1/9/18 34 Dialogue agents / Response Generation • A simple, successful example is the auto-replies available in the Google Inbox app • An application of the powerful, general technique of Neural Language Models, which are an instance of Recurrent Neural Networks 1/9/18 35 Machine Translation • Many levels of translation have been tried in the past: • Traditional MT systems are very large complex systems • What do you think is the interlingua for the DL approach to translation? 1/9/18 36 Die Proteste waren am Wochenende eskaliert <EOS> The protests escalated over the weekend 0.2 0.6 -0.1 -0.7 0.1 0.4 -0.6 0.2 -0.3 0.4 0.2 -0.3 -0.1 -0.4 0.2 0.2 0.4 0.1 -0.5 -0.2 0.4 -0.2 -0.3 -0.4 -0.2 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 -0.1 0.3 -0.1 -0.7 0.1 -0.2 0.6 0.1 0.3 0.1 -0.4 0.5 -0.5 0.4 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 -0.2 -0.1 0.1 0.1 0.2 0.6 -0.1 -0.7 0.1 0.1 0.3 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.4 0.1 0.2 -0.8 -0.1 -0.5 0.1 0.2 0.6 -0.1 -0.7 0.1 -0.4 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 0.3 0.1 -0.1 0.6 -0.1 0.3 0.1 0.2 0.4 -0.1 0.2 0.1 0.3 0.6 -0.1 -0.5 0.1 0.2 0.6 -0.1 -0.7 0.1 0.2 -0.1 -0.1 -0.7 0.1 0.1 0.3 0.1 -0.4 0.2 0.2 0.6 -0.1 -0.7 0.1 0.4 0.4 0.3 -0.2 -0.3 0.5 0.5 0.9 -0.3 -0.2 0.2 0.6 -0.1 -0.5 0.1 -0.1 0.6 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 0.3 0.6 -0.1 -0.7 0.1 0.4 0.4 -0.1 -0.7 0.1 -0.2 0.6 -0.1 -0.7 0.1 -0.4 0.6 -0.1 -0.7 0.1 -0.3 0.5 -0.1 -0.7 0.1 0.2 0.6 -0.1 -0.7 0.1 The protests escalated over the weekend <EOS> Neural Machine Translation Source sentence is mapped to vector, then output sentence generated [Sutskever et al. 2014, Bahdanau et al. 2014, Luong and Manning 2016] Sentence meaning is built up Source sentence Translation generated Feeding in last word Now live for some languages in Google Translate (etc.), with big error reductions! 1/9/18 37 Conclusion: Representation for all levels? Vectors We will study in the next lecture how we can learn vector representations for words and what they actually represent. Next week: how neural networks work and how they can use these vectors for all NLP levels and many different applications 1/9/18 38 https://xkcd.com/1576/ Randall Munroe CC BY NC 2.5 1/9/18 39 "
459,"Natural Language Processing with Deep Learning CS224N/Ling284 Richard Socher Lecture 2: Word Vectors Organization • PSet 1 is released. Coding Session 1/22: (Monday, PA1 due Thursday) • Some of the questions from Piazza: • sharing the choose-your-own final project with another class seems fine--> Yes* • But how about the default final project? Can that also be used as a final project for a different course?--> Yes* • Are we allowing students to bring one sheet of notes for the midterm?--> Yes • Azure computing resources for Projects/PSet4. Part of milestone 1/11/18 2 Lecture Plan 1. Word meaning (15 mins) 2. Word2vec introduction (20 mins) 3. Word2vec objective function gradients (25 mins) 4. Optimization refresher (10 mins) 1/11/18 3 1. How do we represent the meaning of a word? Definition: meaning (Webster dictionary) • the idea that is represented by a word, phrase, etc. • the idea that a person wants to express by using words, signs, etc. • the idea that is expressed in a work of writing, art, etc. Commonest linguistic way of thinking of meaning: signifier (symbol) ⟺signified (idea or thing) = denotation 1/11/18 4 How do we have usable meaning in a computer? Common solution: Use e.g. WordNet, a resource containing lists of synonym sets and hypernyms (“is a” relationships). [Synset('procyonid.n.01'), Synset('carnivore.n.01'), Synset('placental.n.01'), Synset('mammal.n.01'), Synset('vertebrate.n.01'), Synset('chordate.n.01'), Synset('animal.n.01'), Synset('organism.n.01'), Synset('living_thing.n.01'), Synset('whole.n.02'), Synset('object.n.01'), Synset('physical_entity.n.01'), Synset('entity.n.01')] (adj) full, good (adj) estimable, good, honorable, respectable (adj) beneficial, good (adj) good, just, upright (adj) adept, expert, good, practiced, proficient, skillful (adj) dear, good, near (adj) good, right, ripe … (adv) well, good (adv) thoroughly, soundly, good (n) good, goodness (n) commodity, trade good, good e.g. synonym sets containing “good”: e.g. hypernyms of “panda”: 1/11/18 5 Problems with resources like WordNet • Great as a resource but missing nuance • e.g. “proficient” is listed as a synonym for “good”. This is only correct in some contexts. • Missing new meanings of words • e.g. wicked, badass, nifty, wizard, genius, ninja, bombest • Impossible to keep up-to-date! • Subjective • Requires human labor to create and adapt • Hard to compute accurate word similarity à 1/11/18 6 Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols: hotel, conference, motel Words can be represented by one-hot vectors: motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] Vector dimension = number of words in vocabulary (e.g. 500,000) Means one 1, the rest 0s 1/11/18 7 Problem with words as discrete symbols Example: in web search, if user searches for “Seattle motel”, we would like to match documents containing “Seattle hotel”. But: motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] These two vectors are orthogonal. There is no natural notion of similarity for one-hot vectors! Solution: • Could rely on WordNet’s list of synonyms to get similarity? • Instead: learn to encode similarity in the vectors themselves Sec. 9.2.2 1/11/18 8 Representing words by their context • Core idea: A word’s meaning is given by the words that frequently appear close-by • “You shall know a word by the company it keeps” (J. R. Firth 1957: 11) • One of the most successful ideas of modern statistical NLP! • When a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window). • Use the many contexts of w to build up a representation of w …government debt problems turning into banking crises as happened in 2009… …saying that Europe needs unified banking regulation to replace the hodgepodge… …India has just given its banking system a shot in the arm… These context words will represent banking 1/11/18 9 Word vectors We will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts. Note: word vectors are sometimes called word embeddings or word representations. linguistics = 0.286 0.792 −0.177 −0.107 0.109 −0.542 0.349 0.271 1/11/18 10 2. Word2vec: Overview Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. Idea: • We have a large corpus of text • Every word in a fixed vocabulary is represented by a vector • Go through each position t in the text, which has a center word c and context (“outside”) words o • Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa) • Keep adjusting the word vectors to maximize this probability 1/11/18 11 Word2Vec Overview • Example windows and process for computing 𝑃𝑤789 | 𝑤7 … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑤78; | 𝑤7 𝑃𝑤78< | 𝑤7 𝑃𝑤7=; | 𝑤7 𝑃𝑤7=< | 𝑤7 1/11/18 12 Word2Vec Overview • Example windows and process for computing 𝑃𝑤789 | 𝑤7 … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑤78; | 𝑤7 𝑃𝑤78< | 𝑤7 𝑃𝑤7=; | 𝑤7 𝑃𝑤7=< | 𝑤7 1/11/18 13 Word2vec: objective function For each position 𝑡= 1, … , 𝑇, predict context words within a window of fixed size m, given center word 𝑤 9. 𝐿𝜃= F F 𝑃𝑤789 | 𝑤7; 𝜃 H =IJ9JI 9KL M 7N; The objective function 𝐽𝜃 is the (average) negative log likelihood: 𝐽𝜃= −1 𝑇log 𝐿(𝜃) = −1 𝑇S S log 𝑃𝑤789 | 𝑤7; 𝜃 H =IJ9JI 9KL M 7N; Minimizing objective function ⟺Maximizing predictive accuracy Likelihood = 𝜃is all variables to be optimized sometimes called cost or loss function 1/11/18 14 Word2vec: objective function • We want to minimize the objective function: 𝐽𝜃= −1 𝑇S S log 𝑃𝑤789 | 𝑤7; 𝜃 H =IJ9JI 9KL M 7N; • Question: How to calculate 𝑃𝑤789 | 𝑤7; 𝜃? • Answer: We will use two vectors per word w: • 𝑣U when w is a center word • 𝑢U when w is a context word • Then for a center word c and a context word o: 𝑃𝑜𝑐= exp (𝑢Y M𝑣Z) ∑ exp (𝑢U M 𝑣Z) H U∈] 1/11/18 15 Word2Vec Overview with Vectors • Example windows and process for computing 𝑃𝑤789 | 𝑤7 • 𝑃𝑢^_Y`abIc | 𝑣de7Y short for P 𝑝𝑟𝑜𝑏𝑙𝑒𝑚𝑠 | 𝑖𝑛𝑡𝑜 ; 𝑢^_Y`abIc, 𝑣de7Y, 𝜃 … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑢`peqder |𝑣de7Y 𝑃𝑢Z_dcdc |𝑣de7Y 𝑃𝑢7seder | 𝑣de7Y 𝑃𝑢^_Y`abIc | 𝑣de7Y 1/11/18 16 Word2Vec Overview with Vectors • Example windows and process for computing 𝑃𝑤789 | 𝑤7 … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑢Z_dcbc |𝑣`peqder 𝑃𝑢pc |𝑣`peqder 𝑃𝑢de7Y | 𝑣`peqder 𝑃𝑢7s_eder |𝑣`peqder 1/11/18 17 Word2vec: prediction function 𝑃𝑜𝑐= exp (𝑢Y M𝑣Z) ∑ exp (𝑢U M 𝑣Z) H U∈] • This is an example of the softmax function ℝe →ℝe softmax 𝑥d = exp (𝑥d) ∑ exp (𝑥9) e 9N; = 𝑝d • The softmax function maps arbitrary values 𝑥d to a probability distribution 𝑝d • “max” because amplifies probability of largest 𝑥d • “soft” because still assigns some probability to smaller 𝑥d • Frequently used in Deep Learning Dot product compares similarity of o and c. Larger dot product = larger probability After taking exponent, normalize over entire vocabulary 1/11/18 18 To train the model: Compute all vector gradients! • Recall: 𝜃 represents all model parameters, in one long vector • In our case with d-dimensional vectors and V-many words: • Remember: every word has two vectors • We then optimize these parameters 1/11/18 19 3. Derivations of gradient • Whiteboard – see video if you’re not in class ;) • The basic Lego piece • Useful basics: • If in doubt: write out with indices • Chain rule! If y = f(u) and u = g(x), i.e. y = f(g(x)), then: 1/11/18 20 Chain Rule • Chain rule! If y = f(u) and u = g(x), i.e. y = f(g(x)), then: • Simple example: 1/11/18 21 Interactive Whiteboard Session! Let’s derive gradient for center word together For one example window and one example outside word: You then also need the gradient for context words (it’s similar; left for homework). That’s all of the parameters θ here. 1/11/18 22 Calculating all gradients! • We went through gradient for each center vector v in a window • We also need gradients for outside vectors u • Derive at home! • Generally in each window we will compute updates for all parameters that are being used in that window. For example: … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑢Z_dcbc |𝑣`peqder 𝑃𝑢pc |𝑣`peqder 𝑃𝑢de7Y | 𝑣`peqder 𝑃𝑢7s_eder |𝑣`peqder 1/11/18 23 Word2vec: More details Why two vectors? à Easier optimization. Average both at the end. Two model variants: 1. Skip-grams (SG) Predict context (”outside”) words (position independent) given center word 2. Continuous Bag of Words (CBOW) Predict center word from (bag of) context words This lecture so far: Skip-gram model Additional efficiency in training: 1. Negative sampling So far: Focus on naïve softmax (simpler training method) 1/11/18 24 Gradient Descent • We have a cost function 𝐽𝜃we want to minimize • Gradient Descent is an algorithm to minimize 𝐽𝜃 • Idea: for current value of 𝜃, calculate gradient of 𝐽𝜃, then take small step in direction of negative gradient. Repeat. Note: Our objectives are not convex like this :( 1/11/18 25 Intuition For a simple convex function over two parameters. Contour lines show levels of objective function 1/11/18 26 • Update equation (in matrix notation): • Update equation (for single parameter): • Algorithm: Gradient Descent 𝛼= step size or learning rate 1/11/18 27 Stochastic Gradient Descent • Problem: 𝐽𝜃is a function of all windows in the corpus (potentially billions!) • So is very expensive to compute • You would wait a very long time before making a single update! • Very bad idea for pretty much all neural nets! • Solution: Stochastic gradient descent (SGD) • Repeatedly sample windows, and update after each one. • Algorithm: 1/11/18 28 1/11/18 29 PSet1: The skip-gram model and negative sampling • From paper: “Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013) • Overall objective function (they maximize): • The sigmoid function! (we’ll become good friends soon) • So we maximize the probability of two words co-occurring in first log à 1/11/18 30 PSet1: The skip-gram model and negative sampling • Simpler notation, more similar to class and PSet: • We take k negative samples. • Maximize probability that real outside word appears, minimize prob. that random words appear around center word • P(w)=U(w)3/4/Z, the unigram distribution U(w) raised to the 3/4 power (We provide this function in the starter code). • The power makes less frequent words be sampled more often 1/11/18 31 PSet1: The continuous bag of words model • Main idea for continuous bag of words (CBOW): Predict center word from sum of surrounding word vectors instead of predicting surrounding single words from center word as in skip- gram model • To make assignment slightly easier: Implementation of the CBOW model is not required (you can do it for a couple of bonus points!), but you do have to do the theory problem on CBOW. 1/11/18 32 "
460,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 3: More Word Vectors Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Organization. See Calendar • Project advice office hours are today 1/16/18 2 A Deeper Look at Word Vectors • Finish word2vec • What does word2vec capture? • How could we capture this essence more effectively? • How can we analyze word vectors? 1/16/18 3 Review: Main ideas of word2vec • Go through each word of the whole corpus • Predict surrounding words of each word • 𝑃𝑜𝑐= '() (+, -./) ∑ '() (+2 - ./) 3 2∈5 … crises banking into turning problems … as center word at position t outside context words in window of size 2 outside context words in window of size 2 𝑃𝑤789 | 𝑤7 𝑃𝑤78; | 𝑤7 𝑃𝑤7<9 | 𝑤7 𝑃𝑤7<; | 𝑤7 1/16/18 4 Stochastic gradients with word vectors! • Then take gradients at each such window for SGD • But in each window, we only have at most 2m + 1 words, so is very sparse! 1/16/18 5 Stochastic gradients with word vectors! • We may as well only update the word vectors that actually appear! • Solution: either you need sparse matrix update operations to only update certain columns of full embedding matrices U and V, or you need to keep around a hash for word vectors • If you have millions of word vectors and do distributed computing, it is important to not have to send gigantic updates around! [ ] d |V| 1/16/18 6 Approximations: Assignment 1 • The normalization factor is too computationally expensive. • 𝑃𝑜𝑐= '() (+, -./) ∑ '() (+2 - ./) 3 2∈5 • Hence, in Assignment 1, you will implement the skip-gram model with negative sampling • Main idea: train binary logistic regressions for a true pair (center word and word in its context window) versus a couple of noise pairs (the center word paired with a random word) 1/16/18 7 Word2vec improves objective function by putting similar words nearby in space 1/16/18 8 Summary of word2vec • Go through each word of the whole corpus • Predict surrounding words of each word • This captures co-occurrence of words one at a time • Why not capture co-occurrence counts directly? 1/16/18 9 Yes we can! With a co-occurrence matrix X • 2 options: windows vs full document • Window: Similar to word2vec, use window around each word à captures both syntactic (POS) and semantic information • Word-document co-occurrence matrix will give general topics (all sports terms will have similar entries) leading to “Latent Semantic Analysis” 1/16/18 10 Example: Window based co-occurrence matrix • Window length 1 (more common: 5 - 10) • Symmetric (irrelevant whether left or right context) • Example corpus: • I like deep learning. • I like NLP. • I enjoy flying. 1/16/18 11 Window based co-occurrence matrix • Example corpus: • I like deep learning. • I like NLP. • I enjoy flying. counts I like enjoy deep learning NLP flying . I 0 2 1 0 0 0 0 0 like 2 0 0 1 0 1 0 0 enjoy 1 0 0 0 0 0 1 0 deep 0 1 0 0 1 0 0 0 learning 0 0 0 1 0 0 0 1 NLP 0 1 0 0 0 0 0 1 flying 0 0 1 0 0 0 0 1 . 0 0 0 0 1 1 1 0 1/16/18 12 Problems with simple co-occurrence vectors Increase in size with vocabulary Very high dimensional: require a lot of storage Subsequent classification models have sparsity issues à Models are less robust 1/16/18 13 Solution: Low dimensional vectors • Idea: store “most” of the important information in a fixed, small number of dimensions: a dense vector • Usually 25 – 1000 dimensions, similar to word2vec • How to reduce the dimensionality? 1/16/18 14 Method 1: Dimensionality Reduction on X Singular Value Decomposition of co-occurrence matrix X. Rohde, Gonnerman, Plaut Modeling Word r = n n r r X U S S S S S . 2 3 1 r UUU 1 2 3 V m m V V 1 2 3 . . . . . . . . = n X U S m V T V T U U U 1 2 3 Sk 0 0 0 0 V m V V 1 2 3 . . . S S S 2 3 1 . . . k k k n r k Figure 1: The singular value decomposition of matrix X. ˆ X is the best rank k approximation to X, in terms of least squares. tropy of the document distribution of row vector a. Words that are evenly distributed over documents will have high d h l i h i ﬂ i h i i i Table 3. It is unc by the singular v as implied in Dee Harshman (1990) Computing the matrix with dim requires time pro for matrices with However, LSA c sparse and the SV matrices, allowin sands of words a ings tested here pairwise compari site (http://lsa.co the Touchstone A eral reading up to is the best rank k approximation to X , in terms of least squares. Rohde, Gonnerman, Plaut Modeling Word Meaning r = n n r r X U S S S S S . 2 3 1 r UUU 1 2 3 V m m V V 1 2 3 . . . . . . . . = n X U S m V T V T U U U 1 2 3 Sk 0 0 0 0 V m V V 1 2 3 . . . S S S 2 3 1 . . . k k k n r k Figure 1: The singular value decomposition of matrix X. ˆ X is the best rank k approximation to X, in terms of least squares. Table 3. It is unclear wheth by the singular values, S, as implied in Deerwester, D Harshman (1990). Computing the SVD itse matrix with dimensions n requires time proportional for matrices with more than However, LSA co-occurren sparse and the SVD comput matrices, allowing the mod sands of words and docum ings tested here were gene pairwise comparison interfa 1/16/18 15 Simple SVD word vectors in Python Corpus: I like deep learning. I like NLP. I enjoy flying. 1/16/18 16 Simple SVD word vectors in Python Corpus: I like deep learning. I like NLP. I enjoy flying. Printing first two columns of U corresponding to the 2 biggest singular values 1/16/18 17 Hacks to X • Problem: function words (the, he, has) are too frequent à syntax has too much impact. Some fixes: • min(X,t), with t~100 • Ignore them all • Ramped windows that count closer words more • Use Pearson correlations instead of counts, then set negative values to 0 • +++ 1/16/18 18 Interesting semantic patters emerge in the vectors Figure 8: Multidimensional scaling for three noun classes. WRIST ANKLE SHOULDER ARM LEG HAND FOOT HEAD NOSE FINGER TOE FACE EAR EYE TOOTH DOG CAT PUPPY KITTEN COW MOUSE TURTLE OYSTER LION BULL CHICAGO ATLANTA MONTREAL NASHVILLE TOKYO CHINA RUSSIA AFRICA ASIA EUROPE AMERICA BRAZIL MOSCOW FRANCE HAWAII Figure 9: Hierarchical clustering for three noun classes using distances based on vector correlations. 20 An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. 2005 1/16/18 19 Interesting syntactic patters emerge in the vectors An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. 2005 TAKE SHOW TOOK TAKING TAKEN SPEAK EAT CHOOSE SPEAKING GROW GROWING THROW SHOWN SHOWING SHOWED EATING CHOSEN SPOKE CHOSE GROWN GREW SPOKEN THROWN THROWING STEAL ATE THREW STOLEN STEALING CHOOSING STOLE EATEN Figure 11: Multidimensional scaling of present, past, progressive, and past participle forms for eight verb families. 22 1/16/18 20 Interesting semantic patters emerge in the vectors An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. 2005 Rohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurren DRIVE LEARN DOCTOR CLEAN DRIVER STUDENT TEACH TEACHER TREAT PRAY PRIEST MARRY SWIM BRIDE JANITOR SWIMMER Figure 13: Multidimensional scaling for nouns and their associated verbs. Table 10 The 10 nearest neighbors and their percent correlation similarities for a set of nouns, under the COALS-14K mode gun point mind monopoly cardboard lipstick leningrad feet 1) 46 4 h d 32 4 i t 33 5 i d 39 9 li 47 4 l ti 42 9 hi 24 0 59 5 i h 1/16/18 21 Problems with SVD Computational cost scales quadratically for n x m matrix: O(mn2) flops (when n<m) à Bad for millions of words or documents Hard to incorporate new words or documents Different learning regime than other DL models 1/16/18 22 Count based vs direct prediction • LSA, HAL (Lund & Burgess), • COALS, Hellinger-PCA (Rohde et al, Lebret & Collobert) • Fast training • Efficient usage of statistics • Primarily used to capture word similarity • Disproportionate importance given to large counts • Skip-gram/CBOW (Mikolov et al) • NNLM, HLBL, RNN (Bengio et al; Collobert & Weston; Huang et al; Mnih & Hinton) • Scale with corpus size • Inefficient usage of statistics • Can capture complex patterns beyond word similarity • Generate improved performance on other tasks 1/16/18 23 Combining the best of both worlds: GloVe • Fast training • Scalable to huge corpora • Good performance even with small corpus, and small vectors • By Pennington, Socher, Manning (2014) 1/16/18 24 What to do with the two sets of vectors? • We end up with U and V from all the vectors u and v (in columns) • Both capture similar co-occurrence information. It turns out, the best solution is to simply sum them up: Xfinal = U + V • One of many hyperparameters explored in GloVe: Global Vectors for Word Representation, Pennington et al. (2014) 1/16/18 25 Glove results 1. frogs 2. toad 3. litoria 4. leptodactylidae 5. rana 6. lizard 7. eleutherodactylus litoria leptodactylidae rana eleutherodactylus Nearest words to frog: 1/16/18 26 How to evaluate word vectors? • Related to general evaluation in NLP: Intrinsic vs extrinsic • Intrinsic: • Evaluation on a specific/intermediate subtask • Fast to compute • Helps to understand that system • Not clear if really helpful unless correlation to real task is established • Extrinsic: • Evaluation on a real task • Can take a long time to compute accuracy • Unclear if the subsystem is the problem or its interaction or other subsystems • If replacing exactly one subsystem with another improves accuracy à Winning! 1/16/18 27 Intrinsic word vector evaluation • Word Vector Analogies • Evaluate word vectors by how well their cosine distance after addition captures intuitive semantic and syntactic analogy questions • Discarding the input words from the search! • Problem: What if the information is there but not linear? man:woman :: king:? a:b :: c:? king man woman 1/16/18 28 Glove Visualizations 1/16/18 29 Glove Visualizations: Company - CEO 1/16/18 30 Glove Visualizations: Superlatives 1/16/18 31 Other fun word2vec analogies 1/16/18 32 Details of intrinsic word vector evaluation • Word Vector Analogies: Syntactic and Semantic examples from http://code.google.com/p/word2vec/source/browse/trunk/questions- words.txt : city-in-state problem: different cities Chicago Illinois Houston Texas may have same name Chicago Illinois Philadelphia Pennsylvania Chicago Illinois Phoenix Arizona Chicago Illinois Dallas Texas Chicago Illinois Jacksonville Florida Chicago Illinois Indianapolis Indiana Chicago Illinois Austin Texas Chicago Illinois Detroit Michigan Chicago Illinois Memphis Tennessee Chicago Illinois Boston Massachusetts 1/16/18 33 Details of intrinsic word vector evaluation • Word Vector Analogies: Syntactic and Semantic examples from : capital-world problem: can change Abuja Nigeria Accra Ghana Abuja Nigeria Algiers Algeria Abuja Nigeria Amman Jordan Abuja Nigeria Ankara Turkey Abuja Nigeria Antananarivo Madagascar Abuja Nigeria Apia Samoa Abuja Nigeria Ashgabat Turkmenistan Abuja Nigeria Asmara Eritrea Abuja Nigeria Astana Kazakhstan 1/16/18 34 Details of intrinsic word vector evaluation • Word Vector Analogies: Syntactic and Semantic examples from : gram4-superlative bad worst big biggest bad worst bright brightest bad worst cold coldest bad worst cool coolest bad worst dark darkest bad worst easy easiest bad worst fast fastest bad worst good best bad worst great greatest 1/16/18 35 Details of intrinsic word vector evaluation • Word Vector Analogies: Syntactic and Semantic examples from : gram7-past-tense dancing danced decreasing decreased dancing danced describing described dancing danced enhancing enhanced dancing danced falling fell dancing danced feeding fed dancing danced flying flew dancing danced generating generated dancing danced going went dancing danced hiding hid dancing danced hitting hit 1/16/18 36 Analogy evaluation and hyperparameters • Very careful analysis: Glove word vectors X | X r=1 k r↵= kH|X |,↵, (18) ten the last sum in terms of onic number Hn,m. The up- |X|, is the maximum fre- oincides with the number of he matrix X. This number is mum value of r in Eqn. (17) , |X| = k1/↵. Therefore we , X|↵H|X |,↵. (19) ow |X| is related to |C| when ge; therefore we are free to side of the equation for large we use the expansion of gen- mbers (Apostol, 1976), + O(x−s) if s > 0, s , 1 , (20) (↵) |X|↵+ O(1) , (21) mann zeta function. In the nly one of the two terms on g p y ; scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† and CBOW† using the word2vec tool3. See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.1 53.2 HPCA 100 1.6B 4.2 16.4 10.8 GloVe 100 1.6B 67.5 54.3 60.3 SG 300 1B 61 61 61 CBOW 300 1.6B 16.1 52.6 36.1 vLBL 300 1.5B 54.2 64.8 60.0 ivLBL 300 1.5B 65.2 63.0 64.0 GloVe 300 1.6B 80.8 61.5 70.3 SVD 300 6B 6.3 8.1 7.3 SVD-S 300 6B 36.7 46.6 42.1 SVD-L 300 6B 56.6 63.0 60.1 CBOW† 300 6B 63.6 67.4 65.7 SG† 300 6B 73.0 66.0 69.1 GloVe 300 6B 77.4 67.0 71.7 CBOW 1000 6B 57.3 68.9 63.7 SG 1000 6B 66.1 65.1 65.6 SVD-L 300 42B 38.4 58.2 49.2 GloVe 300 42B 81.9 69.3 75.0 dataset for NER (Tjong Kim Sang and De Meul- der, 2003). 1/16/18 37 Analogy evaluation and hyperparameters • Asymmetric context (only words to the left) are not as good • Best dimensions ~300, slight drop-off afterwards • But this might be different for downstream tasks! • Window size of 8 around each center word is good for Glove vectors 0 100 200 300 400 500 600 20 30 40 50 60 70 80 Vector Dimension Accuracy [%] Semantic Syntactic Overall (a) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (b) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (c) Asymmetric context Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012) and RW (Luong et al 2013) has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl5. We tokenize and lowercase each corpus with the Stanford to- kenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of co- occurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right 1/16/18 38 Analogy evaluation and hyperparameters • More training time helps 5 6 20 25 40 50 W) GloVe CBOW W 3 6 9 12 15 18 21 24 60 62 64 66 68 70 72 20 40 60 80 100 1 2 3 4 5 6 7 10 12 15 20 GloVe Skip-Gram Accuracy [%] Iterations (GloVe) Negative Samples (Skip-Gram) Training Time (hrs) (b) GloVe vs Skip-Gram ord analogy task as a function of training time which is governed by 1/16/18 39 Analogy evaluation and hyperparameters • More data helps, Wikipedia is better than news text! ctors. . We MN, 50 55 60 65 70 75 80 85 Overall Syntactic Semantic Wiki2010 1B tokens Accuracy [%] Wiki2014 1.6B tokens Gigaword5 4.3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300- dimensional vectors trained on different corpora. 1/16/18 40 Another intrinsic word vector evaluation • Word vector distances and their correlation with human judgments • Example dataset: WordSim353 http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Word 1 Word 2 Human (mean) tiger cat 7.35 tiger tiger 10.00 book paper 7.46 computer internet 7.58 plane car 5.77 professor doctor 6.62 stock phone 1.62 stock CD 1.31 stock jaguar 0.92 1/16/18 41 Closest words to “Sweden” (cosine similarity) 1/16/18 42 Correlation evaluation • Word vector distances and their correlation with human judgments • Some ideas from Glove paper have been shown to improve skip-gram (SG) model also (e.g. sum both vectors) rs. Doing so typ- ormance, with the analogy task. d results of a va- as well as with the word2vec sing SVDs. With gram (SG†) and W†) models on the ia 2014 + Giga- top 400,000 most ndow size of 10. hich we show in or this corpus. nerate a truncated formation of how ith only the top This step is typi- based methods as a disproportion- the methods are sive Table 3: Spearman rank correlation on word simi- larity tasks. All vectors are 300-dimensional. The CBOW⇤vectors are from the word2vec website and differ in that they contain phrase vectors. Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25.6 SVD-S 6B 56.5 71.5 71.0 53.6 34.7 SVD-L 6B 65.7 72.7 75.1 56.5 37.0 CBOW† 6B 57.2 65.6 68.2 57.0 32.5 SG† 6B 62.8 65.2 69.7 58.1 37.2 GloVe 6B 65.8 72.7 77.8 53.9 38.1 SVD-L 42B 74.0 76.4 74.1 58.3 39.9 GloVe 42B 75.9 83.6 82.9 59.6 47.8 CBOW⇤100B 68.4 79.6 75.4 59.4 45.5 L model on this larger corpus. The fact that this basic SVD model does not scale well to large cor- pora lends further evidence to the necessity of the type of weighting scheme proposed in our model. Table 3 shows results on ﬁve different word similarity datasets. A similarity score is obtained 1/16/18 43 But what about ambiguity? • You may hope that one vector captures both kinds of information (run = verb and noun) but then vector is pulled in different directions • Alternative described in: Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012) • Idea: Cluster word windows around words, retrain with each word assigned to multiple different clusters bank1, bank2, etc 1/16/18 44 But what about ambiguity? • Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012) 1/16/18 45 Extrinsic word vector evaluation • Extrinsic evaluation of word vectors: All subsequent tasks in this class • One example where good word vectors should help directly: named entity recognition: finding a person, organization or location • Next: How to use word vectors in neural net models! Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details. Model Dev Test ACE MUC7 Discrete 91.0 85.4 77.4 73.4 SVD 90.8 85.7 77.3 73.7 SVD-S 91.0 85.5 77.6 74.3 SVD-L 90.5 84.8 73.6 71.5 HPCA 92.6 88.7 81.7 80.7 HSMN 90.5 85.7 78.7 74.7 CW 92.2 87.4 81.7 80.2 CBOW 93.1 88.2 82.2 81.1 GloVe 93.2 88.3 82.9 82.2 shown for neural vectors in (Turian et al., 2010). 4.4 Model Analysis: Vector Length and Context Size In Fig 2 we show the results of experiments that 50 55 60 65 70 75 80 85 Semantic Wiki2010 1B tokens Accuracy [%] Wiki2014 1.6B tokens Figure 3: Accuracy on dimensional vectors tra entries are updated to whereas Gigaword is a outdated and possibly i 4.6 Model Analysis: The total run-time is s and training the mode 1/16/18 46 Next level up: Word and Window classification • Let’s add context by taking in windows and classifying the center word of that window (and not just representing it across all windows)! • Possible: Softmax and cross entropy error or max-margin loss • Next class! 1/16/18 47 "
461,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 4: Word Window Classification and Neural Networks Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Organization • Main midterm: Feb 13 • Alternative midterm: Friday Feb 9 – tough choice • Final project and PSet4 can both have groups of 3. • Highly discouraged (especially for PSet4) and need more results so higher pressure for you to coordinate and deliver. • Python Review session tomorrow (Friday) 3-4:20pm at nVidia auditorium • Coding session: Next Monday, 6-9pm • Project website is up 1/18/18 2 Overview Today: • Classification background • Updating word vectors for classification • Window classification & cross entropy error derivation tips • A single layer neural network! • Max-Margin loss and backprop • This will be a tough lecture for some! à OH 1/18/18 3 Classification setup and notation • Generally we have a training dataset consisting of samples {xi,yi}N i=1 • xi are inputs, e.g. words (indices or vectors!), context windows, sentences, documents, etc. • Dimension d • yi are labels (one of C classes) we try to predict, for example: • classes: sentiment, named entities, buy/sell decision • other words • later: multi-word sequences 1/18/18 4 Classification intuition • Training data: {xi,yi}N i=1 • Simple illustration case: • Fixed 2D word vectors to classify • Using logistic regression • Linear decision boundary • General ML approach: assume xi are fixed, train logistic regression weights (only modifies the decision boundary) • Goal: For each x, predict: Visualizations with ConvNetJS by Karpathy! http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html 1/18/18 5 Details of the softmax classifier We can tease apart the prediction function into two steps: 1. Take the y’th row of W and multiply that row with x: Compute all fc for c=1,…,C 2. Apply softmax function to get normalized probability: = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥𝑓) 1/18/18 6 Training with softmax and cross-entropy error • For each training example {x,y}, our objective is to maximize the probability of the correct class y • Hence, we minimize the negative log probability of that class: 1/18/18 7 Background: Why “Cross entropy” error • Assuming a ground truth (or gold or target) probability distribution that is 1 at the right class and 0 everywhere else: p = [0,…,0,1,0,…0] and our computed probability is q, then the cross entropy is: • Because of one-hot p, the only term left is the negative log probability of the true class 1/18/18 8 Sidenote: The KL divergence • Cross-entropy can be re-written in terms of the entropy and Kullback-Leibler divergence between the two distributions: • Because H(p) is zero in our case (and even if it wasn’t it would be fixed and have no contribution to gradient), to minimize this is equal to minimizing the KL divergence between p and q • The KL divergence is not a distance but a non-symmetric measure of the difference between two probability distributions p and q 1/18/18 9 Classification over a full dataset • Cross entropy loss function over full dataset {xi,yi}N i=1 • Instead of • We will write f in matrix notation: • We can still index elements of it based on class 1/18/18 10 Classification: Regularization! • Really full loss function in practice includes regularization over all parameters 𝜃: • Regularization prevents overfitting when we have a lot of features (or later a very powerful/deep model,++) 1/18/18 11 model power overfitting Details: General ML optimization • For general machine learning 𝜃usually only consists of columns of W: • So we only update the decision boundary Visualizations with ConvNetJS by Karpathy 1/18/18 12 Classification difference with word vectors • Common in deep learning: • Learn both W and word vectors x Very large! Danger of overfitting! 1/18/18 13 A pitfall when retraining word vectors • Setting: We are training a logistic regression classification model for movie review sentiment using single words. • In the training data we have “TV” and “telly” • In the testing data we have “television” • The pre-trained word vectors have all three similar: • Question: What happens when we retrain the word vectors? TV telly television 1/18/18 14 A pitfall when retraining word vectors • Question: What happens when we train the word vectors? • Answer: • Those that are in the training data move around • “TV” and “telly” • Words not in the training data stay • “television” 1/18/18 15 TV telly television This is bad! So what should I do? • Question: Should I train my own word vectors? • Answer: • If you only have a small training data set, don’t train the word vectors. • If you have have a very large dataset, it may work better to train word vectors to the task. 1/18/18 16 TV telly television Side note on word vectors notation • The word vector matrix L is also called lookup table • Word vectors = word embeddings = word representations (mostly) • Usually obtained from methods like word2vec or Glove V L = d … … aardvark a … meta … zebra • These are the word features xword from now on • New development (later in the class): character models :o [ ] 1/18/18 17 Window classification • Classifying single words is rarely done. • Interesting problems like ambiguity arise in context! • Example: auto-antonyms: • ""To sanction"" can mean ""to permit"" or ""to punish.” • ""To seed"" can mean ""to place seeds"" or ""to remove seeds."" • Example: ambiguous named entities: • Paris à Paris, France vs Paris Hilton • Hathaway à Berkshire Hathaway vs Anne Hathaway 1/18/18 18 Window classification • Idea: classify a word in its context window of neighboring words. • For example, Named Entity Recognition is a 4-way classification task: • Person, Location, Organization, None • There are many ways to classify a single word in context • For example: average all the words in a window • Problem: that would lose position information 1/18/18 19 Window classification • Train softmax classifier to classify a center word by taking concatenation of all word vectors surrounding it • Example: Classify “Paris” in the context of this sentence with window length 2: … museums in Paris are amazing … . Xwindow = [ xmuseums xin xParis xare xamazing ]T • Resulting vector xwindow = x ∈R5d , a column vector! 1/18/18 20 Simplest window classifier: Softmax • With x = xwindow we can use the same softmax classifier as before • With cross entropy error as before: • But how do you update the word vectors? same predicted model output probability 1/18/18 21 Deriving gradients for window model • Short answer: Just take derivatives as before • Long answer: Let’s go over steps together (helpful for PSet 1) • Define: • : softmax probability output vector (see previous slide) • : target probability distribution (all 0’s except at ground truth index of class y, where it’s 1) • and fc = c’th element of the f vector • Hard, the first time, hence some tips now :) 1/18/18 22 • Tip 1: Carefully define your variables and keep track of their dimensionality! • Tip 2: Chain rule! If y = f(u) and u = g(x), i.e. y = f(g(x)), then: • Example repeated: Deriving gradients for window model 1/18/18 23 • Tip 2 continued: Know thy chain rule • Don’t forget which variables depend on what and that x appears inside all elements of f’s • Tip 3: For the softmax part of the derivative: First take the derivative wrt fc when c=y (the correct class), then take derivative wrt fc when c¹ y (all the incorrect classes) Deriving gradients for window model 1/18/18 24 • Tip 4: When you take derivative wrt one element of f, try to see if you can create a gradient in the end that includes all partial derivatives: • Tip 5: To later not go insane & implementation! à results in terms of vector operations and define single index-able vectors: Deriving gradients for window model 1/18/18 25 • Tip 6: When you start with the chain rule, first use explicit sums and look at partial derivatives of e.g. xi or Wij • Tip 7: To clean it up for even more complex functions later: Know dimensionality of variables &simplify into matrix notation • Tip 8: Write this out in full sums if it’s not clear! Deriving gradients for window model 1/18/18 26 • What is the dimensionality of the window vector gradient? • x is the entire window, 5 d-dimensional word vectors, so the derivative wrt to x has to have the same dimensionality: Deriving gradients for window model 1/18/18 27 • The gradient that arrives at and updates the word vectors can simply be split up for each word vector: • Let • With xwindow = [ xmuseums xin xParis xare xamazing ] • We have Deriving gradients for window model 1/18/18 28 • This will push word vectors into areas such they will be helpful in determining named entities. • For example, the model can learn that seeing xin as the word just before the center word is indicative for the center word to be a location Deriving gradients for window model 1/18/18 29 • The gradient of J wrt the softmax weights W! • Similar steps, write down partial wrt Wij first! • Then we have full What’s missing for training the window model? 1/18/18 30 A note on matrix implementations • There are two expensive operations in the softmax classifier: • The matrix multiplication and the exp • A large matrix multiplication is always more efficient than a for loop! • Example code on next slide à 1/18/18 31 A note on matrix implementations • Looping over word vectors instead of concatenating them all into one large matrix and then multiplying the softmax weights with that matrix • 1000 loops, best of 3: 639 µs per loop 10000 loops, best of 3: 53.8 µs per loop 1/18/18 32 A note on matrix implementations • Result of faster method is a C x N matrix: • Each column is an f(x) in our notation (unnormalized class scores) • You should speed-test your code a lot too • Tl;dr: Matrices are awesome! Matrix multiplication is better than for loop 33 Softmax (= logistic regression) alone not very powerful • Softmax only gives linear decision boundaries in the original space. • With little data that can be a good regularizer • With more data it is very limiting! 1/18/18 34 Softmax (= logistic regression) is not very powerful • Softmax only linear decision boundaries • à Unhelpful when problem is complex • Wouldn’t it be cool to get these correct? 1/18/18 35 Neural Nets for the Win! • Neural networks can learn much more complex functions and nonlinear decision boundaries! 1/18/18 36 From logistic regression to neural nets 1/18/18 37 Demystifying neural networks Neural networks come with their own terminological baggage But if you understand how softmax models work Then you already understand the operation of a basic neuron! A single neuron A computational unit with n (3) inputs and 1 output and parameters W, b Activation function Inputs Bias unit corresponds to intercept term Output 1/18/18 38 A neuron is essentially a binary logistic regression unit hw,b(x) = f (wTx + b) f (z) = 1 1+e−z w, b are the parameters of this neuron i.e., this logistic regression model b: We can have an “always on” feature, which gives a class prior, or separate it out, as a bias term 1/18/18 39 f = nonlinear activation fct. (e.g. sigmoid), w = weights, b = bias, h = hidden, x = inputs A neural network = running several logistic regressions at the same time If we feed a vector of inputs through a bunch of logistic regression functions, then we get a vector of outputs … But we don’t have to decide ahead of time what variables these logistic regressions are trying to predict! 1/18/18 40 A neural network = running several logistic regressions at the same time … which we can feed into another logistic regression function It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the targets for the next layer, etc. 1/18/18 41 A neural network = running several logistic regressions at the same time Before we know it, we have a multilayer neural network…. 1/18/18 42 Matrix notation for a layer We have In matrix notation where f is applied element-wise: a1 a2 a3 a1 = f (W 11x1 +W 12x2 +W 13x3 + b 1) a2 = f (W21x1 +W22x2 +W23x3 + b2) etc. z = Wx + b a = f (z) f ([z1, z2, z3]) =[ f (z1), f (z2), f (z3)] W12 b3 1/18/18 43 Non-linearities (aka “f”): Why they’re needed • Example: function approximation, e.g., regression or classification • Without non-linearities, deep neural networks can’t do anything more than a linear transform • Extra layers could just be compiled down into a single linear transform: W1 W2 x = Wx • With more layers, they can approximate more complex functions! 1/18/18 44 Binary classification with unnormalized scores • Revisiting our previous example: Xwindow = [ xmuseums xin xParis xare xamazing ] • Assume we want to classify whether the center word is a Location (Named Entity Recognition) • Similar to word2vec, we will go over all positions in a corpus. But this time, it will be supervised and only some positions should get a high score. • The positions that have an actual NER location in their center are called “true” positions. 1/18/18 45 Binary classification for NER • Example: Not all museums in Paris are amazing. • Here: one true window, the one with Paris in its center and all other windows are “corrupt” in terms of not having a named entity location in their center. • “Corrupt“ windows are easy to find and there are many: Any window that isn’t specifically labeled as NER location in our corpus 1/18/18 46 A Single Layer Neural Network • A single layer is a combination of a linear layer and a nonlinearity: • The neural activations a can then be used to compute some output. • For instance, a probability via softmax: 𝑝𝑦𝑥= softmax 𝑊𝑎 • Or an unnormalized score (even simpler): 1/18/18 47 Summary: Feed-forward Computation We compute a window’s score with a 3-layer neural net: • s = score(""museums in Paris are amazing”) xwindow = [ xmuseums xin xParis xare xamazing ] 1/18/18 48 Main intuition for extra layer The layer learns non-linear interactions between the input word vectors. Example: only if “museums” is first vector should it matter that “in” is in the second position Xwindow = [ xmuseums xin xParis xare xamazing ] 1/18/18 49 The max-margin loss • Idea for training objective: Make true window’s score larger and corrupt window’s score lower (until they’re good enough): minimize • s = score(museums in Paris are amazing) • sc = score(Not all museums in Paris) • This is not differentiable but it is Each option continuous --> we can use SGD. is continuous 1/18/18 50 Max-margin loss • Objective for a single window: • Each window with an NER location at its center should have a score +1 higher than any window without a location at its center • xxx |ß 1 à| ooo • For full objective function: Sample several corrupt windows per true one. Sum over all training windows. • Similar to negative sampling in word2vec 1/18/18 51 Deriving gradients for backprop Assuming cost J is > 0, compute the derivatives of s and sc wrt all the involved variables: U, W, b, x à 1/18/18 52 Deriving gradients for backprop • For this function: • Let’s consider the derivative of a single weight Wij • Wij only appears inside ai • For example: W23 is only used to compute a2 not a1 53 x1 x2 x3 +1 f(z1)= a1 a2 =f(z2) s U2 W23 b2 Deriving gradients for backprop Derivative of single weight Wij: 54 x1 x2 x3 +1 f(z1)= a1 a2 =f(z2) s U2 W23 b2 Ignore constants in which Wij doesn’t appear Pull out Ui since it’s constant, apply chain rule Apply definition of a Just define derivative of f as f’ Plug in definition of z where for logistic f Deriving gradients for backprop Derivative of single weight Wij continued: Local error signal Local input signal 55 x1 x2 x3 +1 f(z1)= a1 a2 =f(z2) s U2 W23 b2 • We want all combinations of i = 1, 2 and j = 1, 2, 3 à ? • Solution: Outer product: where is the “responsibility” or error signal coming from each activation a Deriving gradients for backprop • So far, derivative of single Wij only , but we want gradient for full W. S 56 Deriving gradients for backprop • How to derive gradients for biases b? 57 x1 x2 x3 +1 f(z1)= a1 a2 =f(z2) s U2 W23 b2 b Training with Backpropagation That’s almost backpropagation It’s taking derivatives and using the chain rule Remaining trick: we can re-use derivatives computed for higher layers in computing derivatives for lower layers! Example: last derivatives of model, the word vectors in x 1/18/18 58 Training with Backpropagation • Take derivative of score with respect to single element of word vector • Now, we cannot just take into consideration one ai because each xj is connected to all the neurons above and hence xj influences the overall score through all of these, hence: Re-used part of previous derivative 1/18/18 59 Training with Backpropagation • With ,what is the full gradient? à • Observations: The error message 𝛿that arrives at a hidden layer has the same dimensionality as that hidden layer 1/18/18 60 Putting all gradients together: • Remember: Full objective function for each window was: • For example: gradient for just U: 1/18/18 61 Summary Congrats! Super useful basic components and real model • Word vector training • Windows • Softmax and cross entropy error à PSet1 • Scores and max-margin loss • Neural network à PSet1 1/18/18 62 Next lecture: Taking more and deeper derivatives à Full Backprop High level tips for easier derivations Then we have all the basic tools in place to learn about and have fun with more complex and deeper models :) 1/18/18 63 "
462,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 5: Backpropagation Kevin Clark Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Assignment 1 due Thursday, 11:59 • You can use up to 3 late days (making it due Sunday at midnight) • Default ﬁnal project will be released February 1st • To help you choose which project opGon you want to do • Final project proposal due February 8th • See website for details and inspiraGon 2 Overview Today: • From one-layer to mulG layer neural networks! • Fully vectorized gradient computaGon • The backpropagaGon algorithm • (Time permiNng) Class project Gps 3 Remember: One-layer Neural Net x = [ xmuseums xin xParis xare xamazing ] 4 Two-layer Neural Net x = [ xmuseums xin xParis xare xamazing ] 5 Repeat as Needed! x = [ xmuseums xin xParis xare xamazing ] 6 Why Have Mul@ple Layers? • Hierarchical representaGons -> neural net can represent complicated features • BeZer results! # Layers Machine Transla@on Score 2 23.7 4 25.3 8 25.5 From Transformer Network (will cover in a later lecture) 7 Remember: Stochas@c Gradient Descent • Update equaGon: 𝛼 = step size or learning rate 8 Remember: Stochas@c Gradient Descent • Update equaGon: • This Lecture: How do we compute ? • By hand • Algorithmically (the backpropagaGon algorithm) 𝛼 = step size or learning rate 9 Why learn all these details about gradients? • Modern deep learning frameworks compute gradients for you • But why take a class on compilers or systems when they are implemented for you? • Understanding what is going on under the hood is useful! • BackpropagaGon doesn’t always work perfectly. • Understanding why is crucial for debugging and improving models • Example in future lecture: exploding and vanishing gradients 10 Quickly Compu@ng Gradients by Hand • Review of mulGvariable derivaGves • Fully vectorized gradients • Much faster and more useful than non-vectorized gradients • But doing a non-vectorized gradient can be good pracGce, see slides in last week’s lecture for an example • Lecture notes cover this material in more detail 11 Gradients • Given a funcGon with 1 output and n inputs • Its gradient is a vector of parGal derivaGves 12 Jacobian Matrix: Generaliza@on of the Gradient • Given a funcGon with m outputs and n inputs • Its Jacobian is an m x n matrix of parGal derivaGves 13 Chain Rule For Jacobians • For one-variable funcGons: mulGply derivaGves • For mulGple variables: mulGply Jacobians 14 Example Jacobian: Ac@va@on Func@on 15 Example Jacobian: Ac@va@on Func@on FuncGon has n outputs and n inputs -> n by n Jacobian 16 Example Jacobian: Ac@va@on Func@on 17 Example Jacobian: Ac@va@on Func@on 18 Example Jacobian: Ac@va@on Func@on 19 Other Jacobians • Compute these at home for pracGce! • Check your answers with the lecture notes 20 Other Jacobians • Compute these at home for pracGce! • Check your answers with the lecture notes 21 Other Jacobians • Compute these at home for pracGce! • Check your answers with the lecture notes 22 Other Jacobians • Compute these at home for pracGce! • Check your answers with the lecture notes 23 Back to Neural Nets! x = [ xmuseums xin xParis xare xamazing ] 24 Back to Neural Nets! x = [ xmuseums xin xParis xare xamazing ] • Let’s ﬁnd • In pracGce we care about the gradient of the loss, but we will compute the gradient of the score for simplicity 25 1. Break up equa@ons into simple pieces 26 2. Apply the chain rule 27 2. Apply the chain rule 28 2. Apply the chain rule 29 2. Apply the chain rule 30 3. Write out the Jacobians Useful Jacobians from previous slide 31 3. Write out the Jacobians Useful Jacobians from previous slide 32 3. Write out the Jacobians Useful Jacobians from previous slide 33 3. Write out the Jacobians Useful Jacobians from previous slide 34 3. Write out the Jacobians Useful Jacobians from previous slide 35 Re-using Computa@on • Suppose we now want to compute • Using the chain rule again: 36 Re-using Computa@on • Suppose we now want to compute • Using the chain rule again: The same! Let’s avoid duplicated computaGon… 37 Re-using Computa@on • Suppose we now want to compute • Using the chain rule again: 38 Deriva@ve with respect to Matrix • What does look like? • 1 output, nm inputs: 1 by nm Jacobian? • Inconvenient to do 39 Deriva@ve with respect to Matrix • What does look like? • 1 output, nm inputs: 1 by nm Jacobian? • Inconvenient to do • Instead follow convenGon: shape of the gradient is shape of parameters • So is n by m: 40 Deriva@ve with respect to Matrix • Remember • is going to be in our answer • The other term should be because • It turns out 41 Why the Transposes? • Hacky answer: this makes the dimensions work out • Useful trick for checking your work! • Full explanaGon in the lecture notes 42 Why the Transposes? • Hacky answer: this makes the dimensions work out • Useful trick for checking your work! • Full explanaGon in the lecture notes 43 What shape should deriva@ves be? • is a row vector • But convenGon says our gradient should be a column vector because is a column vector… • Disagreement between Jacobian form (which makes the chain rule easy) and the shape convenGon (which makes implemenGng SGD easy) • We expect answers to follow the shape convenGon • But Jacobian form is useful for compuGng the answers 44 What shape should deriva@ves be? • Two opGons: • 1. Use Jacobian form as much as possible, reshape to follow the convenGon at the end: • What we just did. But at the end transpose to make the derivaGve a column vector, resulGng in • 2. Always follow the convenGon • Look at dimensions to ﬁgure out when to transpose and/or reorder terms. 45 Notes on PA1 • Don’t worry if you used some other method for gradient computaGon (as long as your answer is right and you are consistent!) • This lecture we computed the gradient of the score, but in PA1 its of the loss • Don’t forget to replace f’ with the actual derivaGve • PA1 uses for the linear transformaGon: gradients are diﬀerent! 46 Backpropaga@on • Compute gradients algorithmically • ConverGng what we just did by hand into an algorithm • Used by deep learning frameworks (TensorFlow, PyTorch, etc.) 47 Computa@onal Graphs ! + ! • RepresenGng our neural net equaGons as a graph • Source nodes: inputs • Interior nodes: operaGons 48 Computa@onal Graphs ! + ! • RepresenGng our neural net equaGons as a graph • Source nodes: inputs • Interior nodes: operaGons • Edges pass along result of the operaGon 49 Computa@onal Graphs ! + ! • RepresenGng our neural net equaGons as a graph • Source nodes: inputs • Interior nodes: operaGons • Edges pass along result of the operaGon “Forward PropagaGon” 50 Backpropaga@on ! + ! • Go backwards along edges • Pass along gradients 51 Backpropaga@on: Single Node • Node receives an “upstream gradient” • Goal is to pass on the correct “downstream gradient” Upstream gradient 52 Downstream gradient Backpropaga@on: Single Node Downstream gradient Upstream gradient • Each node has a local gradient • The gradient of its output with respect to its input Local gradient 53 Backpropaga@on: Single Node Downstream gradient Upstream gradient • Each node has a local gradient • The gradient of its output with respect to its input Local gradient 54 Chain rule! Backpropaga@on: Single Node Downstream gradient Upstream gradient • Each node has a local gradient • The gradient of its output with respect to its input Local gradient • [downstream gradient] = [upstream gradient] x [local gradient] 55 Backpropaga@on: Single Node * • What about nodes with mulGple inputs? 56 Backpropaga@on: Single Node Downstream gradients Upstream gradient Local gradients * • MulGple inputs -> mulGple local gradients 57 An Example 58 An Example + * max 59 Forward prop steps An Example + * max 60 Forward prop steps 6 3 2 1 2 2 0 An Example + * max 61 Forward prop steps 6 3 2 1 2 2 0 Local gradients An Example + * max 62 Forward prop steps 6 3 2 1 2 2 0 Local gradients An Example + * max 63 Forward prop steps 6 3 2 1 2 2 0 Local gradients An Example + * max 64 Forward prop steps 6 3 2 1 2 2 0 Local gradients An Example + * max 65 Forward prop steps 6 3 2 1 2 2 0 Local gradients upstream * local = downstream 1 1*3 = 3 1*2 = 2 An Example + * max 66 Forward prop steps 6 3 2 1 2 2 0 Local gradients upstream * local = downstream 1 3 2 3*1 = 3 3*0 = 0 An Example + * max 67 Forward prop steps 6 3 2 1 2 2 0 Local gradients upstream * local = downstream 1 3 2 3 0 2*1 = 2 2*1 = 2 An Example + * max 68 Forward prop steps 6 3 2 1 2 2 0 Local gradients 1 3 2 3 0 2 2 Gradients add at branches 69 + Gradients add at branches 70 + Node Intui@ons + * max 71 6 3 2 1 2 2 0 1 2 2 2 • + “distributes” the upstream gradient Node Intui@ons + * max 72 6 3 2 1 2 2 0 1 3 3 0 • + “distributes” the upstream gradient • max “routes” the upstream gradient Node Intui@ons + * max 73 6 3 2 1 2 2 0 1 3 2 • + “distributes” the upstream gradient • max “routes” the upstream gradient • * “switches” the upstream gradient Eﬃciency: compute all gradients at once * + ! • Incorrect way of doing backprop: • First compute 74 Eﬃciency: compute all gradients at once * + ! • Incorrect way of doing backprop: • First compute • Then independently compute • Duplicated computaGon! 75 Eﬃciency: compute all gradients at once * + ! • Correct way: • Compute all the gradients at once • Analogous to using when we computed gradients by hand 76 Backprop Implementa@ons 77 Implementa@on: forward/backward API 78 Implementa@on: forward/backward API 79 Alterna@ve to backprop: Numeric Gradient • For small h, • Easy to implement • But approximate and very slow: • Have to recompute f for every parameter of our model • Useful for checking your implementaGon 80 Summary • BackpropagaGon: recursively apply the chain rule along computaGonal graph • [downstream gradient] = [upstream gradient] x [local gradient] • Forward pass: compute results of operaGon and save intermediate values • Backward: apply chain rule to compute gradient 81 82 Project Types 1. Apply exisGng neural network model to a new task 2. Implement a complex neural architecture(s) • This is what PA4 will have you do! 3. Come up with a new model/training algorithm/etc. • Get 1 or 2 working ﬁrst • See project page for some inspiraGon 83 Must-haves (choose-your-own ﬁnal project) • 10,000+ labeled examples by milestone • Feasible task • AutomaGc evaluaGon metric • NLP is central 84 Details ma_er! • Split your data into train/dev/test: only look at test for ﬁnal experiments • Look at your data, collect summary staGsGcs • Look at your model’s outputs, do error analysis • Tuning hyperparameters is important • Writeup quality is important • Look at last-year’s prize winners for examples 85 Project Advice • Implement simplest possible model ﬁrst (e.g., average word vectors and apply logisGc regression) and improve it • Having a baseline system is crucial • First overﬁt your model to train set (get really good training set results) • Then regularize it so it does well on the dev set • Start early! 86 "
463,"An introduction to TensorFlow! Chip Huyen (chiphuyen@cs.stanford.edu) CS224N 1/25/2018 1 2 Agenda Why TensorFlow Graphs and Sessions Linear Regression tf.data word2vec Structuring your model Managing experiments 3 Why TensorFlow? ● Flexibility + Scalability ● Popularity 4 import tensorflow as tf 5 Graphs and Sessions 6 Data Flow Graphs TensorFlow separates definition of computations from their execution Graph from TensorFlow for Machine Intelligence 7 Data Flow Graphs Phase 1: assemble a graph Phase 2: use a session to execute operations in the graph. 8 Graph from TensorFlow for Machine Intelligence Data Flow Graphs Phase 1: assemble a graph Phase 2: use a session to execute operations in the graph. 9 Graph from TensorFlow for Machine Intelligence This might change in the future with eager mode!! What’s a tensor? 10 What’s a tensor? An n-dimensional array 0-d tensor: scalar (number) 1-d tensor: vector 2-d tensor: matrix and so on 11 Data Flow Graphs import tensorflow as tf a = tf.add(3, 5) Visualized by TensorBoard 12 Data Flow Graphs import tensorflow as tf a = tf.add(3, 5) Why x, y? TF automatically names the nodes when you don’t explicitly name them. x = 3 y = 5 Visualized by TensorBoard 13 Data Flow Graphs import tensorflow as tf a = tf.add(3, 5) Nodes: operators, variables, and constants Edges: tensors Tensors are data. TensorFlow = tensor + flow = data + flow (I know, mind=blown) 14 Interpreted? 5 3 a Data Flow Graphs import tensorflow as tf a = tf.add(3, 5) print(a) >> Tensor(""Add:0"", shape=(), dtype=int32) (Not 8) 15 5 3 a How to get the value of a? Create a session, assign it to variable sess so we can call it later Within the session, evaluate the graph to fetch the value of a 16 How to get the value of a? Create a session, assign it to variable sess so we can call it later Within the session, evaluate the graph to fetch the value of a import tensorflow as tf a = tf.add(3, 5) sess = tf.Session() print(sess.run(a)) sess.close() The session will look at the graph, trying to think: hmm, how can I get the value of a, then it computes all the nodes that leads to a. 17 How to get the value of a? Create a session, assign it to variable sess so we can call it later Within the session, evaluate the graph to fetch the value of a import tensorflow as tf a = tf.add(3, 5) sess = tf.Session() print(sess.run(a)) sess.close() >> 8 8 The session will look at the graph, trying to think: hmm, how can I get the value of a, then it computes all the nodes that leads to a. 18 How to get the value of a? Create a session, assign it to variable sess so we can call it later Within the session, evaluate the graph to fetch the value of a import tensorflow as tf a = tf.add(3, 5) sess = tf.Session() with tf.Session() as sess: print(sess.run(a)) sess.close() 8 19 tf.Session() A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. 20 tf.Session() A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. Session will also allocate memory to store the current values of variables. 21 More graph x = 2 y = 3 op1 = tf.add(x, y) op2 = tf.multiply(x, y) op3 = tf.pow(op2, op1) with tf.Session() as sess: op3 = sess.run(op3) Visualized by TensorBoard 22 Subgraphs Because we only want the value of pow_op and pow_op doesn’t depend on useless, session won’t compute value of useless → save computation useless add_op mul_op pow_op 23 x = 2 y = 3 add_op = tf.add(x, y) mul_op = tf.multiply(x, y) useless = tf.multiply(x, add_op) pow_op = tf.pow(add_op, mul_op) with tf.Session() as sess: z = sess.run(pow_op) Subgraphs Possible to break graphs into several chunks and run them parallelly across multiple CPUs, GPUs, TPUs, or other devices Example: AlexNet Graph from Hands-On Machine Learning with Scikit-Learn and TensorFlow 24 Distributed Computation To put part of a graph on a specific CPU or GPU: # Creates a graph. with tf.device('/gpu:2'): a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a') b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b') c = tf.multiply(a, b) # Creates a session with log_device_placement set to True. sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) # Runs the op. print(sess.run(c)) 25 26 Why graphs 1. Save computation. Only run subgraphs that lead to the values you want to fetch. 27 Why graphs 1. Save computation. Only run subgraphs that lead to the values you want to fetch. 2. Break computation into small, differential pieces to facilitate auto-differentiation 28 Why graphs 1. Save computation. Only run subgraphs that lead to the values you want to fetch. 2. Break computation into small, differential pieces to facilitate auto-differentiation 3. Facilitate distributed computation, spread the work across multiple CPUs, GPUs, TPUs, or other devices 29 Why graphs 1. Save computation. Only run subgraphs that lead to the values you want to fetch. 2. Break computation into small, differential pieces to facilitate auto-differentiation 3. Facilitate distributed computation, spread the work across multiple CPUs, GPUs, TPUs, or other devices 4. Many common machine learning models are taught and visualized as directed graphs A neural net graph from Stanford’s CS224N course 30 TensorBoard 31 import tensorflow as tf a = tf.constant(2, name='a') b = tf.constant(3, name='b') x = tf.add(a, b, name='add') with tf.Session() as sess: print(sess.run(x)) Your first TensorFlow program 32 import tensorflow as tf a = tf.constant(2, name='a') b = tf.constant(3, name='b') x = tf.add(a, b, name='add') writer = tf.summary.FileWriter('./graphs', tf.get_default_graph()) with tf.Session() as sess: # writer = tf.summary.FileWriter('./graphs', sess.graph) print(sess.run(x)) writer.close() # close the writer when you’re done using it Visualize it with TensorBoard 33 Create the summary writer after graph definition and before running your session ‘graphs’ or any location where you want to keep your event files Go to terminal, run: $ python [yourprogram].py $ tensorboard --logdir=""./graphs"" --port 6006 Then open your browser and go to: http://localhost:6006/ Run it 34 6006 or any port you want 35 Constants, Sequences, Variables, Ops 36 import tensorflow as tf a = tf.constant([2, 2], name='a') b = tf.constant([[0, 1], [2, 3]], name='b') x = tf.multiply(a, b, name='mul') with tf.Session() as sess: print(sess.run(x)) # >> [[0 2] # [4 6]] Constants 37 Broadcasting similar to NumPy Tensors filled with a specific value tf.zeros([2, 3], tf.int32) ==> [[0, 0, 0], [0, 0, 0]] # input_tensor is [[0, 1], [2, 3], [4, 5]] tf.zeros_like(input_tensor) ==> [[0, 0], [0, 0], [0, 0]] tf.fill([2, 3], 8) ==> [[8, 8, 8], [8, 8, 8]] Similar to NumPy 38 Constants as sequences tf.lin_space(start, stop, num, name=None) tf.lin_space(10.0, 13.0, 4) ==> [10. 11. 12. 13.] tf.range(start, limit=None, delta=1, dtype=None, name='range') tf.range(3, 18, 3) ==> [3 6 9 12 15] tf.range(5) ==> [0 1 2 3 4] NOT THE SAME AS NUMPY SEQUENCES Tensor objects are not iterable for _ in tf.range(4): # TypeError 39 Randomly Generated Constants tf.random_normal tf.truncated_normal tf.random_uniform tf.random_shuffle tf.random_crop tf.multinomial tf.random_gamma 40 Randomly Generated Constants tf.set_random_seed(seed) 41 TensorFlow integrates seamlessly with NumPy tf.int32 == np.int32 # ⇒ True Can pass numpy types to TensorFlow ops tf.ones([2, 2], np.float32) # ⇒ [[1.0 1.0], [1.0 1.0]] For tf.Session.run(fetches): if the requested fetch is a Tensor , output will be a NumPy ndarray. sess = tf.Session() a = tf.zeros([2, 3], np.int32) print(type(a)) # ⇒ <class 'tensorflow.python.framework.ops.Tensor'> a_out = sess.run(a) print(type(a)) # ⇒ <class 'numpy.ndarray'> TF vs NP Data Types 42 ● Python native types: TensorFlow has to infer Python type Use TF DType when possible 43 ● Python native types: TensorFlow has to infer Python type ● NumPy arrays: NumPy is not GPU compatible Use TF DType when possible 44 Not trainable What’s wrong with constants? 45 my_const = tf.constant([1.0, 2.0], name=""my_const"") with tf.Session() as sess: print(sess.graph.as_graph_def()) Constants are stored in graph definition 46 This makes loading graphs expensive when constants are big Constants are stored in graph definition 47 This makes loading graphs expensive when constants are big Only use constants for primitive types. Use variables or readers for more data that requires more memory Constants are stored in graph definition 48 Variables # create variables with tf.Variable s = tf.Variable(2, name=""scalar"") m = tf.Variable([[0, 1], [2, 3]], name=""matrix"") W = tf.Variable(tf.zeros([784,10])) # create variables with tf.get_variable s = tf.get_variable(""scalar"", initializer=tf.constant(2)) m = tf.get_variable(""matrix"", initializer=tf.constant([[0, 1], [2, 3]])) W = tf.get_variable(""big_matrix"", shape=(784, 10), initializer=tf.zeros_initializer()) 49 You have to initialize your variables The easiest way is initializing all variables at once: with tf.Session() as sess: sess.run(tf.global_variables_initializer()) 50 Initializer is an op. You need to execute it within the context of a session You have to initialize your variables The easiest way is initializing all variables at once: with tf.Session() as sess: sess.run(tf.global_variables_initializer()) Initialize only a subset of variables: with tf.Session() as sess: sess.run(tf.variables_initializer([a, b])) 51 You have to initialize your variables The easiest way is initializing all variables at once: with tf.Session() as sess: sess.run(tf.global_variables_initializer()) Initialize only a subset of variables: with tf.Session() as sess: sess.run(tf.variables_initializer([a, b])) Initialize a single variable W = tf.Variable(tf.zeros([784,10])) with tf.Session() as sess: sess.run(W.initializer) 52 Eval() a variable # W is a random 700 x 100 variable object W = tf.Variable(tf.truncated_normal([700, 10])) with tf.Session() as sess: sess.run(W.initializer) print(W) >> Tensor(""Variable/read:0"", shape=(700, 10), dtype=float32) 53 tf.Variable.assign() W = tf.Variable(10) W.assign(100) with tf.Session() as sess: sess.run(W.initializer) print(W.eval()) # >> ???? 54 tf.Variable.assign() W = tf.Variable(10) W.assign(100) with tf.Session() as sess: sess.run(W.initializer) print(W.eval()) # >> 10 55 Ugh, why? tf.Variable.assign() W = tf.Variable(10) W.assign(100) with tf.Session() as sess: sess.run(W.initializer) print(W.eval()) # >> 10 56 W.assign(100) creates an assign op. That op needs to be executed in a session to take effect. tf.Variable.assign() W = tf.Variable(10) W.assign(100) with tf.Session() as sess: sess.run(W.initializer) print(W.eval()) # >> 10 -------- W = tf.Variable(10) assign_op = W.assign(100) with tf.Session() as sess: sess.run(W.initializer) sess.run(assign_op) print(W.eval()) # >> 100 57 Placeholder 58 A TF program often has 2 phases: 1. Assemble a graph 2. Use a session to execute operations in the graph. A quick reminder 59 A TF program often has 2 phases: 1. Assemble a graph 2. Use a session to execute operations in the graph. ⇒ Assemble the graph first without knowing the values needed for computation Placeholders 60 A TF program often has 2 phases: 1. Assemble a graph 2. Use a session to execute operations in the graph. ⇒ Assemble the graph first without knowing the values needed for computation Analogy: Define the function f(x, y) = 2 * x + y without knowing value of x or y. x, y are placeholders for the actual values. Placeholders 61 Why placeholders? We, or our clients, can later supply their own data when they need to execute the computation. 62 Placeholders tf.placeholder(dtype, shape=None, name=None) # create a placeholder for a vector of 3 elements, type tf.float32 a = tf.placeholder(tf.float32, shape=[3]) b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c)) # >> ??? 63 Placeholders tf.placeholder(dtype, shape=None, name=None) # create a placeholder for a vector of 3 elements, type tf.float32 a = tf.placeholder(tf.float32, shape=[3]) b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c)) # >> InvalidArgumentError: a doesn’t an actual value 64 Supplement the values to placeholders using a dictionary 65 Placeholders tf.placeholder(dtype, shape=None, name=None) # create a placeholder for a vector of 3 elements, type tf.float32 a = tf.placeholder(tf.float32, shape=[3]) b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c, feed_dict={a: [1, 2, 3]})) # the tensor a is the key, not the string ‘a’ # >> [6, 7, 8] 66 Placeholders tf.placeholder(dtype, shape=None, name=None) # create a placeholder for a vector of 3 elements, type tf.float32 a = tf.placeholder(tf.float32, shape=[3]) b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c, feed_dict={a: [1, 2, 3]})) # >> [6, 7, 8] Quirk: shape=None means that tensor of any shape will be accepted as value for placeholder. shape=None is easy to construct graphs and great when you have different batch sizes, but nightmarish for debugging 67 Placeholders tf.placeholder(dtype, shape=None, name=None) # create a placeholder of type float 32-bit, shape is a vector of 3 elements a = tf.placeholder(tf.float32, shape=[3]) # create a constant of type float 32-bit, shape is a vector of 3 elements b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # Short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c, {a: [1, 2, 3]})) # >> [6, 7, 8] Quirk: shape=None also breaks all following shape inference, which makes many ops not work because they expect certain rank. 68 Placeholders are valid ops tf.placeholder(dtype, shape=None, name=None) # create a placeholder of type float 32-bit, shape is a vector of 3 elements a = tf.placeholder(tf.float32, shape=[3]) # create a constant of type float 32-bit, shape is a vector of 3 elements b = tf.constant([5, 5, 5], tf.float32) # use the placeholder as you would a constant or a variable c = a + b # Short for tf.add(a, b) with tf.Session() as sess: print(sess.run(c, {a: [1, 2, 3]})) # >> [6, 7, 8] 69 What if want to feed multiple data points in? You have to do it one at a time with tf.Session() as sess: for a_value in list_of_values_for_a: print(sess.run(c, {a: a_value})) 70 Extremely helpful for testing Feed in dummy values to test parts of a large graph 71 Linear Regression in TensorFlow 72 Model the linear relationship between: ●dependent variable Y ●explanatory variables X 73 74 Visualization made by Google, based on data collected by World Bank X: birth rate Y: life expectancy 190 countries 75 World Development Indicators dataset Find a linear relationship between X and Y to predict Y from X 76 Want Model Inference: Y_predicted = w * X + b Mean squared error: E[(y - y_predicted)2] 77 Interactive Coding birth_life_2010.txt 78 Interactive Coding linreg_starter.py birth_life_2010.txt 79 Phase 1: Assemble our graph 80 Step 1: Read in data I already did that for you 81 Step 2: Create placeholders for inputs and labels tf.placeholder(dtype, shape=None, name=None) 82 Step 3: Create weight and bias tf.get_variable( name, shape=None, dtype=None, initializer=None, … ) 83 No need to specify shape if using constant initializer Step 4: Inference Y_predicted = w * X + b 84 Step 5: Specify loss function loss = tf.square(Y - Y_predicted, name='loss') 85 Step 6: Create optimizer opt = tf.train.GradientDescentOptimizer(learning_rate=0.001) optimizer = opt.minimize(loss) 86 Phase 2: Train our model Step 1: Initialize variables Step 2: Run optimizer (use a feed_dict to feed data into X and Y placeholders) 87 Write log files using a FileWriter writer = tf.summary.FileWriter('./graphs/linear_reg', sess.graph) 88 See it on TensorBoard Step 1: $ python linreg_starter.py Step 2: $ tensorboard --logdir='./graphs' 89 90 91 tf.data 92 Placeholder Pro: put the data processing outside TensorFlow, making it easy to do in Python Cons: users often end up processing their data in a single thread and creating data bottleneck that slows execution down. 93 Placeholder data, n_samples = utils.read_birth_life_data(DATA_FILE) X = tf.placeholder(tf.float32, name='X') Y = tf.placeholder(tf.float32, name='Y') … with tf.Session() as sess: … # Step 8: train the model for i in range(100): # run 100 epochs for x, y in data: # Session runs train_op to minimize loss sess.run(optimizer, feed_dict={X: x, Y:y}) 94 tf.data Instead of doing inference with placeholders and feeding in data later, do inference directly with data 95 tf.data tf.data.Dataset tf.data.Iterator 96 Store data in tf.data.Dataset ● tf.data.Dataset.from_tensor_slices((features, labels)) ● tf.data.Dataset.from_generator(gen, output_types, output_shapes) 97 Store data in tf.data.Dataset tf.data.Dataset.from_tensor_slices((features, labels)) dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1])) 98 Store data in tf.data.Dataset tf.data.Dataset.from_tensor_slices((features, labels)) dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1])) print(dataset.output_types) # >> (tf.float32, tf.float32) print(dataset.output_shapes) # >> (TensorShape([]), TensorShape([])) 99 Can also create Dataset from files ● tf.data.TextLineDataset(filenames) ● tf.data.FixedLengthRecordDataset(filenames) ● tf.data.TFRecordDataset(filenames) 100 tf.data.Iterator Create an iterator to iterate through samples in Dataset 101 tf.data.Iterator ● iterator = dataset.make_one_shot_iterator() ● iterator = dataset.make_initializable_iterator() 102 tf.data.Iterator ● iterator = dataset.make_one_shot_iterator() Iterates through the dataset exactly once. No need to initialization. ● iterator = dataset.make_initializable_iterator() Iterates through the dataset as many times as we want. Need to initialize with each epoch. 103 tf.data.Iterator iterator = dataset.make_one_shot_iterator() X, Y = iterator.get_next() # X is the birth rate, Y is the life expectancy with tf.Session() as sess: print(sess.run([X, Y])) # >> [1.822, 74.82825] print(sess.run([X, Y])) # >> [3.869, 70.81949] print(sess.run([X, Y])) # >> [3.911, 72.15066] 104 tf.data.Iterator iterator = dataset.make_initializable_iterator() ... for i in range(100): sess.run(iterator.initializer) total_loss = 0 try: while True: sess.run([optimizer]) except tf.errors.OutOfRangeError: pass 105 Handling data in TensorFlow dataset = dataset.shuffle(1000) dataset = dataset.repeat(100) dataset = dataset.batch(128) dataset = dataset.map(lambda x: tf.one_hot(x, 10)) # convert each element of dataset to one_hot vector 106 Does tf.data really perform better? 107 Does tf.data really perform better? With placeholder: 9.05271519 seconds With tf.data: 6.12285947 seconds 108 Should we always use tf.data? ● For prototyping, feed dict can be faster and easier to write (pythonic) ● tf.data is tricky to use when you have complicated preprocessing or multiple data sources ● NLP data is normally just a sequence of integers. In this case, transferring the data over to GPU is pretty quick, so the speedup of tf.data isn't that large 109 How does TensorFlow know what variables to update? 110 Optimizers 111 Optimizer optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss) _, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) 112 Optimizer optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss) _, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) Session looks at all trainable variables that loss depends on and update them 113 Optimizer Session looks at all trainable variables that optimizer depends on and update them 114 Trainable variables tf.Variable(initial_value=None, trainable=True,...) 115 Specify if a variable should be trained or not By default, all variables are trainable List of optimizers in TF tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer ... 116 Usually Adam works out-of-the-box better than SGD word2vec skip-gram in TensorFlow 118 Embedding Lookup Illustration by Chris McCormick 119 Embedding Lookup Illustration by Chris McCormick tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None) 120 Negative sampling vs NCE ● Negative sampling is a simplified model of Noise Contrastive Estimation (NCE) ● NCE guarantees approximation to softmax. Negative sampling doesn’t 121 NCE Loss tf.nn.nce_loss( weights, biases, labels, inputs, num_sampled, num_classes, … ) Interactive Coding word2vec_utils.py word2vec_starter.py 122 Embedding visualization Interactive Coding word2vec_visualize.py 124 Visualization from Chris Olah’s blog 125 Visualize vector representation of anything 126 127 Name scope TensorFlow doesn’t know what nodes should be grouped together, unless you tell it to 128 Name scope Group nodes together with tf.name_scope(name) with tf.name_scope(name_of_that_scope): # declare op_1 # declare op_2 # ... 129 Name scope with tf.name_scope('data'): iterator = dataset.make_initializable_iterator() center_words, target_words = iterator.get_next() with tf.name_scope('embed'): embed_matrix = tf.get_variable('embed_matrix', shape=[VOCAB_SIZE, EMBED_SIZE], ...) embed = tf.nn.embedding_lookup(embed_matrix, center_words) with tf.name_scope('loss'): nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE], ...) nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE])) loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, …) with tf.name_scope('optimizer'): optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss) 130 TensorBoard 131 Variable scope Name scope vs variable scope tf.name_scope() vs tf.variable_scope() 132 Variable scope Name scope vs variable scope Variable scope facilitates variable sharing 133 Variable sharing: The problem def two_hidden_layers(x): w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights') b1 = tf.Variable(tf.zeros([50]), name='h1_biases') h1 = tf.matmul(x, w1) + b1 w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights') b2 = tf.Variable(tf.zeros([10]), name='2_biases') logits = tf.matmul(h1, w2) + b2 return logits 134 Variable sharing: The problem def two_hidden_layers(x): w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights') b1 = tf.Variable(tf.zeros([50]), name='h1_biases') h1 = tf.matmul(x, w1) + b1 w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights') b2 = tf.Variable(tf.zeros([10]), name='2_biases') logits = tf.matmul(h1, w2) + b2 return logits logits1 = two_hidden_layers(x1) logits2 = two_hidden_layers(x2) What will happen if we make these two calls? 135 Sharing Variable: The problem Two sets of variables are created. You want all your inputs to use the same weights and biases! 136 tf.get_variable() tf.get_variable(<name>, <shape>, <initializer>) If a variable with <name> already exists, reuse it If not, initialize it with <shape> using <initializer> 137 tf.get_variable() def two_hidden_layers(x): assert x.shape.as_list() == [200, 100] w1 = tf.get_variable(""h1_weights"", [100, 50], initializer=tf.random_normal_initializer()) b1 = tf.get_variable(""h1_biases"", [50], initializer=tf.constant_initializer(0.0)) h1 = tf.matmul(x, w1) + b1 assert h1.shape.as_list() == [200, 50] w2 = tf.get_variable(""h2_weights"", [50, 10], initializer=tf.random_normal_initializer()) b2 = tf.get_variable(""h2_biases"", [10], initializer=tf.constant_initializer(0.0)) logits = tf.matmul(h1, w2) + b2 return logits logits1 = two_hidden_layers(x1) logits2 = two_hidden_layers(x2) 138 def two_hidden_layers(x): assert x.shape.as_list() == [200, 100] w1 = tf.get_variable(""h1_weights"", [100, 50], initializer=tf.random_normal_initializer()) b1 = tf.get_variable(""h1_biases"", [50], initializer=tf.constant_initializer(0.0)) h1 = tf.matmul(x, w1) + b1 assert h1.shape.as_list() == [200, 50] w2 = tf.get_variable(""h2_weights"", [50, 10], initializer=tf.random_normal_initializer()) b2 = tf.get_variable(""h2_biases"", [10], initializer=tf.constant_initializer(0.0)) logits = tf.matmul(h1, w2) + b2 return logits logits1 = two_hidden_layers(x1) logits2 = two_hidden_layers(x2) ValueError: Variable h1_weights already exists, disallowed. Did you mean to set reuse=True in VarScope? tf.get_variable() 139 def two_hidden_layers(x): assert x.shape.as_list() == [200, 100] w1 = tf.get_variable(""h1_weights"", [100, 50], initializer=tf.random_normal_initializer()) b1 = tf.get_variable(""h1_biases"", [50], initializer=tf.constant_initializer(0.0)) h1 = tf.matmul(x, w1) + b1 assert h1.shape.as_list() == [200, 50] w2 = tf.get_variable(""h2_weights"", [50, 10], initializer=tf.random_normal_initializer()) b2 = tf.get_variable(""h2_biases"", [10], initializer=tf.constant_initializer(0.0)) logits = tf.matmul(h1, w2) + b2 return logits with tf.variable_scope('two_layers') as scope: logits1 = two_hidden_layers(x1) scope.reuse_variables() logits2 = two_hidden_layers(x2) Put your variables within a scope and reuse all variables within that scope tf.variable_scope() 140 tf.variable_scope() Only one set of variables, all within the variable scope “two_layers” They take in two different inputs 141 tf.variable_scope() tf.variable_scope implicitly creates a name scope 142 def two_hidden_layers(x): assert x.shape.as_list() == [200, 100] w1 = tf.get_variable(""h1_weights"", [100, 50], initializer=tf.random_normal_initializer()) b1 = tf.get_variable(""h1_biases"", [50], initializer=tf.constant_initializer(0.0)) h1 = tf.matmul(x, w1) + b1 assert h1.shape.as_list() == [200, 50] w2 = tf.get_variable(""h2_weights"", [50, 10], initializer=tf.random_normal_initializer()) b2 = tf.get_variable(""h2_biases"", [10], initializer=tf.constant_initializer(0.0)) logits = tf.matmul(h1, w2) + b2 return logits with tf.variable_scope('two_layers') as scope: logits1 = two_hidden_layers(x1) scope.reuse_variables() logits2 = two_hidden_layers(x2) Reusable code? 143 def fully_connected(x, output_dim, scope): with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope: w = tf.get_variable(""weights"", [x.shape[1], output_dim], initializer=tf.random_normal_initializer()) b = tf.get_variable(""biases"", [output_dim], initializer=tf.constant_initializer(0.0)) return tf.matmul(x, w) + b def two_hidden_layers(x): h1 = fully_connected(x, 50, 'h1') h2 = fully_connected(h1, 10, 'h2') with tf.variable_scope('two_layers') as scope: logits1 = two_hidden_layers(x1) logits2 = two_hidden_layers(x2) Layer ‘em up Fetch variables if they already exist Else, create them 144 Layer ‘em up Manage Experiments tf.train.Saver saves graph’s variables in binary files 146 tf.train.Saver.save(sess, save_path, global_step=None...) tf.train.Saver.restore(sess, save_path) 147 Saves sessions, not graphs! # define model model = SkipGramModel(params) # create a saver object saver = tf.train.Saver() with tf.Session() as sess: for step in range(training_steps): sess.run([optimizer]) # save model every 1000 steps if (step + 1) % 1000 == 0: saver.save(sess, 'checkpoint_directory/model_name', global_step=step) 148 Save parameters after 1000 steps # define model model = SkipGramModel(params) # create a saver object saver = tf.train.Saver() with tf.Session() as sess: for step in range(training_steps): sess.run([optimizer]) # save model every 1000 steps if (step + 1) % 1000 == 0: saver.save(sess, 'checkpoint_directory/model_name', global_step=step) 149 Specify the step at which the model is saved global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step') 150 Global step Very common in TensorFlow program global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step') optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step) 151 Global step Need to tell optimizer to increment global step This can also help your optimizer know when to decay learning rate 152 Your checkpoints are saved in checkpoint_directory 153 tf.train.Saver Only save variables, not graph Checkpoints map variable names to tensors 154 tf.train.Saver Can also choose to save certain variables v1 = tf.Variable(..., name='v1') v2 = tf.Variable(..., name='v2') You can save your variables in one of three ways: saver = tf.train.Saver({'v1': v1, 'v2': v2}) saver = tf.train.Saver([v1, v2]) saver = tf.train.Saver({v.op.name: v for v in [v1, v2]}) # similar to a dict saver.restore(sess, 'checkpoints/name_of_the_checkpoint') e.g. saver.restore(sess, 'checkpoints/skip-gram-99999') 155 Restore variables Still need to first build graph # check if there is checkpoint ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint')) # check if there is a valid checkpoint path if ckpt and ckpt.model_checkpoint_path: saver.restore(sess, ckpt.model_checkpoint_path) 156 Restore the latest checkpoint 1. checkpoint file keeps track of the latest checkpoint 2. restore checkpoints only when there is a valid checkpoint path tf.summary Why matplotlib when you can summarize? 157 Visualize our summary statistics during our training tf.summary.scalar tf.summary.histogram tf.summary.image 158 tf.summary with tf.name_scope(""summaries""): tf.summary.scalar(""loss"", self.loss) tf.summary.scalar(""accuracy"", self.accuracy) tf.summary.histogram(""histogram loss"", self.loss) summary_op = tf.summary.merge_all() 159 Step 1: create summaries merge them all into one summary op to make managing them easier loss_batch, _, summary = sess.run([loss, optimizer, summary_op]) 160 Step 2: run them Like everything else in TF, summaries are ops. For the summaries to be built, you have to run it in a session writer.add_summary(summary, global_step=step) 161 Step 3: write summaries to file Need global step here so the model knows what summary corresponds to what step tf.summary.scalar(""loss"", self.loss) tf.summary.histogram(""histogram loss"", self.loss) summary_op = tf.summary.merge_all() saver = tf.train.Saver() # defaults to saving all variables with tf.Session() as sess: sess.run(tf.global_variables_initializer()) ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint')) if ckpt and ckpt.model_checkpoint_path: saver.restore(sess, ckpt.model_checkpoint_path) writer = tf.summary.FileWriter('./graphs', sess.graph) for index in range(10000): ... loss_batch, _, summary = sess.run([loss, optimizer, summary_op]) writer.add_summary(summary, global_step=index) if (index + 1) % 1000 == 0: saver.save(sess, 'checkpoints/skip-gram', index) 162 Putting it together See summaries on TensorBoard 163 164 Scalar loss 165 Histogram loss 166 Toggle run to compare experiments Questions? Feedback: chiphuyen@cs.stanford.edu Thanks! 167 "
464,"Natural Language Processing with Deep Learning CS224N/Ling284 Richard Socher Lecture 7: Dependency Parsing Organization Reminders/comments: • Final project discussion – come meet with us • Extra credit for most prolific piazza student answerers • Midterm in two weeks • Practice exams are on the website 1/30/18 2 Lecture Plan 1. Syntactic Structure: Constituency and Dependency 2. Dependency Grammar 3. Transition-based dependency parsing 4. Neural dependency parsing 1/30/18 3 Two views of linguistic structure: Constituency = phrase structure grammar = context-free grammars (CFGs) Phrase structure organizes words into nested constituents. Basic unit: words the, cat, cuddly, by, door Words combine into phrases the cuddly cat, by the door Phrases can combine into bigger phrases the cuddly cat by the door 1/30/18 4 Two views of linguistic structure: Constituency = phrase structure grammar = context-free grammars (CFGs) Phrase structure organizes words into nested constituents. Basic unit: words the, cat, cuddly, by, door Words combine into phrases the cuddly cat, by the door Phrases can combine into bigger phrases the cuddly cat by the door NP -> Det Adj N Det NP -> NP PP N Adj P N PP -> P NP Can represent the grammar with CFG rules 1/30/18 5 Example Constituency Trees • PP attachment ambiguities in constituency structure 1/30/18 6 Two views of linguistic structure: Dependency structure • Dependency structure shows which words depend on (modify or are arguments of) which other words. Look for the large barking dog by the door in a crate 1/30/18 7 • Dependency structure shows which words depend on (modify or are arguments of) which other words. • Determiners, adjectives, and (sometimes) verbs modify nouns Look for the large barking dog by the door in a crate Two views of linguistic structure: Dependency structure 1/30/18 8 Two views of linguistic structure: Dependency structure • Dependency structure shows which words depend on (modify or are arguments of) which other words. • Determiners, adjectives, and (sometimes) verbs modify nouns • We will also treat prepositions as modifying nouns Look for the large barking dog by the door in a crate 1/30/18 9 Two views of linguistic structure: Dependency structure • Dependency structure shows which words depend on (modify or are arguments of) which other words. • Determiners, adjectives, and (sometimes) verbs modify nouns • We will also treat prepositions as modifying nouns • The prepositional phrases are modifying the main noun phrase Look for the large barking dog by the door in a crate 1/30/18 10 Two views of linguistic structure: Dependency structure • Dependency structure shows which words depend on (modify or are arguments of) which other words. • Determiners, adjectives, and (sometimes) verbs modify nouns • We will also treat prepositions as modifying nouns • The prepositional phrases are modifying the main noun phrase • The main noun phrase is an argument of “look” Look for the large barking dog by the door in a crate 1/30/18 11 Ambiguity: PP attachments Scientists study whales from space 1/30/18 12 PP attachment ambiguities in dependency structure Scientists study whales from space Scientists study whales from space 1/30/18 13 Attachment ambiguities • A key parsing decision is how we ‘attach’ various constituents • PPs, adverbial or participial phrases, infinitives, coordinations, etc. 1/30/18 14 Attachment ambiguities • A key parsing decision is how we ‘attach’ various constituents • PPs, adverbial or participial phrases, infinitives, coordinations, etc. • Catalan numbers: Cn = (2n)!/[(n+1)!n!] • An exponentially growing series, which arises in many tree-like contexts • But normally, we assume nesting. 1/30/18 15 The rise of annotated data: Universal Dependencies treebanks [Universal Dependencies: http://universaldependencies.org/ ; cf. Marcus et al. 1993, The Penn Treebank, Computational Linguistics] 1/30/18 16 The rise of annotated data Starting off, building a treebank seems a lot slower and less useful than building a grammar But a treebank gives us many things • Reusability of the labor • Many parsers, part-of-speech taggers, etc. can be built on it • Valuable resource for linguistics • Broad coverage, not just a few intuitions • Frequencies and distributional information • A way to evaluate systems 1/30/18 17 Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies Dependency Grammar and Dependency Structure submitted Bills were Senator by immigration Brownback and on ports Republican of Kansas 1/30/18 18 Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies The arrows are commonly typed with the name of grammatical relations (subject, prepositional object, apposition, etc.) Dependency Grammar and Dependency Structure submitted Bills were Senator by nsubj:pass aux obl case immigration conj Brownback cc and on case nmod ports flat Republican of case nmod Kansas appos 1/30/18 19 Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations (“arrows”) called dependencies The arrow connects a head (governor, superior, regent) with a dependent (modifier, inferior, subordinate) Usually, dependencies form a tree (connected, acyclic, single-head) Dependency Grammar and Dependency Structure submitted Bills were Senator by nsubj:pass aux obl case immigration conj Brownback cc and on case nmod ports flat Republican of case nmod Kansas appos 1/30/18 20 21 Dependency Relations Selected dependency relations from the Universal Dependency set. (de Marneffe et al., 2014) https://web.stanford.edu/~jurafsky/slp3/14.pdf 1/30/18 Pāṇini’s grammar (c. 5th century BCE) 22 Gallery: http://wellcomeimages.org/indexplus/image/L0032691.html CC BY 4.0 File:Birch bark MS from Kashmir of the Rupavatra Welcome L0032691.jpg 1/30/18 Dependency Grammar/Parsing History • The idea of dependency structure goes back a long way • To Pāṇini’s grammar (c. 5th century BCE) • Basic approach of 1st millennium Arabic grammarians • Constituency/context-free grammars is a more recent invention • 20th century (R.S. Wells, 1947) • Modern dependency work often linked to work of L. Tesnière (1959) • Was dominant approach in “East” (Russia, China, …) • Good for free-er word order languages • Among the earliest kinds of parsers in NLP, even in the US: • David Hays, one of the founders of U.S. computational linguistics, built early (first?) dependency parser (Hays 1962) 1/30/18 23 ROOT Discussion of the outstanding issues was completed . Dependency Grammar and Dependency Structure • Some people draw the arrows one way; some the other way! • Tesnière had them point from head to dependent… • Ours will point from head to dependent • Usually add a fake ROOT so every word is a dependent of precisely 1 other node 1/30/18 24 What are the sources of information for dependency parsing? 1. Bilexical affinities [discussion à issues] is plausible 2. Dependency distance mostly with nearby words 3. Intervening material Dependencies rarely span intervening verbs or punctuation 4. Valency of heads How many dependents on which side are usual for a head? Dependency Conditioning Preferences ROOT Discussion of the outstanding issues was completed . 1/30/18 25 Dependency Parsing • A sentence is parsed by choosing for each word what other word (including ROOT) it is a dependent of • i.e., find the right outgoing arrow from each word • Usually some constraints: • Only one word is a dependent of ROOT • Don’t want cycles A → B, B → A • This makes the dependencies a tree • Final issue is whether arrows can cross (non-projective) or not 26 I give a on bootstrapping talk tomorrow ROOT ’ll 1/30/18 Methods of Dependency Parsing 1. Dynamic programming 2. Graph algorithms You create a Minimum Spanning Tree for a sentence McDonald et al.’s (2005) MSTParser scores dependencies independently using an ML classifier (he uses MIRA, for online learning, but it can be something else) 3. Constraint Satisfaction Edges are eliminated that don’t satisfy hard constraints. Karlsson (1990), etc. 4. “Transition-based parsing” or “deterministic dependency parsing” Greedy choice of attachments guided by good machine learning classifiers MaltParser (Nivre et al. 2008). Has proven highly effective. 1/30/18 27 4. Greedy transition-based parsing [Nivre 2003] • A simple form of greedy discriminative dependency parser • The parser does a sequence of bottom up actions • Roughly like “shift” or “reduce” in a shift-reduce parser, but the “reduce” actions are specialized to create dependencies with head on left or right • The parser has: • a stack σ, written with top to the right • which starts with the ROOT symbol • a buffer β, written with top to the left • which starts with the input sentence • a set of dependency arcs A • which starts off empty • a set of actions 1/30/18 28 Basic transition-based dependency parser Start: σ = [ROOT], β = w1, …, wn , A = ∅ 1. Shift σ, wi|β, A è σ|wi, β, A 2. Left-Arcr σ|wi|wj, β, A è σ|wj, β, A∪{r(wj,wi)} 3. Right-Arcr σ|wi|wj, β, A è σ|wi, β, A∪{r(wi,wj)} Finish: σ = [w], β = ∅ 1/30/18 29 Arc-standard transition-based parser (there are other transition schemes …) Analysis of “I ate fish” ate fish [root] Start I [root] Shift I ate fish ate [root] fish Shift I Start: σ = [ROOT], β = w1, …, wn , A = ∅ 1. Shift σ, wi|β, A è σ|wi, β, A 2. Left-Arcr σ|wi|wj, β, A è σ|wj, β, A∪{r(wj,wi)} 3. Right-Arcr σ|wi|wj, β, A è σ|wi, β, A∪{r(wi,wj)} Finish: β = ∅ 1/30/18 30 Arc-standard transition-based parser Analysis of “I ate fish” ate [root] ate [root] Left Arc I A += nsubj(ate →I) ate fish [root] ate fish [root] Shift ate [root] [root] Right Arc A += obj(ate →fish) fish ate ate [root] [root] Right Arc A += root([root] →ate) Finish 1/30/18 31 MaltParser [Nivre and Hall 2005] • How could we choose the next action? • Each action is predicted by a discriminative classifier (eg. SVM or logistic regression classifier) over each legal move • Features: top of stack word, POS; first in buffer word, POS; etc. • There is NO search (in the simplest form) • But you can profitably do a beam search if you wish (slower but better) • It provides VERY fast linear time parsing • The model’s accuracy is only slightly below the best dependency parsers 1/30/18 32 Feature Representation Feature templates: usually a combination of 1 ~ 3 elements from the configuration. Indicator features 0 0 0 1 0 0 1 0 0 0 1 0 binary, sparse dim =106 ~ 107 … 33 Evaluation of Dependency Parsing: (labeled) dependency accuracy ROOT She saw the video lecture 0 1 2 3 4 5 Gold 1 2 She nsubj 2 0 saw root 3 5 the det 4 5 video nn 5 2 lecture obj Parsed 1 2 She nsubj 2 0 saw root 3 4 the det 4 5 video nsubj 5 2 lecture ccomp Acc = # correct deps # of deps UAS = 4 / 5 = 80% LAS = 2 / 5 = 40% 1/30/18 34 Dependency paths identify semantic relations – e.g, for protein interaction [Erkan et al. EMNLP 07, Fundel et al. 2007, etc.] KaiC çnsubj interacts nmod:with è SasA KaiC çnsubj interacts nmod:with è SasA conj:andè KaiA KaiC çnsubj interacts prep_withè SasA conj:andè KaiB demonstrated results KaiC interacts rythmically nsubj The mark det ccomp that nsubj KaiB KaiA SasA conj:and conj:and advmod nmod:with with and cc case 1/30/18 35 • Dependencies parallel to a CFG tree must be projective • There must not be any crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words. • But dependency theory normally does allow non-projective structures to account for displaced constituents • You can’t easily get the semantics of certain constructions right without these nonprojective dependencies Who did Bill buy the coffee from yesterday ? Projectivity 1/30/18 36 Handling non-projectivity • The arc-standard algorithm we presented only builds projective dependency trees • Possible directions: 1. Just declare defeat on nonprojective arcs 2. Use a dependency formalism which only admits projective representations (a CFG doesn’t represent such structures…) 3. Use a postprocessor to a projective dependency parsing algorithm to identify and resolve nonprojective links 4. Add extra transitions that can model at least most non-projective structures (e.g., add an extra SWAP transition, cf. bubble sort) 5. Move to a parsing mechanism that does not use or require any constraints on projectivity (e.g., the graph-based MSTParser) 1/30/18 37 Why train a neural dependency parser? Indicator Features Revisited • Problem #1: sparse • Problem #2: incomplete • Problem #3: expensive computation More than 95% of parsing time is consumed by feature computation. Our Approach: learn a dense and compact feature representation 0.1 dense dim = ~1000 0.9-0.2 0.3 -0.1-0.5 … 38 A neural dependency parser [Chen and Manning 2014] • English parsing to Stanford Dependencies: • Unlabeled attachment score (UAS) = head • Labeled attachment score (LAS) = head and label Parser UAS LAS sent. / s MaltParser 89.8 87.2 469 MSTParser 91.4 88.1 10 TurboParser 92.3* 89.6* 8 C & M 2014 92.0 89.7 654 1/30/18 39 • We represent each word as a d-dimensional dense vector (i.e., word embedding) • Similar words are expected to have close vectors. • Meanwhile, part-of-speech tags (POS) and dependency labels are also represented as d-dimensional vectors. • The smaller discrete sets also exhibit many similarities. Distributed Representations come go were was is good NNS (plural noun) should be close to NN (singular noun). num (numerical modifier) should be close to amod (adjective modifier). 40 Extracting Tokens and then vector representations from configuration s1 s2 b1 lc(s1) rc(s1) lc(s2) rc(s2) good has control ∅ ∅ He ∅ JJ VBZ NN ∅ ∅ PRP ∅ ∅ ∅ ∅ ∅ ∅ nsubj ∅ + + word POS dep. • We extract a set of tokens based on the stack / buffer positions: • We convert them to vector embeddings and concatenate them 41 Model Architecture Input layer x lookup + concat Hidden layer h h = ReLU(Wx + b1) Output layer y y = softmax(Uh + b2) Softmax probabilities cross-entropy error will be back-propagated to the embeddings. 42 Non-linearities between layers: Why they’re needed • For logistic regression: map to probabilities • Here: function approximation, e.g., for regression or classification • Without non-linearities, deep neural networks can’t do anything more than a linear transform • Extra layers could just be compiled down into a single linear transform • People use various non-linearities 43 1/30/18 Non-linearities: sigmoid and tanh logistic (“sigmoid”) tanh tanh is just a rescaled and shifted sigmoid tanh is often used and often performs better for deep nets • It’s output is symmetric around 0 tanh(z) = 2logistic(2z)−1 44 1/30/18 Non-linearities: hard tanh • Faster to compute than tanh (no exps or division) • But suffers from “dead neurons” • If our model is initialized such that a neuron is always 1, it will never change! • “Saturated neurons” can also be a problem for regular tanh – initializing NNs right is really important! 45 1/30/18 Non-linearities: ReLU rect(z) = max(z,0) 46 • Also fast to compute, but also can cause dead neurons • Mega common: “go-to” activation function • Transfers a linear activation when active • Lots of variants: LReLU, SELU, ELU, PReLU… 1/30/18 Dependency parsing for sentence structure Neural networks can accurately determine the structure of sentences, supporting interpretation Chen and Manning (2014) was the first simple, successful neural dependency parser The dense representations let it outperform other greedy parsers in both accuracy and speed 1/30/18 Lecture 1, Slide 47 Further developments in transition-based neural dependency parsing This work was further developed and improved by others, including in particular at Google • Bigger, deeper networks with better tuned hyperparameters • Beam search • Global, conditional random field (CRF)-style inference over the decision sequence Leading to SyntaxNet and the Parsey McParseFace model https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html Method UAS LAS (PTB WSJ SD 3.3 Chen & Manning 2014 92.0 89.7 Weiss et al. 2015 93.99 92.05 Andor et al. 2016 94.61 92.79 1/30/18 Lecture 1, Slide 48 Graph-based dependency parsers • Compute a score for every possible dependency • Then add an edge from each word to its highest-scoring candidate head ROOT The big cat sat 0.5 0.3 0.8 2.0 e.g., picking the head for “big” 1/30/18 Lecture 1, Slide 49 Graph-based dependency parsers • Compute a score for every possible dependency • Then add an edge from each word to its highest-scoring candidate head ROOT The big cat sat 0.5 0.3 0.8 2.0 e.g., picking the head for “big” 1/30/18 Lecture 1, Slide 50 Neural graph-based dependency parsers • Compute a score for every possible dependency • Then add an edge from each word to its highest-scoring candidate head • Really great results! • But slower than transition-based parsers: there are n^2 possible dependencies in a sentence of length n. Method UAS LAS (PTB WSJ SD 3.3 Chen & Manning 2014 92.0 89.7 Weiss et al. 2015 93.99 92.05 Andor et al. 2016 94.61 92.79 Dozat & Manning 2017 95.74 93.08 1/30/18 Lecture 1, Slide 51 "
465,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 8: Recurrent Neural Networks and Language Models Abigail See Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Assignment 1: Grades will be released after class • Assignment 2: Coding session next week on Monday; details on Piazza • Midterm logistics: Fill out form on Piazza if you can’t do main midterm, have special requirements, or other special case 2/1/18 2 Announcements • Default Final Project (PA4) release late tonight • Read the handout, look at the code, decide which project you want to do • You may not understand all the technical parts, but you’ll get an overview • You don’t yet have the Azure resources you need to run the code • Project proposal due next week (Thurs Feb 8) • Details released later today • Everyone submits their teams • Custom final project teams also describe their project 2/1/18 3 2/1/18 4 Call for participation Overview Today we will: • Introduce a new NLP task • Language Modeling • Introduce a new family of neural networks • Recurrent Neural Networks (RNNs) THE most important idea for the rest of the class! motivates 2/1/18 5 • Language Modeling is the task of predicting what word comes next. the students opened their ______ • More formally: given a sequence of words , compute the probability distribution of the next word : where is a word in the vocabulary • A system that does this is called a Language Model. Language Modeling exams minds laptops books 2/1/18 6 You use Language Models every day! 2/1/18 7 You use Language Models every day! 2/1/18 8 n-gram Language Models the students opened their ______ • Question: How to learn a Language Model? • Answer (pre- Deep Learning): learn a n-gram Language Model! • Definition: A n-gram is a chunk of n consecutive words. • unigrams: “the”, “students”, “opened”, ”their” • bigrams: “the students”, “students opened”, “opened their” • trigrams: “the students opened”, “students opened their” • 4-grams: “the students opened their” • Idea: Collect statistics about how frequent different n-grams are, and use these to predict next word. 2/1/18 9 n-gram Language Models • First we make a simplifying assumption: depends only on the preceding (n-1) words (statistical approximation) (definition of conditional prob) (assumption) n-1 words prob of a n-gram prob of a (n-1)-gram • Question: How do we get these n-gram and (n-1)-gram probabilities? • Answer: By counting them in some large corpus of text! 2/1/18 10 n-gram Language Models: Example Suppose we are learning a 4-gram Language Model. as the proctor started the clock, the students opened their _____ discard condition on this In the corpus: • “students opened their” occurred 1000 times • “students opened their books” occurred 400 times • à P(books | students opened their) = 0.4 • “students opened their exams” occurred 100 times • à P(exams | students opened their) = 0.1 Should we have discarded the “proctor” context? 2/1/18 11 Problems with n-gram Language Models Note: Increasing n makes sparsity problems worse. Typically we can’t have n bigger than 5. Problem: What if “students opened their” never occurred in data? Then we can’t calculate probability for any ! Sparsity Problem 2 Problem: What if “students opened their ” never occurred in data? Then has probability 0! Sparsity Problem 1 (Partial) Solution: Add small 𝛿 to count for every . This is called smoothing. (Partial) Solution: Just condition on “opened their” instead. This is called backoff. 2/1/18 12 Problems with n-gram Language Models 2/1/18 13 Storage: Need to store count for all possible n-grams. So model size is O(exp(n)). Increasing n makes model size huge! n-gram Language Models in practice • You can build a simple trigram Language Model over a 1.7 million word corpus (Reuters) in a few seconds on your laptop* today the _______ * Try for yourself: https://nlpforhackers.io/language-models/ Otherwise, seems reasonable! company 0.153 bank 0.153 price 0.077 italian 0.039 emirate 0.039 … get probability distribution Sparsity problem: not much granularity in the probability distribution Business and financial news 2/1/18 14 Generating text with a n-gram Language Model • You can also use a Language Model to generate text. today the _______ condition on this company 0.153 bank 0.153 price 0.077 italian 0.039 emirate 0.039 … get probability distribution sample 2/1/18 15 Generating text with a n-gram Language Model • You can also use a Language Model to generate text. today the price _______ condition on this of 0.308 for 0.050 it 0.046 to 0.046 is 0.031 … get probability distribution sample 2/1/18 16 Generating text with a n-gram Language Model • You can also use a Language Model to generate text. today the price of _______ condition on this the 0.072 18 0.043 oil 0.043 its 0.036 gold 0.018 … get probability distribution sample 2/1/18 17 Generating text with a n-gram Language Model • You can also use a Language Model to generate text. today the price of gold _______ 2/1/18 18 Generating text with a n-gram Language Model • You can also use a Language Model to generate text. today the price of gold per ton , while production of shoe lasts and shoe industry , the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks , sept 30 end primary 76 cts a share . Incoherent! We need to consider more than 3 words at a time if we want to generate good text. But increasing n worsens sparsity problem, and exponentially increases model size… 2/1/18 19 How to build a neural Language Model? • Recall the Language Modeling task: • Input: sequence of words • Output: prob dist of the next word • How about a window-based neural model? • We saw this applied to Named Entity Recognition in Lecture 4 2/1/18 20 A fixed-window neural Language Model the students opened their as the proctor started the clock ______ discard fixed window 2/1/18 21 A fixed-window neural Language Model the students opened their books laptops concatenated word embeddings words / one-hot vectors hidden layer a zoo output distribution 2/1/18 22 A fixed-window neural Language Model the students opened their books laptops a zoo Improvements over n-gram LM: • No sparsity problem • Model size is O(n) not O(exp(n)) Remaining problems: • Fixed window is too small • Enlarging window enlarges • Window can never be large enough! • Each uses different rows of . We don’t share weights across the window. We need a neural architecture that can process any length input 2/1/18 23 Recurrent Neural Networks (RNN) hidden states input sequence (any length) … … … Core idea: Apply the same weights repeatedly A family of neural architectures 2/1/18 24 outputs (optional) A RNN Language Model the students opened their words / one-hot vectors books laptops word embeddings a zoo output distribution Note: this input sequence could be much longer, but this slide doesn’t have space! hidden states is the initial hidden state 2/1/18 25 A RNN Language Model the students opened their books laptops a zoo RNN Advantages: • Can process any length input • Model size doesn’t increase for longer input • Computation for step t can (in theory) use information from many steps back • Weights are shared across timesteps à representations are shared RNN Disadvantages: • Recurrent computation is slow • In practice, difficult to access information from many steps back More on these next week 2/1/18 26 Training a RNN Language Model • Get a big corpus of text which is a sequence of words • Feed into RNN-LM; compute output distribution for every step t. • i.e. predict probability dist of every word, given words so far • Loss function on step t is usual cross-entropy between our predicted probability distribution , and the true next word : • Average this to get overall loss for entire training set: 2/1/18 27 Training a RNN Language Model = negative log prob of “students” the students opened their … exams Corpus Loss … 2/1/18 28 Training a RNN Language Model = negative log prob of “opened” Corpus the students opened their … exams Loss … 2/1/18 29 Training a RNN Language Model = negative log prob of “their” Corpus the students opened their … exams Loss … 2/1/18 30 Training a RNN Language Model = negative log prob of “exams” Corpus the students opened their … exams Loss … 2/1/18 31 Training a RNN Language Model + + + + … = Corpus the students opened their … exams Loss … 2/1/18 32 Training a RNN Language Model • However: Computing loss and gradients across entire corpus is too expensive! • Recall: Stochastic Gradient Descent allows us to compute loss and gradients for small chunk of data, and update. • à In practice, consider as a sentence • Compute loss for a sentence (actually usually a batch of sentences), compute gradients and update weights. Repeat. 2/1/18 33 Backpropagation for RNNs … … Question: What’s the derivative of w.r.t. the repeated weight matrix ? Answer: “The gradient w.r.t. a repeated weight is the sum of the gradient w.r.t. each time it appears” 2/1/18 34 Why? Multivariable Chain Rule 2/1/18 35 Source: https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version Backpropagation for RNNs: Proof sketch 2/1/18 36 Source: https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version … In our example: Apply the multivariable chain rule: = 1 Backpropagation for RNNs … … Question: How do we calculate this? Answer: Backpropagate over timesteps i=t,…,0, summing gradients as you go. This algorithm is called “backpropagation through time” 2/1/18 37 Generating text with a RNN Language Model Just like a n-gram Language Model, you can use a RNN Language Model to generate text by repeated sampling. Sampled output is next step’s input. my favorite season is … sample favorite sample season sample is sample spring spring 2/1/18 38 Generating text with a RNN Language Model • Let’s have some fun! • You can train a RNN-LM on any kind of text, then generate text in that style. • RNN-LM trained on Obama speeches: Source: https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0 2/1/18 39 Generating text with a RNN Language Model • Let’s have some fun! • You can train a RNN-LM on any kind of text, then generate text in that style. • RNN-LM trained on Harry Potter: Source: https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6 2/1/18 40 Generating text with a RNN Language Model • Let’s have some fun! • You can train a RNN-LM on any kind of text, then generate text in that style. • RNN-LM trained on Seinfeld scripts: Source: https://www.avclub.com/a-bunch-of-comedy-writers-teamed-up-with-a-computer-to-1818633242 2/1/18 41 Generating text with a RNN Language Model • Let’s have some fun! • You can train a RNN-LM on any kind of text, then generate text in that style. • (character-level) RNN-LM trained on paint colors: Source: http://aiweirdness.com/post/160776374467/new-paint-colors-invented-by-neural-network 2/1/18 42 Evaluating Language Models • The traditional evaluation metric for Language Models is perplexity. • Lower is better! • In Assignment 2 you will show that minimizing perplexity and minimizing the loss function are equivalent. Inverse probability of dataset Normalized by number of words 2/1/18 43 RNNs have greatly improved perplexity n-gram model Increasingly complex RNNs Perplexity improves (lower is better) Source: https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words/ 2/1/18 44 Why should we care about Language Modeling? • Language Modeling is a subcomponent of other NLP systems: • Speech recognition • Use a LM to generate transcription, conditioned on audio • Machine Translation • Use a LM to generate translation, conditioned on original text • Summarization • Use a LM to generate summary, conditioned on original text • Language Modeling is a benchmark task that helps us measure our progress on understanding language These systems are called conditional Language Models 2/1/18 45 Recap • Language Model: A system that predicts the next word • Recurrent Neural Network: A family of neural networks that: • Take sequential input of any length • Apply the same weights on each step • Can optionally produce output on each step • Recurrent Neural Network ≠Language Model • We’ve shown that RNNs are a great way to build a LM. • But RNNs are useful for much more! 2/1/18 46 RNNs can be used for tagging e.g. part-of-speech tagging, named entity recognition knocked over the vase the startled cat VBN IN DT NN DT VBN NN 2/1/18 47 RNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding? e.g. sentiment classification 2/1/18 48 RNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding? Basic way: Use final hidden state e.g. sentiment classification 2/1/18 49 RNNs can be used for sentence classification the movie a lot overall I enjoyed positive Sentence encoding How to compute sentence encoding? Usually better: Take element-wise max or mean of all hidden states e.g. sentiment classification 2/1/18 50 RNNs can be used to generate text e.g. speech recognition, machine translation, summarization what’s the weather the what’s <START> Remember: these are called “conditional language models”. We’ll see Machine Translation in much more detail later. 2/1/18 51 RNNs can be used as an encoder module e.g. question answering, machine translation Question encoding = element-wise max of hidden states Context: Ludwig van Beethoven was a German composer and pianist. A crucial figure … Beethoven ? what nationality was Question: Here the RNN acts as an encoder for the Question. The encoder is part of a larger neural system. Answer: German 2/1/18 52 A note on terminology By the end of the course: You will understand phrases like “stacked bidirectional LSTM with residual connections and self-attention” RNN described in this lecture = “vanilla RNN” Next lecture: You will learn about other RNN flavors like GRU and LSTM 2/1/18 53 and multi-layer RNNs Next time • Problems with RNNs! • Vanishing gradients • Fancy RNN variants! • LSTM • GRU • multi-layer • bidirectional motivates 2/1/18 54 "
466,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 9: Vanishing Gradients and Fancy RNNs (LSTMs and GRUs) Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Assignment 2: due Thursday • Project proposal: due Thursday • Midterm logistics: Fill out form on Piazza if you can’t do main midterm, have special requirements, or other special case • Alternate midterm is this Friday! • Practice midterms are on the website • Midterm review session: in-class this Thursday • Poster session time and location: • 5:30-8:30pm at McCaw Hall at the Alumni Center • Note time has changed 2/6/18 2 This lecture • Vanishing Gradient problem • Fancy RNNs: •GRU •LSTM (!) •Bidirectional •Multi-layer 2/6/18 3 RNN Refresher 2/6/18 4 the students opened their … exams … • Multiply the same matrix at each time step during forward prop Simplify inputs to just x (usually word vectors) 2/6/18 5 … • Ideally inputs from many time steps ago can modify output y • Take for an example RNN with 2 time steps! Insightful! The vanishing/exploding gradient problem • Multiply the same matrix at each time step during backprop 2/6/18 6 … The vanishing gradient problem - Details • Similar but simpler RNN formulation: • Total error is the sum of each error (aka cost function, aka J in previous lectures when it was cross entropy error, could be other cost functions too), but at time steps t • Hardcore chain rule application: 2/6/18 7 The vanishing gradient problem - Details • Similar to backprop but less efficient formulation • Useful for analysis we’ll look at: • Remember: • More chain rule, remember: • Each partial is a Jacobian: 2/6/18 8 We’ll show this can quickly become very small or very large The vanishing gradient problem - Details • Analyzing the norms of the Jacobians yields: • Where we defined 𝛽‘s as upper bounds of the norms • The gradient is a product of Jacobian matrices, each associated with a step in the forward computation. • This can become very small or very large quickly [Bengio et al 1994], and the locality assumption of gradient descent breaks down. à Vanishing or exploding gradient 2/6/18 9 Why is the vanishing gradient a problem? • Ideally, the error Et on step t can flow backwards, via backprop, and allow the weights on a previous timestep (maybe many timesteps ago) to change. 2/6/18 10 … 2018-02-06 11 1. Gradients can be seen as a measure of influence of the past on the future 2. How does the perturbation at time t affect predictions at t+n? Why is the vanishing gradient a problem? 2018-02-06 12 When we only observe going to 0 We cannot tell whether 1. No dependency between t and t+n in data, or 2. Wrong configuration of parameters Why is the vanishing gradient a problem? The vanishing gradient problem for language models • The vanishing gradient problem can cause problems for RNN Language Models: • When predicting the next word, information from many time steps in the past is not taken into consideration. • Example: Jane walked into the room. John walked in too. It was late in the day. Jane said hi to ____ 2/6/18 13 2/6/18 14 Trick for exploding gradient: clipping trick • The solution first introduced by Mikolov is to clip gradients so that their norm has some maximum value. • Makes a big difference in RNNs and many other unstable models ions of this basic ress explicitly the essian-Free opti- damping, a spe- n. This approach nishing gradient, ll missing. Pre- e in high dimen- ity for long term t term ones. This hese components ot guarantee that section 2.3, this ploding gradient nhancement that mall, when the pa- ∆✓. This asks for mall norm, hence adients problem. recurrent neural at while the cur- me with the gradi- te and hence not ng gradient. and Jaeger, 2009) nents element wise (clipping an entry when it exceeds in absolute value a ﬁxed threshold). Clipping has been shown to do well in practice and it forms the backbone of our approach. 3.2. Scaling down the gradients As suggested in section 2.3, one simple mechanism to deal with a sudden increase in the norm of the gradi- ents is to rescale them whenever they go over a thresh- old (see algorithm 1). Algorithm 1 Pseudo-code for norm clipping the gra- dients whenever they explode ˆ g @E @✓ if kˆ gk ≥threshold then ˆ g threshold kˆ gk ˆ g end if This algorithm is very similar to the one proposed by Tomas Mikolov and we only diverged from the original proposal in an attempt to provide a better theoretical foundation (ensuring that we always move in a de- scent direction with respect to the current mini-batch), though in practice both variants behave similarly. The proposed clipping is simple to implement and computationally eﬃcient, but it does however in- troduce an additional hyper-parameter, namely the th h ld O d h i ti f tti thi th h 2/6/18 15 Gradient clipping intuition • Error surface of a single hidden unit RNN, • High curvature walls • Solid lines: standard gradient descent trajectories • Dashed lines gradients rescaled to fixed size On the diﬃculty of training Recurrent Neural Netw Figure 6. We plot the error surface of a single hidden unit recurrent network, highlighting the existence of high cur- vature walls. The solid lines depicts standard trajectories that gradient descent might follow. Using dashed arrow the diagram shows what would happen if the gradients is rescaled to a ﬁxed size when its norm is above a threshold. explode so does the curvature along v leading to a rithm should work even gradient is not the sam (a case for which a se as the ratio between th still explode). Our hypothesis could cent success of the He to other second order m ferences between Hessi order algorithms. First and hence can deal wit not necessarily axis-al new estimate of the H date step and can take curvature (such as the sis) while most other a sumption, i.e., averagin steps. 3. Dealing with t Figure from paper: On the difficulty of training Recurrent Neural Networks, Pascanu et al. 2013 2/6/18 16 One solution: Initialization + ReLus! • You can improve the Vanishing Gradient Problem with good initialization and ReLUs. • Initialize W(*)‘s to identity matrix I and f(z) = • à Huge difference! • Initialization idea first introduced in Parsing with Compositional Vector Grammars, Socher et al. 2013 • New experiments with recurrent neural nets in A Simple Way to Initialize Recurrent Networks of Rectified Linear Units, Le et al. 2015 gits and repeated the experiments. l networks have 100 recurrent hidden units. We stop the optimization after it converges or when eaches 1,000,000 iterations and report the results in ﬁgure 3 (best hyperparameters are listed in le 2). 0 1 2 3 4 5 6 7 8 9 10 x 10 5 0 10 20 30 40 50 60 70 80 90 100 Steps Test Accuracy Pixel−by−pixel MNIST LSTM RNN + Tanh RNN + ReLUs IRNN 0 1 2 3 4 5 6 7 8 9 10 x 10 5 0 10 20 30 40 50 60 70 80 90 100 Steps Test Accuracy Pixel−by−pixel permuted MNIST LSTM RNN + Tanh RNN + ReLUs IRNN gure 3: The results of recurrent methods on the “pixel-by-pixel MNIST” problem. We report the t set accuracy for all methods. Left: normal MNIST. Right: permuted MNIST. Problem LSTM RNN + Tanh RNN + ReLUs IRNN MNIST lr = 0.01, gc = 1 lr = 10−8, gc = 10 lr = 10−8, gc = 10 lr = 10−8, gc = 1 fb = 1.0 8 6 9 rect(z) = max(z,0) 2/6/18 17 Main solution for better RNNs: Better Units • The main solution to the Vanishing Gradient Problem is to use a more complex hidden unit computation in recurrence! • Gated Recurrent Units (GRU) introduced by [Cho et al. 2014] and LSTMs [Hochreiter & Schmidhuber, 1999] • Main ideas: • keep around memories to capture long distance dependencies • allow error messages to flow at different strengths depending on the inputs 2/6/18 18 GRUs • Standard RNN computes hidden layer at next time step directly: • #Simplified from last lecture’s: • GRU first computes an update gate (another layer) based on current input word vector and hidden state • Compute reset gate similarly but with different weights 2/6/18 19 GRUs • Update gate • Reset gate • New memory content: If reset gate unit is ~0, then this ignores previous memory and only stores the new word information • Final memory at time step combines current and previous time steps: 2/6/18 20 GRU illustration rt rt-1 zt-1 ~ ht ~ ht-1 zt ht-1 ht xt xt-1 Input: Reset gate Update gate Memory (reset) Final memory 2/6/18 21 GRU intuition • If reset is close to 0, ignore previous hidden state à Allows model to drop information that is irrelevant in the future • Update gate z controls how much of past state should matter now. • If z close to 1, then we can copy information in that unit through many time steps! Less vanishing gradient! • Units with short-term dependencies often have reset gates very active 2/6/18 22 GRU intuition • Units with long term dependencies have active update gates z • Illustration: • Derivative of ? à rest is same chain rule, but implement with modularization or automatic differentiation odel parameters and sequence, output se- ng set. In our case, starting from the in- use a gradient-based del parameters. ecoder is trained, the ys. One way is to use et sequence given an hand, the model can r of input and output simply a probability 4). ptively Remembers       Figure 2: An illustration of the proposed hidden activation function. The update gate z selects whether the hidden state is to be updated with a new hidden state ˜ h. The reset gate r decides whether the previous hidden state is ignored. See Eqs. (5)–(8) for the detailed equations of r, z, h and ˜ h. only. This effectively allows the hidden state to drop any information that is found to be irrelevant 2/6/18 23 2018-02-06 24 • Is the problem with standard RNNs the naïve transition function? • It implies that the error must backpropagate through all the intermediate nodes: • Perhaps we can create shortcut connections. How do Gated Recurrent Units fix vanishing gradient problems? • Perhaps we can create adaptive shortcut connections. • Let the net prune unnecessary connections adaptively. • That’s what the gates do. 2018-02-06 25 How do Gated Recurrent Units fix vanishing gradient problems? 2018-02-06 26 Execution Registers 1. Read the whole register h 2. Update the whole register h GRU Comparison to Standard tanh-RNN Vanilla RNN … 2018-02-06 27 GRU … Execution Registers 1. Select a readable subset h 2. Read the subset 3. Select a writable subset 4. Update the subset Gated recurrent units are much more versatile and adaptive in which elements of the hidden vector h they update! GRU Comparison to Standard tanh-RNN Long-short-term-memories (LSTMs) • LSTM is even more complex than GRU • Allow each time step to modify • Input gate (current cell matters) • Forget (gate 0, forget past) • Output (how much cell is exposed) • New memory cell • Final memory cell: • Final hidden state: 2/6/18 28 Some visualizations By Chris Olah: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 2/6/18 29 Most illustrations a bit overwhelming ;) http://people.idsia.ch/~juergen/lstm/sld017.htm http://deeplearning.net/tutorial/lstm.html Intuition: memory cells can keep information intact, unless inputs makes them forget it or overwrite it with new input. Cell can decide to output this information or just store it Long Short-Term Memory by Hochreiter and Schmidhuber (1997) inj inj out j out j w i c j wic j y c j g h 1.0 net w i w i y inj y out j net c j g y inj = g + sc j sc j y inj h y out j net 2/6/18 30 Another LSTM visualization inspired by code 31 Picture courtesy of Tim Rocktäschel The LSTM 32 The LSTM gates all operations so stuff can be forgotten/ignored rather than it all being crammed on top of everything else The LSTM 33 The non-linear update for the next time step is just like an RNN The LSTM 34 This part is the the secret! (Of other recent things like ResNets too!) Rather than multiplying, we get ct by adding the non-linear stuff and ct−1 ! There is a direct, linear connection between ct and ct−1. LSTM visualization after training for character language modeling (predict the next character) 2/6/18 35 From: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Visualizing activation of tanh(ct): LSTM visualization after training for character language modeling (predict the next character) 2/6/18 36 From: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ LSTMs are a great default for all sequence problems • Very powerful, especially when stacked and made even deeper (each hidden layer is already computed by a deep internal network) • Most useful if you have lots and lots of data 2/6/18 37 Deep LSTMs compared to traditional systems 2015 Method test BLEU score (ntst14) Bahdanau et al. [2] 28.45 Baseline System [29] 33.30 Single forward LSTM, beam size 12 26.17 Single reversed LSTM, beam size 12 30.59 Ensemble of 5 reversed LSTMs, beam size 1 33.00 Ensemble of 2 reversed LSTMs, beam size 12 33.27 Ensemble of 5 reversed LSTMs, beam size 2 34.50 Ensemble of 5 reversed LSTMs, beam size 12 34.81 Table 1: The performance of the LSTM on WMT’14 English to French test set (ntst14). Note that an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of size 12. Method test BLEU score (ntst14) Baseline System [29] 33.30 Cho et al. [5] 34.54 Best WMT’14 result [9] 37.0 Rescoring the baseline 1000-best with a single forward LSTM 35.61 Rescoring the baseline 1000-best with a single reversed LSTM 35.85 Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5 Oracle Rescoring of the Baseline 1000-best lists ∼45 Table 2: Methods that use neural networks together with an SMT system on the WMT’14 English to French test set (ntst14). Sequence to Sequence Learning by Sutskever et al. 2014 2/6/18 38 Deep LSTMs (with a lot more tweaks) WMT 2016 competition results 2/6/18 39 Deep LSTM for Machine Translation e were surprised to discover that the LSTM did well on long sentences, which is shown quantita- vely in ﬁgure 3. Table 3 presents several examples of long sentences and their translations. 8 Model Analysis −8 −6 −4 −2 0 2 4 6 8 10 −6 −5 −4 −3 −2 −1 0 1 2 3 4 John respects Mary Mary respects John John admires Mary Mary admires John Mary is in love with John John is in love with Mary −15 −10 −5 0 5 10 15 20 −20 −15 −10 −5 0 5 10 15 I gave her a card in the garden In the garden , I gave her a card She was given a card by me in the garden She gave me a card in the garden In the garden , she gave me a card I was given a card by her in the garden gure 2: The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained er processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples is marily a function of word order, which would be difﬁcult to capture with a bag-of-words model. Notice tha th clusters have similar internal structure. ne of the attractive features of our model is its ability to turn a sequence of words into a vector ﬁxed dimensionality. Figure 2 visualizes some of the learned representations. The ﬁgure clearly Sequence to Sequence Learning by Sutskever et al. 2014 PCA of vectors from last time step hidden layer 2/6/18 40 Bidirectional RNNs Problem: For classification you want to incorporate information from words both preceding and following Ideas? Bidirectionality h ! t = f (W !"" ! xt +V ! "" h ! t−1 + b ! ) h ! t = f (W !"" "" xt +V ! "" h ! t+1 + b ! ) yt = g(U[h ! t;h ! t]+c) y h x now represents (summarizes) the past and future around a single token. h =[h ! ;h ! ] 2/6/18 41 Deep Bidirectional RNNs Going Deep h !(i) t = f (W !"" ! (i)ht (i−1) +V ! ""(i)h !(i) t−1 + b !(i)) h ! (i) t = f (W !"" "" (i)ht (i−1) +V ! ""(i)h ! (i) t+1 + b !(i)) yt = g(U[h ! t (L);h ! t (L)]+c) y h(3) x Each memory layer passes an intermediate sequential representation to the next. h(2) h(1) 2/6/18 42 Next up! 2/6/18 43 Midterm review! Gated Recurrent Unit [Cho et al., EMNLP2014; Chung, Gulcehre, Cho, Bengio, DLUFL2014] Long Short-Term Memory [Hochreiter & Schmidhuber, NC1999; Gers, Thesis2001] 44 Gated Recurrent Units Comparison (different notation) ht = ut ⊙˜ ht + (1 −ut) ⊙ht−1 ˜ h = tanh(W [xt] + U(rt ⊙ht−1) + b) ut = σ(Wu [xt] + Uuht−1 + bu) rt = σ(Wr [xt] + Urht−1 + br) ht = ot ⊙tanh(ct) ct = ft ⊙ct−1 + it ⊙˜ ct ˜ ct = tanh(Wc [xt] + Ucht−1 + bc) ot = σ(Wo [xt] + Uoht−1 + bo) it = σ(Wi [xt] + Uiht−1 + bi) ft = σ(Wf [xt] + Ufht−1 + bf) Two most widely used gated recurrent units ˜ ht = tanh(W [xt] + U(rt ⊙ht−1) + b) Training a (gated) RNN 1. Use an LSTM or GRU: it makes your life so much simpler! 2. Initialize recurrent matrices to be orthogonal 3. Initialize other matrices with a sensible (small!) scale 4. Initialize forget gate bias to 1: default to remembering 5. Use adaptive learning rate algorithms: Adam, AdaDelta, … 6. Clip the norm of the gradient: 1–5 seems to be a reasonable threshold when used together with Adam or AdaDelta. 7. Either only dropout vertically or learn how to do it right 8. Be patient! 2018-02-06 45 [Saxe et al., ICLR2014; Ba, Kingma, ICLR2015; Zeiler, arXiv2012; Pascanu et al., ICML2013] "
467,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 10: Machine Translation, Sequence-to-sequence and Attention Abigail See Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Honor code issues: Assignment 2 • Assignment 3 released • Azure credits released • Default final project update: • New handout released • Submission instructions released • Custom final project: you should receive feedback on your proposal this week • Midterm grades: released after lecture 2/15/18 2 Happy Valentines Day! 2/15/18 3 Source: http://aiweirdness.com/post/170820844947/more-candy-hearts-by-neural-network RNN-generated candy hearts Welcome to the second half of the course! • Remaining lectures are mostly geared towards projects • We’ll bring you to the cutting-edge of NLP+DL research • Lectures will be more high-level • No more gradient computations! • Sometimes we’ll sketch an overview – if you’re interested in a topic, you can read more after class • However: today’s lecture will cover two core NLP Deep Learning techniques 2/15/18 4 Overview Today we will: • Introduce a new task: Machine Translation • Introduce a new neural architecture: sequence-to-sequence • Introduce a new neural technique: attention 2/15/18 5 is the primary use-case of is improved by Machine Translation Machine Translation (MT) is the task of translating a sentence x from one language (the source language) to a sentence y in another language (the target language). x: L'homme est né libre, et partout il est dans les fers y: Man is born free, but everywhere he is in chains 2/15/18 6 1950s: Early Machine Translation Machine Translation research began in the early 1950s. • Mostly Russian → English (motivated by the Cold War!) • Systems were mostly rule-based, using a bilingual dictionary to map Russian words to their English counterparts • A cool by-product: Quicksort! 2/15/18 7 Source: https://youtu.be/K-HfpsHPmvw 1990s-2010s: Statistical Machine Translation • Core idea: Learn a probabilistic model from data • Suppose we’re translating French → English. • We want to find best English sentence y, given French sentence x • Use Bayes Rule to break this down into two components to be learnt separately: 2/15/18 8 Translation Model Models how words and phrases should be translated. Learnt from parallel data. Language Model Models how to write good English. Learnt from monolingual data. 1990s-2010s: Statistical Machine Translation • Question: How to learn translation model ? • First, need large amount of parallel data (e.g. pairs of human-translated French/English sentences) 2/15/18 9 Ancient Egyptian Demotic Ancient Greek The Rosetta Stone 1990s-2010s: Statistical Machine Translation • Question: How to learn translation model ? • First, need large amount of parallel data (e.g. pairs of human-translated French/English sentences) • Break it down further: we actually want to consider where a is the alignment, i.e. word-level correspondence between French sentence x and English sentence y 2/15/18 10 What is alignment? Alignment is the correspondence between particular words in the translated sentence pair. • Note: Some words have no counterpart 2/15/18 11 Alignments We can factor the translation model P(f | e ) by identifying alignments (correspondences) between words in f and words in e Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes Japan shaken by two new quakes Le Japon secoué par deux nouveaux séismes spurious word Alignment is complex Alignment can be one-to-many (these are “fertile” words) 2/15/18 12 Alignments: harder And the program has been implemented Le programme a été mis en application zero fertility word not translated And the program has been implemented Le programme a été mis en application one-to-many alignment Alignment is complex Alignment can be many-to-one 2/15/18 13 Alignments: harder The balance was the territory of the aboriginal people Le reste appartenait aux autochtones many-to-one alignments The balance was the territory of the aboriginal people Le reste appartenait aux autochtones Alignment is complex Alignment can be many-to-many (phrase-level) 2/15/18 14 Alignments: hardest The poor don’t have any money Les pauvres sont démunis many-to-many alignment The poor dont have any money Les pauvres sont démunis phrase alignment 1990s-2010s: Statistical Machine Translation • Question: How to learn translation model ? • First, need large amount of parallel data (e.g. pairs of human-translated French/English sentences) • Break it down further: we actually want to consider where a is the alignment, i.e. word-level correspondence between French sentence x and English sentence y • We learn as a combination of many factors, including: • Probability of particular words aligning • Also depends on position in sentence • Probability of particular words having particular fertility • Etc. 2/15/18 15 1990s-2010s: Statistical Machine Translation • We could enumerate every possible y and calculate the probability? → Too expensive! • Answer: Use a heuristic search algorithm to gradually build up the the translation, discarding hypotheses that are too low- probability 2/15/18 16 Question: How to compute this argmax? Translation Model Language Model Searching for the best translation 2/15/18 17 Translation Process • Task: translate this sentence from German into English er geht ja nicht nach hause er geht ja nicht nach hause he does not go home • Pick phrase in input, translate Chapter 6: Decoding 6 Searching for the best translation 2/15/18 18 Translation Options he er geht ja nicht nach hause it , it , he is are goes go yes is , of course not do not does not is not after to according to in house home chamber at home not is not does not do not home under house return home do not it is he will be it goes he goes is are is after all does to following not after not to , not is not are not is not a • Many translation options to choose from – in Europarl phrase table: 2727 matching phrase pairs for this sentence – by pruning to the top 20 per phrase, 202 translation options remain Chapter 6: Decoding 8 Decoding: Find Best Path er geht ja nicht nach hause are it he goes does not yes go to home home backtrack from highest scoring complete hypothesis 1990s-2010s: Statistical Machine Translation • SMT is a huge research field • The best systems are extremely complex • Hundreds of important details we haven’t mentioned here • Systems have many separately-designed subcomponents • Lots of feature engineering • Need to design features to capture particular language phenomena • Require compiling and maintaining extra resources • Like tables of equivalent phrases • Lots of human effort to maintain • Repeated effort for each language pair! 2/15/18 19 2014 (dramatic reenactment) 2/15/18 21 2014 MT research Neural Machine Translation (dramatic reenactment) What is Neural Machine Translation? • Neural Machine Translation (NMT) is a way to do Machine Translation with a single neural network • The neural network architecture is called sequence-to-sequence (aka seq2seq) and it involves two RNNs. 2/15/18 22 Encoder RNN Neural Machine Translation (NMT) 2/15/18 23 <START> Source sentence (input) les pauvres sont démunis The sequence-to-sequence model Target sentence (output) Decoder RNN Encoder RNN produces an encoding of the source sentence. Encoding of the source sentence. Provides initial hidden state for Decoder RNN. Decoder RNN is a Language Model that generates target sentence conditioned on encoding. the argmax the argmax poor poor argmax don’t Note: This diagram shows test time behavior: decoder output is fed in as next step’s input have any money <END> don’t have any money argmax argmax argmax argmax Neural Machine Translation (NMT) • The sequence-to-sequence model is an example of a Conditional Language Model. • Language Model because the decoder is predicting the next word of the target sentence y • Conditional because its predictions are also conditioned on the source sentence x • NMT directly calculates : • Question: How to train a NMT system? • Answer: Get a big parallel corpus… 2/15/18 24 Probability of next target word, given target words so far and source sentence x Training a Neural Machine Translation system 2/15/18 25 Encoder RNN Source sentence (from corpus) <START> the poor don’t have any money les pauvres sont démunis Target sentence (from corpus) Seq2seq is optimized as a single system. Backpropagation operates “end to end”. Decoder RNN ! ""# ! ""$ ! ""% ! ""& ! ""' ! ""( ! "") *# *$ *% *& *' *( *) = negative log prob of “the” * = 1 - . /0# 1 */ = + + + + + + = negative log prob of <END> = negative log prob of “have” Better-than-greedy decoding? • We showed how to generate (or “decode”) the target sentence by taking argmax on each step of the decoder • This is greedy decoding (take most probable word on each step) • Problems? 2/15/18 26 <START> the argmax the argmax poor poor argmax don’t have any money <END> don’t have any money argmax argmax argmax argmax Better-than-greedy decoding? • Greedy decoding has no way to undo decisions! • les pauvres sont démunis (the poor don’t have any money) • → the ____ • → the poor ____ • → the poor are ____ • Better option: use beam search (a search algorithm) to explore several hypotheses and select the best one 2/15/18 27 Beam search decoding • Ideally we want to find y that maximizes • We could try enumerating all y →too expensive! • Complexity !(#$) where V is vocab size and T is target sequence length • Beam search: On each step of decoder, keep track of the k most probable partial translations • k is the beam size (in practice around 5 to 10) • Not guaranteed to find optimal solution • But much more efficient! 2/15/18 28 Beam search decoding: example Beam size = 2 2/15/18 29 <START> Beam search decoding: example Beam size = 2 2/15/18 30 <START> the a Beam search decoding: example Beam size = 2 2/15/18 31 poor people poor person <START> the a Beam search decoding: example Beam size = 2 2/15/18 32 poor people poor person are don’t person but <START> the a Beam search decoding: example Beam size = 2 2/15/18 33 poor people poor person are don’t person but always not have take <START> the a Beam search decoding: example Beam size = 2 2/15/18 34 poor people poor person are don’t person but always not have take in with any enough <START> the a Beam search decoding: example Beam size = 2 2/15/18 35 poor people poor person are don’t person but always not have take in with any enough money funds money funds <START> the a Beam search decoding: example Beam size = 2 2/15/18 36 poor people poor person are don’t person but always not have take in with any enough money funds money funds <START> the a Advantages of NMT Compared to SMT, NMT has many advantages: • Better performance • More fluent • Better use of context • Better use of phrase similarities • A single neural network to be optimized end-to-end • No subcomponents to be individually optimized • Requires much less human engineering effort • No feature engineering • Same method for all language pairs 2/15/18 37 Disadvantages of NMT? Compared to SMT: • NMT is less interpretable • Hard to debug • NMT is difficult to control • For example, can’t easily specify rules or guidelines for translation • Safety concerns! 2/15/18 38 How do we evaluate Machine Translation? BLEU (Bilingual Evaluation Understudy) • BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on: • n-gram precision (usually up to 3 or 4-grams) • Penalty for too-short system translations • BLEU is useful but imperfect • There are many valid ways to translate a sentence • So a good translation can get a poor BLEU score because it has low n-gram overlap with the human translation L 2/15/18 39 MT progress over time 2/15/18 40 0 5 10 15 20 25 2013 2014 2015 2016 Phrase-based SMT Syntax-based SMT Neural MT Source: http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf [Edinburgh En-De WMT newstest2013 Cased BLEU; NMT 2015 from U. Montréal] NMT: the biggest success story of NLP Deep Learning Neural Machine Translation went from a fringe research activity in 2014 to the leading standard method in 2016 • 2014: First seq2seq paper published • 2016: Google Translate switches from SMT to NMT • This is amazing! • SMT systems, built by hundreds of engineers over many years, outperformed by NMT systems trained by a handful of engineers in a few months 2/15/18 41 So is Machine Translation solved? • Nope! • Many difficulties remain: • Out-of-vocabulary words • Domain mismatch between train and test data • Maintaining context over longer text • Low-resource language pairs 2/15/18 42 So is Machine Translation solved? • Nope! • Using common sense is still hard 2/15/18 43 ? So is Machine Translation solved? • Nope! • NMT picks up biases in training data 2/15/18 44 Source: https://hackernoon.com/bias-sexist-or-this-is-the-way-it-should-be-ce1f7c8c683c Didn’t specify gender So is Machine Translation solved? • Nope! • Uninterpretable systems do strange things 2/15/18 45 Source: http://languagelog.ldc.upenn.edu/nll/?p=35120#more-35120 NMT research continues NMT is the flagship task for NLP Deep Learning • NMT research has pioneered many of the recent innovations of NLP Deep Learning • In 2018: NMT research continues to thrive • Researchers have found many, many improvements to the “vanilla” seq2seq NMT system we’ve presented today • But one improvement is so integral that it is the new vanilla… ATTENTION 2/15/18 46 Sequence-to-sequence: the bottleneck problem 2/15/18 47 Encoder RNN Source sentence (input) <START> the poor don’t have any money les pauvres sont démunis the poor don’t have any money <END> Decoder RNN Target sentence (output) Problems with this architecture? Encoding of the source sentence. Sequence-to-sequence: the bottleneck problem 2/15/18 48 Encoder RNN Source sentence (input) <START> the poor don’t have any money les pauvres sont démunis the poor don’t have any money <END> Decoder RNN Target sentence (output) Encoding of the source sentence. This needs to capture all information about the source sentence. Information bottleneck! Attention • Attention provides a solution to the bottleneck problem. • Core idea: on each step of the decoder, focus on a particular part of the source sequence • First we will show via diagram (no equations), then we will show with equations 2/15/18 49 Sequence-to-sequence with attention 2/15/18 50 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Sequence-to-sequence with attention 2/15/18 51 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Sequence-to-sequence with attention 2/15/18 52 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Sequence-to-sequence with attention 2/15/18 53 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Sequence-to-sequence with attention 2/15/18 54 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores On this decoder timestep, we’re mostly focusing on the first encoder hidden state (”les”) Attention distribution Take softmax to turn the scores into a probability distribution Sequence-to-sequence with attention 2/15/18 55 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention distribution Attention scores Attention output Use the attention distribution to take a weighted sum of the encoder hidden states. The attention output mostly contains information the hidden states that received high attention. Sequence-to-sequence with attention 2/15/18 56 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention distribution Attention scores Attention output Concatenate attention output with decoder hidden state, then use to compute ! ""# as before ! ""# the Sequence-to-sequence with attention 2/15/18 57 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores the Attention distribution Attention output ! ""# poor Sequence-to-sequence with attention 2/15/18 58 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores Attention distribution Attention output the poor ! ""# don’t Sequence-to-sequence with attention 2/15/18 59 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores Attention distribution Attention output the poor don’t ! ""# have Sequence-to-sequence with attention 2/15/18 60 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores Attention distribution Attention output the poor have ! ""# any don’t Sequence-to-sequence with attention 2/15/18 61 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores Attention distribution Attention output the poor don’t have any ! ""# money Attention: in equations • We have encoder hidden states • On timestep t, we have decoder hidden state • We get the attention scores for this step: • We take softmax to get the attention distribution for this step (this is a probability distribution and sums to 1) • We use to take a weighted sum of the encoder hidden states to get the attention output • Finally we concatenate the attention output with the decoder hidden state and proceed as in the non-attention seq2seq model 2/15/18 62 Attention is great • Attention significantly improves NMT performance • It’s very useful to allow decoder to focus on certain parts of the source • Attention solves the bottleneck problem • Attention allows decoder to look directly at source; bypass bottleneck • Attention helps with vanishing gradient problem • Provides shortcut to faraway states • Attention provides some interpretability • By inspecting attention distribution, we can see what the decoder was focusing on • We get alignment for free! • This is cool because we never explicitly trained an alignment system • The network just learned alignment by itself 2/15/18 63 Alignments: hardest The poor don’t have any money Les pauvres sont démunis many-to-many alignment The poor dont have any money Les pauvres sont démunis phrase li t Recap • We learned the history of Machine Translation (MT) • Since 2014, Neural MT rapidly replaced intricate Statistical MT • Sequence-to-sequence is the architecture for NMT (uses 2 RNNs) • Attention is a way to focus on particular parts of the input • Improves sequence-to-sequence a lot! 2/15/18 64 Sequence-to-sequence is versatile! • Sequence-to-sequence is useful for more than just MT • Many NLP tasks can be phrased as sequence-to-sequence: • Summarization (long text → short text) • Dialogue (previous utterances → next utterance) • Parsing (input text → output parse as sequence) • Code generation (natural language → Python code) 2/15/18 65 Next time • More types of attention • More uses for attention • More advanced sequence-to-sequence techniques 2/15/18 66 "
468,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 11: Paying attention to attention and Tips and Tricks for large MT Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Thanks for your Feedback! What do you most want to learn about in the remaining lectures? • “More cutting edge research topics in NLP & DL.” • “Solutions/approaches to core NLP problems, variations in the the techniques used” • “More state of the art neural networks” • “More neural tricks” • “More state of the art techniques.“ • “State-of-the-art neural network architectures” See Piazza for feedback form – continued feedback welcome! 2/20/18 2 Congrats • The basic/theoretical/mathematical part of the class is over • Now, we can have fun with recent, state-of-the-art ideas • You can start hacking up real stuff now • Today, we will cover more attention details and tips and tricks to make bigger models and systems work 2/20/18 3 Overview • More types and uses of attention • Refresh and math • Pointer-sentinel model • Self-attention/intra-detention for summarization • Practical tricks on how to deal with large output vocabularies in neural machine translation (and other tasks) • UNKs • Vocab reduction • Subword Units 2/20/18 4 Recap: Sequence-to-sequence with attention 2/20/18 5 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Recap: Sequence-to-sequence with attention 2/20/18 6 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Recap: Sequence-to-sequence with attention 2/20/18 7 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Recap: Sequence-to-sequence with attention 2/20/18 8 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores dot product Recap: Sequence-to-sequence with attention 2/20/18 9 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores On this decoder timestep, we’re mostly focusing on the first encoder hidden state (”les”) Attention distribution Take softmax to turn the scores into a probability distribution Recap: Sequence-to-sequence with attention 2/20/18 10 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention distribution Attention scores Attention output Use the attention distribution to take a weighted sum of the encoder hidden states. The attention output mostly contains information the hidden states that received high attention. Recap: Sequence-to-sequence with attention 2/20/18 11 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention distribution Attention scores Attention output Concatenate attention output with decoder hidden state, then use to compute 𝑦 ""# as before 𝑦 ""# the Recap: Sequence-to-sequence with attention 2/20/18 12 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores the Attention distribution Attention output 𝑦 ""% poor Recap: Sequence-to-sequence with attention 2/20/18 13 Encoder RNN Source sentence (input) <START> les pauvres sont démunis Decoder RNN Attention scores Attention distribution Attention output the poor 𝑦 ""& don’t Recap: Basic Attention equations • We have encoder hidden states • On timestep t, we have decoder hidden state • We get the attention scores for this step: • We take softmax to get the attention distribution for this step (this is a probability distribution and sums to 1) • We use to take a weighted sum of the encoder hidden states to get the attention output • Finally we concatenate the attention output with the decoder hidden state and proceed as in the non-attention seq2seq model 2/20/18 14 Recap: Attention is great • Attention significantly improves NMT performance • It’s very useful to allow decoder to focus on certain parts of the source • Attention solves the bottleneck problem • Attention allows decoder to look directly at source; bypass bottleneck • Attention helps with vanishing gradient problem • Provides shortcut to faraway states • Attention provides some interpretability • By inspecting attention distribution, we can see what the decoder was focusing on • We get alignment for free! • This is cool because we never explicitly trained an alignment system • The network just learned alignment by itself 2/20/18 15 Alignments: hardest The poor don’t have any money Les pauvres sont démunis many-to-many alignment The poor dont have any money Les pauvres sont démunis phrase li t Attention is a general Deep Learning technique • Last time: We saw that attention is a great way to improve the sequence-to-sequence model for Machine Translation. • However: Today we’ll see attention is applied to many architectures (not just seq2seq) and many tasks (not just MT) • More general definition of attention: • Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query.* • We sometimes say that the query attends to the values. • For example, in the seq2seq + attention model, each decoder hidden state attends to the encoder hidden states. 16 * Note: This is slightly different to the terminology in the default final project code. See Piazza post for more details. Attention is a general Deep Learning technique More general definition of attention: Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query. • Intuition: • The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on. • Attention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values), dependent on some other representation (the query). 17 There are several attention variants • We have some values and a query • Attention always involves computing the attention output (sometimes called the context vector) from the attention scores (or attention logits) like so: • However, there are several ways you can compute 18 (take softmax) (take weighted sum) Attention variants There are several ways you can compute from and : • Basic dot-product attention: • Note: this assumes • This is the version we saw earlier • Multiplicative attention: • Where is a weight matrix • Additive attention: • Where are weight matrices and is a weight vector 19 More information: http://ruder.io/deep-learning-nlp-best-practices/index.html#attention Attention application: Pointing to words for language modeling • Idea: Mixture Model of softmax and pointers: • Pointer Sentinel Mixture Models by Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher SMERITY@SALESFORCE.COM CXIONG@SALESFORCE.COM JAMES.BRADBURY@SALESFORCE.COM RSOCHER@SALESFORCE.COM A, USA with lan- very Even ords un- tinel dels word om a inel- uage bank ame- p(Yellen) = g pvocab(Yellen) + (1 −g) pptr(Yellen) p(Yellen) = g pvocab(Yellen) + (1 −g) pptr(Yellen) zebra Chair Janet Yellen … raised rates . Ms. ??? Fed … Yellen Rosenthal Bernanke aardvark … … Sentinel … Pointer Softmax RNN pvocab(Yellen) pvocab(Yellen) g pptr(Yellen) pptr(Yellen) Figure 1. Illustration of the pointer sentinel-RNN mixture model. g is the mixture gate which uses the sentinel to dictate how much probability mass to give to the vocabulary. 20 Pointer-Sentinel Model - Details Pointer Sentinel Mixture Models · · · Sentinel x RNN Distribution pvocab(yN|w1, . . . , wN−1) pvocab(yN|w1, . . . , wN−1) Pointer Distribution pptr(yN|w1, . . . , wN−1) pptr(yN|w1, . . . , wN−1) Output Distribution p(yN|w1, . . . , wN−1) p(yN|w1, . . . , wN−1) Sentinel Query RNN Embed + ··· ··· Softmax Softmax · · · · · · · · · Mixture gate g Figure 2. Visualization of the pointer sentinel-RNN mixture model. The query, produced from applying an MLP to the last output of the RNN, is used by the pointer network to identify likely matching words from the past. The ⊙nodes are inner products between the query and the RNN hidden states. If the pointer component is not conﬁdent, probability mass can be directed to the RNN by increasing the value of the mixture gate g via the sentinel, seen in grey. If g = 1 then only the RNN is used. If g = 0 then only the pointer is used. 2. The Pointer Sentinel for Language Modeling Given a sequence of words w1, . . . , wN−1, our task is to predict the next word wN. 2 1 Th f RNN C 2.2. The Pointer Network Component In this section, we propose a modiﬁcation to pointer net- works for language modeling. To predict the next word in the sequence, a pointer network would select the member of the input sequence p(w1, . . . , wN−1) with the maximal attention score as the output 21 Pointer Sentinel for Language Modeling Pointer Sentinel Mixture Models Model Parameters Validation Test Mikolov & Zweig (2012) - KN-5 2M‡ − 141.2 Mikolov & Zweig (2012) - KN5 + cache 2M‡ − 125.7 Mikolov & Zweig (2012) - RNN 6M‡ − 124.7 Mikolov & Zweig (2012) - RNN-LDA 7M‡ − 113.7 Mikolov & Zweig (2012) - RNN-LDA + KN-5 + cache 9M‡ − 92.0 Pascanu et al. (2013a) - Deep RNN 6M − 107.5 Cheng et al. (2014) - Sum-Prod Net 5M‡ − 100.0 Zaremba et al. (2014) - LSTM (medium) 20M 86.2 82.7 Zaremba et al. (2014) - LSTM (large) 66M 82.2 78.4 Gal (2015) - Variational LSTM (medium, untied) 20M 81.9 ± 0.2 79.7 ± 0.1 Gal (2015) - Variational LSTM (medium, untied, MC) 20M − 78.6 ± 0.1 Gal (2015) - Variational LSTM (large, untied) 66M 77.9 ± 0.3 75.2 ± 0.2 Gal (2015) - Variational LSTM (large, untied, MC) 66M − 73.4 ± 0.0 Kim et al. (2016) - CharCNN 19M − 78.9 Zilly et al. (2016) - Variational RHN 32M 72.8 71.3 Zoneout + Variational LSTM (medium) 20M 84.4 80.6 Pointer Sentinel-LSTM (medium) 21M 72.4 70.9 Table 2. Single model perplexity on validation and test sets for the Penn Treebank language modeling task. For our models and the models of Zaremba et al. (2014) and Gal (2015), medium and large refer to a 650 and 1500 units two layer LSTM respectively. The medium pointer sentinel-LSTM model achieves lower perplexity than the large LSTM model of Gal (2015) while using a third of the parameters and without using the computationally expensive Monte Carlo (MC) dropout averaging at test time Parameter numbers with 22 Attention application: Intra-Decoder attention for Summarization • Longer document summarization. Example: • Tony Blair has said he does not want to retire until he is 91 – as he unveiled plans to set up a ‘cadre’ of ex-leaders to advise governments around the world. The defiant 61-year-old former Prime Minister said he had ‘decades’ still in him and joked that he would ‘turn to drink’ if he ever stepped down from his multitude of global roles. He told Newsweek magazine that his latest ambition was to recruit former heads of government to go round the world to advise presidents and prime ministers on how to run their countries. In an interview with the magazine Newsweek Mr Blair said he did not want to retire until he was 91 years old Mr Blair said his latest ambition is to recruit former heads of government to advise presidents and prime ministers on how to run their countries Mr Blair said he himself had been ‘mentored’ by US president Bill Clinton when he took office in 1997. And he said he wanted to build up his organisations, such as his Faith Foundation, so they are ‘capable of changing global policy’. Last night, Tory MPs expressed horror at the prospect of Mr Blair remaining in public life for another 30 years. Andrew Bridgen said: ‘We all know weak Ed Miliband’s called on Tony to give his flailing campaign a boost, but the attention’s clearly gone to his head.’ (...) • Summary: The former Prime Minister claimed he has 'decades' of work left in him. Joked he would 'turn to drink' if he ever stepped down from global roles. Wants to recruit former government heads to advise current leaders. He was 'mentored' by US president Bill Clinton when he started in 1997. 2/20/18 23 Attention application: Intra-Decoder attention for Summarization • Based on paper: • Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A Deep Reinforced Model for Abstractive Summarization • But similar ideas appear elsewhere also • Two necessary, new ingredients • Attention during generation à Today • Reinforcement learning à Not in this class 2/20/18 24 Attention application: Similar Seq2Seq Idea as in Translation • Problems: For longer outputs (MT was just single sentences), the decoder starts to repeat itself 2/20/18 25 More advanced attention 1. More advanced encoder attention 2. Self-attention (= intra-decoder attention) 2/20/18 26 1. Details of this attention mechanism • More advanced similarity function than simple inner product: • Temporal attention function, penalizing input tokens that have obtained high attention scores in past decoding steps: • Improves coverage and prevent repeated attention to same inputs 2/20/18 27 1. Details of this attention mechanism • Combine softmax’ed weighted hidden states from encoder: • Remember softmax-normalized encoder 𝛼(𝑠for later! 2/20/18 28 2. Self-attention on decoder 2/20/18 29 • Self-attention: A general idea used in many RNN models (e.g. Language Models). The hidden states “attend to themselves”, i.e. each hidden state attends to the previous hidden states of the same RNN. • On step t, attends to previous decoder hidden states : • Apply softmax to get attention distribution over previous hidden states for t’ = 1,…,t-1: • Compute decoder attention output: 2. Combine softmax and pointers using both attention computations • Compute probability of copying/pointing to word from input: • If not copying/pointing, use standard softmax: • If pointing, use encoder attention weights (from 2 slides ago) • Combine everything: 2/20/18 30 Summarization Results 31 Summarization Results 32 Similar ideas explored simultaneously by Abi et al. • Get To The Point: Summarization with Pointer-Generator Networks, Abigail See, Peter J. Liu, Christopher Manning, 2017 Blog post: http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html 33 Preview • Hopefully, you can see how useful and versatile attention is • Next lecture we will go even further and cover a model that only has attention (The Transformer) • But, for now, we will cover some tips and tricks to actually scale up machine translation. 2/20/18 34 Extending NMT to more languages • “Copy” mechanisms are not sufficient. • Transliteration: Christopher ↦Kryštof • Multi-word alignment: Solar system ↦Sonnensystem • Need to handle large, open vocabulary • Rich morphology: • nejneobhospodařovávatelnějšímu - Czech = “to the worst farmable one” • Donaudampfschiffahrtsgesellschaftskapitän – German = Danube steamship company captain • Informal spelling: goooooood morning !!!!! Need to be able to operate at sub-word levels! 35 Dealing with a large output vocabulary in MT++ am a student _ Je suis Je suis étudia I Je suis étudiant _ Softmax parameters Hidden state P(Je| …) |V| 36 The word generation problem • Word generation problem am a student _ Je suis Je suis étudia I Je suis étudiant _ Softmax parameters Hidden state P(Je| …) |V| 37 Softmax computation is expensive. The word generation problem • Word generation problem • If vocabs are modest, e.g., 50K am a student _ Je Je suis ét I The <unk> portico in <unk> Le <unk> <unk> de <unk> The ecotax portico in Pont-de-Buis Le portique écotaxe de Pont-de-Buis 38 First thought: scale the softmax • Lots of ideas from the neural LM literature! • Hierarchical models: tree-structured vocabulary • [Morin & Bengio, AISTATS’05], [Mnih & Hinton, NIPS’09]. • Complex, sensitive to tree structures. • Noise-contrastive estimation: binary classification • [Mnih & Teh, ICML’12], [Vaswani et al., EMNLP’13]. • Different noise samples per training example.* Not GPU-friendly 39 *We’ll mention a simple fix for this! • GPU-friendly. • Training: a subset of the vocabulary at a time. • Testing: smart on the set of possible translations. Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio. On Using Very Large Target Vocabulary for Neural Machine Translation. ACL’15. Fast at both train & test time. 40 Large-vocab NMT Training • Each time train on a smaller vocab Vʹ ≪V How do we select Vʹ? 41 |Vʹ| Training • Each time train on a smaller vocab Vʹ ≪V • Partition training data in subsets: • Each subset has 𝜏distinct target words, |Vʹ| = 𝜏. 42 |Vʹ| Training – Segment data • Sequentially select examples: |Vʹ| = 5. 43 she loves cats he likes dogs cats have tails dogs have tails dogs chase cats she loves dogs cats hate dogs Vʹ = {she, loves, cats, he, likes} Training – Segment data • Sequentially select examples: |Vʹ| = 5. 44 she loves cats he likes dogs cats have tails dogs have tails dogs chase cats she loves dogs cats hate dogs Vʹ = {cats, have, tails, dogs, chase} Training – Segment data • Sequentially select examples: |Vʹ| = 5. 45 she loves cats he likes dogs cats have tails dogs have tails dogs chase cats she loves dogs cats hate dogs Vʹ = {she, loves, dogs, cats, hate} • Practice: |V| = 500K, |Vʹ| = 30K or 50K. Testing – Select candidate words • K most frequent words: unigram prob. 46 de, , la . et des les … Testing – Select candidate words • K most frequent words: unigram prob. • Candidate target words • Kʹ choices per source word. Kʹ = 3. 47 cats elle celle ceci She de, , la . et des les … loves aime amour aimer chats chat félin Testing – Select candidate words 48 de, , la . et des les … cats elle celle ceci She loves aime amour aimer chats chat félin + = Candidate list • Produce translations within the candidate list • Practice: Kʹ = 10 or 20, K = 15k, 30k, or 50k. Kʹ K More on large-vocab techniques • “BlackOut: Speeding up Recurrent Neural Network Language Models with very Large Vocabularies” – [Ji, Vishwanathan, Satish, Anderson, Dubey, ICLR’16]. • Good survey over many techniques. • “Simple, Fast Noise Contrastive Estimation for Large RNN Vocabularies” – [Zoph, Vaswani, May, Knight, NAACL’16]. • Use the same samples per minibatch. GPU efficient. 49 Sub-word NMT: two trends • Same seq2seq architecture: • Use smaller units. • [Sennrich, Haddow, Birch, ACL’16a], [Chung, Cho, Bengio, ACL’16]. • Hybrid architectures: • RNN for words + something else for characters. • [Costa-Jussà & Fonollosa, ACL’16], [Luong & Manning, ACL’16]. 50 Byte Pair Encoding Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with Subword Units. ACL 2016. • A compression algorithm: • Most frequent byte pair ↦a new byte. 51 Replace bytes with character ngrams Byte Pair Encoding • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. 52 Byte Pair Encoding • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. 53 5 l o w 2 l o w e r 6 n e w e s t 3 w i d e s t (Example from Sennrich) l, o, w, e, r, n, w, s, t, i, d Vocabulary Dictionary Start with all characters in vocab Byte Pair Encoding • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. 54 5 l o w 2 l o w e r 6 n e w es t 3 w i d es t (Example from Sennrich) l, o, w, e, r, n, w, s, t, i, d, es Vocabulary Dictionary Add a pair (e, s) with freq 9 Byte Pair Encoding • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. 55 5 l o w 2 l o w e r 6 n e w est 3 w i d est (Example from Sennrich) l, o, w, e, r, n, w, s, t, i, d, es, est Vocabulary Dictionary Add a pair (es, t) with freq 9 Byte Pair Encoding • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. 56 5 lo w 2 lo w e r 6 n e w est 3 w i d est (Example from Sennrich) l, o, w, e, r, n, w, s, t, i, d, es, est, lo Vocabulary Dictionary Add a pair (l, o) with freq 7 Byte Pair Encoding 57 • A word segmentation algorithm: • Start with a vocabulary of characters. • Most frequent ngram pairs ↦a new ngram. • Automatically decide vocabs for NMT Top places in WMT 2016! https://github.com/rsennrich/nematus Character-based LSTM 58 u n y l … … (unfortunately) Bi-LSTM builds word representations Ling, Luís, Marujo, Astudillo, Amir, Dyer, Black, Trancoso. Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation. EMNLP’15. Ling, Luís, Marujo, Astudillo, Amir, Dyer, Black, Trancoso. Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation. EMNLP’15. Character-based LSTM 59 Recurrent Language Model u n y l … … (unfortunately) the the bank bank was was closed Bi-LSTM builds word representations • A best-of-both-worlds architecture: • Translate mostly at the word level • Only go to the character level when needed. • More than 2 BLEU improvement over a copy mechanism. Thang Luong and Chris Manning. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL 2016. Hybrid NMT 60 Hybrid NMT Word-level (4 layers) End-to-end training 8-stacking LSTM layers. 61 2-stage Decoding 62 • Word-level beam search 2-stage Decoding Init with word hidden states. 63 • Word-level beam search • Char-level beam search for <unk>. English-Czech Results 30x data 3 systems 64 Systems BLEU Winning WMT’15 (Bojar & Tamchyna, 2015) 18.8 Word-level NMT (Jean et al., 2015) 18.3 Large vocab + copy mechanism • Train on WMT’15 data (12M sentence pairs) • newstest2015 English-Czech Results 30x data 3 systems 65 Systems BLEU Winning WMT’15 (Bojar & Tamchyna, 2015) 18.8 Word-level NMT (Jean et al., 2015) 18.3 Hybrid NMT (Luong & Manning, 2016)* 20.7 New SOTA! Large vocab + copy mechanism • Train on WMT’15 data (12M sentence pairs) • newstest2015 Sample English-Czech translations source Her 11-year-old daughter , Shani Bart , said it felt a little bit weird human Její jedenáctiletá dcera Shani Bartová prozradila , že je to trochu zvláštní word Její <unk> dcera <unk> <unk> řekla , že je to trochu divné Její 11-year-old dcera Shani , řekla , že je to trochu divné hybrid Její <unk> dcera , <unk> <unk> , řekla , že je to <unk> <unk> Její jedenáctiletá dcera , Graham Bart , řekla , že cítí trochu divný • Word-based: identity copy fails. 66 Sample English-Czech translations source Her 11-year-old daughter , Shani Bart , said it felt a little bit weird human Její jedenáctiletá dcera Shani Bartová prozradila , že je to trochu zvláštní word Její <unk> dcera <unk> <unk> řekla , že je to trochu divné Její 11-year-old dcera Shani , řekla , že je to trochu divné hybrid Její <unk> dcera , <unk> <unk> , řekla , že je to <unk> <unk> Její jedenáctiletá dcera , Graham Bart , řekla , že cítí trochu divný 67 • Hybrid: correct, 11-year-old – jedenáctiletá.. Using attention for coverage • Caption generation 68 How to not miss an important image patch? Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel, Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ICML’15 RNNs are Slow • RNNs are the basic building block for deepNLP • Idea: Take the best and parallelizable parts of RNNs and CNNs • Quasi-Recurrent Neural Networks by James Bradbury, Stephen Merity, Caiming Xiong & Richard Socher 69 Quasi-Recurrent Neural Network • Parallelism computation across time: • Element-wise gated recurrence for parallelism across channels: Under review as a conference paper at ICLR 2017 LSTM CNN LSTM/Linear Linear LSTM/Linear Linear fo-Pool Convolution fo-Pool Convolution Max-Pool Convolution Max-Pool Convolution QRNN Figure 1: Block diagrams showing the computation structure of the QRNN compared with typical LSTM and CNN architectures. Red signiﬁes convolutions or matrix multiplications; a continuous block means that those computations can proceed in parallel. Blue signiﬁes parameterless functions that operate in parallel along the channel/feature dimension. LSTMs can be factored into (red) linear blocks and (blue) elementwise blocks, but computation at each timestep still depends on the results from the previous timestep. 2 MODEL Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions. Gi i X RT ⇥n f T di i l h l i l b parameters and allows fully parallel computation across minibatch and feature Given an input sequence X 2 RT ⇥n of T n-dimensional vectors x1 . . . xT , t component of a QRNN performs convolutions in the timestep dimension wit producing a sequence Z 2 RT ⇥m of m-dimensional candidate vectors zt. In tasks that include prediction of the next token, the ﬁlters must not allow the given timestep to access information from future timesteps. That is, with ﬁlte depends only on xt−k+1 through xt. This concept, known as a masked convo et al., 2016), is implemented by padding the input to the left by the convoluti one. We apply additional convolutions with separate ﬁlter banks to obtain sequen elementwise gates that are needed for the pooling function. While the candida through a tanh nonlinearity, the gates use an elementwise sigmoid. If the pooli forget gate ft and an output gate ot at each timestep, the full set of computation component is then: Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), where Wz,Wf, and Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter ba masked convolution along the timestep dimension. Note that if the ﬁlter width reduce to the LSTM-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1xt 1 + W2xt) y p g p y l convolutions with separate ﬁlter banks to obtain sequences of vectors for the hat are needed for the pooling function. While the candidate vectors are passed inearity, the gates use an elementwise sigmoid. If the pooling function requires a output gate ot at each timestep, the full set of computations in the convolutional Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), (1) nd Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter banks and ⇤denotes a along the timestep dimension. Note that if the ﬁlter width is 2, these equations M-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1 oxt−1 + W2 oxt). (2) of larger width effectively compute higher n-gram features at each timestep; thus pecially important for character-level tasks. or the pooling subcomponent can be constructed from the familiar elementwise nal LSTM cell. We seek a function controlled by gates that can mix states across h acts independently on each channel of the state vector. The simplest option, Ghifary (2016) term “dynamic average pooling”, uses only a forget gate: Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), Wz,Wf, and Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter banks and ⇤denote onvolution along the timestep dimension. Note that if the ﬁlter width is 2, these equatio the LSTM-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1 oxt−1 + W2 oxt). ion ﬁlters of larger width effectively compute higher n-gram features at each timestep; th dths are especially important for character-level tasks. functions for the pooling subcomponent can be constructed from the familiar elementw he traditional LSTM cell. We seek a function controlled by gates that can mix states acr , but which acts independently on each channel of the state vector. The simplest opti lduzzi & Ghifary (2016) term “dynamic average pooling”, uses only a forget gate: ht = ft ⊙ht−1 + (1 −ft) ⊙zt, 2 70 Q-RNNs for Language Modeling • Better • Faster LSTM s recurrent weights, providing structural regularization over the recurrence. Without zoneout, early stopping based upon validation loss was required as the QRNN would begin overﬁtting. By applying a small amount of zoneout (p = 0.1), no early stopping is required and the QRNN achieves competitive levels of perplexity to the variational LSTM of Gal & Ghahramani Model Parameters Validation Test LSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7 Variational LSTM (medium) (Gal & Ghahramani, 2016) 20M 81.9 79.7 LSTM with CharCNN embeddings (Kim et al., 2016) 19M − 78.9 Zoneout + Variational LSTM (medium) (Merity et al., 2016) 20M 84.4 80.6 Our models LSTM (medium) 20M 85.7 82.0 QRNN (medium) 18M 82.9 79.9 QRNN + zoneout (p = 0.1) (medium) 18M 82.1 78.3 Table 2: Single model perplexity on validation and test sets for the Penn Treebank language model- ing task. Lower is better. “Medium” refers to a two-layer network with 640 or 650 hidden units per layer. All QRNN models include dropout of 0.5 on embeddings and between layers. MC refers to Monte Carlo dropout averaging at test time. 6 Under review as a conference paper at ICLR 2017 Sequence length 32 64 128 256 512 Batch size 8 5.5x 8.8x 11.0x 12.4x 16.9x 16 5.5x 6.7x 7.8x 8.3x 10.8x 32 4.2x 4.5x 4.9x 4.9x 6.4x 64 3.0x 3.0x 3.0x 3.0x 3.7x 128 2.1x 1.9x 2.0x 2.0x 2.4x 256 1.4x 1.4x 1.3x 1.3x 1.3x Figure 4: Left: Training speed for two-layer 640-unit PTB LM on a batch of 20 examples of 105 timesteps. “RNN” and “softmax” include the forward and backward times, while “optimization overhead” includes gradient clipping, L2 regularization, and SGD computations. Right: Inference speed advantage of a 320-unit QRNN layer alone over an equal-sized cuDNN LSTM layer for data with the given batch size and sequence length Training results are similar 71 Q-RNNs for Sentiment Analysis • Often better and faster than LSTMs • More interpretable • Example: • Initial positive review • Review starts out positive At 117: “not exactly a bad story” At 158: “I recommend this movie to everyone, even if you’ve never played the game” Under review as a conference paper at ICLR 2017 Model Time / Epoch (s) Test Acc (%) BSVM-bi (Wang & Manning, 2012) − 91.2 2 layer sequential BoW CNN (Johnson & Zhang, 2014) − 92.3 Ensemble of RNNs and NB-SVM (Mesnil et al., 2014) − 92.6 2-layer LSTM (Longpre et al., 2016) − 87.6 Residual 2-layer bi-LSTM (Longpre et al., 2016) − 90.1 Our models Deeply connected 4-layer LSTM (cuDNN optimized) 480 90.9 Deeply connected 4-layer QRNN 150 91.4 D.C. 4-layer QRNN with k = 4 160 91.1 Table 1: Accuracy comparison on the IMDb binary sentiment classiﬁcation task. All of our models use 256 units per layer; all layers other than the ﬁrst layer, whose ﬁlter width may vary, use ﬁlter width k = 2. Train times are reported on a single NVIDIA K40 GPU. We exclude semi-supervised models that conduct additional training on the unlabeled portion of the dataset. 3 EXPERIMENTS We evaluate the performance of the QRNN on three different natural language tasks: document-level sentiment classiﬁcation, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of equal hidden size on all three tasks while dra- matically improving computation speed. Experiments were implemented in Chainer (Tokui et al.). 3.1 SENTIMENT CLASSIFICATION We evaluate the QRNN architecture on a popular document-level sentiment classiﬁcation bench- mark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)). Our best performance on a held-out development set was achieved using a four-layer densely- connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014). Dropout of 0.3 was applied between layers, and we used L2 regularization of 4 ⇥10−6 Optimization was performed on minibatches of 24 examples Under review as a conference paper at ICLR 2017 Figure 3: Visualization of the ﬁnal QRNN layer’s hidden state vectors cL t in the IMDb task, with timesteps along the vertical axis. Colors denote neuron activations. After an initial positive statement “This movie is simply gorgeous” (off graph at timestep 9), timestep 117 triggers a reset of most hidden states due to the phrase “not exactly a bad story” (soon after “main weakness is its story”). Only at timestep 158, after “I recommend this movie to everyone, even if you’ve never played the game”, do the hidden units recover. each layer, it was more computationally convenient to use a multiple of 32. As the Penn Treebank is a relatively small dataset, preventing overﬁtting is of considerable importance and a major focus of recent research. It is not obvious in advance which of the many RNN regularization schemes 72 "
469,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 13: Transformer Networks and Convolutional Neural Networks Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Projects Project advice OH. Come every week from now on. 2/22/18 Lecture 1, Slide 1 Overview of today • Attention is all you need? – Transformer Networks • CNN Variant 1: Simple single layer • Application: Sentence classification • Some practical details and tricks for CNNs 2/22/18 Lecture 1, Slide 2 Problems with RNNs = Motivation for Transformers • Sequential computation prevents parallelization • Despite GRUs and LSTMs, RNNs still need attention mechanism to deal with long range dependencies – path length for co- dependent computation between states grows with sequence • But if attention gives us access to any state… maybe we don’t need the RNN? 2/22/18 Lecture 1, Slide 3 Transformer Overview • Sequence-to-sequence • Encoder-Decoder • Task: machine translation with parallel corpus • Predict each translated word • Final cost/error function is standard cross-entropy error on top of a softmax classifier This and related figures from paper: https://arxiv.org/pdf/1706.03762.pdf 2/22/18 Lecture 1, Slide 4 Transformer Paper • Attention Is All You Need [2017] • by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin • Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self- attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. 2/22/18 Lecture 1, Slide 5 Transformer Basics • Let’s define the basic building blocks of transformer networks first: new attention layers! 2/22/18 Lecture 1, Slide 6 Dot-Product Attention (Extending our previous def.) • Inputs: a query q and a set of key-value (k-v) pairs to an output • Query, keys, values, and output are all vectors • Output is weighted sum of values, where • Weight of each value is computed by an inner product of query and corresponding key • Queries and keys have same dimensionality dk value have dv 2/22/18 Lecture 1, Slide 7 Dot-Product Attention – Matrix notation • When we have multiple queries q, we stack them in a matrix Q: • Becomes: [|Q| x dk] x [dk x |K|] x [|K| x dv] softmax = [|Q| x dv] row-wise 2/22/18 Lecture 1, Slide 8 Scaled Dot-Product Attention • Problem: As dk gets large, the variance of qTk increases à some values inside the softmax get large à the softmax gets very peaked --> hence its gradient gets smaller. • Solution: Scale by length of query/key vectors: 2/22/18 Lecture 1, Slide 9 Self-attention and Multi-head attention • The input word vectors could be the queries, keys and values • In other words: the word vectors themselves select each other • Word vector stack = Q = K = V • Problem: Only one way for words to interact with one-another • Solution: Multi-head attention • First map Q, K, V into h many lower dimensional spaces via W matrices • Then apply attention, then concatenate outputs and pipe through linear layer 2/22/18 Lecture 1, Slide 10 Complete transformer block • Each block has two “sublayers” 1. Multihead attention 2. 2 layer feed-forward Nnet (with relu) Each of these two steps also has: Residual (short-circuit) connection and LayerNorm: LayerNorm(x + Sublayer(x)) Layernorm changes input to have mean 0 and variance 1, per layer and per training point (and adds two more parameters) Layer Normalization by Ba, Kiros and Hinton, https://arxiv.org/pdf/1607.06450.pdf 2/22/18 Lecture 1, Slide 11 Encoder Input • Actual word representations are byte-pair encodings (see last lecture) • Also added is a positional encoding so same words at different locations have different overall representations: 2/22/18 Lecture 1, Slide 12 Complete Encoder • For encoder, at each block, we use the same Q, K and V from the previous layer • Blocks are repeated 6 times 2/22/18 Lecture 1, Slide 13 Attention visualization in layer 5 • Words start to pay attention to other words in sensible ways 2/22/18 Lecture 1, Slide 14 Attention visualization: Implicit anaphora resolution In 5th layer. Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 2/22/18 Lecture 1, Slide 15 Transformer Decoder • 2 sublayer changes in decoder • Masked decoder self-attention on previously generated outputs: • Encoder-Decoder Attention, where queries come from previous decoder layer and keys and values come from output of encoder • Blocks repeated 6 times also 2/22/18 Lecture 1, Slide 16 Tips and tricks of the Transformer Details in paper and later lectures: • Byte-pair encodings • Checkpoint averaging • ADAM optimizer with learning rate changes • Dropout during training at every layer just before adding residual • Label smoothing • Auto-regressive decoding with beam search and length penalties • à Overall, they are hard to optimize and unlike LSTMs don’t usually work out of the box and don’t play well yet with other building blocks on tasks. 2/22/18 Lecture 1, Slide 17 Experimental Results for MT 2/22/18 Lecture 1, Slide 18 Experimental Results for Parsing 2/22/18 Lecture 1, Slide 19 CNNs 2/22/18 Lecture 1, Slide 20 Convolutional Neural Networks (CNNs) • Main CNN idea: What if we compute multiple vectors for every possible phrase in parallel? • Regardless of whether phrase is grammatical • Example: “the country of my birth” computes vectors for: • the country, country of, of my, my birth, the country of, country of my, of my birth, the country of my, country of my birth • Then group them afterwards (more soon) • Not very linguistically or cognitively plausible but very fast! 2/22/18 Lecture 1, Slide 21 What is convolution anyway? • 1d discrete convolution generally: • Convolution is great to extract features from images • 2d example à • Yellow and red numbers show filter weights • Green shows input Stanford UFLDL wiki 2/22/18 Lecture 1, Slide 22 Single Layer CNN • A simple variant using one convolutional layer and pooling • Based on Collobert and Weston (2011) and Kim (2014) “Convolutional Neural Networks for Sentence Classification” • Word vectors: • Sentence: (vectors concatenated) • Concatenation of words in range: • Convolutional filter: (goes over window of h words) • Could be 2 (as before) higher, e.g. 3: n Natural Language Processing (EMNLP), pages 1746–1751, Association for Computational Linguistics g extractors were trained. 2 Model The model architecture, shown in ﬁgure 1, is a slight variant of the CNN architecture of Collobert et al. (2011). Let xi 2 Rk be the k-dimensional word vector corresponding to the i-th word in the sentence. A sentence of length n (padded where 1https://code.google.com/p/word2vec/ 46 the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 n x k representation of sentence with static and non-static channels Convolutional layer with multiple filter widths and feature maps Ma Figure 1: Model architecture with two channels for an necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn, (1) where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, . . . , xi+j. A convolution operation in- volves a ﬁlter w 2 Rhk, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) that is kept s is ﬁne-tuned In the multic ure 1, each ﬁ the results a (2). The mod gle channel a 2.1 Regula For regulari penultimate the weight v Figure 1: Model arc necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn where ⊕is the concatenation operato eral, let xi:i+j refer to the concatenatio xi, xi+1, . . . , xi+j. A convolution op volves a ﬁlter w 2 Rhk, which is a window of h words to produce a new f example, a feature ci is generated from of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). sentence with static and non-static channels multiple filter widths and feature maps Figure 1: Model architecture with two channels for an necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn, (1) where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, . . . , xi+j. A convolution operation in- volves a ﬁlter w 2 Rhk, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the that is kept is ﬁne-tuned In the multi ure 1, each the results (2). The mo gle channel 2.1 Regul For regular penultimate the weight v prevents co domly drop portion p o 1.1 2/22/18 Lecture 1, Slide 23 Single layer CNN • Convolutional filter: (goes over window of h words) • Note, filter is vector! • Window size h could be 2 (as before) or higher, e.g. 3: • To compute feature for CNN layer: where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, . . . , xi+j. A convolution operation in- volves a ﬁlter w 2 Rhk, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map c = [c1, c2, . . . , cn−h+1], (3) with c 2 Rn−h+1. We then apply a max-over- ti li ti (C ll b t t l 2011) ure 1, e the resu (2). Th gle chan 2.1 R For reg penultim the wei prevent domly d portion backpro layer z ﬁlters), for outp the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 1.1 2/22/18 Lecture 1, Slide 24 Single layer CNN • Filter w is applied to all possible windows (concatenated vectors) • Sentence: • All possible windows of length h: • Result is a feature map: Figure 1: Model architecture with two channels for an necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn, (1) where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, . . . , xi+j. A convolution operation in- volves a ﬁlter w 2 Rhk, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent This ﬁlter that is kept s is ﬁne-tuned In the multic ure 1, each ﬁ the results a (2). The mod gle channel a 2.1 Regula For regulari penultimate l the weight v prevents co- domly dropp i p , i g of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map c = [c1, c2, . . . , cn−h+1], (3) with c 2 Rn−h+1. We then apply a max-over- time pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ˆ c = max{c} as the feature corresponding to this particular ﬁlter. The idea is to capture the most im- portant feature—one with the highest value—for ci = f(w · xi Here b 2 R is a bias ter function such as the hyper is applied to each possible sentence {x1:h, x2:h+1, . . a feature map c = [c1, c2, . with c 2 Rn−h+1. We time pooling operation ( over the feature map and ˆ c = max{c} as the featu particular ﬁlter. The idea i portant feature—one with each feature map. This p deals with variable senten W h d ib d th the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 1.1 3.5 … 2.4 ?????????? example, a feature ci is generated from a wi of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). Here b 2 R is a bias term and f is a non- function such as the hyperbolic tangent. This is applied to each possible window of words sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to pr a feature map c = [c1, c2, . . . , cn−h+1], with c 2 Rn−h+1. We then apply a max time pooling operation (Collobert et al., over the feature map and take the maximum ˆ c = max{c} as the feature corresponding t particular ﬁlter. The idea is to capture the mo portant feature—one with the highest value 2/22/18 Lecture 1, Slide 25 Single layer CNN • Filter w is applied to all possible windows (concatenated vectors) • Sentence: • All possible windows of length h: • Result is a feature map: example, a feature ci is generated from a wi of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). Here b 2 R is a bias term and f is a non- function such as the hyperbolic tangent. This is applied to each possible window of words sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to pr a feature map c = [c1, c2, . . . , cn−h+1], with c 2 Rn−h+1. We then apply a max time pooling operation (Collobert et al., over the feature map and take the maximum ˆ c = max{c} as the feature corresponding t particular ﬁlter. The idea is to capture the mo portant feature—one with the highest value Figure 1: Model architecture with two channels for an necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn, (1) where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words xi, xi+1, . . . , xi+j. A convolution operation in- volves a ﬁlter w 2 Rhk, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent This ﬁlter that is kept s is ﬁne-tuned In the multic ure 1, each ﬁ the results a (2). The mod gle channel a 2.1 Regula For regulari penultimate l the weight v prevents co- domly dropp i p , i g of words xi:i+h−1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map c = [c1, c2, . . . , cn−h+1], (3) with c 2 Rn−h+1. We then apply a max-over- time pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ˆ c = max{c} as the feature corresponding to this particular ﬁlter. The idea is to capture the most im- portant feature—one with the highest value—for ci = f(w · xi Here b 2 R is a bias ter function such as the hyper is applied to each possible sentence {x1:h, x2:h+1, . . a feature map c = [c1, c2, . with c 2 Rn−h+1. We time pooling operation ( over the feature map and ˆ c = max{c} as the featu particular ﬁlter. The idea i portant feature—one with each feature map. This p deals with variable senten W h d ib d th the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 1.1 3.5 … 2.4 0 0 0 0 2/22/18 Lecture 1, Slide 26 Single layer CNN: Pooling layer • New building block: Pooling • In particular: max-over-time pooling layer • Idea: capture most important activation (maximum over time) • From feature map • Pooled single number: • But we want more features! is applied to each possible window o sentence {x1:h, x2:h+1, . . . , xn−h+1: a feature map c = [c1, c2, . . . , cn−h+1 with c 2 Rn−h+1. We then apply time pooling operation (Collobert over the feature map and take the ma ˆ c = max{c} as the feature correspo particular ﬁlter. The idea is to capture portant feature—one with the highe each feature map. This pooling sch deals with variable sentence lengths We have described the process b feature is extracted from one ﬁlter ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map c = [c1, c2, . . . , cn−h+1], (3) with c 2 Rn−h+1. We then apply a max-over- time pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ˆ c = max{c} as the feature corresponding to this particular ﬁlter. The idea is to capture the most im- portant feature—one with the highest value—for each feature map. This pooling scheme naturally deals with variable sentence lengths. W h d ib d th b hi h the w prev dom porti back laye ﬁlter for o uses whe tor a rand Grad Here b 2 R is a bias term an function such as the hyperboli is applied to each possible win sentence {x1:h, x2:h+1, . . . , xn a feature map c = [c1, c2, . . . , c with c 2 Rn−h+1. We then time pooling operation (Coll over the feature map and take ˆ c = max{c} as the feature c particular ﬁlter. The idea is to portant feature—one with the each feature map. This poolin deals with variable sentence le We have described the pro feature is extracted from one uses multiple ﬁlters (with var 2/22/18 Lecture 1, Slide 27 Solution: Multiple filters • Use multiple filter weights w • Useful to have different window sizes h • Because of max pooling , length of c irrelevant • So we can have some filters that look at unigrams, bigrams, tri- grams, 4-grams, etc. o wo ds i:i+h 1 by ci = f(w · xi:i+h−1 + b). (2) Here b 2 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This ﬁlter is applied to each possible window of words in the sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produce a feature map c = [c1, c2, . . . , cn−h+1], (3) with c 2 Rn−h+1. We then apply a max-over- time pooling operation (Collobert et al., 2011) over the feature map and take the maximum value ˆ c = max{c} as the feature corresponding to this particular ﬁlter. The idea is to capture the most im- portant feature—one with the highest value—for each feature map. This pooling scheme naturally penultimate laye the weight vecto prevents co-ada domly dropping portion p of t backpropagation layer z = [ˆ c1, . ﬁlters), instead o for output unit y uses y where ◦is the e tor and r 2 Rm random variable ci = f(w · xi:i+h−1 + b). Here b 2 R is a bias term and f is a non function such as the hyperbolic tangent. Th is applied to each possible window of word sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to p a feature map c = [c1, c2, . . . , cn−h+1], with c 2 Rn−h+1. We then apply a ma time pooling operation (Collobert et al., over the feature map and take the maximum ˆ c = max{c} as the feature corresponding particular ﬁlter. The idea is to capture the m portant feature—one with the highest valu each feature map. This pooling scheme n deals with variable sentence lengths. We have described the process by whi f i d f ﬁl Th is applied to each possible window of words in t sentence {x1:h, x2:h+1, . . . , xn−h+1:n} to produ a feature map c = [c1, c2, . . . , cn−h+1], ( with c 2 Rn−h+1. We then apply a max-ove time pooling operation (Collobert et al., 201 over the feature map and take the maximum val ˆ c = max{c} as the feature corresponding to th particular ﬁlter. The idea is to capture the most im portant feature—one with the highest value—f each feature map. This pooling scheme natural deals with variable sentence lengths. We have described the process by which on feature is extracted from one ﬁlter. The mod uses multiple ﬁlters (with varying window size to obtain multiple features. These features for the penultimate layer and are passed to a fully co nected softmax layer whose output is the probab 2/22/18 Lecture 1, Slide 28 Multi-channel idea • Initialize with pre-trained word vectors (word2vec or Glove) • Start with two copies • Backprop into only one set, keep other “static” • Both channels are added to ci before max-pooling 2/22/18 Lecture 1, Slide 29 Classification after one CNN layer • First one convolution, followed by one max-pooling • To obtain final feature vector: (assuming m filters w) • Simple final softmax layer y (w · xi:i+h−1 + b). (2) bias term and f is a non-linear e hyperbolic tangent. This ﬁlter possible window of words in the h+1, . . . , xn−h+1:n} to produce 1, c2, . . . , cn−h+1], (3) 1. We then apply a max-over- ration (Collobert et al., 2011) ap and take the maximum value he feature corresponding to this e idea is to capture the most im- ne with the highest value—for This pooling scheme naturally sentence lengths. bed the process by which one d from one ﬁlter. The model penultimate layer with a constraint on l2-norm the weight vectors (Hinton et al., 2012). Drop prevents co-adaptation of hidden units by domly dropping out—i.e., setting to zero—a p portion p of the hidden units during fow backpropagation. That is, given the penultim layer z = [ˆ c1, . . . , ˆ cm] (note that here we hav ﬁlters), instead of using y = w · z + b for output unit y in forward propagation, drop uses y = w · (z ◦r) + b, where ◦is the element-wise multiplication op tor and r 2 Rm is a ‘masking’ vector of Berno random variables with probability p of being Gradients are backpropagated only through unmasked units. At test time, the learned we l d b h h ˆ 2/22/18 Lecture 1, Slide 30 Figure from Kim (2014) wait for the video and do n't rent it n x k representation of sentence with static and non-static channels Convolutional layer with multiple filter widths and feature maps Max-over-time pooling Fully connected layer with dropout and softmax output Figure 1: Model architecture with two channels for an example sentence. necessary) is represented as x1:n = x1 ⊕x2 ⊕. . . ⊕xn, (1) where ⊕is the concatenation operator. In gen- eral, let xi:i+j refer to the concatenation of words that is kept static throughout training and one tha is ﬁne-tuned via backpropagation (section 3.2). In the multichannel architecture, illustrated in ﬁg ure 1, each ﬁlter is applied to both channels and the results are added to calculate ci in equation (2) Th d l i th i i l t t th i n words (possibly zero padded) and each word vector has k dimensions 2/22/18 Lecture 1, Slide 31 Tricks to make it work better: Dropout • Idea: randomly mask/dropout/set to 0 some of the feature weights z • Create masking vector r of Bernoulli random variables with probability p (a hyperparameter) of being 1 • Delete features during training: • Reasoning: Prevents co-adaptation (overfitting to seeing specific feature constellations) • Paper: Hinton et al. 2012: Improving neural networks by preventing co-adaptation of feature detectors 2/22/18 Lecture 1, Slide 32 Tricks to make it work better: Dropout • At training time, gradients are backpropagated only through those elements of z vector for which ri = 1 • At test time, there is no dropout, so feature vectors z are larger. • Hence, we scale final vector by Bernoulli probability p • Kim (2014) reports 2 – 4% improved accuracy and ability to use very large networks without overfitting 2/22/18 Lecture 1, Slide 33 Another regularization trick • Constrain l2 norms of weight vectors of each class (row in softmax weight W(S)) to fixed number s (also a hyperparameter) • If , then rescale it so that: • Not very common 2/22/18 Lecture 1, Slide 34 All hyperparameters in Kim (2014) • Find hyperparameters based on dev set • Nonlinearity: reLu • Window filter sizes h = 3,4,5 • Each filter size has 100 feature maps • Dropout p = 0.5 • L2 constraint s for rows of softmax s = 3 • Mini batch size for SGD training: 50 • Word vectors: pre-trained with word2vec, k = 300 • During training, keep checking performance on dev set and pick highest accuracy weights for final evaluation 2/22/18 Lecture 1, Slide 35 Experiments Model MR SST-1 SST-2 Subj TREC CR MPQA CNN-rand 76.1 45.0 82.7 89.6 91.2 79.8 83.4 CNN-static 81.0 45.5 86.8 93.0 92.8 84.7 89.6 CNN-non-static 81.5 48.0 87.2 93.4 93.6 84.3 89.5 CNN-multichannel 81.1 47.4 88.1 93.2 92.2 85.0 89.4 RAE (Socher et al., 2011) 77.7 43.2 82.4 − − − 86.4 MV-RNN (Socher et al., 2012) 79.0 44.4 82.9 − − − − RNTN (Socher et al., 2013) − 45.7 85.4 − − − − DCNN (Kalchbrenner et al., 2014) − 48.5 86.8 − 93.0 − − Paragraph-Vec (Le and Mikolov, 2014) − 48.7 87.8 − − − − CCAE (Hermann and Blunsom, 2013) 77.8 − − − − − 87.2 Sent-Parser (Dong et al., 2014) 79.5 − − − − − 86.3 NBSVM (Wang and Manning, 2012) 79.4 − − 93.2 − 81.8 86.3 MNB (Wang and Manning, 2012) 79.0 − − 93.6 − 80.0 86.3 G-Dropout (Wang and Manning, 2013) 79.0 − − 93.4 − 82.1 86.1 F-Dropout (Wang and Manning, 2013) 79.1 − − 93.6 − 81.9 86.3 Tree-CRF (Nakagawa et al., 2010) 77.3 − − − − 81.4 86.1 CRF-PR (Yang and Cardie, 2014) − − − − − 82.7 − SVMS (Silva et al., 2011) − − − − 95.0 − − Table 2: Results of our CNN models against other methods. RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN: 2/22/18 Lecture 1, Slide 36 Problem with comparison? • Dropout gives 2 – 4 % accuracy improvement • Several ‘’baselines’’ didn’t use dropout • Still remarkable results and simple architecture! • Difference to window and RNN architectures we described in previous lectures: pooling, many filters and dropout • Some of these ideas can be used in RNNs too and have since been improved à more in later lectures 2/22/18 Lecture 1, Slide 37 CNN alternatives • Narrow vs wide convolution • Complex pooling schemes (over sequences) and deeper convolutional layers • Kalchbrenner et al. (2014) as . de- els ls. hat gh m- as as- rs. ure ve ler nn he de of ee. re s1 s1 ss ss c1 c5 c5 Figure 2: Narrow and wide types of convolution. The ﬁlter m has size m = 5. a sequence c 2 Rs−m+1 with j ranging from m to s. The wide type of convolution does not have requirements on s or m and yields a sequence c 2 Rs+m−1 where the index j ranges from 1 to s + m −1. Out-of-range input values si where i < 1 or i > s are taken to be zero. The result of the narrow convolution is a subsequence of the result of the wide convolution. The two types of one- dimensional convolution are illustrated in Fig. 2. The trained weights in the ﬁlter m correspond to a linguistic feature detector that learns to recog- nise a speciﬁc class of n-grams. These n-grams have size n m, where m is the width of the ﬁlter. Applying the weights m in a wide convo- lution has some advantages over applying them in tor wi 2 Rd of a word in the sentence: s = 2 4w1 . . . ws 3 5 (2) To address the problem of varying sentence lengths, the Max-TDNN takes the maximum of each row in the resulting matrix c yielding a vector of d values: cmax = 2 6 4 max(c1,:) . . . max(cd,:) 3 7 5 (3) The aim is to capture the most relevant feature, i.e. the one with the highest value, for each of the d rows of the resulting matrix c. The ﬁxed-sized vector cmax is then used as input to a fully con- nected layer for classiﬁcation. The Max-TDNN model has many desirable properties. It is sensitive to the order of the words in the sentence and it does not depend on external language-speciﬁc features such as dependency or constituency parse trees It also gives largely uni- K-Max pooling (k=3) Fully connected layer Folding Wide convolution (m=2) Dynamic k-max pooling (k= f(s) =5) Projected sentence matrix (s=7) Wide convolution (m=3) The cat sat on the red mat Figure 3: A DCNN for the seven word input sen- W d b ddi h i d 4 Th 2/22/18 Lecture 1, Slide 38 CNN application: Translation • One of the first successful neural machine translation efforts • Uses CNN for encoding and RNN for decoding • Kalchbrenner and Blunsom (2013) “Recurrent Continuous Translation Models” • Many more recent models we may cover in future lectures RCTM I P( f | e ) e e S csm 2/22/18 Lecture 1, Slide 39 Next Lectures: • Coreference resolution with Kevin • Recursive Neural Networks • Then model comparisons and tuning tricks 2/22/18 Lecture 1, Slide 40 2/22/18 Lecture 1, Slide 41 "
470,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 13: Coreference Resolution Kevin Clark Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Assignment 3 due today at 11:59pm • Final project milestone due tomorrow • ALL teams submit to Gradescope • Project mentoring: • If you haven’t received an email, Richard is your mentor • Go to his OH to discuss your project! 2 Lecture Plan: • What is Coreference ResoluJon? • MenJon DetecJon • Some LinguisJcs: Types of Reference • 3 Kinds of Coreference ResoluJon Models • Including the current state-of-the-art coreference system! 3 What is Coreference Resolu;on? 4 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty What is Coreference Resolu;on? 5 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty What is Coreference Resolu;on? 6 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty What is Coreference Resolu;on? 7 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty What is Coreference Resolu;on? 8 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty What is Coreference Resolu;on? 9 Barack Obama nominated Hillary Rodham Clinton as his secretary of state on Monday. He chose her because she had foreign aﬀairs experience as a former First Lady. • IdenJfy all men;ons that refer to the same real world enJty Applica;ons 10 • Full text understanding • informaJon extracJon, quesJon answering, summarizaJon, … • “He was born in 1961” Applica;ons 11 • Full text understanding • Machine translaJon • languages have diﬀerent features for gender, number, dropped pronouns, etc. Applica;ons 12 • Full text understanding • Machine translaJon • languages have diﬀerent features for gender, number, dropped pronouns, etc. Applica;ons 13 • Full text understanding • Machine translaJon • Dialogue Systems “Book Jckets to see James Bond” “Spectre is playing near you at 2:00 and 3:00 today. How many Jckets would you like?” “Two Jckets for the showing at three” Coreference Resolu;on is Really Diﬃcult! 14 • “She poured water from the pitcher into the cup unJl it was full” • Requires reasoning /world knowledge to solve Coreference Resolu;on is Really Diﬃcult! 15 • “She poured water from the pitcher into the cup unJl it was full” • “She poured water from the pitcher into the cup unJl it was empty” • Requires reasoning /world knowledge to solve Coreference Resolu;on is Really Diﬃcult! 16 • “She poured water from the pitcher into the cup unJl it was full” • “She poured water from the pitcher into the cup unJl it was empty” • The trophy would not ﬁt in the suitcase because it was too big. • The trophy would not ﬁt in the suitcase because it was too small. • These are called Winograd Schema Coreference Resolu;on is Really Diﬃcult! 17 • “She poured water from the pitcher into the cup unJl it was full” • “She poured water from the pitcher into the cup unJl it was empty” • The trophy would not ﬁt in the suitcase because it was too big. • The trophy would not ﬁt in the suitcase because it was too small. • These are called Winograd Schema • Recently proposed as an alternaJve to the Turing test • Turing test: how can we tell if we’ve built an AI system? A human can’t disJnguish it from a human when chafng with it. • But requires a person, people are easily fooled • If you’ve fully solved coreference, arguably you’ve solved AI Coreference Resolu;on in Two Steps 18 1. Detect the menJons (easy) 2. Cluster the menJons (hard) “[I] voted for [Nader] because [he] was most aligned with [[my] values],” [she] said • menJons can be nested! “[I] voted for [Nader] because [he] was most aligned with [[my] values],” [she] said Men;on Detec;on 19 • MenJon: span of text referring to some enJty • Three kinds of menJons: 1. Pronouns • I, your, it, she, him, etc. 2. Named enJJes • People, places, etc. 3. Noun phrases • “a dog,” “the big ﬂuﬀy cat stuck in the tree” Men;on Detec;on 20 • Span of text referring to some enJty • For detecJon: use other NLP systems 1. Pronouns • Use a part-of-speech tagger 2. Named enJJes • Use a NER system (like hw3) 3. Noun phrases • Use a consJtuency parser (next lecture!) Men;on Detec;on: Not so Simple 21 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny Men;on Detec;on: Not so Simple 22 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny • Every student Men;on Detec;on: Not so Simple 23 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny • Every student • No student Men;on Detec;on: Not so Simple 24 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny • Every student • No student • The best donut in the world Men;on Detec;on: Not so Simple 25 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny • Every student • No student • The best donut in the world • 100 miles Men;on Detec;on: Not so Simple 26 • Marking all pronouns, named enJJes, and NPs as menJons over-generates menJons • Are these menJons? • It is sunny • Every student • No student • The best donut in the world • 100 miles • Some gray area in deﬁning “menJon”: have to pick a convenJon and go with it How to deal with these bad men;ons? 27 • Could train a classiﬁer to ﬁlter out spurious menJons • Much more common: keep all menJons as “candidate menJons” • Aoer your coreference system is done running discard all singleton menJons (i.e., ones that have not been marked as coreference with anything else) Can we avoid a pipelined system? 28 • We could instead train a classiﬁer speciﬁcally for menJon detecJon instead of using a POS tagger, NER system, and parser. • Or even jointly do menJon-detecJon and coreference resoluJon end-to-end instead of in two steps • Will cover later in this lecture! On to Coreference! First, some linguis;cs 29 • Coreference is when two menJons refer to the same enJty in the world • Barack Obama traveled to … Obama • Another kind of reference is anaphora: when a term (anaphor) refers to another term (antecedent) and the interpretaJon of the anaphor is in some way determined by the interpretaJon of the antecedent • Barack Obama said he would sign the bill. anaphor antecedent • Coreference with named enJJes • Anaphora 30 Anaphora vs Coreference text world Barack Obama he Barack Obama Obama text world Anaphora vs. Coreference 31 • Not all anaphoric relaJons are coreferenJal We went to see a concert last night. The ;ckets were really expensive. • This is referred to as bridging anaphora. bridging anaphora Barack Obama … Obama pronominal anaphora coreference anaphora 32 • Usually the antecedent comes before the anaphor (e.g., a pronoun), but not always Cataphora 33 Cataphora “From the corner of the divan of Persian saddle- bags on which he was lying, smoking, as was his custom, innumerable cigareEes, Lord Henry WoEon could just catch the gleam of the honey- sweet and honey-coloured blossoms of a laburnum…” (Oscar Wilde – The Picture of Dorian Gray) 34 Cataphora “From the corner of the divan of Persian saddle- bags on which he was lying, smoking, as was his custom, innumerable cigareEes, Lord Henry WoEon could just catch the gleam of the honey- sweet and honey-coloured blossoms of a laburnum…” (Oscar Wilde – The Picture of Dorian Gray) Next Up: Three Kinds of Coreference Models 35 • MenJon Pair • MenJon Ranking • Clustering  Coreference Models: Men;on Pair 36 “I voted for Nader because he was most aligned with my values,” she said. I Nader he my she Coreference Cluster 1 Coreference Cluster 2 • Train a binary classiﬁer that assigns every pair of menJons a probability of being coreferent: • e.g., for “she” look at all candidate antecedents (previously occurring menJons) and decide which are coreferent with it Coreference Models: Men;on Pair 37 I Nader he my she “I voted for Nader because he was most aligned with my values,” she said. coreferent with she? Coreference Models: Men;on Pair 38 I Nader he my Positive examples: want to be near 1 she “I voted for Nader because he was most aligned with my values,” she said. • Train a binary classiﬁer that assigns every pair of menJons a probability of being coreferent: • e.g., for “she” look at all candidate antecedents (previously occurring menJons) and decide which are coreferent with it Coreference Models: Men;on Pair 39 I Nader he my Negative examples: want to be near 0 she “I voted for Nader because he was most aligned with my values,” she said. • Train a binary classiﬁer that assigns every pair of menJons a probability of being coreferent: • e.g., for “she” look at all candidate antecedents (previously occurring menJons) and decide which are coreferent with it • N menJons in a document • yij = 1 if menJons mi and mj are coreferent, -1 if otherwise • Just train with regular cross-entropy loss (looks a bit diﬀerent because it is binary classiﬁcaJon) Men;on Pair Training 40 Iterate through menJons Iterate through candidate antecedents (previously occurring menJons) Coreferent menJons pairs should get high probability, others should get low probability Men;on Pair Test Time 41 I Nader he my she • Coreference resoluJon is a clustering task, but we are only scoring pairs of menJons… what to do? Men;on Pair Test Time 42 • Coreference resoluJon is a clustering task, but we are only scoring pairs of menJons… what to do? • Pick some threshold (e.g., 0.5) and add coreference links between menJon pairs where is above the threshold I Nader he my she “I voted for Nader because he was most aligned with my values,” she said. Men;on Pair Test Time 43 I Nader he my she “I voted for Nader because he was most aligned with my values,” she said. Even though the model did not predict this coreference link, I and my are coreferent due to transitivity • Coreference resoluJon is a clustering task, but we are only scoring pairs of menJons… what to do? • Pick some threshold (e.g., 0.5) and add coreference links between menJon pairs where is above the threshold • Take the transiJve closure to get the clustering Men;on Pair Test Time 44 I Nader he my she “I voted for Nader because he was most aligned with my values,” she said. Adding this extra link would merge everything into one big coreference cluster! • Coreference resoluJon is a clustering task, but we are only scoring pairs of menJons… what to do? • Pick some threshold (e.g., 0.5) and add coreference links between menJon pairs where is above the threshold • Take the transiJve closure to get the clustering Men;on Pair Models: Disadvantage 45 • Suppose we have a long document with the following menJons • Ralph Nader … he … his … him … <several paragraphs> … voted for Nader because he … Ralph Nader he his him Nader almost impossible he Relatively easy Men;on Pair Models: Disadvantage 46 • Suppose we have a long document with the following menJons • Ralph Nader … he … his … him … <several paragraphs> … voted for Nader because he … Ralph Nader he his him Nader almost impossible he Relatively easy • Many menJons only have one clear antecedent • But we are asking the model to predict all of them • SoluJon: instead train the model to predict only one antecedent for each menJon • More linguisJcally plausible • Assign each menJon its highest scoring candidate antecedent according to the model • Dummy NA menJon allows model to decline linking the current menJon to anything Coreference Models: Men;on Ranking 47 NA I Nader he my best antecedent for she? she • Assign each menJon its highest scoring candidate antecedent according to the model • Dummy NA menJon allows model to decline linking the current menJon to anything Coreference Models: Men;on Ranking 48 NA I Nader he my she Positive examples: model has to assign a high probability to either one (but not necessarily both) • Assign each menJon its highest scoring candidate antecedent according to the model • Dummy NA menJon allows model to decline linking the current menJon to anything Coreference Models: Men;on Ranking 49 NA I Nader he my best antecedent for she? p(NA, she) = 0.1 p(I, she) = 0.5 p(Nader, she) = 0.1 p(he, she) = 0.1 p(my, she) = 0.2 Apply a softmax over the scores for candidate antecedents so probabilities sum to 1 she • Assign each menJon its highest scoring candidate antecedent according to the model • Dummy NA menJon allows model to decline linking the current menJon to anything Coreference Models: Men;on Ranking 50 NA I Nader he my p(NA, she) = 0.1 p(I, she) = 0.5 p(Nader, she) = 0.1 p(he, she) = 0.1 p(my, she) = 0.2 Apply a softmax over the scores for candidate antecedents so probabilities sum to 1 she only add highest scoring coreference link i−1 X j=1 (yij = 1)p(mj, mi) J = −log 0 @ N Y i=2 i−1 X j=1 (yij = 1)p(mj, mi) 1 A • We want the current menJon mj to be linked to any one of the candidate antecedents it’s coreferent with. • MathemaJcally, we want to maximize this probability: Coreference Models: Training 51 Iterate through candidate antecedents (previously occurring menJons) For ones that are coreferent to mj… …we want the model to assign a high probability i−1 X j=1 (yij = 1)p(mj, mi) J = −log 0 @ N Y i=2 i−1 X j=1 (yij = 1)p(mj, mi) 1 A • We want the current menJon mj to be linked to any one of the candidate antecedents it’s coreferent with. • MathemaJcally, we want to maximize this probability: • The model could produce 0.9 probability for one of the correct antecedents and low probability for everything else, and the sum will sJll be large Coreference Models: Training 52 Iterate through candidate antecedents (previously occurring menJons) For ones that are coreferent to mj… …we want the model to assign a high probability i−1 X j=1 (yij = 1)p(mj, mi) J = −log 0 @ N Y i=2 i−1 X j=1 (yij = 1)p(mj, mi) 1 A • We want the current menJon mj to be linked to any one of the candidate antecedents it’s coreferent with. • MathemaJcally, we want to maximize this probability: • Turning this into a loss funcJon: Coreference Models: Training 53 Usual trick of taking negaJve log to go from likelihood to loss Iterate over all the menJons in the document i−1 X j=1 (yij = 1)p(mj, mi) J = −log 0 @ N Y i=2 i−1 X j=1 (yij = 1)p(mj, mi) 1 A J = N X i=2 −log 0 @ i−1 X j=1 (yij = 1)p(mj, mi) 1 A • Prety much the same as menJon-pair model except each menJon is assigned only one antecedent Men;on Ranking Models: Test Time 54 I Nader he my she NA • Prety much the same as menJon-pair model except each menJon is assigned only one antecedent Men;on Ranking Models: Test Time 55 I Nader he my she NA How do we compute the probabili;es? 56 1. Non-neural staJsJcal classiﬁer 2. Simple neural network 3. More advanced model using LSTMs, atenJon 1. Non-Neural Coref Model: Features 57 • Person/Number/Gender agreement • Jack gave Mary a gio. She was excited. • SemanJc compaJbility • … the mining conglomerate … the company … • Certain syntacJc constraints • John bought him a new car. [him can not be John] • More recently menJoned enJJes preferred for referenced • John went to a movie. Jack went as well. He was not busy. • GrammaJcal Role: Prefer enJJes in the subject posiJon • John went to a movie with Jack. He was not busy. • Parallelism: • John went with Jack to a movie. Joe went with him to a bar. • … 2. Neural Coref Model • Standard feed-forward neural network • Input layer: word embeddings and a few categorical features 58 Candidate Antecedent Embeddings Candidate Antecedent Features Mention Features Mention Embeddings Hidden Layer h2 Input Layer h0 Hidden Layer h1 ReLU(W1h0 + b1) ReLU(W2h1 + b2) ReLU(W3h2 + b3) Additional Features Hidden Layer h3 Score s W4h3 + b4 2. Neural Coref Model: Inputs • Embeddings • Previous two words, ﬁrst word, last word, head word, … of each menJon • The head word is the “most important” word in the menJon – you can ﬁnd it using a parser. e.g., The ﬂuﬀy cat stuck in the tree • SJll need some other features: • Distance • Document genre • Speaker informaJon 59 3. End-to-end Model 60 • Current state-of-the-art model for coreference resoluJon (Lee et al., EMNLP 2017) • MenJon ranking model • Improvements over simple feed—forward NN • Use an LSTM • Use atenJon • Do menJon detecJon and coreference end-to-end • No menJon detecJon step! • Instead consider every span of text (up to a certain length) as a candidate menJon • a span is just a conJguous sequence of words 3. End-to-end Model 61 • First embed the words in the document using a word embedding matrix and a character-level CNN General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model 62 • Then run a bidirecJonal LSTM over the document General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model 63 • Next, represent each span of text i going from START(i) to END(i) as a vector General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model 64 • Next, represent each span of text i going from START(i) to END(i) as a vector • General, General Electric, General Electric said, … Electric, Electric said, … will all get its own vector representaJon General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model • Next, represent each span of text i going from START(i) to END(i) as a vector. Span representaJon: , END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vectors in span i. The weights ai,t are automatically learned and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include h b d i ∗ d In the traini is observed. optimize the antecedents l where GOLD ter containin to a gold clu pruned, GOL By optim rally learns initial pruni mentions rec quickly leve ate credit as General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model • Next, represent each span of text i going from START(i) to END(i) as a vector. For example, for “the postal service” Span representaJon: , END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vectors in span i. The weights ai,t are automatically learned and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include h b d i ∗ d In the traini is observed. optimize the antecedents l where GOLD ter containin to a gold clu pruned, GOL By optim rally learns initial pruni mentions rec quickly leve ate credit as General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model • Next, represent each span of text i going from START(i) to END(i) as a vector. For example, for “the postal service” Span representaJon: , END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vectors in span i. The weights ai,t are automatically learned and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include h b d i ∗ d In the traini is observed. optimize the antecedents l where GOLD ter containin to a gold clu pruned, GOL By optim rally learns initial pruni mentions rec quickly leve ate credit as BILSTM hidden states for span’s start and end General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model • Next, represent each span of text i going from START(i) to END(i) as a vector. For example, for “the postal service” Span representaJon: , END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vectors in span i. The weights ai,t are automatically learned and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include h b d i ∗ d In the traini is observed. optimize the antecedents l where GOLD ter containin to a gold clu pruned, GOL By optim rally learns initial pruni mentions rec quickly leve ate credit as AtenJon-based representaJon (details next slide) of the words in the span BILSTM hidden states for span’s start and end General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model • Next, represent each span of text i going from START(i) to END(i) as a vector. For example, for “the postal service” Span representaJon: , END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vectors in span i. The weights ai,t are automatically learned and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include h b d i ∗ d In the traini is observed. optimize the antecedents l where GOLD ter containin to a gold clu pruned, GOL By optim rally learns initial pruni mentions rec quickly leve ate credit as AtenJon-based representaJon (details next slide) of the words in the span AddiJonal features BILSTM hidden states for span’s start and end • is an atenJon-weighted average of the word embeddings in the span ,δ = ot,δ ◦tanh(ct,δ) ∗ t = [ht,1, ht,−1] ∈{−1, 1} indicates the directionality of TM, and x∗ t is the concatenated output idirectional LSTM. We use independent for every sentence, since cross-sentence was not helpful in our experiments. ctic heads are typically included as fea- n previous systems (Durrett and Klein, ark and Manning, 2016b,a). Instead of re- syntactic parses, our model learns a task- notion of headedness using an attention sm (Bahdanau et al., 2014) over words in n: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" exp(αk) with the highest mention scores and co up to K antecedents for each. We a non-crossing bracketing structures wit suppression scheme.2 We accept sp creasing order of the mention scores, un considering span i, there exists a pre cepted span j such that START(i) < S END(i) < END(j) ∨START(j) < ST END(j) < END(i). Despite these aggressive pruning str maintain a high recall of gold mention periments (over 92% when λ = 0.4). For the remaining mentions, the joi tion of antecedents for each document i in a forward pass over a single computa The ﬁnal prediction is the clustering p the most likely conﬁguration. 6 Learning In the training data, only clustering i is observed Since the antecedents are AtenJon scores dot product of weight vector and transformed hidden state mation is concatenated to tation gi of span i: x∗ END(i), ˆ xi, φ(i)] recurrent span repre- oposed for question- 16), which only include tations x∗ START(i) and e soft head word vector (i) encoding the size of odel described above is length T. To maintain g p p g to a gold cluster or all gold antecedents have been pruned, GOLD(i) = {ϵ}. By optimizing this objective, the model natu- rally learns to prune spans accurately. While the initial pruning is completely random, only gold mentions receive positive updates. The model can quickly leverage this learning signal for appropri- ate credit assignment to the different factors, such as the mention scores sm used for pruning. Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over- all model with respect to mention detection. It also prevents the span pruning from introducing 2The ofﬁcial CoNLL-2012 evaluation only considers pre- dictions without crossing mentions to be valid. Enforcing this consistency is not inherently necessary in our model. General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for 3. End-to-end Model 3. End-to-end Model ,δ = ot,δ ◦tanh(ct,δ) ∗ t = [ht,1, ht,−1] ∈{−1, 1} indicates the directionality of TM, and x∗ t is the concatenated output idirectional LSTM. We use independent for every sentence, since cross-sentence was not helpful in our experiments. ctic heads are typically included as fea- n previous systems (Durrett and Klein, ark and Manning, 2016b,a). Instead of re- syntactic parses, our model learns a task- notion of headedness using an attention sm (Bahdanau et al., 2014) over words in n: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" exp(αk) with the highest mention scores and co up to K antecedents for each. We a non-crossing bracketing structures wit suppression scheme.2 We accept sp creasing order of the mention scores, un considering span i, there exists a pre cepted span j such that START(i) < S END(i) < END(j) ∨START(j) < ST END(j) < END(i). Despite these aggressive pruning str maintain a high recall of gold mention periments (over 92% when λ = 0.4). For the remaining mentions, the joi tion of antecedents for each document i in a forward pass over a single computa The ﬁnal prediction is the clustering p the most likely conﬁguration. 6 Learning In the training data, only clustering i is observed Since the antecedents are ot,δ σ(Wo[xt, ht+δ,δ] + bo) ! ct,δ = tanh(Wc[xt, ht+δ,δ] + bc) ct,δ = ft,δ ◦! ct,δ + (1 −ft,δ) ◦ct+δ,δ ht,δ = ot,δ ◦tanh(ct,δ) x∗ t = [ht,1, ht,−1] where δ ∈{−1, 1} indicates the directionality of each LSTM, and x∗ t is the concatenated output of the bidirectional LSTM. We use independent LSTMs for every sentence, since cross-sentence context was not helpful in our experiments. Syntactic heads are typically included as fea- tures in previous systems (Durrett and Klein, 2013; Clark and Manning, 2016b,a). Instead of re- lying on syntactic parses, our model learns a task- speciﬁc notion of headedness using an attention mechanism (Bahdanau et al., 2014) over words in each span: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" ai,t · xt compute their unary m ﬁned in Section 4). To of spans to consider, w with the highest mentio up to K antecedents f non-crossing bracketin suppression scheme.2 creasing order of the m considering span i, th cepted span j such tha END(i) < END(j) ∨ END(j) < END(i). Despite these aggres maintain a high recall periments (over 92% w For the remaining m tion of antecedents for in a forward pass over The ﬁnal prediction is the most likely conﬁgu 6 Learning In the training data, o is observed. Since the optimize the marginal antecedents implied by AtenJon scores AtenJon distribuJon just a soomax over atenJon scores for the span dot product of weight vector and transformed hidden state General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for • is an atenJon-weighted average of the word embeddings in the span mation is concatenated to tation gi of span i: x∗ END(i), ˆ xi, φ(i)] recurrent span repre- oposed for question- 16), which only include tations x∗ START(i) and e soft head word vector (i) encoding the size of odel described above is length T. To maintain g p p g to a gold cluster or all gold antecedents have been pruned, GOLD(i) = {ϵ}. By optimizing this objective, the model natu- rally learns to prune spans accurately. While the initial pruning is completely random, only gold mentions receive positive updates. The model can quickly leverage this learning signal for appropri- ate credit assignment to the different factors, such as the mention scores sm used for pruning. Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over- all model with respect to mention detection. It also prevents the span pruning from introducing 2The ofﬁcial CoNLL-2012 evaluation only considers pre- dictions without crossing mentions to be valid. Enforcing this consistency is not inherently necessary in our model. 3. End-to-end Model ,δ = ot,δ ◦tanh(ct,δ) ∗ t = [ht,1, ht,−1] ∈{−1, 1} indicates the directionality of TM, and x∗ t is the concatenated output idirectional LSTM. We use independent for every sentence, since cross-sentence was not helpful in our experiments. ctic heads are typically included as fea- n previous systems (Durrett and Klein, ark and Manning, 2016b,a). Instead of re- syntactic parses, our model learns a task- notion of headedness using an attention sm (Bahdanau et al., 2014) over words in n: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" exp(αk) with the highest mention scores and co up to K antecedents for each. We a non-crossing bracketing structures wit suppression scheme.2 We accept sp creasing order of the mention scores, un considering span i, there exists a pre cepted span j such that START(i) < S END(i) < END(j) ∨START(j) < ST END(j) < END(i). Despite these aggressive pruning str maintain a high recall of gold mention periments (over 92% when λ = 0.4). For the remaining mentions, the joi tion of antecedents for each document i in a forward pass over a single computa The ﬁnal prediction is the clustering p the most likely conﬁguration. 6 Learning In the training data, only clustering i is observed Since the antecedents are ot,δ σ(Wo[xt, ht+δ,δ] + bo) ! ct,δ = tanh(Wc[xt, ht+δ,δ] + bc) ct,δ = ft,δ ◦! ct,δ + (1 −ft,δ) ◦ct+δ,δ ht,δ = ot,δ ◦tanh(ct,δ) x∗ t = [ht,1, ht,−1] where δ ∈{−1, 1} indicates the directionality of each LSTM, and x∗ t is the concatenated output of the bidirectional LSTM. We use independent LSTMs for every sentence, since cross-sentence context was not helpful in our experiments. Syntactic heads are typically included as fea- tures in previous systems (Durrett and Klein, 2013; Clark and Manning, 2016b,a). Instead of re- lying on syntactic parses, our model learns a task- speciﬁc notion of headedness using an attention mechanism (Bahdanau et al., 2014) over words in each span: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" ai,t · xt compute their unary m ﬁned in Section 4). To of spans to consider, w with the highest mentio up to K antecedents f non-crossing bracketin suppression scheme.2 creasing order of the m considering span i, th cepted span j such tha END(i) < END(j) ∨ END(j) < END(i). Despite these aggres maintain a high recall periments (over 92% w For the remaining m tion of antecedents for in a forward pass over The ﬁnal prediction is the most likely conﬁgu 6 Learning In the training data, o is observed. Since the optimize the marginal antecedents implied by where δ ∈{−1, 1} indicates the direction each LSTM, and x∗ t is the concatenated of the bidirectional LSTM. We use indep LSTMs for every sentence, since cross-se context was not helpful in our experiments Syntactic heads are typically included tures in previous systems (Durrett and 2013; Clark and Manning, 2016b,a). Instea lying on syntactic parses, our model learns speciﬁc notion of headedness using an at mechanism (Bahdanau et al., 2014) over w each span: αt = wα · FFNNα(x∗ t ) ai,t = exp(αt) END(i) "" k=START(i) exp(αk) ˆ xi = END(i) "" t=START(i) ai,t · xt where ˆ xi is a weighted sum of word vec span i. The weights ai,t are automatically AtenJon scores AtenJon distribuJon Final representaJon just a soomax over atenJon scores for the span AtenJon-weighted sum of word embeddings dot product of weight vector and transformed hidden state General Electric said the Postal Service contacted the company General Electric + Electric said the + the Postal Service + Service contacted the + the company + Mention score (sm) Span representation (g) Span head (ˆ x) Bidirectional LSTM (x∗) Word & character embedding (x) e 1: First step of the end-to-end coreference resolution model, which computes embedding r tions of spans for scoring potential entity mentions. Low-scoring spans are pruned, so that o geable number of spans is considered for coreference decisions. In general, the model conside ble spans up to a maximum width, but we depict here only a small subset. ax (P(y | D)) above are computed via standard feed for • is an atenJon-weighted average of the word embeddings in the span mation is concatenated to tation gi of span i: x∗ END(i), ˆ xi, φ(i)] recurrent span repre- oposed for question- 16), which only include tations x∗ START(i) and e soft head word vector (i) encoding the size of odel described above is length T. To maintain g p p g to a gold cluster or all gold antecedents have been pruned, GOLD(i) = {ϵ}. By optimizing this objective, the model natu- rally learns to prune spans accurately. While the initial pruning is completely random, only gold mentions receive positive updates. The model can quickly leverage this learning signal for appropri- ate credit assignment to the different factors, such as the mention scores sm used for pruning. Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over- all model with respect to mention detection. It also prevents the span pruning from introducing 2The ofﬁcial CoNLL-2012 evaluation only considers pre- dictions without crossing mentions to be valid. Enforcing this consistency is not inherently necessary in our model. 3. End-to-end Model • Why include all these diﬀerent terms in the span? p g , y and correlate strongly with traditional deﬁnitions of head words as we will see in Section 9.2. The above span information is concatenated to produce the ﬁnal representation gi of span i: gi = [x∗ START(i), x∗ END(i), ˆ xi, φ(i)] This generalizes the recurrent span repre- sentations recently proposed for question- answering (Lee et al., 2016), which only include the boundary representations x∗ START(i) and x∗ END(i). We introduce the soft head word vector ˆ xi and a feature vector φ(i) encoding the size of span i. 5 Inference The size of the full model described above is O(T 4) in the document length T. To maintain where GOLD(i) is ter containing spa to a gold cluster o pruned, GOLD(i) By optimizing rally learns to pru initial pruning is mentions receive quickly leverage t ate credit assignm as the mention sco Fixing score of removes a spuriou all model with re also prevents the 2The ofﬁcial CoN dictions without cross consistency is not inh hidden states for span’s start and end Represents the context to the leW and right of the span AtenJon-based representaJon Represents the span itself AddiJonal features Represents other informa;on not in the text 3. End-to-end Model • Lastly, score every pair of spans to decide if they are coreferent menJons nd Klein, Manning, is most son over ions and ules to re- alized reg- for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. 5), or (4) tt and Klein, nd Manning, ach is most reason over mentions and use rules to re- lexicalized reg- context is unambiguous. There are three factors for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. Are spans i and j coreferent menJons? Is i a menJon? Is j a menJon? Do they look coreferent? 3. End-to-end Model • Lastly, score every pair of spans to decide if they are coreferent menJons nd Klein, Manning, is most son over ions and ules to re- alized reg- for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. 5), or (4) tt and Klein, nd Manning, ach is most reason over mentions and use rules to re- lexicalized reg- context is unambiguous. There are three factors for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. Are spans i and j coreferent menJons? Is i a menJon? Is j a menJon? Do they look coreferent? aid the Postal Service contacted the company rence resolution model, which computes embedding repre- ty mentions. Low-scoring spans are pruned, so that only a or coreference decisions. In general, the model considers all we depict here only a small subset. ny ervice) above are computed via standard feed-forward neural networks: sm(i) = wm · FFNNm(gi) sa(i, j) = wa · FFNNa([gi, gj, gi ◦gj, φ(i, j)]) where · denotes the dot product, ◦denotes element-wise multiplication, and FFNN denotes a • Scoring funcJons take the span representaJons as input 3. End-to-end Model • Lastly, score every pair of spans to decide if they are coreferent menJons nd Klein, Manning, is most son over ions and ules to re- alized reg- for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. 5), or (4) tt and Klein, nd Manning, ach is most reason over mentions and use rules to re- lexicalized reg- context is unambiguous. There are three factors for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a men- tion, and (3) whether j is an antecedent of i: s(i, j) = # 0 j = ϵ sm(i) + sm(j) + sa(i, j) j ̸= ϵ Here sm(i) is a unary score for span i being a men- tion, and sa(i, j) is pairwise score for span j being an antecedent of span i. Are spans i and j coreferent menJons? Is i a menJon? Is j a menJon? Do they look coreferent? aid the Postal Service contacted the company rence resolution model, which computes embedding repre- ty mentions. Low-scoring spans are pruned, so that only a or coreference decisions. In general, the model considers all we depict here only a small subset. ny ervice) above are computed via standard feed-forward neural networks: sm(i) = wm · FFNNm(gi) sa(i, j) = wa · FFNNa([gi, gj, gi ◦gj, φ(i, j)]) where · denotes the dot product, ◦denotes element-wise multiplication, and FFNN denotes a • Scoring funcJons take the span representaJons as input include mulJplicaJve interacJons between the representaJons again, we have some extra features 3. End-to-end Model • Intractable to score every pair of spans • O(T^2) spans of text in a document (T is the number of words) • O(T^4) runJme! • So have to do lots of pruning to make work (only consider a few of the spans that are likely to be menJons) • AtenJon learns which words are important in a menJon (a bit like head words) 1 (A ﬁre in a Bangladeshi garment factory) has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee (the blaze) in the four-story building. A ﬁre in (a Bangladeshi garment factory) has left at least 37 people dead and 100 hospitalized. Most of the deceased were killed in the crush as workers tried to ﬂee the blaze in (the four-story building). 2 We are looking for (a region of central Italy bordering the Adriatic Sea). (The area) is mostly mountainous and includes Mt. Corno, the highest peak of the Apennines. (It) also includes a lot of sheep, good clean-living, healthy sheep, and an Italian entrepreneur has an idea about how to make a little money of them. 3 (The ﬂight attendants) have until 6:00 today to ratify labor concessions. (The pilots’) union and ground crew did so yesterday Last Coreference Approach: Clustering-Based 78 • Coreference is a clustering task, let’s use a clustering algorithm! • In parJcular we will use agglomeraJve clustering • Start with each menJon in it’s own singleton cluster • Merge a pair of clusters at each step • Use a model to score which cluster merges are good Coreference Models: Clustering-Based 79 Google recently … the company announced Google Plus ... the product features ... Coreference with Agglomera3ve Clustering 4 Google the company Cluster 1 Google Plus the product Cluster 2 Cluster 3 Cluster 4 Coreference Models: Clustering-Based 80 Google recently … the company announced Google Plus ... the product features ... Coreference with Agglomera3ve Clustering 4 Google the company Cluster 1 Google Plus the product Cluster 2 Cluster 3 Cluster 4 Coreference with Agglomera3ve Clustering 6 Google the company Cluster 1 Google Plus the product Cluster 2 Cluster 3 Cluster 4 ✔ merge Google the company Cluster 1 Google Plus the product Cluster 2 Cluster 3 ✔ merge Google the company Cluster 1 Google Plus the product Cluster 2 ✖ do not merge s(c1, c2) = 5 s(c2, c3) = 4 s(c1, c2) = -3 Coreference Models: Clustering-Based 81 The Advantage of Clustering: Exploi3ng En3ty-level Informa3on 8 Google the company Cluster 1 Google Plus the product Cluster 2 Google Google Plus ? coreferent Mention-pair decision is difﬁcult Cluster-pair decision is easier ? coreferent Clustering Model Architecture 82 Neural Network Architecture 12 Merge clusters c1 = {Google, the company} and c2 = {Google Plus, the product} ? s(MERGE[c1,c2]) Men3on Pairs Men3on-Pair Representa3ons Cluster-Pair Representa3on Score (Google, Google Plus) (Google, the product) (the company, Google Plus) (the company, the product) From Clark & Manning, 2016 Me! Clustering Model Architecture 83 • First produce a vector for each pair of menJons • e.g., the output of the hidden layer in the feedforward neural network model Producing Cluster-Pair Representa3ons First produces representaHons for relevant menHon pairs using the menHon-pair-encoder. Mention-Pair Representations !! ! c2 c1 Mention-Pair Encoder !! ! !! ! !! ! Clustering Model Architecture 84 • Then apply a pooling operaJon over the matrix of menJon-pair representaJons to get a cluster-pair representaJon Producing Cluster-Pair Representa3ons 8 Then applies a pooling operaHon over the matrix of menHon- pair representaHons (we use max and average pooling) Cluster-Pair Representation Mention-Pair Representations Pooling !! ! c2 c1 Mention-Pair Encoder !! ! !! ! !! ! max avg rc(c1, c2) Rm(c1, c2) Clustering Model Architecture 85 • Score the candidate cluster merge by taking the dot product of the representaJon with a weight vector Producing Cluster-Pair Representa3ons 18 • Then applies a pooling operaHon over the matrix of menHon- pair representaHons (we use max and average pooling) Cluster-Pair Representation Mention-Pair Representations Pooling !! ! c2 c1 Mention-Pair Encoder !! ! !! ! !! ! max avg rc(c1, c2) Rm(c1, c2) Clustering Model: Training 86 • Current candidate cluster merges depend on previous ones it already made • So can’t use regular supervised learning • Instead use something like Reinforcement Learning to train the model • Reward for each merge: the change in a coreference evaluaJon metric Coreference Evalua;on 87 • Many diﬀerent metrics: MUC, CEAF, LEA, B-CUBED, BLANC • Ooen report the average over a few diﬀerent metrics System Cluster 1 System Cluster 2 Gold Cluster 1 Gold Cluster 2 Coreference Evalua;on 88 • An example: B-cubed • For each menJon, compute a precision and a recall System Cluster 1 System Cluster 2 Gold Cluster 1 Gold Cluster 2 P = 4/5 R= 4/6 Coreference Evalua;on 89 • An example: B-cubed • For each menJon, compute a precision and a recall System Cluster 1 System Cluster 2 Gold Cluster 1 Gold Cluster 2 P = 4/5 R= 4/6 P = 1/5 R= 1/3 Coreference Evalua;on 90 • An example: B-cubed • For each menJon, compute a precision and a recall • Then average the individual Ps and Rs System Cluster 1 System Cluster 2 Gold Cluster 1 Gold Cluster 2 P = 2/4 R= 2/3 P = 4/5 R= 4/6 P = 1/5 R= 1/3 P = 2/4 R= 2/6 P = [4(4/5) + 1(1/5) + 2(2/4) + 2(2/4)] / 9 = 0.6 Coreference Evalua;on 91 100% Precision, 33% Recall 50% Precision, 100% Recall, 92 System Performance • OntoNotes dataset: ~3000 documents labeled by humans • English and Chinese data • Report an F1 score averaged over 3 coreference metrics 93 Model English Chinese Lee et al. (2010) ~55 ~50 Chen & Ng (2012) [CoNLL 2012 Chinese winner] 54.5 57.6 Fernandes (2012) [CoNLL 2012 English winner] 60.7 51.6 Wiseman et al. (2015) 63.3 — Clark & Manning (2016) 65.4 63.7 Lee et al. (2017) 67.2 -- System Performance Rule-based system, used to be state-of-the-art! Non-neural machine learning models Neural menJon ranker End-to-end neural menJon ranker Neural clustering model Where do neural scoring models help? • Especially with NPs and named enJJes with no string matching. Neural vs non-neural scores: 18.9 F1 vs 10.7 F1 on this type compared to 68.7 vs 66.1 F1 These kinds of coreference are hard and the scores are sJll low! 94 Anaphor Antecedent the country’s leoist rebels the guerillas the company the New York ﬁrm 216 sailors from the ``USS cole’’ the crew the gun the riﬂe Example Wins Conclusion • Coreference is a useful, challenging, and linguisJcally interesJng task • Many diﬀerent kinds of coreference resoluJon systems • Systems are gefng beter rapidly, largely due to beter neural models • But overall, results are sJll not amazing • Try out a coreference system yourself! htps://huggingface.co/coref/ Lecture 1, Slide 95 "
471,"Natural Language Processing with Deep Learning CS224N/Ling284 Richard Socher Lecture 14: Tree Recursive Neural Networks and Constituency Parsing Lecture Plan 1. Motivation: Compositionality and Recursion 2. Structure prediction with simple Tree RNN: Parsing 3. Backpropagation through Structure 4. More complex units Reminders/comments: Learn up on GPUs, Azure, Docker Ass 4: Get something working, using a GPU for milestone Final project discussions – come meet with us! OH today after class. You have to come to every OH. No additional feedback beyond OH. Nothing on gradescope. 3/1/18 1 1. The spectrum of language in CS 2 3/1/18 Semantic interpretation of language – Not just word vectors How can we know when larger units are similar in meaning? • The snowboarder is leaping over a mogul • A person on a snowboard jumps into the air People interpret the meaning of larger text units – entities, descriptive terms, facts, arguments, stories – by semantic composition of smaller elements 3/1/18 3 Compositionality Language understanding – & Artificial Intelligence – requires being able to understand bigger things from knowing about smaller parts 3/1/18 6 7 Are languages recursive? • Cognitively somewhat debatable • But: recursion is natural for describing language • [The man from [the company that you spoke with about [the project] yesterday]] • noun phrase containing a noun phrase containing a noun phrase • Arguments for now: 1) Helpful in disambiguation: 3/1/18 Lecture 1, Slide 8 Is recursion useful? 2) Helpful for some tasks to refer to specific phrases: • John and Jane went to a big festival. They enjoyed the trip and the music there. • “they”: John and Jane • “the trip”: went to a big festival • “there”: big festival 3) Works better for some tasks to use grammatical tree structure • It’s a powerful prior for language structure 3/1/18 Lecture 1, Slide 9 Building on Word Vector Space Models 10 x2 x1 0 1 2 3 4 5 6 7 8 9 10 5 4 3 2 1 Monday 9 2 Tuesday 9.5 1.5 By mapping them into the same vector space! 1 5 1.1 4 the country of my birth the place where I was born How can we represent the meaning of longer phrases? France 2 2.5 Germany 1 3 3/1/18 How should we map phrases into a vector space? the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 2.5 3.8 5.5 6.1 1 3.5 1 5 Use principle of compositionality The meaning (vector) of a sentence is determined by (1) the meanings of its words and (2) the rules that combine them. Models in this section can jointly learn parse trees and compositional vector representations x2 x1 0 1 2 3 4 5 6 7 8 9 10 5 4 3 2 1 the country of my birth the place where I was born Monday Tuesday France Germany 11 3/1/18 Constituency Sentence Parsing: What we want 9 1 5 3 8 5 9 1 4 3 NP NP PP S 7 1 VP The cat sat on the mat. 12 3/1/18 Learn Structure and Representation NP NP PP S VP 5 2 3 3 8 3 5 4 7 3 The cat sat on the mat. 9 1 5 3 8 5 9 1 4 3 7 1 13 3/1/18 Recursive vs. recurrent neural networks 3/1/18 the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 2.5 3.8 5.5 6.1 1 3.5 1 5 the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 4.5 3.8 5.5 6.1 1 3.5 1 5 2.5 3.8 Lecture 1, Slide 14 Recursive vs. recurrent neural networks 3/1/18 • Recursive neural nets require a tree structure • Recurrent neural nets cannot capture phrases without prefix context and often capture too much of last words in final vector the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 2.5 3.8 5.5 6.1 1 3.5 1 5 the country of my birth 0.4 0.3 2.3 3.6 4 4.5 7 7 2.1 3.3 4.5 3.8 5.5 6.1 1 3.5 1 5 2.5 3.8 Lecture 1, Slide 15 Recursive Neural Networks for Structure Prediction on the mat. 9 1 4 3 3 3 8 3 8 5 3 3 Neural Network 8 3 1.3 Inputs: two candidate children’s representations Outputs: 1. The semantic representation if the two nodes are merged. 2. Score of how plausible the new node would be. 8 5 16 3/1/18 Recursive Neural Network Definition score = UTp p = tanh(W + b), Same W parameters at all nodes of the tree 8 5 3 3 Neural Network 8 3 1.3 score = = parent c1 c2 c1 c2 17 3/1/18 Parsing a sentence with an RNN Neural Network 0.1 2 0 Neural Network 0.4 1 0 Neural Network 2.3 3 3 9 1 5 3 8 5 9 1 4 3 7 1 Neural Network 3.1 5 2 Neural Network 0.3 0 1 The cat sat on the mat. 18 3/1/18 Parsing a sentence 9 1 5 3 5 2 Neural Network 1.1 2 1 Neural Network 0.1 2 0 Neural Network 0.4 1 0 Neural Network 2.3 3 3 5 3 8 5 9 1 4 3 7 1 19 The cat sat on the mat. 3/1/18 Parsing a sentence 5 2 Neural Network 1.1 2 1 Neural Network 0.1 2 0 3 3 Neural Network 3.6 8 3 9 1 5 3 5 3 8 5 9 1 4 3 7 1 20 The cat sat on the mat. 3/1/18 Parsing a sentence 5 2 3 3 8 3 5 4 7 3 9 1 5 3 5 3 8 5 9 1 4 3 7 1 21 The cat sat on the mat. 3/1/18 Max-Margin Framework - Details • The score of a tree is computed by the sum of the parsing decision scores at each node: • x is sentence; y is parse tree 8 5 3 3 RNN 8 3 1.3 22 3/1/18 Max-Margin Framework - Details • Similar to max-margin parsing (Taskar et al. 2004), a supervised max-margin objective • The loss penalizes all incorrect decisions • Structure search for A(x) was greedy (join best nodes each time) • Instead: Beam search with chart 23 3/1/18 Backpropagation Through Structure Introduced by Goller & Küchler (1996) Principally the same as general backpropagation Three differences resulting from the recursion and tree structure: 1. Sum derivatives of W from all nodes (like RNN) 2. Split derivatives at each node (for tree) 3. Add error messages from parent + node itself 24 = 0 @X j=1 W (nl−1) ji δ(nl) j ) 1 A f 0(z(nl−1) i ) | {z } a(nl−2) j (57) = δ(nl−1) i a(nl−2) j (58) ne that the top layer is linear. This is a very detailed account of essentially rors of all layers l (except the top layer) (in vector format, using the Hadamard δ(l) = ⇣ (W (l))T δ(l+1)⌘ ◦f 0(z(l)), (59) 7 where the sigmoid derivative from eq. 14 gives f 0(z(l)) = (1 −a(l))a(l). U hidden layer backprop derivatives: @ @W (l) ij ER = a(l) j δ(l+1) i + λW (l) ij Which in one simpliﬁed vector notation becomes: @ @W (l) ER = δ(l+1)(a(l))T + λW (l). In summary, the backprop procedure consists of four steps: 1. Apply an input xn and forward propagate it through the network activations using eq. 18. 2. Evaluate δ(nl) for output units using eq. 42. 3. Backpropagate the δ’s to obtain a δ(l) for each hidden layer in the n 4. Evaluate the required derivatives with eq. 62 and update all the procedure such as conjugate gradient or L-BFGS. CG seems to b using mini-batches of training data to estimate the derivatives. 3/1/18 BTS: 1) Sum derivatives of all nodes You can actually assume it’s a different W at each node Intuition via example: If we take separate derivatives of each occurrence, we get same: 25 3/1/18 BTS: 2) Split derivatives at each node During forward prop, the parent is computed using 2 children Hence, the errors need to be computed wrt each of them: where each child’s error is n-dimensional 8 5 3 3 8 3 c1 p = tanh(W + b) c1 c2 c2 8 5 3 3 8 3 c1 c2 26 3/1/18 BTS: 3) Add error messages • At each node: • What came up (fprop) must come down (bprop) • Total error messages = error messages from parent + error message from own score 3/1/18 Lecture 1, Slide 27 8 5 3 3 8 3 c1 c2 parent score BTS Python Code: forwardProp 3/1/18 Lecture 1, Slide 28 BTS Python Code: backProp 3/1/18 Lecture 1, Slide 29 = δ( l ) i a( l ) j where we used in the ﬁrst line that the top layer is linear. This is a v just the chain rule. So, we can write the δ errors of all layers l (except the top layer) (in v product ◦): δ(l) = ⇣ (W (l))T δ(l+1)⌘ ◦f 0(z(l)), 7 where the sigmoid derivative from eq. 14 gives f 0(z(l)) = (1 −a(l))a(l hidden layer backprop derivatives: @ @W (l) ij ER = a(l) j δ(l+1) i + λW (l) ij Which in one simpliﬁed vector notation becomes: @ @W (l) ER = δ(l+1)(a(l))T + λW (l). In summary, the backprop procedure consists of four steps: 1. Apply an input xn and forward propagate it through the netw activations using eq. 18. 2. Evaluate δ(nl) for output units using eq. 42. 3. Backpropagate the δ’s to obtain a δ(l) for each hidden layer in t 4. Evaluate the required derivatives with eq. 62 and update all procedure such as conjugate gradient or L-BFGS. CG seems t using mini-batches of training data to estimate the derivatives. Discussion: Simple RNN • Decent results with single matrix TreeRNN • Single weight matrix TreeRNN could capture some phenomena but not adequate for more complex, higher order composition and parsing long sentences • There is no real interaction between the input words • The composition function is the same for all syntactic categories, punctuation, etc. W c1 c2 p Wscore s 3/1/18 Lecture 1, Slide 30 Version 2: Syntactically-Untied RNN • A symbolic Context-Free Grammar (CFG) backbone is adequate for basic syntactic structure • We use the discrete syntactic categories of the children to choose the composition matrix • A TreeRNN can do better with different composition matrix for different syntactic environments • The result gives us a better semantics 3/1/18 Lecture 1, Slide 31 Compositional Vector Grammars • Problem: Speed. Every candidate score in beam search needs a matrix-vector product. • Solution: Compute score only for a subset of trees coming from a simpler, faster model (PCFG) • Prunes very unlikely candidates for speed • Provides coarse syntactic categories of the children for each beam candidate • Compositional Vector Grammar = PCFG + TreeRNN 3/1/18 Lecture 1, Slide 32 Related Work for parsing • Resulting CVG Parser is related to previous work that extends PCFG parsers • Klein and Manning (2003a) : manual feature engineering • Petrov et al. (2006) : learning algorithm that splits and merges syntactic categories • Lexicalized parsers (Collins, 2003; Charniak, 2000): describe each category with a lexical item • Hall and Klein (2012) combine several such annotation schemes in a factored parser. • CVGs extend these ideas from discrete representations to richer continuous ones 3/1/18 Lecture 1, Slide 33 Experiments • Standard WSJ split, labeled F1 • Based on simple PCFG with fewer states • Fast pruning of search space, few matrix-vector products • 3.8% higher F1, 20% faster than Stanford factored parser Parser Test, All Sentences Stanford PCFG, (Klein and Manning, 2003a) 85.5 Stanford Factored (Klein and Manning, 2003b) 86.6 Factored PCFGs (Hall and Klein, 2012) 89.4 Collins (Collins, 1997) 87.7 SSN (Henderson, 2004) 89.4 Berkeley Parser (Petrov and Klein, 2007) 90.1 CVG (RNN) (Socher et al., ACL 2013) 85.0 CVG (SU-RNN) (Socher et al., ACL 2013) 90.4 Charniak - Self Trained (McClosky et al. 2006) 91.0 Charniak - Self Trained-ReRanked (McClosky et al. 2006) 92.1 3/1/18 Lecture 1, Slide 34 SU-RNN / CVG [Socher, Bauer, Manning, Ng 2013] Learns soft notion of head words Initialization: NP-CC NP-PP PP-NP PRP$-NP 3/1/18 Lecture 1, Slide 35 SU-RNN / CVG [Socher, Bauer, Manning, Ng 2013] ADJP-NP ADVP-ADJP JJ-NP DT-NP 3/1/18 Lecture 1, Slide 36 Analysis of resulting vector representations All the figures are adjusted for seasonal variations 1. All the numbers are adjusted for seasonal fluctuations 2. All the figures are adjusted to remove usual seasonal patterns Knight-Ridder wouldn’t comment on the offer 1. Harsco declined to say what country placed the order 2. Coastal wouldn’t disclose the terms Sales grew almost 7% to $UNK m. from $UNK m. 1. Sales rose more than 7% to $94.9 m. from $88.3 m. 2. Sales surged 40% to UNK b. yen from UNK b. 3/1/18 Lecture 1, Slide 37 Version 3: Compositionality Through Recursive Matrix-Vector Spaces One way to make the composition function more powerful was by untying the weights W But what if words act mostly as an operator, e.g. “very” in very good Proposal: A new composition function p = tanh(W + b) c1 c2 Before: 3/1/18 38 Compositionality Through Recursive Matrix-Vector Recursive Neural Networks p = tanh(W + b) c1 c2 p = tanh(W + b) C2c1 C1c2 39 3/1/18 Matrix-vector RNNs [Socher, Huval, Bhat, Manning, & Ng, 2012] p = A B =P 3/1/18 40 Predicting Sentiment Distributions Good example for non-linearity in language 41 3/1/18 Classification of Semantic Relationships • Can an MV-RNN learn how a large syntactic context conveys a semantic relationship? • My [apartment]e1 has a pretty large [kitchen] e2 à component-whole relationship (e2,e1) • Build a single compositional semantics for the minimal constituent including both terms 3/1/18 Lecture 1, Slide 42 Classification of Semantic Relationships Classifier Features F1 SVM POS, stemming, syntactic patterns 60.1 MaxEnt POS, WordNet, morphological features, noun compound system, thesauri, Google n-grams 77.6 SVM POS, WordNet, prefixes, morphological features, dependency parse features, Levin classes, PropBank, FrameNet, NomLex-Plus, Google n-grams, paraphrases, TextRunner 82.2 RNN – 74.8 MV-RNN – 79.1 MV-RNN POS, WordNet, NER 82.4 3/1/18 Lecture 1, Slide 43 Scene Parsing • The meaning of a scene image is also a function of smaller regions, • how they combine as parts to form larger objects, • and how the objects interact. Similar principle of compositionality. 44 3/1/18 Algorithm for Parsing Images Same Recursive Neural Network as for natural language parsing! (Socher et al. ICML 2011) Features Grass Tree Segments Semantic Representations People Building Parsing Natural Scene Images Parsing Natural Scene Images 45 3/1/18 Multi-class segmentation Method Accuracy Pixel CRF (Gould et al., ICCV 2009) 74.3 Classifier on superpixel features 75.9 Region-based energy (Gould et al., ICCV 2009) 76.4 Local labelling (Tighe & Lazebnik, ECCV 2010) 76.9 Superpixel MRF (Tighe & Lazebnik, ECCV 2010) 77.5 Simultaneous MRF (Tighe & Lazebnik, ECCV 2010) 77.5 Recursive Neural Network 78.1 Stanford Background Dataset (Gould et al. 2009) 46 3/1/18 Next lecture • Model overview, comparison, extensions, combinations, etc 3/1/18 Lecture 1, Slide 47 "
472,"Natural Language Processing with Deep Learning CS224N/Ling284 Richard Socher Lecture 15: Model Overview and Memory Networks Outline • Last minute tips for projects • Model overview and combinations • Dynamic memory networks 3/6/18 Richard Socher Lecture 1, Slide 1 Last minute tips • Nothing works and everything is too slow à Panic • Simplify model à Go back to basics: bag of vectors + nnet • Make a smaller network and dataset for debugging • Once no bugs: increase model size • Make sure you can overfit to your dataset • Plot your training and dev errors over training iterations • Then regularize with L2 and Dropout • Then do hyperparameter search • Come to OH! ( 3/6/18 Richard Socher Lecture 1, Slide 2 Model comparison • Bag of Vectors: Surprisingly good baseline for simple text classification problems. Especially if followed by a few relu layers! • Window Model: Good for single word classification for problems that do not need wide context, e.g. POS • CNNs: good for classification, unclear how to incorporate phrase level annotation (can only take a single label), need zero padding for shorter phrases, hard to interpret, easy to parallelize on GPUs, can be very efficient and versatile • Recurrent Neural Networks: Cognitively plausible (reading from left to right, keeping a state), not best for classification (n-gram), slower than CNNs, can do sequence tagging and classification, very active research, amazing with attention mechanisms • TreeRNNs: Linguistically plausible, hard to parallelize, tree structures are discrete and harder to optimize, need a parser • Combinations and extensions! But, there’s more • Combine and extend creatively • Rarely do we use the vanilla models as is 3/6/18 Richard Socher Lecture 1, Slide 4 TreeLSTMs • LSTMs are great • TreeRNNs can benefit from gates too à TreeRNNs + LSTMs • Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks by Kai Sheng Tai, Richard Socher, Christopher D. Manning 3/6/18 Richard Socher Lecture 1, Slide 5 TreeLSTMs 3/6/18 Richard Socher Lecture 1, Slide 6 • Standard LSTM TreeLSTM • Only has one child Has multiple child nodes: RNNs are Slow à Combine with CNNs • RNNs are the most common basic building block for deepNLP • Idea: Take the best and parallelizable parts of RNNs and CNNs • Quasi-Recurrent Neural Networks by James Bradbury, Stephen Merity, Caiming Xiong & Richard Socher Quasi-Recurrent Neural Network • Parallelism computation across time: • Element-wise gated recurrence for parallelism across channels: Under review as a conference paper at ICLR 2017 LSTM CNN LSTM/Linear Linear LSTM/Linear Linear fo-Pool Convolution fo-Pool Convolution Max-Pool Convolution Max-Pool Convolution QRNN Figure 1: Block diagrams showing the computation structure of the QRNN compared with typical LSTM and CNN architectures. Red signiﬁes convolutions or matrix multiplications; a continuous block means that those computations can proceed in parallel. Blue signiﬁes parameterless functions that operate in parallel along the channel/feature dimension. LSTMs can be factored into (red) linear blocks and (blue) elementwise blocks, but computation at each timestep still depends on the results from the previous timestep. 2 MODEL Each layer of a quasi-recurrent neural network consists of two kinds of subcomponents, analogous to convolution and pooling layers in CNNs. The convolutional component, like convolutional layers in CNNs, allows fully parallel computation across both minibatches and spatial dimensions, in this case the sequence dimension. The pooling component, like pooling layers in CNNs, lacks trainable parameters and allows fully parallel computation across minibatch and feature dimensions. Gi i X RT ⇥n f T di i l h l i l b parameters and allows fully parallel computation across minibatch and feature Given an input sequence X 2 RT ⇥n of T n-dimensional vectors x1 . . . xT , t component of a QRNN performs convolutions in the timestep dimension wit producing a sequence Z 2 RT ⇥m of m-dimensional candidate vectors zt. In tasks that include prediction of the next token, the ﬁlters must not allow the given timestep to access information from future timesteps. That is, with ﬁlte depends only on xt−k+1 through xt. This concept, known as a masked convo et al., 2016), is implemented by padding the input to the left by the convoluti one. We apply additional convolutions with separate ﬁlter banks to obtain sequen elementwise gates that are needed for the pooling function. While the candida through a tanh nonlinearity, the gates use an elementwise sigmoid. If the pooli forget gate ft and an output gate ot at each timestep, the full set of computation component is then: Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), where Wz,Wf, and Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter ba masked convolution along the timestep dimension. Note that if the ﬁlter width reduce to the LSTM-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1xt 1 + W2xt) y p g p y l convolutions with separate ﬁlter banks to obtain sequences of vectors for the hat are needed for the pooling function. While the candidate vectors are passed inearity, the gates use an elementwise sigmoid. If the pooling function requires a output gate ot at each timestep, the full set of computations in the convolutional Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), (1) nd Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter banks and ⇤denotes a along the timestep dimension. Note that if the ﬁlter width is 2, these equations M-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1 oxt−1 + W2 oxt). (2) of larger width effectively compute higher n-gram features at each timestep; thus pecially important for character-level tasks. or the pooling subcomponent can be constructed from the familiar elementwise nal LSTM cell. We seek a function controlled by gates that can mix states across h acts independently on each channel of the state vector. The simplest option, Ghifary (2016) term “dynamic average pooling”, uses only a forget gate: Z = tanh(Wz ⇤X) F = σ(Wf ⇤X) O = σ(Wo ⇤X), Wz,Wf, and Wo, each in Rk⇥n⇥m, are the convolutional ﬁlter banks and ⇤denote onvolution along the timestep dimension. Note that if the ﬁlter width is 2, these equatio the LSTM-like zt = tanh(W1 zxt−1 + W2 zxt) ft = σ(W1 fxt−1 + W2 fxt) ot = σ(W1 oxt−1 + W2 oxt). ion ﬁlters of larger width effectively compute higher n-gram features at each timestep; th dths are especially important for character-level tasks. functions for the pooling subcomponent can be constructed from the familiar elementw he traditional LSTM cell. We seek a function controlled by gates that can mix states acr , but which acts independently on each channel of the state vector. The simplest opti lduzzi & Ghifary (2016) term “dynamic average pooling”, uses only a forget gate: ht = ft ⊙ht−1 + (1 −ft) ⊙zt, 2 Q-RNNs for Language Modeling • Better • Faster LSTM s recurrent weights, providing structural regularization over the recurrence. Without zoneout, early stopping based upon validation loss was required as the QRNN would begin overﬁtting. By applying a small amount of zoneout (p = 0.1), no early stopping is required and the QRNN achieves competitive levels of perplexity to the variational LSTM of Gal & Ghahramani Model Parameters Validation Test LSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7 Variational LSTM (medium) (Gal & Ghahramani, 2016) 20M 81.9 79.7 LSTM with CharCNN embeddings (Kim et al., 2016) 19M − 78.9 Zoneout + Variational LSTM (medium) (Merity et al., 2016) 20M 84.4 80.6 Our models LSTM (medium) 20M 85.7 82.0 QRNN (medium) 18M 82.9 79.9 QRNN + zoneout (p = 0.1) (medium) 18M 82.1 78.3 Table 2: Single model perplexity on validation and test sets for the Penn Treebank language model- ing task. Lower is better. “Medium” refers to a two-layer network with 640 or 650 hidden units per layer. All QRNN models include dropout of 0.5 on embeddings and between layers. MC refers to Monte Carlo dropout averaging at test time. 6 Under review as a conference paper at ICLR 2017 Sequence length 32 64 128 256 512 Batch size 8 5.5x 8.8x 11.0x 12.4x 16.9x 16 5.5x 6.7x 7.8x 8.3x 10.8x 32 4.2x 4.5x 4.9x 4.9x 6.4x 64 3.0x 3.0x 3.0x 3.0x 3.7x 128 2.1x 1.9x 2.0x 2.0x 2.4x 256 1.4x 1.4x 1.3x 1.3x 1.3x Figure 4: Left: Training speed for two-layer 640-unit PTB LM on a batch of 20 examples of 105 timesteps. “RNN” and “softmax” include the forward and backward times, while “optimization overhead” includes gradient clipping, L2 regularization, and SGD computations. Right: Inference speed advantage of a 320-unit QRNN layer alone over an equal-sized cuDNN LSTM layer for data with the given batch size and sequence length Training results are similar Q-RNNs for Sentiment Analysis • Often better and faster than LSTMs • More interpretable • Example: • Initial positive review • Review starts out positive At 117: “not exactly a bad story” At 158: “I recommend this movie to everyone, even if you’ve never played the game” Under review as a conference paper at ICLR 2017 Model Time / Epoch (s) Test Acc (%) BSVM-bi (Wang & Manning, 2012) − 91.2 2 layer sequential BoW CNN (Johnson & Zhang, 2014) − 92.3 Ensemble of RNNs and NB-SVM (Mesnil et al., 2014) − 92.6 2-layer LSTM (Longpre et al., 2016) − 87.6 Residual 2-layer bi-LSTM (Longpre et al., 2016) − 90.1 Our models Deeply connected 4-layer LSTM (cuDNN optimized) 480 90.9 Deeply connected 4-layer QRNN 150 91.4 D.C. 4-layer QRNN with k = 4 160 91.1 Table 1: Accuracy comparison on the IMDb binary sentiment classiﬁcation task. All of our models use 256 units per layer; all layers other than the ﬁrst layer, whose ﬁlter width may vary, use ﬁlter width k = 2. Train times are reported on a single NVIDIA K40 GPU. We exclude semi-supervised models that conduct additional training on the unlabeled portion of the dataset. 3 EXPERIMENTS We evaluate the performance of the QRNN on three different natural language tasks: document-level sentiment classiﬁcation, language modeling, and character-based neural machine translation. Our QRNN models outperform LSTM-based models of equal hidden size on all three tasks while dra- matically improving computation speed. Experiments were implemented in Chainer (Tokui et al.). 3.1 SENTIMENT CLASSIFICATION We evaluate the QRNN architecture on a popular document-level sentiment classiﬁcation bench- mark, the IMDb movie review dataset (Maas et al., 2011). The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words (Wang & Manning, 2012). We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., Miyato et al. (2016)). Our best performance on a held-out development set was achieved using a four-layer densely- connected QRNN with 256 units per layer and word vectors initialized using 300-dimensional cased GloVe embeddings (Pennington et al., 2014). Dropout of 0.3 was applied between layers, and we used L2 regularization of 4 ⇥10−6 Optimization was performed on minibatches of 24 examples Under review as a conference paper at ICLR 2017 Figure 3: Visualization of the ﬁnal QRNN layer’s hidden state vectors cL t in the IMDb task, with timesteps along the vertical axis. Colors denote neuron activations. After an initial positive statement “This movie is simply gorgeous” (off graph at timestep 9), timestep 117 triggers a reset of most hidden states due to the phrase “not exactly a bad story” (soon after “main weakness is its story”). Only at timestep 158, after “I recommend this movie to everyone, even if you’ve never played the game”, do the hidden units recover. each layer, it was more computationally convenient to use a multiple of 32. As the Penn Treebank is a relatively small dataset, preventing overﬁtting is of considerable importance and a major focus of recent research. It is not obvious in advance which of the many RNN regularization schemes Neural Architecture Search! • Manual process of finding best units requires a lot of expertise • What if we could use AI to find the right architecture for any problem? • Neural architecture search with reinforcement learning by Zoph and Le, 2016 Neural Architecture Search Example: CNN Controller Used Reinforcement Learning to train the RNN Controller LSTM Cell vs NAS Cell Nice Perplexity Reduction for Language Modeling More complex tasks need more complex architectures • So far, we looked at basic sequence models and seq2seq models • As you know from the default final project, some tasks require more complex memory components • One of the first ones that was shown to work on both synthetic problems and real NLP tasks: • Dynamic Memory Networks by Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher 3/6/18 Richard Socher Lecture 1, Slide 16 High level idea for harder questions • Imagine having to read an article, memorize it, then get asked various questions à Hard! • You can't store everything in working memory • Optimal: give you the input data, give you the question, allow as many glances as possible Dynamic Memory Network Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. The Modules: Input Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. Standard GRU. The last hidden state of each sentence is accessible. Further Improvement: BiGRU Dynamic Memory Networks for Visual and Textu Input fusion layer Sentence reader Facts GRU f1 f1 w1 w2 w3 w4 GRU Positional Encoder GRU f2 f2 w1 w2 w3 w4 GRU Positional Encoder GRU f3 f3 w1 w2 w3 w4 GRU Positional Encoder 1 1 1 1 2 2 2 2 3 3 3 3 Textual Input Module Visual featu extraction Feature embeddin Input fusion laye Facts Figure 3 V The Modules: Question Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. to take multiple passes over the facts focusing attention on different facts at each pass Each pass Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. to take multiple passes over the facts focusing attention on different facts at each pass Each pass above. The latter is stored in terms of an embed include tables with single facts like lists of city ets like (dog,has-part,tail). If a KB is being used ective similar to Socher et al. [4] to distinguish of the full DMN objective function. nput into a representation that can then be used Assume each question consists of a sequence o for each via qt = GRU(vt, qt−1), where the GR nal question vector is deﬁned as q = qTq. The Modules: Episodic Memory Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. to take multiple passes over the facts, focusing attention on different facts at each pass. Each pass produces an episode and these episodes are then summarized into the memory Endowing our Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. to take multiple passes over the facts, focusing attention on different facts at each pass. Each pass produces an episode and these episodes are then summarized into the memory Endowing our ℎ"" # = 𝑔"" #𝐺𝑅𝑈𝑠"", ℎ""+, # + 1 −𝑔"" # ℎ""+, # Last hidden state: mt The Modules: Episodic Memory • Gates are activated if sentence relevant to the question or memory • When the end of the input is reached, the relevant facts are summarized in another GRU 𝑧"" # = [𝑠"" ∘𝑞 ; 𝑠"" ∘𝑚#+,; |𝑠"" −𝑞| ; |𝑠"" −𝑚#+,|] The Modules: Episodic Memory • If summary is insufficient to answer the question, repeat sequence over input Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs We do not draw connections for gates that are close to zero See Section Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs We do not draw connections for gates that are close to zero See Section The Modules: Answer dden state of the answer sequence model is the last hidden state of the m he other hidden elements are computed with a separate GRU which takes den state and the previously predicted output yt−1 as well as the question: at = GRU([yt−1, q], at−1), yt = softmax(W (a)at), ) is a standard softmax layer. The output is simply trained with the cross n of the correct sequences. This is the last but most important part of the ove sequence tagging case, we simply predict an output at every hidden state o = softmax(W smt). Note that, with enough training data, this case can b d answer sequence prediction which just outputs the same number of label ing unsupervised over word input sequnces to learn word vectors [2] and stor mory. Question-answer training is cast as a supervised classiﬁcation problem y errors at either each word (in the case of sequence models) or at the end o Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section Related work • Sequence to Sequence (Sutskever et al. 2014) • Neural Turing Machines (Graves et al. 2014) • Teaching Machines to Read and Comprehend (Hermann et al. 2015) • Learning to Transduce with Unbounded Memory (Grefenstette 2015) • Structured Memory for Neural Turing Machines (Wei Zhang 2015) • Memory Networks (Weston et al. 2015) • End to end memory networks (Sukhbaatar et al. 2015) à Comparison to MemNets Similarities: • MemNets and DMNs have input, scoring, attention and response mechanisms Differences: • For input representations MemNets use bag of word, nonlinear or linear embeddings that explicitly encode position • MemNets iteratively run functions for attention and response • DMNs show that neural sequence models can be used for input representation, attention and response mechanisms à naturally captures position and temporality • Enables broader range of applications babI 1k, with gate supervision 4.1 Question Answering The Facebook bAbI dataset is a synthetic dataset meant to test a model’s ability to retrieve facts and reason over them. Each task tests a different skill that a good question answering model ought to have, such as coreference resolution, deduction, and induction. Training on the bAbI dataset Task MemNN DMN Task MemNN DMN 1: Single Supporting Fact 100 100 11: Basic Coreference 100 99.9 2: Two Supporting Facts 100 98.2 12: Conjunction 100 100 3: Three Supporting facts 100 95.2 13: Compound Coreference 100 99.8 4: Two Argument Relations 100 100 14: Time Reasoning 99 100 5: Three Argument Relations 98 99.3 15: Basic Deduction 100 100 6: Yes/No Questions 100 100 16: Basic Induction 100 99.4 7: Counting 85 96.9 17: Positional Reasoning 65 59.6 8: Lists/Sets 91 96.5 18: Size Reasoning 95 95.3 9: Simple Negation 100 100 19: Path Finding 36 34.5 10: Indeﬁnite Knowledge 98 97.5 20: Agent’s Motivations 100 100 Mean Accuracy (%) 93.3 93.6 Table 1: Test accuracies on the bAbI dataset. MemNN numbers taken from Weston et al. [18]. The DMN passes (accuracy > 95%) 18 tasks, whereas the MemNN passes 16. uses the following objective function: J = ↵ECE(Gates) + βECE(Answers), where ECE is the standard cross-entropy cost and ↵and β are hyperparameters. In practice, we begin training with ↵ set to 1 and β set to 0, and then later switch β to 1 while keeping ↵at 1. We subsample the facts from the input module by end-of-sentence tokens. The gate supervision aims to select one sentence per pass; thus, we also experimented with modifying Eq. 6 to a simple softmax instead of a GRU. T Experiments: Sentiment Analysis Stanford Sentiment Treebank Test accuracies: • MV-RNN and RNTN: Socher et al. (2013) • DCNN: Kalchbrenner et al. (2014) • PVec: Le & Mikolov. (2014) • CNN-MC: Kim (2014) • DRNN: Irsoy & Cardie (2015) • CT-LSTM: Tai et al. (2015) sk Me Anything: Dynamic Memory Networks for Natural Language Processing MemNN DMN g Fact 100 100 Facts 100 98.2 g Facts 100 95.2 elations 100 100 Relations 98 99.3 s 100 100 85 96.9 91 96.5 100 100 wledge 98 97.5 nce 100 99.9 100 100 eference 100 99.8 g 99 100 n 100 100 Task Binary Fine-grained MV-RNN 82.9 44.4 RNTN 85.4 45.7 DCNN 86.8 48.5 PVec 87.8 48.7 CNN-MC 88.1 47.4 DRNN 86.6 49.8 CT-LSTM 88.0 51.0 DMN 88.6 52.1 Table 2. Test accuracies for sentiment analysis on the Sta Sentiment Treebank. MV-RNN and RNTN: Socher et al. (2 DCNN: Kalchbrenner et al. (2014). PVec: Le & Mikolov. (2 CNN-MC: Kim (2014). DRNN: Irsoy & Cardie (2015), CT-LSTM: Tai et al. (2015) Analysis of Number of Episodes • How many attention + memory passes are needed in the episodic memory? mory Networks for Natural Language Processing Max passes task 3 three-facts task 7 count task 8 lists/sets sentiment (ﬁne grain) 0 pass 0 48.8 33.6 50.0 1 pass 0 48.8 54.0 51.5 2 pass 16.7 49.1 55.6 52.1 3 pass 64.7 83.4 83.4 50.1 5 pass 95.2 96.9 96.5 N/A Table 4. Effectiveness of episodic memory module across tasks. Each row shows the ﬁnal accuracy in term of percentages with a different maximum limit for the number of passes the episodic memory module can take. Note that for the 0-pass DMN, the Analysis of Attention for Sentiment [done reading] Table 5. An example of what the DMN focuses on during each episod attention weight is higher. • Sharper attention when 2 passes are allowed. • Examples that are wrong with just one pass Analysis of Attention for Sentiment Figure 4 Attention weights for sentiment examples that were Analysis of Attention for Sentiment day. ning. afternoon. each episode on a real query in the bAbI task. Darker colors mean that the • Examples where full sentence context from first pass changes attention to words more relevant for final prediction Analysis of Attention for Sentiment • Examples where full sentence context from first pass changes attention to words more relevant for final prediction erately. The power of the episodic memory module is evident in tasks 7 and 8, where the DM niﬁcantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts a e them in a representation that slowly incorporates more of the relevant information of the inp uence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspe is due to the MemNN using n-gram features as well as explicit sequence position features. Sequence Tagging: Part of Speech Tagging -of-speech tagging is traditionally modeled as a sequence tagging problem: every word in ence is to be classiﬁed into its part-of-speech class (see Fig. 1). We evaluate on the standa l Street Journal dataset included in Penn-III [26]. We use the standard splits of sections 0- training, 19-21 for development and 22-24 for test sets [27]. Since this is a word level taggi , DMN memories are produced at the word -rather than sentence- level. We compare the DM Model SVMTool Sogaard Suzuki et al. Spoustova et al. SCNN DMN Acc (%) 97.15 97.27 97.40 97.44 97.50 97.56 Table 2: Test accuracies on WSJ-PTB h the results in [27]. The DMN achieves state-of-the-art accuracy with a single model, reachi evelopment set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets 58 dev and 97.56 test accuracies, achieving a new state-of-the-art (Table 2). 7 Experiments: POS Tagging • PTB WSJ, standard splits • Episodic memory does not require multiple passes, single pass enough Modularization Allows for Different Inputs Episodic Memory Answer Question Input Module Episodic Memory Answer Question Input Module (a) Text Question-Answering (b) Visual Question-Answering John moved to the garden. John got the apple there. John moved to the kitchen. Sandra picked up the milk there. John dropped the apple. John moved to the ofﬁce. Where is the apple? Kitchen What kind of tree is in the backgrou nd? Palm Input Module for Images r Visual and Textual Question Answering ed As en m th p- et- on a- del or al sh 512 14 14 W W W GRU GRU GRU GRU GRU GRU CNN Visual feature extraction Feature embedding Input fusion layer Input Module Figure 3. VQA input module to represent images for the DMN. ﬁrst rescale the input image to 448 ⇥448 and take the out- Dynamic Memory Networks for Visual and Textual Question Answering, Caiming Xiong, Stephen Merity, Richard Socher Accuracy: Visual Question Answering Dynamic Memory Networks for Visual and Textual Question Answering DMN+ E2E NR 0.3 0.3 - 1.1 2.1 - s 0.5 0.8 - 0.0 0.1 - 2.4 2.0 - 0.0 0.9 - 0.0 0.3 - 0.0 0.1 - 0.2 0.1 - 45.3 51.8 - ng 4.2 18.6 0.9 2.1 5.3 - 0.0 2.3 1.6 2.8 4.2 - ) 1 3 - rious model architectures on tasks dataset. E2E = End-To-End Mem- test-dev test-std Method All Y/N Other Num All VQA Image 28.1 64.0 3.8 0.4 - Question 48.1 75.7 27.1 36.7 - Q+I 52.6 75.6 37.4 33.7 - LSTM Q+I 53.7 78.9 36.4 35.2 54.1 ACK 55.7 79.2 40.1 36.1 56.0 iBOWIMG 55.7 76.5 42.6 35.0 55.9 DPPnet 57.2 80.7 41.7 37.2 57.4 D-NMN 57.9 80.5 43.1 37.4 58.0 SAN 58.7 79.3 46.1 36.6 58.9 DMN+ 60.3 80.5 48.3 36.8 60.4 Table 3. Performance of various architectures and approaches on VQA test-dev and test-standard data. VQA numbers are from Antol et al. (2015); ACK Wu et al. (2015);iBOWIMG -Zhou et al. (2015);DPPnet - Noh et al. (2015); D-NMN - Andreas et al. (2016); SAN -Yang et al. (2015) VQA test-dev and test-standard: • Antol et al. (2015) • ACK Wu et al. (2015); • iBOWIMG - Zhou et al. (2015); • DPPnet - Noh et al. (2015); D-NMN - Andreas et al. (2016); • SAN - Yang et al. (2015) Attention Visualization Dynamic Memory Networks for Visual and Textual Question Whic Wha What is the main color on the bus ? Answer: blue How many pink ﬂags are there ? Answer: 2 What type of trees are in the background ? Answer: pine Is this in the wild ? Answer: no Attention Visualization al Question Answering Which man is dressed more ﬂamboyantly ? Answer: right What time of day was this picture taken ? Answer: night ine What time of day was this picture taken ? Answer: night What is the boy holding ? Answer: surfboard Who is on both photos ? Answer: girl : 2 tal ripes Did the player hit the ball ? Answer: yes What color are the bananas ? Answer: green Is this in the wild ? Answer: no lts of attention for VQA. Each image (left) is shown with the attention that the episodic memory Answers are given by the DMN+ Attention Visualization Wh W W How many pink ﬂags are there ? Answer: 2 What is this sculpture made out of ? Answer: metal What is the pattern on the cat ' s fur on its tail ? Answer: stripes Did the player hit the ball ? Answer: yes What color are the bananas ? Answer: green Is this in the wild ? Answer: no Figure 4. Examples of qualitative results of attention for VQA. Each image (left) is shown w Summary • Basic blocks can be combined or learned with NAS • Memory is useful. DMN accurately solves variety of tasks • Next week: Most recent research and fun future outlook "
473,"Natural Language Processing with Deep Learning CS224N Lecture 17 Semi-Supervised Learning for NLP Kevin Clark Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Announcements • Final project due soon: go to office hours! • More poster session details on the website • If you can’t make some or all of the poster session, fill out the Google form ASAP! 1 Lecture Plan: • Why use semi-supervised learning for NLP? • Semi-supervised learning algorithms • Pre-training • Three recent papers • Self-training • Consistency regularization • Two recent papers 2 Why has deep learning been so successful recently? Lecture 1, Slide 3 Why has deep learning been so successful recently? Lecture 1, Slide 4 Why has deep learning been so successful recently? Lecture 1, Slide 5 Why has deep learning been so successful recently? Lecture 1, Slide 6 • Better “tricks” (dropout, improved optimizers (e.g., Adam), batch norm, attention) • Better hardware (thanks video games!) -> larger models • Larger datasets Big deep learning successes • Image Recognition: Widely used by Google, Facebook, etc. • Machine Translation: Google translate, etc. • Game Playing: Atari Games, AlphaGo, and more Lecture 1, Slide 7 Big deep learning successes • Image Recognition: ImageNet: 14 million examples • Machine Translation: WMT: Millions of sentence pairs • Game Playing: 10s of millions of frames for Atari AI 10s of millions of self-play games for AlphaZero Lecture 1, Slide 8 NLP Dataset Sizes Lecture 1, Slide 9 Dataset (English) Size (# sentences) NER 15K (CoNLL 2003) Coreference Resolution 75K (OntoNotes) Parsing 40K (Penn Treebank) Question Answering 100k questions (SQuAD) Textual Entailment 570k (SNLI) Sentiment Analysis 10k (SST) …And that’s for core tasks in English! • Most tasks have less data • There are thousands of languages • Hundreds with > 1 million native speakers • Less than 10% of people speak English as their first language • Little-to-no annotated data for many other languages Lecture 1, Slide 10 What to do? • Just collect more data? • Expensive! • Crowdsourcing alleviates this a bit • Requires linguistic knowledge for some tasks • Semi-Supervised Learning • Use unlabeled examples during training • Easy to find for NLP! Lecture 1, Slide 11 Two Moons Dataset Lecture 1, Slide 12 • Toy dataset that we will use as an example throughout the rest of the lecture Two Moons Dataset Lecture 1, Slide 13 • Semi-supervised: most examples don’t have a label Two Moons Dataset Lecture 1, Slide 14 • A supervised model would learn something like this Two Moons Dataset Lecture 1, Slide 15 • But this of course is wrong, the model will make lots of mistakes Semi-Supervised Learning • We will cover three semi-supervised learning techniques • Pre-training • One of the tricks that started to make NNs successful • You learned about this in week 1 (word2vec)! • Self-training • One of the oldest and simplest semi-supervised learning algorithms (1960s) • Consistency regularization • Recent idea (2014, lots of active research) • Great results for both computer vision and NLP Lecture 1, Slide 16 Pre-training • First train an unsupervised model on unlabeled data • Then incorporate the model’s learned weights into a supervised model and train it on the labeled data • Optional: continue fine-tuning the unsupervised weights. Lecture 1, Slide 17 big corpus of unlabeled data 1. pre-training phase unsupervised learning smaller corpus of labeled data 2. supervised learning phase shared part of the model supervised-only part of the model shared part of the model supervised learning unsupervised-only part of the model initialize weights Pre-training: Word2Vec • Shared part is word embeddings • No unsupervised-only part • Supervised-only part is the rest of the model • Unsupervised learning: skip-gram/cbow/glove/etc • Supervised learning: training on some NLP task Lecture 1, Slide 18 big corpus of unlabeled data 1. pre-training phase unsupervised learning smaller corpus of labeled data 2. supervised learning phase shared part of the model supervised-only part of the model shared part of the model supervised learning unsupervised-only part of the model initialize weights Why does pre-training work? • ”Smart” initialization for the model • More meaningful representations in the model • e.g., GloVe vectors capture a lot about word meaning, our model no longer has to learn the meanings itself Lecture 1, Slide 19 input supervised NN input pre-trained NN supervised NN Supervised learning: have to learn everything from “raw” input Pre-training: supervised part gets more useful representations as input Why does pre-training work? Lecture 1, Slide 20 original representation space learned representation space after pre-training Why does pre-training work? Lecture 1, Slide 21 original representation space learned representation space after pre-training Supervised part of the model has a much easier job after pre-training Pre-Training for NLP Lecture 1, Slide 22 • Most neural NLP models looks (roughly) like this Inputs (words) Embedding lookup Encoder NN(s) (CNN/BiLSTM/Transformer/Etc.) Prediction NN (e.g., mean-pool then softmax layer) Prediction(s) Pre-Training for NLP Lecture 1, Slide 23 • With pre-trained embeddings Inputs (words) Embedding lookup Encoder NN(s) (CNN/BiLSTM/Transformer/Etc.) Prediction NN (e.g., mean-pool then softmax layer) Prediction(s) Supervised Pre-trained Pre-Training for NLP Lecture 1, Slide 24 • Recent research: pre-train more of the model (e.g., the first LSTM layer) Inputs (words) Embedding lookup Pre-trained Encoder NN Prediction NN Prediction(s) Supervised Pre-trained Supervised Encoder NN Pre-Training Strategies: Auto-Encoder (Dai & Le, 2015) • For pre-training, train an autoencoder: seq2seq model (without attention) where the target sequence is the input sequence • the encoder converts the input into a vector that contains enough information that the input can be recovered • Initialize the LSTM for a sentence classification model with the encoder Lecture 1, Slide 25 “It was good” LSTM Encoder LSTM Decoder “It was good” pre-training supervised learning “It was good” LSTM Encoder Softmax Layer positive sentiment initialize Pre-Training Strategies: Auto-Encoder (Dai & Le, 2015) • For pre-training, train an autoencoder: seq2seq model (without attention) where the target sequence is the input sequence • the encoder converts the input into a vector that contains enough information that the input can be recovered • Initialize the LSTM for a sentence classification model with the encoder Lecture 1, Slide 26 Dataset Previous Best Result Supervised Baseline With Pretraining IMDB 7.42 10.00 7.24 Rotten Tomatoes 18.5 20.5 16.7 20 Newsgroups 17.1 18.0 15.6 Pre-Training Strategies: CoVe (McCann et al., 2017) • Pre-train the encoder for machine translation • So really this could be considered transfer learning, not semi- supervised learning • Don’t update the pre-trained part of the model during training • Train bigger NN on top of the pre-trained encoder • e.g., BiLSTM layers as well as just a softmax layer Lecture 1, Slide 27 “It was good” LSTM Encoder LSTM Decoder “fue bueno” pre-training supervised learning “It was good” LSTM Encoder Supervised Model positive sentiment initialize, don’t fine-tune Pre-Training Strategies: CoVe (McCann et al., 2017) • Why not fine-tune the pre-trained encoder? • Much faster during training! In a preprocessing step run the encoder once over each example • Then treat the outputs as fixed vectors (like GloVe vectors) that the model takes as input Lecture 1, Slide 28 It was good positive sentiment Supervised BiLSTM GloVe vectors CoVe vectors (produced by NMT encoder running over the sentence) Pre-Training Strategies: CoVe (McCann et al., 2017) • Why not fine-tune the pre-trained encoder? • Much faster during training! In a preprocessing step run the encoder once over each example • Then treat the outputs as fixed vectors (like GloVe vectors) that the model takes as input Lecture 1, Slide 29 Pre-Training Strategies: ELMo (Peters et al., 2017) • Similar to CoVe but • Pre-train the model for language modeling (both forwards and backwards) • Scaled-up: much larger model, much more data • A few other tricks… Lecture 1, Slide 30 “It was good” LSTM Encoder Softmax over Vocab “was good <EOS>” pre-training supervised learning “It was good” LSTM Encoder Supervised Model positive sentiment initialize, don’t fine-tune Pre-Training Strategies: ELMo (Peters et al., 2018) • Tricks: • Fine-tune the LM on the supervised dataset • Combine the LM representations in a smart way • Pass the ELMo representations into multiple layers of the supervised model, not just the first layer Lecture 1, Slide 31 , collapses all layers in R into a single vector, ELMok = E(Rk; ⇥e). In the simplest case, ELMo just selects the top layer, E(Rk) = hLM k,L , as in TagLM (Peters et al., 2017) and CoVe (Mc- Cann et al., 2017). More generally, we compute a task speciﬁc weighting of all biLM layers: ELMotask k = E(Rk; ⇥task) = γtask L X j=0 stask j hLM k,j . (1) In (1), stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector. γ is of practical im- portance to aid the optimization process (see sup- plemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting. ough L lay ion k, each dent repre- he top layer ict the next ard LM, ex- se, predict- context: +2, . . . , tN). us way to a TM layer j esentations d backward collapses all layers in R into a single vector, ELMok = E(Rk; ⇥e). In the simplest case, ELMo just selects the top layer, E(Rk) = hLM k,L , as in TagLM (Peters et al., 2017) and CoVe (Mc- Cann et al., 2017). More generally, we compute a task speciﬁc weighting of all biLM layers: ELMotask k = E(Rk; ⇥task) = γtask L X j=0 stask j hLM k,j . (1) In (1), stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector. γ is of practical im- portance to aid the optimization process (see sup- plemental material for details). Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM l b f i hti weight controls the magnitude of the representations combine outputs of all layers of the LM other learned weights control which layers contribute the most LM hidden states Pre-Training Strategies: ELMo (Peters et al., 2018) • Amazing Results • This kind of method may become standard-practice in NLP the way pretrained embeddings is standard-practice currently Lecture 1, Slide 32 TASK PREVIOUS SOTA OUR BASELINE ELMO + BASELINE INCREASE (ABSOLUTE/ RELATIVE) SQuAD SAN 84.4 81.1 85.8 4.7 / 24.9% SNLI Chen et al. (2017) 88.6 88.0 88.7 ± 0.17 0.7 / 5.8% SRL He et al. (2017) 81.7 81.4 84.6 3.2 / 17.2% Coref Lee et al. (2017) 67.2 67.2 70.4 3.2 / 9.8% NER Peters et al. (2017) 91.93 ± 0.19 90.15 92.22 ± 0.10 2.06 / 21% SST-5 McCann et al. (2017) 53.7 51.4 54.7 ± 0.5 3.3 / 6.8% Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across six benchmark NLP tasks. The performance metric varies across tasks – accuracy for SNLI and SST-5; F1 for SQuAD, SRL and NER; average F1 for Coref. Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across ﬁve runs with different random seeds. The “increase” column lists both the absolute and relative improvements over our baseline. ral Language Inference (SNLI) corpus (Bowman our experiments with the OntoNotes coreference Pre-Training Overview • Initialize part of the model with a network trained using unsupervised learning • Works great! • But requires training a separate (usually extremely large) model • e.g., ELMo uses a 2-layer BiLSTM with 4096 units in each layer, also incorporated a size 2048 character-level CNN, pre- trained with 10 passes over a 1 billion word corpus Lecture 1, Slide 33 Self-Training • Use unlabeled data without a giant model or long pretraining phase • Old (1960s) and simple semi-supervised algorithm • Algorithm: 1. Train the model on the labeled data. 2. Have the model label the unlabeled data. • Take some of examples the model is most confident about (i.e., the model gives them high probability). Add those examples with the model’s labels to the training set 3. Go back to 1. Lecture 1, Slide 34 Self-Training: Example Lecture 1, Slide 35 • 1. Train our model on the labeled data Self-Training: Example Lecture 1, Slide 36 • 2. Label a few examples Self-Training: Example Lecture 1, Slide 37 • 3. Re-train the model Self-Training: Example Lecture 1, Slide 38 • Repeat! Self-Training: Example Lecture 1, Slide 39 Self-Training: Example Lecture 1, Slide 40 Self-Training: Example Lecture 1, Slide 41 Self-Training: Lecture 1, Slide 42 • Good results on quite a few NLP tasks in the 1990s and 2000s • e.g., for constituency parsing (McClosky et al., 2006) • However, not used as much lately (especially with NNs) because other methods work better Sentences added 1 22 24 0 (baseline) 91.8 92.1 90.5 50k 91.8 92.4 90.8 250k 91.8 92.3 91.0 500k 92.0 92.4 90.9 750k 92.0 92.4 91.1 1,000k 92.1 92.2 91.3 1,500k 92.1 92.1 91.2 1,750k 92.1 92.0 91.3 2,000k 92.2 92.0 91.3 Table 2: f-scores from evaluating the rerank- ing parser on three held-out sections after adding reranked sentences from NANC to WSJ training. These evaluations were performed on all sentences. on the same models. In Table 2, we see that the new NANC data contains some information orthogonal to the reranker and improves parsing accuracy of the reranking parser. Up to this point, we have only considered giving our true training data a relative weight of one. In- creasing the weight of the Wall Street Journal data should improve, or at least not hurt, parsing perfor- mance. Indeed, this is the case for both the parser (ﬁgure not shown) and reranking parser (Figure 1). Adding more weight to the Wall Street Journal data Parsing F1 score vs amount of unlabeled data Online Self-Training Lecture 1, Slide 43 1. Sample a labeled minibatch (xi, yi) and unlabeled minibatch xj 2. Take a step of gradient descent minimizing Regular supervised loss Target output is a human-produced label Model acts as a “teacher” and labels the examples Target is a model-produced label. It will be noisy because the model isn’t as accurate as a person, but hopefully the model can still learn from it Then model acts as a “student” and learns to match the target Online Self-Training Lecture 1, Slide 44 1. Sample a labeled minibatch (xi, yi) and unlabeled minibatch xj 2. Take a step of gradient descent minimizing Rest of this lecture: how do we make this second term (the unsupervised one) better? Regular supervised loss Target output is a human-produced label Model acts as a “teacher” and labels the examples Target is a model-produced label. It will be noisy because the model isn’t as accurate as a person, but hopefully the model can still learn from it Then model acts as a “student” and learns to match the target Hard vs Soft Targets • Why use “hard” one-hot label as the target on unlabeled examples? Wouldn’t a “soft” probability distribution work better? Lecture 1, Slide 45 y1 y2 y3 y1 y2 y3 One-hot vector just tells us y1 is the most likely class Probability distribution also tells use the model isn’t very confident about its prediction and that y2 is the second-most-likely class Hard vs Soft Targets • Why use “hard” one-hot label as the target on unlabeled examples? Wouldn’t a “soft” probability distribution work better? Lecture 1, Slide 46 Hard vs Soft Targets • Why use “hard” one-hot label as the target on unlabeled examples? Wouldn’t a “soft” probability distribution work better? • A bit odd: the student already matches the targets! Lecture 1, Slide 47 Consistency Regularization Lecture 1, Slide 48 • Add noise to the student’s inputs • Where is a vector with a random direction and a small magnitude Soft target Model learns to produce target even when noise is added to its input Consistency Regularization Lecture 1, Slide 49 • Add noise to the student’s inputs • Where is a vector with a random direction and a small magnitude • Train the model so a bit of noise doesn’t mess up its predictions • Equivalently, the model must give consistent predictions to nearby data points The model is trained to give the same prediction for any point in the circle “distributional smoothing” unlabeled example Consistency Regularization: Example Lecture 1, Slide 50 Consistency Regularization: Example Lecture 1, Slide 51 Model should produce the same predictions everywhere in the circles -> overlapping circles should have the same prediction Consistency Regularization: Example Lecture 1, Slide 52 Decision boundary will look like this Amazing Results for Computer Vision! Lecture 1, Slide 53 Model Error Rate Supervised 35.56 Ladder Netwosk (Rasmus et al, 2015) 20.40 CatGAN (Springenberger, 2016) 19.58 GAN (Salimans et al., 2016) 18.63 Consistency Regularization (Sajjadi et al., 2016) 11.29 • Results on small image recognition dataset: 4K labeled examples, 46K unlabeled examples How to Apply Consistency Regularization to NLP? • a • In NLP, xj is a sequence of words • Unlike with pixels in an image, words are discrete. How can we add random noise to them? • 3 ideas: • Miyato et al. (2017) • Add noise to the word embeddings • Clark et al. (2018) • Word dropout • Cross-view Consistency Lecture 1, Slide 54 Virtual Adversarial Training (Miyato et al., 2017) • Apply consistency regularization to text classification • First embed the words • Add the noise to the word embeddings • Have to constrain the word embeddings (e.g., make them have zero mean and unit variance) • Otherwise the model could just make them have really large magnitude so the noise doesn’t change anything • Noise added to the word embeddings is not chosen randomly: it is chosen adversarially Lecture 1, Slide 55 Adversarial Examples • Adversarial examples: Small (imperceptible to humans) tweak to neural network inputs can change its output Lecture 1, Slide 56 Adversarial Examples • Security implications Lecture 1, Slide 57 Adversarial Examples • Adversarial examples: Small (imperceptible to humans) tweak to neural network inputs can change its output • Creating an adversarial example: • Compute the gradient of the loss with respect to the input • Add epsilon times the gradient to the input • Possibly repeat multiple times Lecture 1, Slide 58 Virtual Adversarial Training Lecture 1, Slide 59 Instead of picking a random direction for , pick the one that most increases the loss unlabeled example Virtual Adversarial Training: Results Lecture 1, Slide 60 Model Error Pretraining Only 7.33 Pretraining + consistency reg. (random pertrubation) 6.78 Pretraining + consistency reg. (adversarial pertrubation) 5.91 • Results on sentiment classification task Word Dropout • Much simpler idea: • We can’t add noise to words easily • Instead let’s randomly (10-20% probability) replace words in the input with a special REMOVED token • A lot simpler than Virtual Adversarial Training! • And actually works better in many cases Lecture 1, Slide 61 Cross-View Consistency (Clark et al., 2018) • Word dropout causes the model (when acting as the “student”) to see a restricted view of the input Original input: “They traveled to Washington by plane” Restricted view: “They ______ to Washington by ____” • Cross-view Consistency: instead train the model across many different views of the input at once • Sounds nice, but wouldn’t this train 4n times as slow? Lecture 1, Slide 62 When making a prediction about “Washington”: View 1: They traveled to Washington _____ View 2: They traveled to _______________ View 3: ___________ Washington by plane View 4: _____________________ by plane When making a prediction about “to”: View 1: They traveled to _______________ View 2: They traveled __________________ View 3: _________ to Washington by plane View 4: ___________ Washington by plane Cross-View Consistency (Clark et al., 2018) • Sounds nice, but wouldn’t this train 4n times as slow? • Instead of running full the model multiple times, add multiple “auxiliary” softmax layers to the model • e.g., add one to the forward LSTM in the first BiLSTM layer. It doesn’t see any words to the right of the current one • Trains these predictions to match the “primary” prediction from a softmax layer that sees all of the input Lecture 1, Slide 63 Primary softmax layer that sees all the input Auxiliary softmax layers that see restricted views of the input Cross-View Consistency (Clark et al., 2018) Lecture 1, Slide 64 Semi-Supervised Learning with Cross-View Consistency Learning on an Unlabeled Example “They traveled to Washington by plane.” Model acting as the teacher Model acting as the student ˆ y ˆ yview1 ˆ yview2 ˆ yview3 ˆ yview4 loss Inputs Seen by Student Prediction Layers: view 1: “They traveled to ” view 2: “ by plane” view 3: “They traveled to Washington ” view 4: “ Washington by plane” Learning on a Labeled Example LOCATION “Washington is a state located in...” Model ˆ y loss 1 igure 1. An overview of Cross-View Consistency. The model is trained with standard supervised learning on labeled examples nlabeled examples, auxiliary prediction layers with different views of the input are trained to agree with the primary prediction l lthough the model takes on different roles (i.e., as the teacher or the student), only one set of parameters is trained. This partic xample shows CVC applied to named entity recognition. From the labeled example, the model can learn that “Washington” usu efers to a location. Then, on unlabeled data, auxiliary prediction layers are trained to reach the same prediction without seeing som he input. In doing so, they improve the contextual representations produced by the model, for example, learning that “traveled t sually followed by a location. The improved representations will lead to more accurate predictions from the full model. ross-View Consistency adds k additional prediction layers 1 pk to the model Each layer pj takes as input an inter- model’s inputs are not available. • Model first learns “Washington” is usually a location from the labeled data. So on the unlabeled example it can guess “Washington” refers to a location • Then on the unlabeled example it learns a location usually follows “They traveled to” Auxilliary Prediction Layers for Sequence Tagging Lecture 1, Slide 65 • Forward: attached to forward LSTM, produces predictions without seeing the right context of the current token. • Future: attached to forward LSTM, produces prediction without seeing the right context or the current token itself cation Softmax Layers for Sequence Tagging ˆ yi Predict g Auxiliary Prediction Layers for Machine Translation Lecture 1, Slide 66 Encoder RNN <START> les pauvres sont démunis Attention scores Attention distribution Attention output the poor ! ""# don’t Model: standard seq2seq with attention Auxiliary layers use the same LSTM but different attention and softmax weights Auxiliary Prediction Layers for Machine Translation Lecture 1, Slide 67 Encoder RNN <START> les pauvres sont démunis Attention scores Attention distribution Attention output the poor Model: standard seq2seq with attention Auxiliary prediction layer 1: predict the word after next have ! ""# Auxiliary Prediction Layers for Machine Translation Lecture 1, Slide 68 Model: standard seq2seq with attention Auxiliary prediction layer 2: attention dropout: model only attends to a subset of the source sentence <START> les pauvres sont démunis Attention output the poor ! ""# don’t Encoder RNN Attention scores Attention distribution Cross-View Consistency (Clark et al., 2018) • Model makes multiple predictions • Each one using a different softmax layer • Trains these predictions to match the “primary” prediction from a softmax layer that sees all of the input • Loss function: Lecture 1, Slide 69 Primary softmax layer that sees all the input Auxiliary softmax layerr that see restricted views of the input Cross View Consistency: Advantages • Much more data efficient than word dropout because the model learns to produce good predictions across many views of the input at once instead of just one • Not much slower because a few extra softmax layers are computationally cheap compared to the LSTMs Lecture 1, Slide 70 Cross View Consistency: Results Lecture 1, Slide 71 Method CCG Chunk NER Dep. Parsing MT (English- Vietnamese) Previous state-of- the-art 95.1 96.4 92.2 94.1 26.1 • Chunking and NER results are using ELMo, rest are from supervised classifiers Cross View Consistency: Results Lecture 1, Slide 72 Method CCG Chunk NER Dep. Parsing MT (English- Vietnamese) Previous state-of- the-art 95.1 96.4 92.2 94.1 26.1 Supervised 94.8 94.9 91.2 93.3 28.9 Cross View Consistency: Results Lecture 1, Slide 73 Method CCG Chunk NER Dep. Parsing MT (English- Vietnamese) Previous state-of- the-art 95.1 96.4 92.2 94.1 26.1 Supervised 94.8 94.9 91.2 93.3 28.9 Virtual Adversarial 95.1 95.2 91.6 93.7 -- Word Dropout 95.2 95.8 92.1 93.8 29.3 Cross View Consistency: Results Lecture 1, Slide 74 Method CCG Chunk NER Dep. Parsing MT (English- Vietnamese) Previous state-of- the-art 95.1 96.4 92.2 94.1 26.1 Supervised 94.8 94.9 91.2 93.3 28.9 Virtual Adversarial 95.1 95.2 91.6 93.7 -- Word Dropout 95.2 95.8 92.1 93.8 29.3 CVC 95.6 96.5 92.4 94.2 29.7 Cross View Consistency: Results Lecture 1, Slide 75 Accuracy vs Amount of labeled data Accuracy vs Model size Conclusion • Lots of recent work on semi-supervised learning resulting in big improvements on many tasks! • Provides a way to scale up models even when there isn’t much labeled data Lecture 1, Slide 76 "
474,"Natural Language Processing with Deep Learning CS224N/Ling284 Lecture 18: The Limits and Future of NLP Richard Socher Natural Language Processing with Deep Learning CS224N/Ling284 Christopher Manning and Richard Socher Lecture 2: Word Vectors Poster Session • Everyone expected to attend (or video). • 530pm-830pm next Wednesday. • Check out other student’s posters. • Dinner on us. • Stay until end if you think you have a chance of winning one of the awards • Jobs and funding • Fun :) What has been lost from old NLP work? • An earlier era of work had lofty goals, but modest realities • Today, we have much better realities, but often content ourselves with running LSTMs rather than reaching for the stars 3 Norvig (1986) Ph.D. Peter Norvig’s thesis – 30th anniversary 4 Robert Wilensky Lofti Zadeh Chuck Fillmore The language analyzed • In a poor fishing village built on an island not far from the coast of China, a young boy named Chang Lee lived with his widowed mother. Every day, little Chang bravely set off with his net, hoping to catch a few fish from the sea, which they could sell and have a little money to buy bread. (a) There is a sea, which surrounds the island, is used by the villagers for fishing, and forms part of the coast of China (b) Chang intends to trap fish in his net, which is a fishing net (c) The word which refers to the fish (d) The word they refers to Chang and his mother 5 Basic NLP: Progress has been made! “Arens and Wilensky’s PHRAN program was used where possible [to convert input sentences to KODIAK knowledge representations]. For some input, PHRAN was not up to the task, so a representation was constructed by hand instead.” (p. 4) CoreNLP 6 Building elaborations a la Norvig (1986) 7 What do we still need? • BiLSTMs with attention seem to be taking over the field and improving our ability to do everything • Neural methods are leading to a renaissance for all language generation tasks (i.e., MT, dialog, QA, summarization, …) • There’s a real scientific question of where and whether we need explicit, localist language and knowledge representations and inferential mechanisms 8 What do we still need? • However: We still have very primitive methods for building and accessing memories or knowledge • Current models have almost nothing for developing and executing goals and plans* 9 Answer module Question Module Semantic Memory Module Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 (Glove vectors) w1 w T Figure 3: Real example of an input sentence sequence and the attention gates that are triggered by a speciﬁc question. Gate values gi t are shown above the corresponding vectors. The gates change with each search over inputs. We do not draw connections for gates that are close to zero. See Section 4.1 for details on the dataset that this example comes from. to take multiple passes over the facts, focusing attention on different facts at each pass. Each pass produces an episode, and these episodes are then summarized into the memory. Endowing our module with this episodic component allows its attention mechanism to attend more selectively to Progress on goals and plans • Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning, Tianmin Shu, Caiming Xiong, and Richard Socher International Conference on Learning Representations (ICLR 2018) What do we still need? • We still have quite inadequate abilities for understanding and using inter-sentential relationships. • We still can’t, at a large scale, do elaborations from a situation using common sense knowledge BUT also have bias 11 The Limits of Single Task Learning • Great performance improvements • Projects start from random • Single unsupervised task can’t fix it • We will never get to a truly general NLP model this way. Towards NLP-Complete Super Tasks • How to express different tasks in the same framework, e.g. – Sequence tagging: aspect specific sentiment – Text classification: dialogue intent classification – Seq2seq: machine translation, summarization, etc. The 3 Equivalent NLP-Complete Super Tasks • Language modeling • Question answering • Dialogue systems Usefulness and complexity in their current interpretation Framework for Tackling NLP A joint model for comprehensive QA QA Examples I: Mary walked to the bathroom. I: Sandra went to the garden. I: Daniel went back to the garden. I: Sandra took the milk there. Q: Where is the milk? A: garden I: Everybody is happy. Q: What’s the sentiment? A: positive I: Jane has a baby in Dresde Q: What are the named enti A: Jane - person, Dresden - I: Jane has a baby in Dresde Q: What are the POS tags? A: NNP VBZ DT NN IN N I: I think this model is incre Q: In French? A: Je pense que ce mod` ele Figure 1: Example inputs and questions together with answers all of whic same dynamic memory network. 2 The Dynamic Memory Network The Dynamic memory network (DMNs) is a general model for asking que ed to the bathroom. t to the garden. t back to the garden. k the milk there. he milk? is happy. sentiment? I: Jane has a baby in Dresden. Q: What are the named entities? A: Jane - person, Dresden - location I: Jane has a baby in Dresden. Q: What are the POS tags? A: NNP VBZ DT NN IN NNP . I: I think this model is incredible Q: In French? A: Je pense que ce mod` ele est incroyable. Example inputs and questions together with answers all of which are generated b ic memory network. ynamic Memory Network c memory network (DMNs) is a general model for asking questions over inputs he DMN is to represent inputs and their parts in vector form. In this paper, we will estions over natural language inputs. Seman&c( Memory( Episodic(Memory( Answer( Wh W How many pink ﬂags are there ? Answer: 2 What is this sculpture made out of ? Answer: metal What color are the bananas ? Answer: green Is this in the wild ? Answer: no I: Q: What color are the bananas? A: Green. Move from {xi,yi} to {xi,qi,yi} First of Several Major Obstacles • For NLP no single model architecture with consistent state of the art results across tasks Task State of the art model Question answering (babI) Strongly Supervised MemNN (Weston et al 2015) Sentiment Analysis (SST) Tree-LSTMs (Tai et al. 2015) Part of speech tagging (PTB-WSJ) Bi-directional LSTM-CRF (Huang et al. 2015) Tackling Obstacle: Dynamic Memory Network Answer module Question Module c Memory Episodic Memory Module Input Module Mary got the milk there. John moved to the bedroom. Sandra went back to the kitchen. Mary travelled to the hallway. John got the football there. John went to the hallway. John put down the football. Mary went to the garden. s1 s2 s3 s4 s5 s6 s7 s8 Where is the fooball? q 0.0 0.3 0.0 0.0 0.0 0.9 0.0 0.0 0.3 0.0 0.0 0.0 0.0 0.0 1.0 0.0 e1 e2 e3 e4 e5 e6 e7 e8 1 1 1 1 1 1 1 1 e1 e2 e3 e4 e5 e6 e7 e8 2 2 2 2 2 2 2 2 hallway <EOS> m 1 m 2 vectors) w1 w T : Real example of an input sentence sequence and the attention gates that are triggered by a question. Gate values gi t are shown above the corresponding vectors. The gates change with arch over inputs. We do not draw connections for gates that are close to zero. See Section details on the dataset that this example comes from. ✔ But, now known, it’s not enough Obstacle: Joint Many-task Learning • Fully joint multitask learning* is hard: – Usually restricted to lower layers – Usually helps only if tasks are related – Often hurts performance if tasks are not related – We lose powerful accuracy improvement techniques such as task-specific architecture and hyperparameter tuning * meaning: same decoder/classifier and not only transfer learning with source target task pairs, no swappable modeling blocks per task Tackling Joint Training • A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka & Richard Socher • Final Model à Under review as a conference paper at ICLR 2017 CHUNK POS DEP Relatedness encoder Relatedness Entailment encoder Entailment word representation Sentence1 CHUNK POS DEP Relatedness encoder Entailment encoder word representation Sentence2 semantic level syntactic level word level Figure 1: Overview of the joint many-task model predicting different linguistic outp sively deeper layers Model Details • Include character n-grams and short-circuits • State of the art purely feedforward parser Under review as a conference paper at ICLR 2017 LSTM LSTM LSTM LSTM x1 x2 x3 x4 softmax softmax softmax softmax POS Tagging: h(1) 1 h(1) 2 h(1) 3 h(1) 4 y(pos) 1 y(pos) 2 y(pos) 3 y(pos) 4 label embedding label embedding label embedding label embedding LSTM LSTM LSTM LSTM x1 x2 x3 x4 softmax softmax softmax softmax Chunking: h(2) 1 h(2) 2 h(2) 3 h(2) 4 h(1) 1 h(1) 2 h(1) 3 h(1) 4 y(chk) 1 y(chk) 2 y(chk) 3 y(chk) 4 y(pos) 1 y(pos) 2 y(pos) 3 y(pos) 4 label embedding label embedding label embedding label embedding Figure 2: Overview of the POS tagging and chunking tasks in the ﬁrst and second layers of the JMT model. where we deﬁne the input gt as gt = [− ! h t−1; xt], i.e. the concatenation of the previous hidden state and the word representation of wt. The backward pass is expanded in the same way, but a different et of weights are used. Dependency Parsing LSTM LSTM LSTM LSTM x1 softmax softmax softmax Dependency Parsing: h(3) 1 h(3) 2 h(3) 3 h(3) 4 h(2) 1 y(chk) 1 y(pos) 1 Multi Sentence Tasks: Semantic Relatedness re 3: Overview of dependency parsing in the third layer of the JMT m LSTM LSTM LSTM x1 softmax Semantic relatedness: LSTM LSTM LSTM Sentence1 Sentence2 temporal max-pooling temporal max-pooling Feature extracton h(4) 1 h(4) 2 h(4) 3 label embedding h(3) 1 y(chk) 1 y(pos) 1 y(rel) Training Details: Regularized Idea tion is based on the idea that we do not want the model to forget the informatio her tasks. In the case of POS tagging, the regularization is applied to ✓e, and g parameter after training the ﬁnal task in the top-most layer at the previous traini erparameter. AINING THE CHUNKING LAYER tive function is deﬁned as follows: − X s X t log p(y(2) t = ↵|h(2) t ) + λkWchunkk2 + δk✓POS −✓0 POSk2, similar to that of POS tagging, and ✓chunk is (Wchunk, bchunk, EPOS, ✓e), wher k are the weight and bias parameters including those in ✓POS, and EPOS is the embeddings. ✓0 POS is the one after training the POS layer at the current training AINING THE DEPENDENCY LAYER tive function is deﬁned as follows: X t log p(↵|h(3) t )p(β|h(3) t , h(3) ↵) + λ(kWdepk2 + kWdk2) + δk✓chunk −✓0 chunk |h(3)) i th b bilit l i d t th t t d f AINING THE RELATEDNESS LAYER Tai et al. (2015), the objective function is deﬁned as follows: X (s,s0) KL ⇣ ˆ p(s, s0) # # #p(h(4) s , h(4) s0 ) ⌘ + λkWrelk2 + δk✓dep −✓0 depk2 , s0) is the gold distribution over the deﬁned relatedness scores, p(h(4) s , h(4) s0 ) is tribution given the the sentence representations, and KL ⇣ ˆ p(s, s0) # # #p(h(4) s , h(4) s0 ence between the two distributions. ✓rel is deﬁned as (Wrel, brel, EPOS, Echunk, AINING THE ENTAILMENT LAYER tive function is deﬁned as follows: − X (s,s0) log p(y(5) (s,s0) = ↵|h(5) s , h(5) s0 ) + λkWentk2 + δk✓rel −✓0 relk2, (5) (s,s0) = ↵|h(5) s , h(5) s0 ) is the probability value that the correct label ↵is assign ypothesis pair (s, s0). ✓ent is deﬁned as (Went, bent, EPOS, Echunk, Erel, ✓e), wh of the relatedness label embeddings. Chunking training Entailment training Joint Training Helps Here! New State of the Art on 4 of 5 Tasks Dependency LAS 91.42 92.90 n/a 92.92 n/a D Relatedness 0.247 0.233 n/a n/a 0.238 E Entailment 81.8 86.2 n/a n/a 86.8 Table 1: Test set results for the ﬁve tasks. In the relatedness task, the lower scores are better. Method Acc. JMTall 97.55 Ling et al. (2015) 97.78 Kumar et al. (2016) 97.56 Ma & Hovy (2016) 97.55 Søgaard (2011) 97.50 Collobert et al. (2011) 97.29 Tsuruoka et al. (2011) 97.28 Toutanova et al. (2003) 97.27 Table 2: POS tagging results. Method F1 JMTAB 95.77 Søgaard & Goldberg (2016) 95.56 Suzuki & Isozaki (2008) 95.15 Collobert et al. (2011) 94.32 Kudo & Matsumoto (2001) 93.91 Tsuruoka et al. (2011) 93.81 Table 3: Chunking results. Method UAS LAS JMTall 94.67 92.90 Single 93.35 91.42 Andor et al. (2016) 94.61 92.79 Alberti et al. (2015) 94.23 92.36 Weiss et al. (2015) 93.99 92.05 Dyer et al. (2015) 93.10 90.90 Bohnet (2010) 92.88 90.71 Table 4: Dependency results. Method MSE JMTall 0.233 JMTDE 0.238 Zhou et al. (2016) 0.243 Tai et al. (2015) 0.253 Table 5: Semantic relatedness results. Method Acc. JMTall 86.2 JMTDE 86.8 Yin et al. (2016) 86.2 Lai & Hockenmaier (2014) 84.6 Table 6: Textual entailment results. 6.2 COMPARISON WITH PUBLISHED RESULTS POS tagging: Table 2 shows the results of POS tagging, and our JMT model achieves the score Progress of Recent Weeks • Joint model trained on harder tasks • Single task models Joint many-task model • First solution described in class Obstacle: Duplicate Word Representations • Different encodings for encoder (Word2Vec and GloVe word vectors) and decoder (softmax classification weights for words) • Duplicate parameters/meaning Pointer Sentinel Mixture Models · · · Sentinel x RNN Distribution pvocab(yN|w1, . . . , wN−1) pvocab(yN|w1, . . . , wN−1) Pointer Distribution pptr(yN|w1, . . . , wN−1) pptr(yN|w1, . . . , wN−1) Output Distribution p(yN|w1, . . . , wN−1) p(yN|w1, . . . , wN−1) Sentinel Query RNN Embed + ··· ··· Softmax Softmax · · · · · · · · · Mixture gate g Tackling Obstacle by Tying Word Vectors • Simple but theoretically motivated idea: tie word vectors and train single weights jointly • Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling, Hakan Inan, Khashayar Khosravi, Richard Socher (ICLR 2017) Language Modeling With Tying Word Vectors Table 2: Comparison of our work to previous state of the art on word-level validation and test perplexities on the Penn Treebank corpus. MODEL PARAMETERS VALIDATION TEST KN-5 (Mikolov & Zweig) 2M - 141.2 KN-5 + Cache (Mikolov & Zweig) 2M - 125.7 RNN (Mikolov & Zweig) 6M - 124.7 RNN+LDA (Mikolov & Zweig) 7M - 113.7 RNN+LDA+KN-5+Cache (Mikolov & Zweig) 9M - 92.0 Deep RNN (Pascanu et al., 2013a) 6M - 107.5 Sum-Prod Net (Cheng et al., 2014) 5M - 100.0 LSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7 LSTM (large) (Zaremba et al., 2014) 66M 82.2 78.4 VD-LSTM (medium, untied) (Gal, 2015) 20M 81.9 ± 0.2 79.7 ± 0.1 VD-LSTM (medium, untied, MC) (Gal, 2015) 20M - 78.6 ± 0.1 VD-LSTM (large, untied) (Gal, 2015) 66M 77.9 ± 0.3 75.2 ± 0.2 VD-LSTM (large, untied, MC) (Gal, 2015) 66M - 73.4 ± 0.0 CharCNN (Kim et al., 2015) 19M - 78.9 VD-RHN (Zilly et al., 2016) 32M 72.8 71.3 Pointer Sentinel-LSTM(medium) (Merity et al., 2016) 21M 72.4 70.9 38 Large LSTMs (Zaremba et al., 2014) 2.51B 71.9 68.7 10 Large VD-LSTMs (Gal, 2015) 660M - 68.7 VD-LSTM +REAL (medium) 14M 75.7 73.2 VD-LSTM +REAL (large) 51M 71.1 68.5 Obstacle: Necessary Inputs to QA • We need to be able to understand text, images and databases to really answer all kinds of questions Database QA • Question: Who was drafted with the 3rd pick of the 1st round? • Answer: Jayson Tatum Seq2SQL Overview Seq2SQL for DB QA Database QA Results Recent Visual Question Answering • Interpretable Reinforcement Learning Counter • Good for discrete reasoning over images • Interpretable Counting for Visual Question Answering, Alexander Trott, Caiming Xiong, Richard Socher. ICLR 2018 Results Results Obstacle: Architecture Engineering • We don’t yet know the right model architecture for comprehensive QA & joint multitask learning • Architecture Search is an active area of research but usually applied to simpler/known tasks, e.g. • A Flexible Approach to Automated RNN Architecture Generation, Stephen Merity, Martin Schrimpf, James Bradbury, Richard Socher. (ICLR 2018 Workshop Track) Architecture Search Overview When trained on language modeling Interesting new building blocks like cos Very different activation patterns! Recent Work on Architecture Search • Efficient Neural Architecture Search via Parameter Sharing • Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean • Mon, 12 Feb 2018 :) • 1000x more efficient and finds better models • Shares parameters between models instead of training from scratch Lots of Limits for deepNLP • Comprehensive QA • Multitask learning • Combined multimodal, logical and memory-based reasoning • Learning from few examples onference paper at ICLR 2017 STM CNN fo-Pool Convolution fo-Pool Convolution Max-Pool Convolution Max-Pool Convolution QRNN Under review as a conference paper at ICLR 2017 AQ AD document product concat product bi-LSTM bi-LSTM bi-LSTM bi-LSTM bi-LSTM concat n+1 m+1 D: Q: CQ CD ut U: ℓ ℓ Figure 2: Coattention encoder The afﬁnity matrix L is not shown here We instead directly show Under review as a conference paper at ICLR 2017 CHUNK POS DEP Relatedness encoder Relatedness Entailment encoder Entailment word representation Sentence1 CHUNK POS DEP Relatedness encoder Entailment encoder word representation Sentence2 semantic level syntactic level word level Figure 1: Overview of the joint many-task model predicting different linguistic sively deeper layers 2 THE JOINT MANY-TASK MODEL In this section, we assume that the model is trained and describe its inference pr at the lowest level and work our way to higher layers and more complex tasks. 2.1 WORD REPRESENTATIONS For each word wt in the input sentence s of length L, we construct a representatio a word and a character embedding. DeepNLP Congratulations! Good luck with the projects "
475,"Reinforcement Learning for NLP Caiming Xiong Salesforce Research CS224N/Ling284 Outline Introduction to Reinforcement Learning Policy-based Deep RL Value-based Deep RL Examples of RL for NLP Many Faces of RL By David Silver What is RL? ● RL is a general-purpose framework for sequential decision-making ● Usually describe as agent interacting with unknown environment ● Goal: select action to maximize a future cumulative reward Agent Environment Action 𝑎 Reward 𝑟, Observation 𝑜 Motor Control ● Observations: images from camera, joint angle ● Actions: joint torques ● Rewards: navigate to target location, serve and protect humans Business Management ● Observations: current inventory levels and sales history ● Actions: number of units of each product to purchase ● Rewards: future profit Similarly, there also are resource allocation and routing problems …. Games State ● Experience is a sequence of observations, actions, rewards ● The state is a summary of experience RL Agent Major components: ● Policy: agent’s behavior function ● Value function: how good would be each state and/or action ● Model: agent’s prediction/representation of the environment Policy A function that maps from state to action: ● Deterministic policy: ● Stochastic policy: Value Function ● Q-value function gives expected future total reward ○ from state and action (𝑠, 𝑎) ○ under policy 𝜋 ○ with discount factor 𝛾∈(0,1) ○ Show how good current policy ● Value functions can be defined using Bellman equation ● Bellman backup operator 𝐵𝑄/ 𝑠, 𝑎= B/Q s, a = E67,87[r + 𝛾𝑄/(𝑠<, 𝑎<)|𝑠, 𝑎] Value Function ● For optimal Q-value function 𝑄∗𝑠, 𝑎= max / 𝑄/(𝑠, 𝑎) , then policy function is deterministic, the Bellman equation becomes: B/Q s, a = E67[r + 𝛾max B7 𝑄/(𝑠<, 𝑎<)|𝑠, 𝑎] What is Deep RL? ● Use deep neural network to approximate ○ Policy ○ Value function ○ Model ● Optimized by SGD Approaches ● Policy-based Deep RL ● Value-based Deep RL ● Model-based Deep RL Deep Policy Network ● Represent policy by deep neural network that max C 𝐸B~G(B|C,H)[𝑟(𝑎)|𝜃, 𝑠] ● Ideas: given a bunch of trajectories, ○ Make the good trajectories/action more probable ○ Push the actions towards good actions Policy Gradient How to make high-reward actions more likely: ● Let's 𝑟𝑎say that measures how good the sample is. ● Moving in the direction of gradient pushes up the probability of the sample, in proportion to how good it is. Deep Q-Learning ● Represent value function by Q-network Deep Q-Learning ● Optimal Q-values should obey Bellman equation ● Treat right-hand side as target network, given 𝑠, 𝑎, 𝑟, 𝑠< , optimize MSE loss via SGD: ● Converges to optimal Q using table lookup representation Deep Q-Learning But diverges using neural networks due to: ● Correlations between samples ● Non-stationary targets Deep Q-Learning Experience Replay: remove correlations, build data-set from agent's own experience ● Sample experiences from data-set and apply update ● To deal with non-stationarity, target parameters is fixed Deep Q-Learning in Atari Network architecture and hyperparameters fixed across all games By David Silver By David Silver Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto Second Edition, in progress MIT Press, Cambridge, MA, 2017 If you want to know more about RL, suggest to read: RL in NLP ● Article summarization ● Question answering ● Dialogue generation ● Dialogue System ● Knowledge-based QA ● Machine Translation ● Text generation RL in NLP ● Article summarization ● Question answering ● Dialogue generation ● Dialogue System ● Knowledge-based QA ● Machine Translation ● Text generation Article Summarization Text summarization is the process of automatically generating natural language summaries from an input document while retaining the important points. • extractive summarization • abstractive summarization A Deep Reinforced Model for Abstractive Summarization Paulus et. al. Given x = {𝑥L, 𝑥M, ⋯, 𝑥O} represents the sequence of input (article) tokens, 𝑦= {𝑦L, 𝑦M, ⋯, 𝑦R}, the sequence of output (summary) tokens Coping word Generating word Paulus et. al. The maximum-likelihood training objective: Training with teacher forcing algorithm. A Deep Reinforced Model for Abstractive Summarization There is discrepancy between training and test performance, because • exposure bias • potentially valid summaries • metric difference Paulus et. al. A Deep Reinforced Model for Abstractive Summarization Paulus et. al. Using reinforcement learning framework, learn a policy that maximizes a specific discrete metric. Action: 𝑢T ∈𝑐𝑜𝑝𝑦, 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒and word 𝑦T H State: hidden states of encoder and previous outputs Reward: ROUGH score A Deep Reinforced Model for Abstractive Summarization Where 𝑝𝑦T H 𝑦L H, ⋯, 𝑦T[L H , 𝑥= 𝑝𝑢T = 𝑐𝑜𝑝𝑦𝑝𝑦T H 𝑦L H, ⋯, 𝑦T[L H , 𝑥, 𝑢T = 𝑐𝑜𝑝𝑦 +𝑝𝑢T = 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒𝑝(𝑦T H|𝑦L H, ⋯, 𝑦T[L H , 𝑥, 𝑢T = 𝑔𝑒𝑛𝑒𝑟𝑎𝑡𝑒) Paulus et. al. A Deep Reinforced Model for Abstractive Summarization Human readability scores on a random subset of the CNN/Daily Mail test dataset Paulus et. al. A Deep Reinforced Model for Abstractive Summarization RL in NLP ● Article summarization ● Question answering ● Dialogue generation ● Dialogue System ● Knowledge-based QA ● Machine Translation ● Text generation Text Question Answering Example from SQuaD dataset Text Question Answering Encoder Layer Attention Layer Decoder Pointer Encoder Layer Loss function layer P Q LSTM, GRU Self-attention biAttention Coattention LSTM + MLP GRU + MLP Cross Entropy DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING Constraints of Cross-Entropy loss: P: “Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history,…” Q: “which team is considered to be one of the greatest teams in NBA history” GT: “the Golden State Warriors team of 2017” Ans1: “Warriors” Ans2: “history” Xiong et. al. To address this, we introduce F1 score as extra objective combining with traditional cross entropy loss: Not necessary for variable length. Xiong et. al. DCN+: MIXED OBJECTIVE AND DEEP RESIDUAL COATTENTION FOR QUESTION ANSWERING RL in NLP ● Article summarization ● Question answering ● Dialogue generation ● Dialogue System ● Knowledge-based QA ● Machine Translation ● Text generation Deep Reinforcement Learning for Dialogue Generation Li et. al. To generate responses for conversational agents. The LSTM sequence-to-sequence (SEQ2SEQ) model is one type of neural generation model that maximizes the probability of generating a response given the previous dialogue turn. However, • One concrete example is that SEQ2SEQ models tend to generate highly generic responses • stuck in an infinite loop of repetitive responses Li et. al. To solve these, the model needs: • integrate developer-defined rewards that better mimic the true goal of chatbot development • model the long term influence of a generated response in an ongoing dialogue Deep Reinforcement Learning for Dialogue Generation Definitions: Action: infinite since arbitrary-length sequences can be generated. State: A state is denoted by the previous two dialogue turns [𝑝\, 𝑞\]. Reward: Ease of answering, Information Flow and Semantic Coherence Li et. al. Deep Reinforcement Learning for Dialogue Generation ● Ease of answering: avoid utterance with a dull response. The is a list of dull responses such as “I don’t know what you are talking about”, “I have no idea”, etc. Li et. al. Deep Reinforcement Learning for Dialogue Generation S ● Information Flow: penalize semantic similarity between consecutive turns from the same agent. Li et. al. Deep Reinforcement Learning for Dialogue Generation Where ℎG_ and ℎG_`adenote representations obtained from the encoder for two consecutive turns 𝑝\ and 𝑝\bL ● Semantic Coherence: avoid situations in which the generated replies are highly rewarded but are ungrammatical or not coherent ● The final reward for action a is a weighted sum of the rewards Li et. al. Deep Reinforcement Learning for Dialogue Generation ● Simulation of two agents taking turns that explore state-action space and learning a policy ○ Supervised learning for Seq2Seq models ○ Mutual Information for pretraining policy model ○ Dialogue Simulation between Two Agents Li et. al. Deep Reinforcement Learning for Dialogue Generation ● Simulation of two agents taking turns that explore state-action space and learning a policy ○ Supervised learning for Seq2Seq models ○ Mutual Information for pretraining policy model ○ Dialogue Simulation between Two Agents Li et. al. Deep Reinforcement Learning for Dialogue Generation ● Mutual Information for previous sequence 𝑆and response 𝑇 ● MMI objective Li et. al. Deep Reinforcement Learning for Dialogue Generation 𝜆∶ controls the penalization for generic response Li et. al. Deep Reinforcement Learning for Dialogue Generation Consider 𝑆as (𝒒𝒊, 𝒑𝒊), 𝑇as 𝑎, we can have ● Simulation ○ Supervised learning for Seq2Seq models ○ Mutual Information for pretraining policy model ○ Dialogue Simulation between Two Agents Li et. al. Deep Reinforcement Learning for Dialogue Generation ○ Dialogue Simulation between Two Agents Li et. al. Deep Reinforcement Learning for Dialogue Generation • Using the simulated turns and reward, maximize the expected future reward. • Training trick: Curriculum Learning Li et. al. Deep Reinforcement Learning for Dialogue Generation Summary ● The introduction of Reinforcement Learning ● Deep Policy Learning ● Deep Q-Learning ● Applications on NLP ○ Article summarization ○ Question answering ○ Dialogue generation "
476,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 1: Introduction Semester 2 2017/18 Prof. Xavier Bresson xbresson@ntu.edu.sg Xavier Bresson 2 About Me - Prof of Computer Science - Undergraduate in France, PhD in Switzerland, Postdoctorate in the US - Member of NTU Data Science and AI Research Centre - NRF Fellowship for DS research - Teach Master, PhD courses in DS since 2015 - Consulting/training in DS for companies - Publications at NIPS, ICML, JMLR http://www.ntu.edu.sg/home/xbresson https://github.com/xbresson https://twitter.com/xbresson https://www.facebook.com/xavier.bresson.1 https://www.linkedin.com/in/xavier-bresson-738585b Office: N4-02b-40 Tel: 6790 6300 Xavier Bresson 3 Students CE9010 Class Logistics Xavier Bresson 4 Ø Class Schedule Ø Lectures: 2 hours on Wednesdays 12:30-14:30, LT10 Ø Tutorials: 1 hour on Wednesdays 16:30-17:30, LT10, starting Week 2 Ø Laboratories: 5 x 2 hours on Wednesdays 14:30-16:30, SWLAB1/N4-01a-02, Weeks 5,7,9,11,13 Ø Happy for extra discussion on Wednesdays 17:30-18:30 at my office N4-02b-40. Ø Use your own laptops for the coding exercises (tutorials, labs) ⇒It is important to learn to code (e.g. 15min/day). Ø Use NTULearn for course material and https://github.com/xbresson/CE9010_2018 Ø Evaluation Ø Laboratory: 30% Ø Project: 30% Ø Final Examination: 40% Course Schedule Xavier Bresson 5 Week Topic 1 Introduction to data science learning techniques 2-4 Supervised regression 5 Supervised classification 6 Gradient descent tricks 7 Generalization and regularization 8-9 Developing data science projects 10-11 Unsupervised learning 12 Recommender systems 13 Neural Networks Ø The course structure follows the successful Andrew Ng Coursera course on Machine learning: Data Scientist Xavier Bresson 6 Best job in the U.S since 2015 [Forbes, LinkedIn]. Salary has jumped from $125,000 to $200,000+ [Glassdoor]. McKinsey projected in 2015 that “by 2018, the U.S. alone may face a 50 percent to 60 percent gap between supply and requisite demand of deep analytic talent.” Source: Drew Conway In the News Xavier Bresson 7 What is Data Science? Ø Data science is an emerging field that develops mathematical and algorithmic techniques to analyze (big) data and solve real- world problems. Xavier Bresson 8 FIND KNOLEDGE IN DATA YOU MUST DATA SCIENTIST YOU MUST BECOME Ø Data science has different names: machine learning, AI. Ø Data science solves tasks by learning from data (like the brain). Example Ø Google search engine Xavier Bresson 9 Example Ø Netflix: Movie recommendations Xavier Bresson 10 Example Ø Facebook face recognition Xavier Bresson 11 Example Ø Google translate machine Xavier Bresson 12 Example Ø Self-driving cars Xavier Bresson 13 Example Ø Robots Xavier Bresson 14 Example Ø Medical diagnostic for skin cancer Xavier Bresson 15 Example Ø Playing games Xavier Bresson 16 Data Collection of massive amounts of data at increasing rate. E.g. Social networks, sensor networks, mobile devices, biological networks, administrative, economics data Issues of privacy, security, ownership Data Science Multidisciplinary field: “1+1=3” Computer Science Scalable databases for storing, accessing data. E.g. Cloud computing, Amazon EC2, Hadoop. Distributed and parallel frameworks for data processing. E.g. MapReduce, GraphLab. Intelligent Systems E.g. Autonomous cars, chatbots, security, interactive tools for data organization and exploration. Personalized Services E.g. Healthcare (enhanced diagnostics) Commerce (products) Knowledge Discovery E.g. Physics, genomics, social sciences. Mathematical Modeling Design algorithms that transform data into knowledge. Use Linear algebra, optimization, graph theory, statistics. Domain Expertise Sciences E.g. Economy, Biology, Physics, Neuroscience, sociology. Government E.g. Healthcare, Defense, Education, Transportation. Industry E.g. E-commerce, Telecommunications, Finance. What is Data Science? - Long Answer Major challenges: Multidisciplinary integration, large-scale databases, scalable computational infrastructures, design math algorithms for massive datasets, trade-off speed and accuracy for real- time decisions, interactive visualization tools. Q: What are the fields of Data Science? Q: What are the applications? Q: What are the main challenges? What is Data Science? – Short Answer Xavier Bresson 18 Data Science = Big Data + Computational Infrastructure + Artificial Intelligence 3rd industrial revolution Cloud computing GPU Not new! Q: Is AI new? Q: What is big data? A brief history of artificial neural networks Xavier Bresson 19 1958 Perceptron Rosenblatt 1982 Backprop Werbos 1959 Visual primary cortex Hubel-Wiesel 1987 Neocognitron Fukushima SVM/Kernel techniques Vapnik 1995 1997 1998 1999 2006 2012 2014 2015 Data scientist 1st Job in US Facebook Center OpenAI Center Deep Learning Breakthough Hinton GAN, Goodfellow ResNet, He Deep RL, Mnih Attention, Bengio First NVIDIA GPU First Amazon Cloud Center RNN/LSTM Schmidhuber CNN LeCun-Bengio AI Winter [1960’s-2012] AI Birth AI Renaissance Hardware GPU speed 2x/1.5 year Kernel techniques Graphical models Wavelets, numerical PDEs Handcrafted Features 4th industrial revolution? Digital intelligence revolution or new AI bubble? Big Data Volume 2x/1.5 year Google AI TensorFlow Facebook AI Torch 1962 Birth of Data Science Split from Statistics Tukey First NIPS 1989 First KDD 2010 Kaggle Platform Q: Have you heard about the 4th industrial revolution? Intended Learning Outcomes (ILO) Ø By the end of this course, you (as a student) would be expected to be able to: Ø Identify and apply data analysis techniques to different data problems. Ø Implement data science techniques with Python. Ø Analyze and solve real-world data science projects. Ø Engage more professionally in written and oral scientific communication. Ø Work cohesively and effectively in teams. Xavier Bresson 20 Why you should consider this course? Xavier Bresson 21 1. Short-term goal: Learn how to apply the most essential data analysis tools. 2. Long-term goal: Data scientist has become the most- wanted job in all industries. Data Science is coming. Better be ready! Reference book Ø Pattern Recognition and Machine Learning Christopher Bishop Director at Microsoft Research Cambridge Springer-Verlag New York 2006 ISBN 978-0-387-31073-2 Xavier Bresson 22 Questions are welcome! Xavier Bresson 23 "
477,"CE9010: Introduction to Data Science Xavier Bresson !""#$%&'(&%))*+ , School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 2: Learning Techniques Semester 2 2017/18 Learning techniques !""#$%&'(&%))*+ - Supervised learning (recent breakthrough 2012-) Unsupervised learning (still incomplete) Reinforcement learning (recent breakthrough 2015-) Predictive learning (main obstacle to general AI) Learning techniques !""#$%&'(&%))*+ . Supervised learning (recent breakthrough 2012-) Supervised learning !""#$%&'(&%))*+ / Definition: Given a set of data and labels (some properties of the data), design a learning algorithm to predict label for new data (never observed before). Supervised learning algorithm Data Label Learn to predict Regression task Classification task Movie Rating Regression: Housing price prediction !""#$%&'(&%))*+ 0 Predict the price (continuous value) of houses given existing data label (house size). Fit a curve to the data Size Price 200K$ 50m2 What is the price for a house of size 125m2? Supervised regression predicts 180K$. 125m2 280K$ 125m2 280K$ Classification: Disease prediction !""#$%&'(&%))*+ 1 Predict disease class (discrete value) of patient given existing medical data (tumor size). 0/No 1/Yes Size Disease classes Is the tumor benign/malign? Supervised classification predicts benign. Malign Benign Multiple classes !""#$%&'(&%))*+ 2 From binary class to multi-class prediction Example: Disease classes can be {benign, type I, type II, type III}. 0 Size Disease classes Type I Benign 1 2 3 Type II Type III What is the disease class? Supervised classification predicts Type I cancer. Multiple features !""#$%&'(&%))*+ 3 Data have 2 features: Size Age Benign Malign Feature/ variable #2 Feature/ variable #1 What is the disease class? Data dimension is the number of features (here dim(data)=2). High-dimensional learning !""#$%&'(&%))*+ 4 Generalization: Data with N>2 features, for examples 1K, 1M features  Curse of dimensionality (too many variables) Examples dim( Go ) = 19 x 19 5 103 dim( Images ) = 512 x 512 5 106 dim( Books ) = 80,000 5 105 Quiz Xavier Bresson 10 You’re running a company, and you want to develop learning algorithms to address each of two problems. Problem 1: You have a large inventory of identical items. You want to predict how many of these items will sell over the next 3 months. Problem 2: You’d like software to examine individual customer accounts, and for each account decide if it has been hacked/compromised. Should you treat these as classification or as regression problems? Treat both as classification problems. Treat problem 1 as a classification problem, problem 2 as a regression problem. Treat problem 1 as a regression problem, problem 2 as a classification problem. Treat both as regression problems. Learning techniques !""#$%&'(&%))*+ ,, Unsupervised learning (still incomplete) Unsupervised learning !""#$%&'(&%))*+ ,- Difference between supervised and unsupervised learning: Supervised learning {data,label} Unupervised learning {data,-} No label, no additional information Definition: Find structures in data that can solve tasks like clustering, classification, compression, generative model. labels No labels Example 1: Data clustering !""#$%&'(&%))*+ ,. Definition: Group similar data into groups Example: News articles (Google News) Example 2: Google engine !""#$%&'(&%))*+ ,/ Google PageRank recommender system: Example 3: Data analysis !""#$%&'(&%))*+ ,0 Gene expression with PCA (unsupervised data representation): Example 4: Data generation !""#$%&'(&%))*+ ,1 Generative Adversarial Networks (GANs): NVIDIA’17 Quiz Xavier Bresson 17 Of the following examples, which would you address using an unsupervised learning algorithm? Given email labeled as spam/not spam, learn a spam filter. Given a set of news articles found on the web, group them into set of articles about the same story. Given a database of customer data, automatically discover market segments and group customers into different market segments. Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabetes or not. Learning techniques !""#$%&'(&%))*+ ,3 Reinforcement learning (recent breakthrough 2015-) Atari Pong !""#$%&'(&%))*+ ,4 AI gamer: Move Up or Down Reward: +1 if the ball makes it past the opponent (winning game) -1 if we missed the ball (lost game) Goal: Move paddle to maximize reward Atari Pong !""#$%&'(&%))*+ -6 Actions: Move Up or Down Reward: +1 if the ball makes it past the opponent (winning game) -1 if we missed the ball (lost game) Goal: Move paddle to maximize reward Examples !""#$%&'(&%))*+ -, AI Gaming (Doom), Go Examples !""#$%&'(&%))*+ -- Robots Reinforcement learning !""#$%&'(&%))*+ -. Definition: Learn a sequence of actions that maximizes a reward. Agent Environment/ World Action Reward RL agents learn to plan the future to win. Big issue: Lots of data required because the reward is sparse (more difficult than supervised learning because at each action, a positive reward/label is given). Quiz Xavier Bresson 24 Which of these examples use supervised learning and reinforcement learning? Given a robot that can move up, down, left, right in known mazes, develop a strategy to find the exit in a new unseen maze. Given the chess game and a professional player who evaluates and provides all moves for several games, design a technique to win new games. Given a self-driving car and millions of miles collected from human driving, develop a smart car system. Learning techniques !""#$%&'(&%))*+ -0 Predictive learning (main obstacle to general AI) Predictive learning/common sense [LeCun] !""#$%&'(&%))*+ -1 Definition: Unsupervised learning of the world and its laws by observing data, acting, and predicting the future, like Physics. Main obstacle to general AI today. Common sense Xavier Bresson 27 Human learning is unsupervised, and use observation-action on the world. Humans are able to learn in the first years physical perceptions like depth, 3-dimensionality, gravity, object permanence, etc. No need for advanced mathematical concepts like PDEs (Newton’s laws). Peek-a-boo Learn object permanence Early concept acquisition Xavier Bresson 28 How do we get the machines to learn common sense? Predicting past, present or future from available information? Humans learn common sense through perception and production Conclusion !""#$%&'(&%))*+ -4 Supervised learning (recent breakthrough 2012-) Unsupervised learning (still incomplete) Reinforcement learning (recent breakthrough 2015-) Predictive learning (main obstacle to general AI) Questions? Xavier Bresson 30 "
478,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 3: Supervised Linear Regression Semester 2 2017/18 Outline Xavier Bresson 2 Regression task Supervised learning Linear regression Loss function Optimization with gradient descent Generic case Linear regression case Regression with multiple variables Non-linear regression Optimization with normal equations Conclusion Outline Xavier Bresson 3 Regression task Supervised learning Housing price prediction Xavier Bresson 4 Supervised regression problem: Predict the price (continuous value) of houses given existing data features/properties (house size). Fit a curve to the data Size Price 200K$ 50m2 What is the price for a house of size 125m2? Supervised regression predicts 280K$. 125m2 280K$ Formalization Xavier Bresson 5 Supervised learning algorithm f Input: x (data) Output: y (data label) f : x→ y or y=f(x) Training dataset (x,y) here: house size here: house price For regression, y is a scalar f : prediction function f maps data feature x to data label y Supervised regression as an example of supervised learning: Xavier Bresson 6 Training set All supervised learning techniques use a training set to design the prediction function f. Notations: n=number of training data, here n=8 x=input variable/feature, here x=size y=output variable/label, here y=price One training data/sample is identified as (xi,yi) where i is the index of the training data. Examples: (x1,y1)=(31,124), (x2,y2)=(54,156). Size/x Price/y 200K$ 50m2 Size (x) 31 54 … Price (y) 124 156 … Xavier Bresson 7 Training set Performance of SL techniques depend on the size of the training sets. The bigger the better (super-human for some tasks): Object recognition: 1.5M images Self-driving cars: Millions of miles (several years) Go: Millions of games Medical diagnostic: Thousands of scans and radiologist reports The challenging issue is data collection: Time consuming, expensive, sometimes not available. Humans transfer biases to data. Talk by Kate Crawford at NIPS17: https://nips.cc/Conferences/2017/Schedule?showEvent=8742 Xavier Bresson 8 Quiz Quality of supervised learning depends on training set. Let’s take an example, we want to develop a learning algorithm that predict student marks for a particular course like algebra given students profile: How much time to collect students profile? How much time to collect final exams? How much time to digitalize them? How many profiles do we need to get a good estimation? Outline Xavier Bresson 9 Regression task Linear regression Xavier Bresson 10 What is the simplest model representation to regress the data? Straight line. Fit a straight line to the data Size Price 200K$ 50m2 What is the price for a house of size 125m2? Supervised regression predicts 265K$. 125m2 265K$ Linear regression Xavier Bresson 11 How to represent the prediction function f ? Case of linear regression: Model representation y = f(x) y = f(x) Non-linear function Example: Neural networks Linear function Xavier Bresson 12 Predictor function f as a straight line: Linear representation y = fw(x) = w0 + w1x x y Linear predictive function/ Linear regression model with one single variable x Compact representation of all variables/parameters of prediction function f w = (w0, w1) Model parameters Data feature Xavier Bresson 13 Prediction function: Influence of different parameter values (w0,w1) on the prediction: Prediction function parameters fw(x) = w0 + w1x Model parameters w0=1.3 w1=0 w0=2.9 w1=-0.5 w0=-1.4 w1=10.4 fw(x) fw(x) fw(x) How to find the best possible straight line (w0,w1) that fits all data? A fitness measure is required  Loss function. Xavier Bresson 14 Quiz Linear prediction function with 2 data features: What is the representation of the prediction function with 2 variables? fw(x(1), x(2)) = w0 + w1x(1) + w2x(2) fw(x(1), x(2)) x(2) x(1) Outline Xavier Bresson 15 Regression task Loss function Xavier Bresson 16 Idea: Choose parameters (w0,w1) such that fw(xi) is close to yi for all training data (xi,yi), that is How to choose the parameters? yi ⇡fw(xi) xi yi fw(xi) Xavier Bresson 17 Find parameters (w0,w1) by solving a minimization problem: Formalization min w=(w0,w1) 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 xi yi fw(xi) Prediction given xi Prediction error yi −fw(xi) Sum over the n training data Average prediction error L(w) Loss function Remarks: Generic optimization problem in supervised learning (f is not specified here). Loss function is also called cost function. This loss function is called mean square error (most used regression loss). Xavier Bresson 18 Linear regression loss min w=(w0,w1) 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 min w=(w0,w1) 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 fw(xi) = w0 + w1xi ) L(w0, w1) From generic to linear regression task: Linear regression loss Xavier Bresson 19 Prediction function: Parameters: Loss function: Optimization: Let us simplify with a single parameter: w0=0, w1=w Loss analysis L(w0, w1) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 min w=(w0,w1) L(w0, w1) w0, w1 fw(x) = w0 + w1x w0=0 fw(x) = wx min w L(w) = 1 n n X i=1 ⇣ wxi −yi ⌘2 Xavier Bresson 20 Loss analysis Prediction function fw(x) Loss function L(w) Data feature Prediction parameter 1 2 3 1 2 3 fw(x) = x, w = 1 x fw(x) 0.5 1 1.5 0 w L(w) L(w = 1) = 1 n n X i=1 ⇣ xi −yi ⌘2 1 3 ⇣ (1 −1)2 + (2 −2)2 + (3 −3)2⌘ = 0 L(w = 1) = 0 No error in prediction  loss=0 Xavier Bresson 21 Loss analysis Prediction function fw(x) Loss function L(w) Prediction parameter Data feature 1 2 3 1 2 3 x fw(x) fw(x) = x, w = 0.5 0.5 1 1.5 0 w L(w) L(w = 1) = 1 n n X i=1 ⇣ 0.5xi −yi ⌘2 1 3 ⇣ (0.5 −1)2 + (1 −2)2 + (1.5 −3)2⌘ = 1.16 L(w = 0.5) = 1.16 Xavier Bresson 22 Loss analysis Prediction function fw(x) Loss function L(w) Prediction parameter Data feature 1 2 3 1 2 3 x fw(x) 0.5 1 1.5 0 w L(w) fw(x) = x, w = 1.5 L(w = 1.5) = 1 n n X i=1 ⇣ 1.5xi −yi ⌘2 1 3 ⇣ (1.5 −1)2 + (3 −2)2 + (4.5 −3)2⌘ = 1.16 L(w = 1.5) = 1.16 Xavier Bresson 23 Compute L(w) for multiple values w: Loss analysis min w L(w) = 1 n n X i=1 ⇣ wxi −yi ⌘2 w = 1 ) 0.5 1 1.5 0 w L(w) Quadratic loss function 1 2 3 1 2 3 x fw(x) 1 2 3 1 2 3 1 2 3 1 2 3 Parameter w=1 provides a perfect prediction function for all training data. Xavier Bresson 24 Loss function with 2 parameters Prediction function: Parameters: Loss function: Optimization: L(w0, w1) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 min w=(w0,w1) L(w0, w1) w0, w1 fw(x) = w0 + w1x w0=2.9 w1=-0.5 fw(x) w0 L(w0) Single parameter Loss is represented by a curve Two parameters What is the loss representation? 3D surface w0 w1 L(w0, w1) Xavier Bresson 25 Loss function with 2 parameters w0 w1 L(w0, w1) Loss is represented by a 3D surface. It is called the landscape. fw(x) fw(x) fw(x) min w=(w0,w1) L(w0, w1) Minimum Xavier Bresson 26 Level sets/iso-contours = curves with the same loss values. They offer a 2D visualization of the 3D loss surface. Contours of L min L L=1 L=2 L=3 w0 w1 L(w0, w1) w0 w1 L(w0, w1) Xavier Bresson 27 Loss function with 2 parameters How to find the value of the parameters (w0,w1) that minimize the loss?  Gradient descent. w0 w1 L(w0, w1) fw(x) fw(x) fw(x) min w=(w0,w1) L(w0, w1) Minimum Xavier Bresson 28 Quiz Expression of the mean square error loss: L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 Can we use alternate regression loss? Mean absolute error loss: Preferably, any convex function: L(w) w L(w) = 1 n n X i=1 "" "" ""fw(xi) −yi "" "" "" Outline Xavier Bresson 29 Optimization with gradient descent Generic case Xavier Bresson 30 Optimization problem Prediction function: Parameters: Loss function: Optimization: L(w0, w1) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 min w=(w0,w1) L(w0, w1) w0, w1 fw(x) = w0 + w1x w0 w1 L(w0, w1) fw(x) min w=(w0,w1) L(w0, w1) Minimum Xavier Bresson 31 Optimization by brute-force approach Try all possible parameter values (w0,w1) to find the minimizer: w0 w1 L(w0, w1) Complexity: Ndim  Exponential L Here, N=8, dim=2 Xavier Bresson 32 Gradient descent technique: Initialization: Start with some values (w0,w1). Iterate (loop): Keep updating (w0,w1) to reduce the loss value L(w0,w1) until a minimum is reached (when no possible update is possible). Optimization by gradient descent L(w0, w1) w0 w1 Initialization (w0,w1) Loss landscape Xavier Bresson 33 Dynamic process L(w0, w1) w0 w1 Idea: Move the values of (w0,w1) in the direction of the steepest descent to decrease the value of the loss L as fast as possible. Init 1st move 2nd move kth move Minimum (w0,w1) Xavier Bresson 34 How to start? Initialization can be random. It can also be estimated if additional information on L is available. Initialization L(w0, w1) w0 w1 L(w0, w1) w0 w1 Initialization #1 Initialization #2 Optimization scheme is faster with initialization #2 because the optimization starts closer to the solution. Init Init Xavier Bresson 35 Good initialization can be essential: Start closer to the minimizer  Quicker convergence. If bad choice then gradient descent captures local minimizer (not as good as global minimizer). Initialization L(w0, w1) w0 w1 Local minimizer L(w0, w1) w0 w1 Global minimizer min w=(w0,w1) L(w0, w1) Xavier Bresson 36 Stationary/critical points: Points where the gradient/slope of the function is zero. Characterization of stationary points: Global minimizers (maximizers) Local minimizers (maximizers) Saddle points Minimizers L(w) w Global minimum Local minimum Local maximum Saddle point gradient/slope not zero gradient/slope is zero min w=(w0,w1) L(w0, w1) 2D non-convex function 1D non-convex function Xavier Bresson 37 Convex losses are mathematically well studied: Fast optimization techniques (Newton, Nesterov), well understood behaviors and properties (existence of global minimum). Non-convex losses are in general not well understood, are slow to optimize (gradient descent), have critical points, but they offer large learning capacity. Convexity vs non-convexity 2D convex function 1D convex function Xavier Bresson 38 Repeat until convergence (until reaching a stationary point): Formalization wj wj −⌧ @ @wj L(w) Computer notation wk+1 j = wk j −⌧ @ @wj L(wk) Math notation Assignment operator jth variable of w w=(w0,w1,w2,…) j=(0,1,2,…) Learning rate Derivative w.r.t. jth variable Gradient of L w.r.t. jth variable Iteration number k+1 Iteration number k Xavier Bresson 39 Gradient descent with one variable Let us consider a single parameter w: Prediction function: Loss optimization: Gradient descent: fw(x) = wx min w L(w) = 1 n n X i=1 ⇣ wxi −yi ⌘2 w w −⌧@ @wL(w) w L(w) Gradient/ slope of L at winit w = winit −⌧@ @wL(winit) winit −⌧@ @wL(winit) w @ @wL(winit) ≥0 wmin L(wmin) Xavier Bresson 40 Any initialization of the gradient descent does converge to the solution of the optimization problem. Only the optimization time is affected. Why? Loss is convex. Different initialization Gradient/ slope of L at winit L(w) −⌧@ @wL(winit) L(wmin) w w = winit −⌧@ @wL(winit) winit wmin w @ @wL(winit) 0 Xavier Bresson 41 Gradient descent techniques with convex function: Only global minimizers, GD will converge for all initializations. Convexity vs non-convexity with GD Gradient descent techniques with non-convex function: Critical points  Choice of initialization is critical for global minimizers, local minimizer, saddle points. Xavier Bresson 42 Influence of learning rate/ time step in gradient descent: Learning rate w w −⌧@ @wL(w) If τ is too small, then the optimization process requires a lot of iterations to converge to the solution. If τ is too large, then the optimization process does not converge (diverges) and cannot capture the minimum (loss value explodes). L(w) w winit L(w) w winit Xavier Bresson 43 Do we need to decrease the learning rate τ to guarantee convergence? No. The slope/gradient decreases its value when we get closer to the minimum. Convergence speed w L(w) Gradient/ slope of L winit w wmin L(wmin) @ @wL(w) w w −⌧@ @wL(w) @ @wL(w) = 0 Xavier Bresson 44 If you start at the solution, i.e. the minimum, what one gradient step will do? Quiz L(w) w winit @ @wL(winit) = 0 w winit −⌧@ @wL(winit) w winit Nothing. Outline Xavier Bresson 45 Optimization with gradient descent Linear regression case Xavier Bresson 46 Gradient descent for linear regression Prediction function: Parameters: Loss function: Optimization: Gradient descent: L(w0, w1) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 min w=(w0,w1) L(w0, w1) w0, w1 fw(x) = w0 + w1x wj wj −⌧ @ @wj L(w) Loss gradient w0 w1 L(w0, w1) Init fw(x) min w=(w0,w1) L(w0, w1) Minimum x Xavier Bresson 47 For w0: Loss gradient @ @wj L(w0, w1) = @ @wj h 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 i @ @w0 L(w0, w1) = @ @w0 h 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 i = 1 n n X i=1 @ @w0 ⇣ w0 + w1xi −yi ⌘2 = 2 n n X i=1 (w0 + w1xi −yi). @ @w0 (w0 + w1xi −yi) = 2 n n X i=1 (w0 + w1xi −yi).1 For any wj: Xavier Bresson 48 For w1: Loss gradient @ @w1 L(w0, w1) = @ @w1 h 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 i = 1 n n X i=1 @ @w1 ⇣ w0 + w1xi −yi ⌘2 = 2 n n X i=1 (w0 + w1xi −yi). @ @w1 (w0 + w1xi −yi) = 2 n n X i=1 (w0 + w1xi −yi).xi Xavier Bresson 49 Gradient descent equation w1 w1 −⌧2 n n X i=1 (w0 + w1xi −yi)xi w0 w0 −⌧2 n n X i=1 (w0 + w1xi −yi) Iterate until convergence: Initialize: w0, w1 Xavier Bresson 50 fw(x) fw(x) fw(x) min w=(w0,w1) L(w0, w1) fw(x) w0 w1 L(w0, w1) Init Min Gradient descent in action Xavier Bresson 51 Training set size is n: Stochastic vs deterministic GD wj wj −⌧ @ @wj L(w) with L(w) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 wj wj −⌧1 n n X i=1 @ @wj ⇣ w0 + w1xi −yi ⌘2 ( If n is small a few K, but if n is as large as M or B?  Stochastic gradient descent (neural networks) Xavier Bresson 52 What is the gradient descent technique for the mean absolute loss? Quiz For any wj: @ @wj L(w0, w1) = @ @wj h 1 n n X i=1 # # #w0 + w1xi −yi # # # i L(w0, w1) = 1 n n X i=1 "" "" ""w0 + w1xi −yi "" "" "" Iterate until convergence: w0 w0 −⌧1 n n X i=1 sign(w0 + w1xi −yi) w1 w1 −⌧1 n n X i=1 sign(w0 + w1xi −yi)xi |x| @|x| @x = x |x| = sign(x) Outline Xavier Bresson 53 Regression with multiple variables Xavier Bresson 54 Data may have K even M features/attributes (curse of dimensionality). Multiple variables Size x 31 54 … Price y 124 156 … fw(x) = w0 + w1x Size x(1) 31 54 … #Rooms x(2) 2 3 … Age x(3) 6 12 … Price y 124 156 … d=1 variable d=3 variables n data n data Prediction function: Prediction function: Notation: x(j) = jth variable of x data fw(x(1), x(2), x(3)) = w(0) + w1x(1) + w2x(2) + w3x(3) Xavier Bresson 55 Notation: n = number of training data d = number of variables/features, dim(x) xi = ith training data, size(xi) = d x 1 x(j) = jth feature of training data x, scalar X = data matrix, size(X)= n x d Xij = xi(j)= jth variable of ith training data Example: Data matrix X x1 = 2 4 2 1 7 3 5 xT 1 = [2 1 7] All training data in a single matrix (very efficient for linear algebra computations) d variables Data matrix n x d d variables n data X = 2 6 4 xT 1 . . . xT n 3 7 5 d x 1 1 x d Xavier Bresson 56 d=1 variable: d variables: Example: d=2 Prediction function with d variables fw(x) = w0 + w1x fw(x(1), ..., x(d)) = w(0) + w1x(1) + ... + wdx(d) fw(x(1), x(2)) = w(0) + w1x(1) + wdx(2) fw(x(1), x(2)) = −0.3 + 1.2x(1) + 9.3x(2) fw(x(1), x(2)) x(1) x(2) Xavier Bresson 57 Define the vectors: Re-write the prediction function (as vector-vector multiplication): Vector representation f is called multivariate linear regression function. w = 2 6 6 6 6 6 4 w0 w1 w2 . . . wd 3 7 7 7 7 7 5 (d+1) x 1 (d+1) x 1 1 x (d+1) (d+1) x 1 x = 2 6 6 6 6 6 4 1 x(1) x(2) . . . x(d) 3 7 7 7 7 7 5 fw(x) = w0.1 + w1x(1) + ... + wdx(d) = wT x One line of code Xavier Bresson 58 Gradient descent with d=1 variable Prediction function: Parameters: Loss function: Optimization: Gradient descent: L(w0, w1) = 1 n n X i=1 ⇣ w0 + w1xi −yi ⌘2 min w=(w0,w1) L(w0, w1) w0, w1 fw(x) = w0 + w1x wj wj −⌧ @ @wj L(w) w0 w1 L(w0, w1) Init fw(x) min w=(w0,w1) L(w0, w1) Minimum x Xavier Bresson 59 Gradient descent with d variables Prediction function: Parameters: Loss function: Optimization: Gradient descent: wj wj −⌧ @ @wj L(w) fw(x) = wT x = w0 + w1x(1) + ... + wdx(d) L(w) = 1 n n X i=1 ⇣ wT xi −yi ⌘2 w = [w0, w1, ..., wd] min w L(w) Init Minimum x(1) x(2) fw(x) min w L(w) L(w) Xavier Bresson 60 d=1 (one variable): Gradient descent equations @ @wj L(w0, w1) = @ @wj h 1 n n X i=1 ⇣ w0.1 + w1xi −yi ⌘2 i w0 w0 −⌧2 n n X i=1 (w0 + w1xi −yi).1 w1 w1 −⌧2 n n X i=1 (w0 + w1xi −yi).xi d variables: @ @wj L(w) = @ @wj h 1 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2 i Xavier Bresson 61 Gradient descent equations wj wj −⌧2 n n X i=1 (w0 + w1xi(1) + ... + wdxi(d) −yi).xi(j) Data matrix Xij @ @wj L(w) = @ @wj h 1 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2 i = 1 n n X i=1 @ @wj h⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2 i = 2 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘ . @ @wj ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘ = 2 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘ xi(j) Gradient: Gradient descent: wj wj −⌧2 nXT j (Xw −y) Xavier Bresson 62 Matrix-vector representation wj wj −⌧2 n n X i=1 (w0 + w1xi(1) + ... + wdxi(d) −yi).xi(j) Vectorize gradient descent scheme: xi = 2 6 6 6 4 1 xi(1) . . . xi(d) 3 7 7 7 5 w = 2 6 6 6 6 6 4 w0 w1 w2 . . . wd 3 7 7 7 7 7 5 (d+1) x 1 (d+1) x 1 wj wj −⌧2 n n X i=1 Xij.(xT i w −yi) X = 2 6 4 1 xT 1 . . . 1 xT n 3 7 5 n x (d+1) Xj = 2 6 4 x1(j) . . . xn(j) 3 7 5 n x 1 w w −⌧2 nXT (Xw −y) y = 2 6 4 y1 . . . yn 3 7 5 n x 1 (d+1) x 1 (d+1) x n n x 1 Data matrix w0 for 1 line of code Xavier Bresson 63 How much speed up can we gain between a vectorized and un-vectorized algorithm? Quiz wj wj −⌧2 n n X i=1 (w0 + w1xi(1) + ... + wdxi(d) −yi).xi(j)  Up to 2-3 orders of magnitudes faster! Existing computational components (CPU, multi-core CPU, GPU) are built for fast linear algebra computations. w w −⌧2 nXT (Xw −y) Outline Xavier Bresson 64 Non-linear regression Xavier Bresson 65 Example: Housing prices prediction function Beyond linear regression Handcrafted prediction function: Domain expertise allows to define better families of predictive regression functions. Example: Linear prediction Quadratic prediction Polynomial prediction fw(x) = w0 + w1x + w2x2 fw(x) = w0 + w1x More learning capacity  More learning capacity  fw(x) = w0 + w1 px + w2e−x Best non-linear regression technique: Neural networks. fw(x) = w0 + w1x + ... w2x2 + ... + w6x6 Outline Xavier Bresson 66 Optimization with normal equations Gradient descent is the most generic optimization technique. But it has limitations: Choice of learning rate τ Convergence speed (even with optimal τ) Xavier Bresson 67 Beyond gradient descent L(w) w winit min w L(w) L(w) w winit min w L(w) wj wj −⌧ @ @wj L(w) min w L(w) We can leverage some mathematical properties to speed up the optimization. Prediction function is linear: Loss function is convex (quadratic): Xavier Bresson 68 Beyond gradient descent fw(x) = wT x = w0 + w1x(1) + ... + wdx(d) L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 Xavier Bresson 69 Normal equation: Solution of mean square error loss The minimum is obtained when the gradient/slope is zero: Normal equation for d=1 and n=1 min w n L(w) = (wx −y)2 o L(w) w @ @wL(w) = 0 One line of code Solution @ @w(wx −y)2 = 2x(wx −y) = 0 ) w = x−1y Xavier Bresson 70 Loss L: Gradient of the loss L w.r.t. w: Normal equation for d=1 and n data L(w) = 1 n n X i=1 ⇣ wxi–yi ⌘2 @ @w h 1 n n X i=1 ⇣ wxi–yi ⌘2i = 1 n n X i=1 @ @w h⇣ wxi–yi ⌘2i = 2 n n X i=1 (wxi–yi) @ @w(wxi–yi) = 2 n n X i=1 (wxi–yi)xi = 2 nw n X i=1 x2 i −2 n n X i=1 yixi = 0 ) w = Pn i=1 yixi Pn i=1 x2 i @ @wL(w) = 0 ) min w L(w) Solution Xavier Bresson 71 Vectorization L(w) = 1 n n X i=1 ⇣ wxi–yi ⌘2 = 1 n $ wx–y %T $ wx–y % x = 2 6 4 x1 . . . xn 3 7 5 y = 2 6 4 y1 . . . yn 3 7 5 Loss L: Gradient of the loss L w.r.t. w: n x 1 n x 1 n x 1 1 x n @ @w h 1 n "" wx–y #T "" wx–y #i = 1 n @ @w h"" wxT –yT #"" wx–y #i = 2 nxT "" wx–y # = 2 n "" wxT x–xT y # = 0 ) w = (xT x)−1xT y One line of code @ @wL(w) = 0 ) min w L(w) Xavier Bresson 72 Loss L: Gradient of the loss L w.r.t. wj: Normal equation for d features and n data @ @wj L(w0, ..., wd) = 0 8j L(w0, ..., wd) = 1 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2 @ @wj L(w) = @ @wj h 1 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2i = 2 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘ xi(j) = 0 ) wj = P k6=j P i wkxi(k)xi(j) P i x2 i(j) xi(0) = 1 with Solution Xavier Bresson 73 Vectorization Loss L: w = 2 6 4 w0 . . . wd 3 7 5 (d+1) x 1 xi = 2 6 4 xi(0) . . . xi(d) 3 7 5 (d+1) x 1 X = 2 6 4 xT 1 . . . xT n 3 7 5 n x (d+1) y = 2 6 4 y1 . . . yn 3 7 5 n x 1 xi(0) = 1 with Data matrix L(w0, ..., wd) = 1 n n X i=1 ⇣ w0 + w1xi(1) + ... + wdxi(d) −yi ⌘2 L(w) = 1 n n X i=1 ⇣ xT i w −yi ⌘2 L(w) = 1 n ⇣ Xw −y ⌘T ⇣ Xw −y ⌘ Xavier Bresson 74 Vectorization Gradient of the loss L w.r.t. wj: One line of code @ @wL(w) = 1 n @ @w h⇣ Xw −y ⌘T ⇣ Xw −y ⌘i = 1 n @ @w h⇣ wT XT −yT ⌘⇣ Xw −y ⌘i = 2 nXT ⇣ Xw −y ⌘ = 2 n ⇣ XT Xw −XT y ⌘ = 0 ) w = (XT X)−1XT y Xavier Bresson 75 Gradient descent vs normal equation? Quiz Advantages: Works well for large n (big data). Complexity is O(n). Limitations: Select learning rate τ. May requite lots of iterations. Advantages: Very fast for small n (solve linear system of equations). No need to select learning rate τ. Limitations: Works well for small n (n<104). Complexity is O(n3). n=10 < 1sec n=100 O(sec) n=1,000 O(min) n=10,000 O(hour) Outline Xavier Bresson 76 Conclusion Xavier Bresson 77 Linear regression is the most common regression technique. Gradient descent technique is one approach to optimize the regression loss, but there exist better optimization techniques for small, medium-scale datasets (quadratic optimization (normal equations), convex optimization). Code vectorization benefits from existing architectures s.a. CPU, multi-core CPU, GPU (much faster than for loops). Any learning technique, including regression, can be scaled up to billions of data with stochastic gradient descent. Gradient descent technique (a.k.a. backpropagation) is the only available technique to train neural networks with large-scale datasets (it works very well but it is slow). Conclusion Xavier Bresson 78 tutorial01.ipynb Introduction to Python Xavier Bresson 79 tutorial02.ipynb Linear algebra with Python Xavier Bresson 80 tutorial03.ipynb Coding exercise on supervised regression Questions? Xavier Bresson 81 "
479,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 4: Supervised Classification Semester 2 2017/18 Xavier Bresson 2 Classification problem Predictive function Decision boundary Loss function Gradient descent Multi-class classification Conclusion Outline Xavier Bresson 3 Classification problem Outline Disease class prediction Xavier Bresson 4 Supervised classification problem: Predict the disease class (discrete value) of patient given existing medical data features (tumor size). Tumor size Disease class Is the tumor benign/malign? Supervised classification predicts benign. Malign (1) Benign (0) y = {0, 1} More examples Xavier Bresson 5 Examples of binary classification tasks: Email: Spam (1) or not spam (0) Online financial transaction: Fraudulent (1) or legitimate (0) From binary to multi-class classification: Email: Spam (0), work (1), friends (2), family (3) Medical diseases: Benign (0), malign I (1), malign II (2), malign III (3) y = {0, 1, 2, ..., K} Benign Malign Binary variable Multi-value variable: ≠ Continuous variable (regression) y 2 R Formalization Xavier Bresson 6 Supervised classification algorithm f Input: x (data) Output: y (data label) f : x→ y or y=f(x) Training dataset (x,y) here: tumor size here: disease class For classification, y is a discrete variable f : class prediction function f maps data feature x to data class y Supervised classification learning: Reminder: For regression, y is a scalar Xavier Bresson 7 Predictive function Outline Xavier Bresson 8 How to represent a (discrete) class prediction function? Linear model? (like for regression) Class prediction might be: Model representation fw(x) = w0 + w1x if fw(x) ≥0.5 then predict y = 1 if fw(x) < 0.5 then predict y = 0 Tumor size Disease class Malign (1) Benign (0) 1/2 x y = 0 y = 1 fw(x) = w0 + w1x Xavier Bresson 9 Linear classification models are not robust to large variations of data features: Limitation of linear model Tumor size Disease class Malign (1) Benign (0) 1/2 x y = 0 y = 1 fw(x) = w0 + w1x New data The new data has changed significantly the classification result. ⇒Linear model is not a good solution to the classification problem. Xavier Bresson 10 Prediction function for classification of d-dim data: Model representation x = 2 6 6 6 6 6 4 1 x(1) x(2) . . . x(d) 3 7 7 7 7 7 5 wT x = w0 + w1x(1) + ... + wdx(d) w = 2 6 6 6 6 6 4 w0 w1 w2 . . . wd 3 7 7 7 7 7 5 with Logistic/ sigmoid function fw(x) = σ(wT x) σ(⌘) = 1 1 + e−⌘ fw(x) = 1 1 + e−wT x Logistic regression/ classification function 1 0.5 0 σ(⌘) ⌘ Sigmoid is like a smooth gate fw(x) ) pw(x) = Prw(y = 1|x) = 1 1 + e−wT x Xavier Bresson 11 The prediction function with logistic regression is a probability function: Probabilistic interpretation Example: If x = 5mm (tumor size) and fw(x) = 0.3 then the patient has 30% chance of tumor being malign. New notation for prediction function: Probability function fw(x) = Prw(y = 1|x) Probability to have y=1 given data x Probability is parametrized by w Xavier Bresson 12 What is the probability to have y=0 given data x? Quiz Prw(y = 0|x) = 1 −Prw(y = 1|x) = 1 −pw(x) = 1 − 1 1 + e−wT x Probability to have y=0 given data x Prw(y = 1|x) + Prw(y = 0|x) = 1 8x Use probability property that Xavier Bresson 13 Decision boundary Outline Xavier Bresson 14 Soft (continuous) class predictive function: Observe this predicative function is not a “hard” prediction (discrete value {0,1} to have class 1 or class 2), but a “soft” prediction (probability value between [0,1] to have class 1 or class 2). Hard (discrete) class predicative function: Class prediction pw(x) = σ(wT x) = 1 1 + e−wT x if pw(x) ≥0.5 then y = 1 if pw(x) < 0.5 then y = 0 Xavier Bresson 15 Interpretation of Decision boundary 1 0.5 0 if pw(x) ≥0.5 then y = 1 if pw(x) < 0.5 then y = 0 As σ(⌘= wT x) ≥0.5 when ⌘= wT x ≥0 Therefore pw(x) = σ(wT x) ≥0.5 if wT x ≥0 (and y = 1) And pw(x) = σ(wT x) < 0.5 if wT x < 0 (and y = 0) wT x pw(x) = σ(wT x) y = 1 y = 0 wT x ≥0 wT x < 0 Decision boundary (it is a single point for d=1) wT x = 0 ) x = 0 Xavier Bresson 16 Decision boundary in higher dimensional spaces: Decision boundary for d=2 features x = 2 4 1 x(1) x(2) 3 5 w = 2 4 w0 w1 w2 3 5 y = 0 y = 1 x(1) x(2) wT x ≥0 wT x < 0 Decision boundary (it is a straight line for d=2) pw(x) = σ(w0 + w1x(1) + w2x(2)) = σ(wT x) All x such that wT x = 0 Xavier Bresson 17 Plan in 3D and hyper-plan in d-D Decision boundary for d features All x such that wT x = 0 x(1) x(2) x(3) Decision boundary (it is a plane for d=3) y = 1 wT x ≥0 y = 0 wT x < 0 Xavier Bresson 18 Beyond flat boundaries (straight lines, plans): Class decision function: Non-linear decision boundary Quadratic function pw(x) = σ(w0 + w1x(1) + w2x(2) + w3x2 (1) + w4x2 (2)) = σ(wT x) w = 2 6 6 6 6 4 w0 w1 w2 w3 w4 3 7 7 7 7 5 x = 2 6 6 6 6 4 1 x(1) x(2) x2 (1) x2 (2) 3 7 7 7 7 5 if pw(x) ≥0.5 or wT x ≥0 then y = 1 if pw(x) < 0.5 or wT x < 0 then y = 0 Xavier Bresson 19 Example: w0=-R2, w1=w2=0, w3=w4=1 Non-linear decision boundary ) wT x = −R2 + x2 (1) + x2 (2) x(1) x(2) y = 1 wT x ≥0 y = 0 wT x < 0 All x such that wT x = 0 Decision boundary Circle equation x2 (1) + x2 (2) = R2 Xavier Bresson 20 What is the most general shape/geometry of a non-linear boundary decision? Quiz y = 0 x(1) x(2) All x such that wT x = 0 Decision boundary (it is a curve for d=2) y = 1 wT x ≥0 y = 0 wT x < 0 Xavier Bresson 21 Loss function Outline Xavier Bresson 22 Loss function Predictive function: How to choose the parameters w of the predictive function pw? We need: A loss/cost function to assess the prediction. A training set of examples (xi,yi) (supervised learning) Candidate: Loss function used for regression? Good choice for classification? pw(x) = σ(wT x) = 1 1 + e−wT x L(w) = 1 n n X i=1 ⇣ pw(xi) −yi ⌘2 Mean square error (MSE) Xavier Bresson 23 Linear regression predictive function: MSE loss: L function is convex J GD guarantees to find (global) minimum MSE loss for regression and classification fw(x) = wT x Classification predictive function: MSE loss: L function is non-convex L GD no guaranteed to converge to global minimum pw(x) = 1 1 + e−wT x L(w) = 1 n n X i=1 ⇣ wT xi −yi ⌘2 L(w) = 1 n n X i=1 ⇣ 1 1 + e−wT xi −yi ⌘2 min w L(w) min w L(w) Xavier Bresson 24 Logistic regression loss L(w) w Most popular classification loss is the logistic regression loss. Note: The name “logistic regression” may be confusing as we deal with the classification task (not the regression task). Definition: Convexity: Logistic regression function L(w) is convex J. L(w) = 1 n n X i=1 `(pw(xi), yi) with `(pw(xi), yi) = ⇢−log pw(xi) if yi = 1 −log(1 −pw(xi)) if yi = 0 and pw(xi) = 1 1 + e−wT xi Xavier Bresson 25 Properties of the logistic regression loss: If yi=1 and the predictive function pw(xi) predict 1 (correct), we should have: If yi=1 and the predictive function pw(xi) predict 0 (mistake), we should penalize: Loss analysis `(pw(xi), yi) = 0 `(pw(xi), yi) = +1 pw(xi) 1 0 if yi = 1 : −log pw(xi) Properties of the logistic regression loss: If yi=0 and the predictive function pw(xi) predict 0 (correct), we should have: If yi=0 and the predictive function pw(xi) predict 1 (mistake), we should penalize: `(pw(xi), yi) = 0 `(pw(xi), yi) = +1 pw(xi) 1 0 if yi = 0 : −log(1 −pw(xi)) Xavier Bresson 26 Can we re-write the logistic regression loss is one line? Quiz L(w) = 1 n n X i=1 `(pw(xi), yi) with `(pw(xi), yi) = ⇢−log pw(xi) if yi = 1 −log(1 −pw(xi)) if yi = 0 L(w) = 1 n n X i=1 `(pw(xi), yi) with `(pw(xi), yi) = −yi log pw(xi) −(1 −yi) log(1 −pw(xi)) Term=0 if yi=0 Term=0 if yi=1 ) L(w) = −1 n n X i=1 ⇣ yi log pw(xi) + (1 −yi) log(1 −pw(xi)) ⌘ One line expression: Logistic regression loss Yes. Xavier Bresson 27 Gradient descent Outline Xavier Bresson 28 Gradient descent for logistic regression Prediction function: Parameters: Loss function: Optimization: Gradient descent: wj wj −⌧ @ @wj L(w) pw(x) = 1 1 + e−wT x w = [w0, w1, ..., wd] L(w) = −1 n n X i=1 ⇣ yi log pw(xi) + (1 −yi) log(1 −pw(xi)) ⌘ min w L(w) Init Minimum min w L(w) L(w) y = 0 y = 1 x(1) x(2) Xavier Bresson 29 Gradient descent for logistic regression L(w) = −1 n n X i=1 ⇣ yi log pw(xi) + (1 −yi) log(1 −pw(xi)) ⌘ RHS1 RHS2 Gradient of RHS1: @ @wj h −1 n n X i=1 yi log pw(xi) i = −1 n n X i=1 yi @ @wj h log σ(wT xi) i = −1 n n X i=1 yi σ0 σ @ @wj h wT xi i = −1 n n X i=1 yi σ(1 −σ) σ xi(j) = 1 n n X i=1 yi(σ −1)xi(j) Loss: σ0 = dσ d⌘= (1 −σ(⌘))σ(⌘) Chain rule: @ @w h log σ(wT x) i = @f @z @z @w f z @ log σ(z) @z = σ0 σ @(wT x) @w = x Chain rule Xavier Bresson 30 Gradient descent for logistic regression Gradient of RHS2: @ @wj h −1 n n X i=1 (1 −yi) log(1 −pw(xi)) i = −1 n n X i=1 (1 −yi) @ @wj h log(1 −σ(wT xi)) i = −1 n n X i=1 (1 −yi)(1 −σ)0 (1 −σ) @ @wj h wT xi i = −1 n n X i=1 (1 −yi)−σ(1 −σ) (1 −σ) xi(j) = 1 n n X i=1 (1 −yi)σxi(j) L(w) = −1 n n X i=1 ⇣ yi log pw(xi) + (1 −yi) log(1 −pw(xi)) ⌘ RHS1 RHS2 Loss: (1 −σ)0 = −σ0 = −σ(1 −σ(⌘)) Chain rule @ @z h log(1 −σ) i = (1 −σ)0 (1 −σ) Xavier Bresson 31 Putting gradients together: Gradient descent for logistic regression L(w) = −1 n n X i=1 ⇣ yi log pw(xi) + (1 −yi) log(1 −pw(xi)) ⌘ Loss: wj wj −⌧ @ @wj L(w) wj −⌧1 n n X i=1 (σ(wT xi) −yi)xi(j) wj −⌧1 n n X i=1 (pw(xi) −yi)xi(j) Xavier Bresson 32 Have you seen this gradient descent expression before? Quiz Interestingly, this is the same gradient expression than linear regression. The only difference is the predictive function: pw(x) = 1 1 + e−wT x fw(x) = wT x Linear regression Logistic regression/ classifier Yes, linear supervised regression. wj wj −⌧1 n n X i=1 (pw(xi) −yi)xi(j) wj wj −⌧1 n n X i=1 (fw(xi) −yi)xi(j) Xavier Bresson 33 Multi-class classification Outline Xavier Bresson 34 Multi-class problem Examples of binary classification tasks: Email: Spam (1) or not spam (0) Online financial transaction: Fraudulent (1) or legitimate (0) From binary to multi-class classification: Email: Spam (0), work (1), friends (2), family (3) Medical diseases: Benign (0), malign I (1), malign II (2), malign III (3) y = {0, 1} y = {0, 1, 2, ..., K} Binary variable Multi-value variable y = 0 y = 1 x(1) x(2) y = 0 y = 1 x(1) x(2) y = 2 Binary classification Multi-variate classification Xavier Bresson 35 Two steps: Step 1 : Learning: Learn K classifiers to recognize of K classes One-vs-all classification problem y = 0 y = 1 x(1) x(2) y = 2 y = 0 y = 1 x(1) x(2) p1 w(x) y = 0 y = 1 x(1) x(2) p2 w(x) y = 0 y = 1 x(1) x(2) p0 w(x) pk w(x) = Prw(y = i|x) i = 0, 1, 2 Xavier Bresson 36 Step 2 : Testing: Classify a new data x with the class k that provides the highest probability: One-vs-all classification problem y = 0 y = 1 x(1) x(2) y = 2 k = arg max c pc w(x) Xavier Bresson 37 Why do we want a multi-value variable for classification? Why not also using a scalar value like in regression? Quiz Supervised classification algorithm f Input: x (data) Output: y (data label) For classification, y is a multi-value variable For regression, y is a scalar For probabilistic interpretation: pk w(x) = Prw(y = i|x) i = 0, 1, 2 Xavier Bresson 38 Conclusion Outline Xavier Bresson 39 Classification and regression are the two most fundamental tasks in machine learning and data science (many problems can be reduced to solve these tasks). Supervised classification is similar to supervised regression: Require training data. Parameter learning can be carried out the same way (optimization). Predictive and loss functions are different. Logistic regression loss (classification loss) is the most popular for linear models, and also non-linear models (neural networks). Conclusion Xavier Bresson 40 tutorial04.ipynb Coding exercise Xavier Bresson 41 tutorial05.ipynb Coding exercise Questions? Xavier Bresson 42 "
480,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 5: Gradient Descent Tricks Semester 2 2017/18 Xavier Bresson 2 Feature scaling Gradient descent with unbalanced scaling Feature normalization Max normalization Z-scoring Learning rate Stopping condition Conclusion Outline Xavier Bresson 3 Feature scaling Outline Xavier Bresson 4 Data features with similar scaling: Example: pixels in images x1 = pixel1 (0-255) x2 = pixel2 (0-255) Etc Consequence: Loss landscape is well distributed along all variable directions. Feature scaling Loss landscape with x1 and x2 having similar scaling w1 w2 L(w1, w2) Minimum L(w1, w2) = 1 n n X i=1 ⇣ w1xi(1) + w2xi(2) −yi ⌘2 Loss function fw(x) = w1x(1) + w2x(2) Predictive function Xavier Bresson 5 Real-world data features may have different scales. Example: House pricing prediction x1 = house size (0-2000m2) x2 = #rooms (1-5) Consequence: Loss landscape may be skew along some variable directions. Unbalanced scaling w1 w2 L(w1, w2) w1 w2 L(w1, w2) What consequence on the gradient descent technique? Skewed landscapes Xavier Bresson 6 Gradient descent with unbalanced scaling Outline Xavier Bresson 7 Reminder: GD follows the direction of the steepest descent, given by the gradient direction: Gradient descent w1 w2 L(w1, w2) Starting point Minimum w1 w2 L(w1, w2) Starting point Minimum GD requires a few steps to converge GD may require many steps to converge as its trajectory oscillates. Gradient direction Xavier Bresson 8 Scaling the features x(i) to the same order of values “flattens” the loss landscape: Normalization w1 w2 L(w1, w2) x1 = house size [0-2000](m2) x2 = #rooms [1-5] w1 w2 L(w1, w2) x1 = house size [0,1] x2 = #rooms [0,1] ⇒ ⇒ Data normalization Xavier Bresson 9 Feature normalization Max normalization Outline Xavier Bresson 10 Most common data normalization: Normalize max value of xi to 1. Example: x1 = house size 0-2000m2 ⇒x1 = x1/2000 x2 = #rooms 1-5 ⇒x2 = x2/5 If xi takes negative values, then Max normalization xi xi maxi xi ) 0 xi 1 xi xi maxi |xi| ) −1 xi 1 Xavier Bresson 11 Feature normalization Z-scoring Outline Xavier Bresson 12 Most common statistical normalization: Step 1: Center the data (zero-mean). Step 2: Normalize data variance to 1 (unit-variance). Z-scoring xi xi −µ σ µ = mean(xi) = 1 n n X i=1 xi σ2 = variance(xi) = 1 n n X i=1 (xi −µ)2 σ = standard deviation(xi) = v u u t 1 n n X i=1 (xi −µ)2 Xavier Bresson 13 It is one of the most common pre-processing steps: It allows comparison between data distributions (data exploration). Part of data science pipeline: data acquisition, data cleaning (by exploration), data pre-processing, data analysis, decision. Gradient descent works better if data is centered. Step required to normalize the variance of data. Step 1: Center the data xi xi −µ xi Pr(xi) Xavier Bresson 14 Step 1: Center the data 100 50 60 20 -60 -20 0 Data not centered Data are centered xi xi −µ ⇒ Data centering Example: x1 = house size 0-2000m2 ⇒x1 = x1-1000 (-1000≤x1 ≤1000) x2 = #rooms 1-5 ⇒x2 = x2-2.5 (-2.5≤x2 ≤2.5) Illustration: Xavier Bresson 15 The data variance is normalized to 1. It allows easier comparison between data distributions (data exploration). Sometimes, it decreases the performances of learning techniques. Step 2: Variance normalization xi xi −µ σ Xavier Bresson 16 Step 2: Variance normalization 100 50 60 20 -60 -20 0 0.1 0.05 -0.1 -0.05 0 ⇒ Data centering ⇒ Data Variance normalization xi xi −µ xi xi σ (µ = 0) Example: x1 = house size 0-2000m2 ⇒x1 = (x1-1000)/250 (-0.1≤x1 ≤0.1) x2 = #rooms 1-5 ⇒x2 = (x2-2.5)/2 (-0.1≤x2 ≤0.1) Illustration: Same scaling Xavier Bresson 17 Learning rate Outline Xavier Bresson 18 Learning rate Good choice of learning rate τ is essential: Too small and the convergence takes lots of time. Too large and the technique diverges. How to select a good value τ? Monitor the loss decrease: The loss is guaranteed to decrease at each iteration: The speed of loss decrease, a.k.a. convergence speed, should be as fast as possible. L(wk+1) L(wk) 8k k is the iteration index L(wk+1) L(wk) k L(w) 10 11 Xavier Bresson 19 Convergence During the minimization: At convergence (steady state): wk+1 = wk wk+1 = wk −⌧@ @wL(wk) ) @ @wL(wk) = 0 L(wk+1) = L(wk) 8k > 100 Definition of a minimum wk+1 = wk −⌧@ @wL(wk) L(wk+1) L(wk) 8k 100 L(wk+1) L(wk) k L(w) 10 11 100 How to get to the flat region as fast as possible? Xavier Bresson 20 Goal: Find the best value τ that makes GD converges as fast as possible. Convergence speed Good value τ: convergence to minimum (50 iterations) k L(w) Best value τ: fast convergence (10 iterations) Bad value τ: very slow convergence to minimum (10,000 iterations) Worst value τ: divergence (after a few iterations) L(w) w winit min w L(w) L(w) w winit min w L(w) Experimentally, test τ = 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10. Xavier Bresson 21 Stopping condition Outline Xavier Bresson 22 When to stop the iterative GD process? Manually: Visual inspection of the loss decay. Stop when the steady state is reached. Example: Loss = 2.5 1.8 1.2 0.6 0.24 0.22 0.21 0.21 0.21 0.21 Automatically: Mathematical, convergence is defined when Data measure (like accuracy), convergence is obtained when error rate does not decrease after e.g. 10 iterations: Stopping condition L(wk+1) −L(wk) "", "" = 10−3 Difficult to select Acc(wk+1) −Acc(wk) 5% Easy to select Xavier Bresson 23 Conclusion Outline Xavier Bresson 24 Data pre-processing is useful for Gradient descent Most data analysis techniques Most common data pre-processing: Centering Max or variance normalization Gradient descent technique limitations: Skewed loss landscape Learning rate hyper-parameter Slow Improved GD techniques (used for neural networks): Momentum, RMSprop, Adams Conclusion Questions? Xavier Bresson 25 "
481,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 6: Generalization and Regularization Semester 2 2017/18 Xavier Bresson 2 Generalization The problem of over-fitting Addressing over-fitting Regularization Regularization by gradient descent Regularization by normal equation Conclusion Outline Xavier Bresson 3 Generalization Outline Xavier Bresson 4 Generalization is the goal of all predictive models. The generalization problem consists at making accurate predictions to new data points (never seen before). Any data science task has two stages: Learning stage: Use training data to learn the parameters of a predictive model. Testing stage: Once training is over, apply the predictive model to unseen data points. Generalization Xavier Bresson 5 Example: Supervised regression Generalization Predictive model Size Price New data point (never seen before) Xavier Bresson 6 What generalization means for supervised classification? Quiz Tumor size Disease class Malign (1) Benign (0) Predictive model New data point (never seen before) Xavier Bresson 7 The problem of over-fitting Outline Xavier Bresson 8 Example: Regression task The problem of data fitting Linear prediction Quadratic prediction Polynomial prediction fw(x) = w0 + w1x + w2x2 fw(x) = w0 + w1x More learning capacity ⇒ More learning capacity ⇒ fw(x) = w0 + w1x + ... w2x2 + ... + w6x6 Under-fitting The predictive model does not fit well the training data. The data assumption (linear data) does not reflect the true housing price (too simple model). Right-fitting The predictive model fits “right” the training data. The data assumption (quadratic data) reflects the true housing price as it will predict well prices on new examples. Over-fitting The predictive model fits perfectly the training data. The data assumption (polynominal data) does not reflect the true housing price, (too complex model). Xavier Bresson 9 Bias and variance Linear prediction Quadratic prediction Polynomial prediction fw(x) = w0 + w1x + w2x2 fw(x) = w0 + w1x More learning capacity ⇒ More learning capacity ⇒ fw(x) = w0 + w1x + ... w2x2 + ... + w6x6 Under-fitting High bias Small variance Over-fitting Small bias High variance Right-fitting Good balance between bias and variance Model bias: Variations between predictive value and training value Model variance: Variations of predictive function Unlikely to generalize Unlikely to generalize Good generalization Xavier Bresson 10 The problem of over-fitting Over-fitting is the most common fitting problem (e.g. neural networks) because predictive models have usually large capacity to learn (high number of weight parameters). Problem: The predictive function f fits almost perfectly all training data: but fails to generalize to new data examples. Dynamic of the gradient descent: L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 ⇡0 k L(wk) 0 Xavier Bresson 11 The problem of over-fitting Problem (different view): The space of prediction is too large ⇒Non-meaningful predictions will be done. Curse of dimensionality: The over-fitting problem is more important when data have too many variables because they lie in high- dimensional spaces. Xavier Bresson 12 What over-fitting means for the classification task? Quiz Right-fitting Good balance between bias and variance Good generalization Quadratic function pw(x) = σ(w0 + w1x(1) + ... + w4x2 (2)) x(1) x(2) Under-fitting High bias Small variance Unlikely to generalize x(1) x(2) Linear function pw(x) = σ(w0 + w1x(1) + w2x(2)) Over-fitting Small bias High variance Unlikely to generalize Polynomial function pw(x) = σ(w0 + w1x(1) + ... + w6x6 (2)) x(1) x(2) Xavier Bresson 13 Addressing over-fitting Outline Xavier Bresson 14 Generalization needs to fix the issue of over-fitting. They are two options: Reduce the number of data features: Arbitrary select less features or use feature selection model to automatically select the most meaningful features. Two problems: We may lose good information about data. Feature selection models are not perfect. Regularization: Adapt the importance of the features depending on the data and the task at hand. Advantages: Keep all features. Learning stage will compute the weight values. Good for high-dim data How to address over-fitting? Xavier Bresson 15 Regularization Outline Xavier Bresson 16 Intuition Idea: Starting from high-capacity (cubic) function, we enforce the weight values w3,w4 to be small ⇒The high-capacity function becomes a simpler, smoother (quadratic) function: Cubic prediction fw(x) = w0 + w1x + w2x2 + w3x3 + w4x4 Less learning capacity ⇒ w3, w4 ⇡0 Quadratic prediction fw(x) = w0 + w1x + w2x2 Over-fitting Right-fitting How to make the weights small? Xavier Bresson 17 We can enforce the weights to be small by penalizing their values: Penalization min w 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 + 1000.w2 3 + 1000.w2 4 Arbitrary large number Minimizing this loss function will force w3,w4 to have a small value, that is w3,w4 ≈0. fw(x) = w0 + w1x + w2x2 + w3x3 + w4x4 fw(x) = w0 + w1x + w2x2 ⇒ w3, w4 ⇡0 Xavier Bresson 18 Idea: Start with a high-capacity predictive function and learn smoother, lower-capacity function by optimizing a (regularized) loss that penalizes the values of the parameter weights of the predictive function: Regularization Fitting loss between predictive function and training data Penalty/ regularization loss (enforce small values) a.k.a. L2 loss Regularization parameter Regularized loss Trade-off between perfect fit and smooth predictive function min w L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 + λ d d X j=1 w2 j Xavier Bresson 19 Quiz The regularization parameter λ controls the generalization/over- fitting issue. It is an hyper-parameter that can be estimated by cross-validation (later discussed). What is the predictive function for a large λ value? λ = 0 λ = 1000 Over-fitting Right-fitting fw(x) = w0 + w1x + ... + wdxd λ = 1, 000, 000 Under-fitting fw(x) = w0 λ = 106 ) w1 = w2 = w3 = w4 = 0 ⇒ min w L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 + λ d (w2 1 + w2 2 + w2 3 + w2 4) Xavier Bresson 20 Regularization by gradient descent Outline Xavier Bresson 21 Regularized regression loss: Regularized loss Regularized classification loss: pw(x) = σ(wT x) fw(x) = wT x L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 + λ d d X j=1 w2 j L(w) = −1 n n X i=1 yi log pw(xi) + (1 −yi) log(1 −pw(xi)) + λ d d X j=1 w2 j @ @wj L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘ xi(j) + λ d @ @wj ⇣ d X j=1 w2 j ⌘ 1 n n X i=1 ⇣ fw(xi) −yi ⌘ xi(j) + 2λ d wj Xavier Bresson 22 Optimization by gradient descent Optimization: Gradient descent: Gradient: min w L(w) wj wj −⌧ @ @wj L(w) Xavier Bresson 23 Gradient descent: Decreasing effect wj wj −⌧ @ @wj L(w) Value smaller than 1 (e.g. 0.9) Decrease the value of wj towards 0. Shrinking effect wj (1 −2⌧λ d )wj −⌧1 n n X i=1 (yi −fw(xi))xi(j) Xavier Bresson 24 How fast is the decreasing property? Quiz Value smaller than 1 (e.g. 0.9) Decrease the value of wj towards 0 exponentially fast: 0.9#iter, examples: 0.910=0.34 and 0.9100=0.00001 wk=0 = w0 wk=1 = 0.9wk=0 = 0.9w0 wk=2 = 0.9wk=1 = 0.92w0 . . . wk = 0.9wk−1 = 0.9kw0 Shrinking effect wj (1 −2⌧λ d )wj −⌧1 n n X i=1 (yi −fw(xi))xi(j) Xavier Bresson 25 Regularization by normal equation Outline Xavier Bresson 26 Solution of MSE loss with linear predictive function: Normal equation min w L(w) , @ @wL(w) = 0 ) w = (XT X)−1XT y ) w = 2 6 4 w0 . . . wd 3 7 5 (d+1) x 1 xi = 2 6 4 xi(0) . . . xi(d) 3 7 5 (d+1) x 1 X = 2 6 4 xT 1 . . . xT n 3 7 5 n x (d+1) y = 2 6 4 y1 . . . yn 3 7 5 n x 1 xi(0) = 1 with Data matrix One line of code L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 Xavier Bresson 27 Regularized regression loss: Regularization with normal equation Matrix-vector representation ) L(w) = 1 n ⇣ Xw −y ⌘T ⇣ Xw −y ⌘ + λ d wT w L(w) = 1 n n X i=1 ⇣ fw(xi) −yi ⌘2 + λ d d X j=1 w2 j Xavier Bresson 28 Gradient: Regularization with normal equation Identity matrix Iw = w One line of code @ @wL(w) = @ @w h 1 n ⇣ Xw −y ⌘T ⇣ Xw −y ⌘ + λ d wT w i = 1 n @ @w h⇣ wT XT −yT ⌘⇣ Xw −y ⌘i + λ d @ @w h wT w i = 2 nXT ⇣ Xw −y ⌘ + 2λ d w = 2 n ⇣ (XT X + λ d I)w −XT y ⌘ = 0 ) w = (XT X + λ d I)−1XT y Xavier Bresson 29 NE do not admit a solution when n<d, i.e. when the number of training data n is smaller than the data dimensionality d. Example genetics: 100K genes and 1K training data. This is called an over-parametrized linear system of equations. The matrix XTX is not invertible. Examples: Solution is to simply regularize: (Bonus) Normal equation for non-invertible matrix a11w1 + a12w2 = b1 a21w1 + a22w2 = b2 2 unknowns w1,w2 and 2 equations ✓ 2 unknowns w1,w2 but 1 equation ✘ (Infinite number of solutions) a11w1 + a12w2 = b1 w = (XT X + λ d I)−1XT y Xavier Bresson 30 Conclusion Outline Xavier Bresson 31 Generalization is the ultimate goal of learning techniques, and one obstacle is over-fitting. Over-fitting can be reduced by Selecting of small number of features (domain expertise, limited) Regularization (L2 loss, dropout for neural networks) Data augmentation (can be challenging to produce more data) Selection of regularization hyper-parameter can be time-consuming (cross-validation). Conclusion Questions? Xavier Bresson 32 "
482,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 7: Developing Data Science Projects Semester 2 2017/18 Xavier Bresson 2 Strategy for developing a data analysis system Cross-validation for hyper-parameter estimation Learning curves for over- and under-fitting control The more data the better Hand-crafted features selection Error metric for unbalanced classes Good practices Conclusion Outline Xavier Bresson 3 Strategy for developing a data analysis system Outline Xavier Bresson 4 How to develop a data analysis system? Guidelines and best practices Example: How to build a spam classifier to distinguish between spam and non-spam emails? Step 1: Identify the task and the class of learning techniques. For this example, we build a classifier using supervised learning. Strategy Xavier Bresson 5 Data representation Step 2: What data representation (features/variables)? Develop hand-crafted features (domain expertise). Learn the features from the data (end-to-end systems). For this example, the features of the email can be hand-crafted with 100 words indicating of spams and non-spams like: deal, buy, discount,… for spams and my name, chit-chat words, etc for non- spams.  Representation of an email x = 2 6 6 6 6 4 0 2 1 0 0 3 7 7 7 7 5 Email (data) deal discount xavier … 100 elements dim(x)=100 x 2 R100 x(j) = ⇢nb of words j appearing in the email 0 otherwise Xavier Bresson 6 Step 3: How to make the spam classifier with the lowest possible error? Collect (lots of) training data. Select a smaller set of hand-crafted features. Get additional hand-crafted features. Increase learning capacity with polynomial features (x2,x3,…), neural networks. Increase/decrease regularization How to select the number of features, of polynomial terms, the regularization constant, other hyper-parameters of the learning systems? Cross-validation technique. Design optimal classifier Xavier Bresson 7 Cross-validation for hyper-parameter estimation Outline Xavier Bresson 8 Terminology: Parameters of learning algorithms are the variables that can be computed by any gradient-based techniques (gradient descent). Example: Parameters w in supervised regression and classification. Hyper-parameters of learning algorithms are the variable that cannot be computed by gradient-based techniques (but by cross-validation). Examples: Degree d of polynomial terms, regularization constant λ, number of hidden layers and neurons in neural networks. Hyper-parameter selections are also called model selection problems. Cross-validation is a technique for estimating the values of the hyper-parameters of the learning system. Cross-validation Xavier Bresson 9 We split the original set into 3 sets: A train set A validation set (a.k.a. cross-validation set) A test set Example: Train/validation/test sets Original set (100%) + Train set (60%) Validation set (20%) Test set (20%) Xavier Bresson 10 We define 3 losses (for each dataset): Remember the loss is a measure of fitness between the predictive model and the data. If the fit is almost perfect then the loss is close to zero. Train/validation/test sets (x1, y1), ..., (xn, yn) (xv 1, yv 1), ..., (xv n, yv n) (xt 1, yt 1), ..., (xt n, yt n) LTrain(w) = 1 n n X i=1 `(pw(xi), yi) LVal(w) = 1 nv nv X i=1 `(pw(xv i ), yv i ) LTest(w) = 1 nt nt X i=1 `(pw(xt i), yt i) nb of training data nb of test data nb of validation data Train set (60%) Validation set (20%) Test set (20%) Xavier Bresson 11 The validation test is essential to avoid the poor generalization problem (bad predictions for new data). If the validation test is not used to evaluate the prediction performances on the hyper-parameters, then the learning model will over-fit the test data and will not be able to generalize to new data. Example: Let us consider the selection of the polynomial degree d for the prediction function: Why the validation set? pw(x) = σ(wT d x) wT d x = w0 + w1x(1) + w2x(2) + w3x2 (1) + w4x2 (2) + ... + wdxd (1) + wd+1xd (2) polynomial degree d d=1  Linear prediction d=2  Quadratic prediction … d=10  Polynomial prediction Xavier Bresson 12 Suppose we do not have an evaluation set but only the train and test sets to evaluate d. Then, we may want to select d by learning the predictive function with different values d and select d that minimizes the test loss value: Direct approach d=1  Linear prediction: d=2  Quadratic prediction: … d=10  Polynomial prediction: min w LTrain(w) ) wd=1 ) LTest(wd=1) min w LTrain(w) ) wd=2 ) LTest(wd=2) … min w LTrain(w) ) wd=10 ) LTest(wd=10) Compute the value w for the hyper-parameter d=1. Compute the test loss value with the computed w. Compute the test loss values for all values of d. Select the d=8 value that minimizes the test loss. Is this good idea? Xavier Bresson 13 It is a bad solution to select the value of the hyper-parameter d that minimizes the test loss because we select the d that over-fits the test set  The generalization performance may be poor. Bad approach x(1) x(2) Test set: All { } Over-fitting test set Unlikely to generalize Never use the test set! (very common error) This new point will not be well-classified Xavier Bresson 14 Use the validation set as a “test set” to select the hyper-parameter values (the real test set stays untouched): Cross-validation approach d=1  Linear prediction: d=2  Quadratic prediction: … d=10  Polynomial prediction: … Compute the value w for the hyper-parameter d=1. Compute the validation loss value with the computed w. Compute the validation loss values for all values of d. Select the d=4 value that minimizes the validation loss. min w LTrain(w) ) wd=1 ) LVal(wd=1) min w LTrain(w) ) wd=2 ) LVal(wd=2) min w LTrain(w) ) wd=10 ) LVal(wd=10) Evaluate the generalization performance: LTest(wd=4) As the test data has not been used – it can be used to evaluate the generalization performance of the learning system. Xavier Bresson 15 k-fold cross validation The k-fold cross validation allows to evaluate the generalization performance on all the training data*. Step 1: Split the original set into 2 datasets: A training set A test set Original set (100%) + Train set* (80%) Test set (20%) Xavier Bresson 16 Step 2: Split the new training set into k=4 folds/sub-sets: k-fold cross validation + k=4 folds Train set (80%) Test set (20%) Xavier Bresson 17 Step 3: Loop over the k folds and define the evaluation set as one of the k fold and the rest of the data as the train set. Then, learn the parameters of the learning system using the training set and evaluate the hyper-parameter value on the evaluation set. k-fold cross validation min w LTrain(w) ) wd=1 ) LVal(wd=1) … min w LTrain(w) ) wd=2 ) LVal(wd=2) min w LTrain(w) ) wd=10 ) LVal(wd=10) Fold 1: Train set Val set d LVal 4 1 2 10 Xavier Bresson 18 k-fold cross validation Fold 2: Train set Val set min w LTrain(w) ) wd=1 ) LVal(wd=1) … min w LTrain(w) ) wd=2 ) LVal(wd=2) min w LTrain(w) ) wd=10 ) LVal(wd=10) Train set d LVal 4 1 2 10 Xavier Bresson 19 k-fold cross validation Fold 3: Train set Val set Train set Fold 4: Train set Val set d LVal 4 1 2 10 Xavier Bresson 20 Step 4: Compute the mean value of the evaluation loss over the all k folds. Select the d value that minimizes the evaluation loss. k-fold cross validation d LVal 1 2 10 4 Select the d=4 value that minimizes the mean validation loss. Evaluate the generalization performance: LTest(wd=4) Train set (80%) Test set (20%) Generic technique: k-fold cross validation can be applied to estimate any hyper-parameter (s.a. regularization constant, neural network architectures with number of hidden layers, neurons). Xavier Bresson 21 How do we select the hyper-parameter values to use? Quiz Grid search Hyper- parameter 1 Hyper- parameter 2 Random search Hyper- parameter 1 Hyper- parameter 2 1 2 3 4 5 6 7 … 1e-3 1e-2 1e-1 1e0 1e1 1e2 … linear scale logarithmic scale Grid search (linear or logarithmic scales) Random search Xavier Bresson 22 Learning curves for over- and under-fitting control Outline Xavier Bresson 23 Learning curves are the shapes of L w.r.t. hyper-parameters. They are useful to identify over-fitting and under-fitting problems, and therefore solve the generalization problem. Reminder: Over-fitting is due to high variance of the learning model and under-fitting is caused by high bias. Learning curves Right-fitting Good balance between bias and variance Good generalization x(1) x(2) Under-fitting High bias Small variance Unlikely to generalize x(1) x(2) Over-fitting High variance Small bias Unlikely to generalize x(1) x(2) Xavier Bresson 24 Plot the losses w.r.t. polynomial degree d, which acts as the learning capacity of the system: Learning curve for capacity 1 4 10 LVal LTrain d Learning capacity Loss/error High bias (under-fitting) Right- fitting High variance (over-fitting) x(1) x(2) x(1) x(2) x(1) x(2) LTrain ⇡LVal = large LTrain ⇡0 LVal = large ≫LTrain Xavier Bresson 25 We remind the regularized training loss L with the hyper-parameter λ, which acts as the regularization of the learning system: Learning curve for regularization We first minimize the regularized loss L (it gives us w) for different values of the hyper-parameter λ and then we plot the training and validation losses: L(w) = LTrain(w) + λLReg(w) 1 n n X i=1 `(pw(xi), yi) + λ d d X j=1 w2 j LTrain(w) = 1 n n X i=1 `(pw(xi), yi) LVal(w) = 1 nv nv X i=1 `(pw(xv i ), yv i ) Xavier Bresson 26 Plot of the losses w.r.t. hyper-parameter λ: Learning curve for regularization 0.1 10 100 LVal LTrain Regularization Loss/error High variance (over-fitting) Right- fitting High bias (under-fitting) x(1) x(2) x(1) x(2) x(1) x(2) LTrain ⇡LVal = large LTrain ⇡0 LVal = large ≫LTrain λ Xavier Bresson 27 First, let us suppose that the system has a small learning capacity (like linear learning systems). Then, we plot the losses w.r.t. the number of training data n: Learning curve for training size 1 10 100 LVal LTrain Number of training data Loss/error n x(1) x(2) Loss value is high At low capacity (high bias), there is no need to collect lots of training data. Besides, the losses (errors) can be high. x(1) x(2) Xavier Bresson 28 Second, let us increase the learning capacity of the system (with quadratic or polynomial terms). We plot the losses w.r.t. the number of training data n: Learning curve for training size 1 10 100 LVal LTrain Number of training data Loss/error n x(1) x(2) Loss value is low At high capacity (high variance), collecting lots of training data is very helpful as the losses (errors) may decrease significantly. x(1) x(2) Xavier Bresson 29 Learning curves are good debugging tools for learning systems as they allow to derive the best possible generalization property. The limitation is the expensive computational time to perform cross-validation on all hyper-parameters. This is why it is important to start with a small dataset (extract a sub-part of the original training set)  Compute the learning curves on this small datasets to roughly estimate the hyper- parameters. Then, use the full dataset to fine-tune the hyper- parameters. Good practices Xavier Bresson 30 How to design optimal classifier? Step 3: How to make the spam classifier with the lowest error? Collect (lots of) training data  Fixes Select a smaller set of hand-crafted features  Fixes high variance Get additional hand-crafted features  Fixes Increase learning capacity with polynomial features (x2,x3,…), neural networks  Fixes Increase regularization  Fixes Decrease regularization  Fixes Quiz high variance high bias high bias high variance high bias high variance Xavier Bresson 31 The more data the better Outline Xavier Bresson 32 How much data to train on? It is not clear how much data to get x% accuracy. It is clear that the more data the better. Big data Number of training data n Accuracy Deep NNs Large NNs Small NNs SVM, Decision tree, others Small data All algorithms perform the same. Big data Best algorithms are NNs. It's not who has the best algorithm that wins, it's who has the most data! A. Ng Xavier Bresson 33 Assume that x in Rd has sufficient d features (information) to predict accurately any output (millions) The learning capacity is high (a NN with billion parameters) The training set is big (millions) Then, the generalization performance is maximum. Why? (1) High capacity means that Ltrain will be small. (2) A learning algorithm (even with high capacity) will not be able to over-fit a large-scale datasets, so we will have Ltrain ≈Ltest. Putting (1) and (2) together implies Ltest (test error) to be small! Big data Xavier Bresson 34 Hand-crafted features selection Outline Xavier Bresson 35 Error analysis technique: Use a simple (linear) algorithm and a sub-set of the training data. Learn the parameters of the learning system with gradient-based technique and cross-validation. Manually select the examples in the validation set where the algorithm fails. Study the data and identify potential systematic errors. Example: Spam classifier. Let us extract 500 spam/non-spam emails. We learn the parameters and hyper-parameters. We observe 100 emails have been misclassified.  What types of emails, what features should be removed or added? Designing better hand-crafted features Xavier Bresson 36 Error metric for unbalanced classes Outline Xavier Bresson 37 Example: Disease classification task. Train a classifier pw(x) to predict illness (y=1) or not (y=1). We find 1% error on the test set (or 99% correct diagnostic)  Seems an excellent classifier but there is actually only 0.5% of patients who are ill (so the classifier finds as much healthy patients as ill ones). The problem is due to unbalanced/skewed classes. For this example, we have 995 heathy patients and only 5 ill patients. A new error metric is required to deal with unbalanced class sizes: Precision and recall are standard metrics for this situation. Error for unbalanced classes Xavier Bresson 38 Assume that y=1 for the rare/small classes. Confusion matrix (a.k.a. error matrix): Each row of the matrix represents the instances in a predicted class and each column represents the instances in an actual class. Confusion matrix 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) Actual classes Predicted classes Algorithm predicted y=1 (positive) and the truth is y=1 Algorithm predicted y=0 (negative) and the truth is actually y=1 Algorithm predicted y=1 (positive) and the truth is y=0 Algorithm predicted y=0 (negative) and the truth is actually y=0 Xavier Bresson 39 Precision: Fraction of patients who are actually ill among all patients who are predicted ill. Precision and recall P = = #True Positive #Predicted Positive #True Positive #TP + #FP  High P value (close to 1) is good. Recall: Fraction of patients who are predicted ill among all patients who are actually ill. R = = #True Positive #Actual Positive #True Positive #TP + #FN  High R value (close to 1) is good. 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) Xavier Bresson 40 Example: Disease classification task for 1000 patients Actual class: 995 (y=0) and 5 (y=1) Predicted class: 990 (y=0) and 10 (y=1) Algorithm predicted 10 patients ill (y=1) Among these 10 patients: 5 patients are actual ill (y=1)  True positive=5 5 patients are actual healthy (y=0)  False positive=5 Algorithm predicted 990 patients healthy (y=0) Among these 990 patients: 990 patients are actual healthy (y=0)  True negative=990 0 patients are actual ill (y=1)  False negative=0 Precision and recall Xavier Bresson 41 Confusion matrix: Precision and recall 1 0 1 TP = 5 FP = 5 0 FN = 0 TN = 990 Actual classes Predicted classes P = = 0.5 5 5 + 5 R = = 1 5 5 + 0 Precision is low Recall is high Xavier Bresson 42 Precision and recall Recipe to compute precision and recall: Learn the predictive logistic regression function pw(x). Define the two predicted classes y=1 and y=0 with pw(x) and thresholding value 0.5: A patient with probability larger than 50% belong to the class y=1: Predict y=1 if pw(x) ≥0.5 A patient with probability larger than 50% belong to the class y=0: Predict y=0 if pw(x) < 0.5 Compute the confusion matrix (TP, FP, FN, TN) given the predicted and actual classes. Compute precision P and recall R. 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) Xavier Bresson 43 Trading precision and recall Suppose we want to predict patients who are ill only if we are very confident (minimize false positive). We can bias the classifier for this task. As classes are defined according to a threshold (0.5): Predict y=1 if pw(x) ≥0.5 and y=0 if pw(x) < 0.5, we can change the threshold to e.g. 0.7 to bias the classifier to high precision (and lower recall): Predict y=1 if pw(x) ≥0.7 and y=0 if pw(x) < 0.7. But we increase the number of patients predicted with no illness but actually are ill. Suppose we want to prevent to avoid missing cases of illness (minimize false negative). Then, we can bias the classifier to high recall (and lower precision): Predict y=1 if pw(x) ≥0.3 and y=0 if pw(x) < 0.3. But we increase the number of patients predicted with illness but actually are healthy. 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) 1 0 1 True positive (TP) False positive (FP) 0 False negative (FN) True negative (TN) Xavier Bresson 44 Classes are defined for each threshold μ : Predict y=1 if pw(x) ≥μ and y=0 if pw(x) < μ And the classes fix the values of precision P and recall R. The values of P and R can be plotted as follows: Trading precision and recall Recall Precision 1 1 threshold μ = 0.1 threshold μ = 0.5 threshold μ = 0.9 Recall Precision 1 1 Other shapes are possible Recall Precision 1 1 Optimal shape P=1, R=1 Xavier Bresson 45 There is an approach to select the threshold value automatically. Ideally, we want to have P=1 and R=1: A naive and bad metric is the mean: The F1 score is a good metric: Example: Threshold selection P R M F1 μ = 0.9 0.5 0.4 0.45 0.44 μ = 0.5 0.7 0.1 0.4 0.17 μ = 0.1 0.01 1 0.51 0.03 Predict y=1 all the time F1 = 2 PR P + R M = P + R 2 Best F1  best compromise precision and recall Best recall, but worst precision Xavier Bresson 46 What is the difference between the loss function and the error function (or equivalently the accuracy function)? Quiz Continuous and differentiable function  Gradient-descent techniques can be used. Discrete (not-differentiable) function  No gradient- descent techniques can be used. Alternative are grid search (brute force) and genetic algorithms (may be slow and not applicable to NNs). Loss function is a surrogate of the error function. Xavier Bresson 47 Good practices Outline Xavier Bresson 48 A recipe for designing learning systems: Step 1: Pre-process data (zero-mean, unit variance) Step 2: Choose a learning algorithm (linear, NNs) Step 3: Make sure you have enough learning capacity. Extract a sub-set of training data and over-fit them, i.e. LTrain ≈ 0 (LVal is high) by manually selecting the hyper-parameters. Step 4: Add regularization and evaluate the generalization performance on the validation set. We should have LVal ↘and LTrain ↗. The gap between LVal and LTrain should be as small as possible. Step 5: Use all training data and cross-validation to estimate the parameters and the hyper-parameters (long running time). Ideally, LVal ≈ LTrain ≈ small value. Good practices Xavier Bresson 49 Conclusion Outline Xavier Bresson 50 Designing high-quality learning systems depends on: Complexity and availability of data Mathematical (and intuitive) understanding of learning algorithms Coding skills (Python, Java, etc) Computational infrastructure (CPU, GPU, cloud, data storage, database, etc) Best practices (different recipes exist) Personal practice.. Conclusion Xavier Bresson 51 tutorial06.ipynb Coding exercise Questions? Xavier Bresson 52 "
483,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 8: Unsupervised Learning Semester 2 2017/18 Xavier Bresson 2 Supervised vs unsupervised learning Unsupervised clustering with k-means Clustering k-means algorithm Loss k-means properties Unsupervised representation with PCA Introduction and motivations Principal directions and components Formalization Algorithm PCA properties Conclusion Outline Xavier Bresson 3 Supervised vs unsupervised learning Outline Supervised learning Xavier Bresson 4 Supervised learning algorithm f Input: xi (data) Output: yi (data label/target) f : x→ y or y=f(x) Training dataset (xi,yi) x is the question Ex: what is the house price given its size? y is the answer Ex: house price f : predictive function f maps data feature x to data label y Supervised learning: Design a predictive function given a training set composed of data points (xi,yi) with xi the data features and yi the data label. Unsupervised learning Xavier Bresson 5 Supervised learning algorithm f Input: xi (data) Output: yi (property) f : x→ y or y=f(x) Training dataset (xi) x is the question Ex: what is the class of the input data? y is the answer given by inference (not by example) Ex: data class f : predictive function f maps data feature x to data label y No more label yi! Not a label! Supervised vs unsupervised Xavier Bresson 6 Difference between supervised and unsupervised learning: Consider the task of finding clusters of data: Supervised learning { data, label } Unupervised learning { data , - } Labels Class 0 Class 1 No labels Cluster 1 Cluster 2 Cluster 1 Cluster 2 Labels are highly valuable to solve the learning task. Xavier Bresson 7 Unsupervised learning: Design a predictive function given a training set which has no label. The training set is simply composed of the data points (xi) with the data features (no yi). Main idea: Find structures in data that can solve data analysis tasks. Common tasks: Unsupervised clustering: k-means (structures are the means) Unsupervised representation: PCA (structures are the variances) Unsupervised learning Xavier Bresson 8 Unsupervised clustering with k-means Clustering Outline Xavier Bresson 9 Definition: Find groups of data that are similar. Applications: Unsupervised clustering Social network analysis (ads for groups of similar people) Market analysis (segment customers or products in homogeneous groups) Image segmentation (group pixels of same objects) Molecular analysis (find molecules related to drugs) Xavier Bresson 10 Unsupervised clustering with k-means k-means algorithm Outline Xavier Bresson 11 k-means algorithm is the most popular unsupervised clustering algorithm. Definition: The mean (a.k.a. cluster center, centroid) of a cluster is the center of this cluster. Center definition: The center is a point not necessarily belonging to the set of data. The center is a point from the set of data. k-means Data points In this lecture, we choose the center to be a point from the set of data. Xavier Bresson 12 k-means algorithm steps: Step 1: Initialization. Select randomly k points called means: k-means Initial mean of cluster 2 Initial mean of cluster 1 + Original data points Step 1: Initialization Xavier Bresson 13 Step 2: Loop until convergence. Assign each data to one of the 2 means. Update the means of the clusters given the assigned data. k-means + + Updated mean of cluster 2 Updated mean of cluster 1 Updated cluster 2 Updated cluster 1 Xavier Bresson 14 Step 2: Loop until convergence. Assign each data to one of the 2 means. Update the means of the clusters given the assigned data. k-means + + Updated mean of cluster 2 Mean of cluster 1 (no update) Updated cluster 2 Updated cluster 1 Xavier Bresson 15 Step 2: Loop until convergence. Assign each data to one of the 2 means. Update the means of the clusters given the assigned data. k-means + + Mean of cluster 2 (no update) Mean of cluster 1 (no update) Cluster 2 (no update) Cluster 1 (no update) Xavier Bresson 16 Inputs: K = number of clusters Training set {x1,…,xn}, xi in Rd, d=number of features/dim Outputs: Clusters = {C1,…,CK } and means = { μ1,…, μK }. K-means algorithm: Initialization: Randomly initialize K means: { μ1,…, μK }∈Rd. Repeat until convergence: 1. Clusters assignment {C1,…,CK } 2. Means update { μ1,…, μK }. Formalization Xavier Bresson 17 1. Cluster assigmnent For all i = 1 to n: For all k = 1 to K: Formalization ai = arg min k kxi −µkk2 2 a = 2 6 6 6 6 6 4 2 1 1 . . . 2 3 7 7 7 7 7 5 1st data in C2 3rd data in C1 n x 1 Ck = {xi : ai = k} ai gives the index of the cluster the closest to xi. Distance between xi and the mean μk. Take the k that minimizes the distance with kxk2 2 = d X j=1 x2 j Euclidean distance µ2 µ1 ai = 2 ai = 1 kxi −µ2k2 2 kxi −µ1k2 2 µ2 µ1 C1 C2 Xavier Bresson 18 Formalization 2. Mean update For all k = 1 to K: d x 1 μk is the mean of all data points in cluster Ck µ2 µ1 C1 C2 µ2 µ1 C1 C2 + nb of data in cluster Ck µk = P xi2Ck xi |Ck| Xavier Bresson 19 Unsupervised clustering with k-means Loss Outline Xavier Bresson 20 k-means loss Measure of fitness/accuracy between the k-means model and the data points: Clusters Means µ2 µ1 xi C1 C2 kxi −µkk2 2 µ2 µ1 C1 C2 Loss is small (minimized) Loss is large L(C1, ..., CK, µ1, ..., µK) = 1 n n X i=1 kxi −µaik2 2 = 1 n K X k=1 X xi2Ck kxi −µkk2 2 Xavier Bresson 21 Optimize the k-means loss: Re-visiting k-means algorithm min C1, ..., CK µ1, ..., µK L(C1, ..., CK, µ1, ..., µK) Find Ck and μk that minimize the loss function. Xavier Bresson 22 Re-visiting k-means algorithm Repeat: 1. Cluster assignment 2. Mean update min C1,...,CK L(C1, ..., CK) for µ1, ..., µK ﬁxed min µ1,...,µK L(µ1, ..., µK) for C1, ..., CK ﬁxed Ck = {xi : ai = k} + ai = arg min k kxi −µkk2 2 + µk = P xi2Ck xi |Ck| Xavier Bresson 23 Unsupervised clustering with k-means k-means properties Outline Xavier Bresson 24 Monotonicity: k-means is guaranteed to decrease the loss at each iteration. Monotonicity and non-convexity iter Loss Liter Loss decrease Convergence (solution) Non-convex: k-means loss is non- convex ⇒No guarantee to find the global minimizer of the loss. ⇒Initial condition is important. global minimizer local minimizer Xavier Bresson 25 k-means has no guarantee to find a global solution: Initialization µ2 µ1 µ3 + µ2 µ1 µ3 µ2 µ1 µ3 + µ2 µ1 µ3 global solution local solution Xavier Bresson 26 Reduce the dependence w.r.t. initialization: Run the k-means algorithm 100 times with random initialization. Save clusters and the loss value for each run. Pick the solution with the smallest loss value. Initialization Xavier Bresson 27 No obvious answer. It depends on the data and the next task that will make use of the clusters (sometimes, post-processing selection after k-means ran). Example: Ambiguous “optimal” choice of k? How to choose the number k of clusters? ✓ ✓ Xavier Bresson 28 K-means algorithm cannot be applied to non-separated/ overlapping clusters. It is designed to find separated clusters. Separated clusters Overlapping clusters Separated clusters Xavier Bresson 29 k-means algorithm assumes the data are linearly separable ⇒Data can be separated by straight lines. Limitation k-means is not supposed to work for data with more complex distributions (solution is given by kernel k-means). Xavier Bresson 30 k-means is the oldest unsupervised clustering algorithm (by physicist Lloyd in 1957). k-means is sound: Loss decrease and convergence guaranteed, speed is linear w.r.t. the number of data, the number of features and the number of clusters, easy to implement, and multiple extensions exist (k-medians, k-plans) k-means is limited: linear data and existence of local minimizers. Summary Xavier Bresson 31 Unsupervised representation with PCA Introduction and motivations Outline Xavier Bresson 32 Principal component analysis (PCA) is a dimensionality reduction technique (introduced by statistician Karl Pearson in 1901). Why dimensionality reduction is useful? Data compression (compact representation) Data visualization (2D and 3D) Dimensionality reduction Data compression 64x64≈4k reduced to 10 variables Data visualization Video games Xavier Bresson 33 Example: Let’s consider some country properties Data compression motivation Country GDP (gross domestic product in trillions of US$) Human development index US 14.3 0.91 Canada 1.5 0.90 China 5.8 0.68 India 1.6 0.5 Singapore 0.2 0.8 Russia 1.4 0.7 x(1) x(2) Data are close to a line. There is some redundancy between the 2 features representing the country ⇒Compression is possible. US Canada Singapore x(1) x(2) Human development index GDP Line Xavier Bresson 34 We can reduce the data from 2 features (2D representation) to 1 feature (1D representation): Data compression motivation New compressed feature z(1) 2D representation 1D representation Compression + z 2 R1 x 2 R2 Singapore Canada US Projection of 2D points onto 1D line US Canada Singapore x(1) x(2) Human development index GDP Line Xavier Bresson 35 Data compression motivation Country GDP (gross domestic product in trillions of US$) Human development index Life expectancy US 14.3 0.91 78.3 Canada 1.5 0.90 80.7 China 5.8 0.68 73 India 1.6 0.5 64.7 Singapore 0.2 0.8 80 Russia 1.4 0.7 65.5 GDP Human development index US Canada Singapore x(1) x(2) Life expectancy x(3) Data are close to a plane. There is some redundancy between the 3 features representing the country ⇒ Compression is again possible. Plane x(1) x(2) x(3) Example: Let’s consider three country properties Xavier Bresson 36 Data compression motivation z 2 R2 x 2 R3 GDP Human development index US Canada Singapore x(1) x(2) Life expectancy x(3) z(1) Singapore Canada US z(2) Projection of 3D points onto 2D plane New compressed feature New compressed feature 3D representation 2D representation Compression + We can also reduce the data from 3 features (3D representation) to 2 features (2D representation): Xavier Bresson 37 General case: Data features can be compressed from 10,000-D compressed to 100-D. Data compression motivation 10,0000-D representation 100-D representation Dim reduction/ Compression + x = 2 6 6 6 4 x(1) x(2) . . . x(10,000) 3 7 7 7 5 2 R10,000 z = 2 6 6 6 4 z(1) z(2) . . . z(100) 3 7 7 7 5 2 R100 Main advantage of compression: Speeding up learning algorithm and less memory consuming. Xavier Bresson 38 Data visualization is an exploratory tool to get insights about the data, and therefore better understand them. The problem is that data with more than 3 features cannot be visualized. How to visualize 10,000 dimensions? ⇒Dimensionality reduction Visualization motivation Country GDP (gross domestic product in trillions of US$) Per capita GDP Human developme nt index Life expectanc y Poverty index (Gini) Mean household income (thousand s of US$) US 14.3 46.7 0.91 78.3 40.8 84.3 Canada 1.5 39.1 0.90 80.7 32.6 67.2 China 5.8 7.5 0.68 73 46.9 10.22 India 1.6 3.4 0.5 64.7 36.8 0.73 Singapore 0.2 56.6 0.8 80 42.5 67.1 Russia 1.4 19.8 0.7 65.5 39.9 0.72 Each country has 6 features x(3) x(1) x(2) x(4) x(5) x(6) Xavier Bresson 39 Visualization motivation Country GDP (gross domestic product in trillions of US$) Per capita GDP Human developme nt index Life expectancy Poverty index (Gini) Mean household income (thousands of US$) US 14.3 46.7 0.91 78.3 40.8 84.3 Canada 1.5 39.1 0.90 80.7 32.6 67.2 China 5.8 7.5 0.68 73 46.9 10.22 India 1.6 3.4 0.5 64.7 36.8 0.73 Singapore 0.2 56.6 0.8 80 42.5 67.1 Russia 1.4 19.8 0.7 65.5 39.9 0.72 x(3) x(1) x(2) x(4) x(5) x(6) Country Learned feature 1 Learned feature 2 US -4.5 46.7 Canada 1.6 6.7 China -5.8 7.5 India 1.6 -3.4 Singapore 0.2 3.2 Russia 2.4 -7.0 z(1) z(2) Dim reduction/ Compression + Dimensionality reduction interpretation: The meaning of learned features can be hard to interpret. The meaning of hand-crafted features are easy to interpret. Xavier Bresson 40 Plot/visualization: Visualization motivation Country Learned feature 1 Learned feature 2 US -4.5 46.7 Canada 1.6 6.7 China -5.8 7.5 India 1.6 -3.4 Singapore 0.2 3.2 Russia 2.4 -7.0 z(1) z(2) New compressed feature z(1) Singapore Canada US z(2) New compressed feature Although interpreting the new features may be hard, the dim reduction technique always put together data that are similar ⇒Clustering property of dim reduction. It makes easier to analyze data. Xavier Bresson 41 Unsupervised representation with PCA Principal directions and components Outline Xavier Bresson 42 Reduce the dimensionality of data by projecting the data on the closest line for 1D reduction, the closest plane for 2D reduction, etc. Basic idea How to define a “good” line, a “good” plane, etc? Data xi Line x(1) x(2) Projection of data xi on the blue line Xavier Bresson 43 What is a good line? The line that minimizes as much as possible the projection errors. Basic idea x(1) x(2) Projection of data xi on the blue line Data xi Projection error: distance between xi and its projection on the line The best blue line if the line that minimizes the sum of all projection errors. Xavier Bresson 44 Different lines: Basic idea x(1) x(2) x(1) x(2) Projection error Projection error Sum of projection errors is small (and minimized). Sum of projection errors is high. Xavier Bresson 45 xi : ith data point xi(j) : jth data feature of the ith data point zi : projection of the ith data point zi(j) : jth learned feature of the ith projected data point uk : unit vector of the kth principal direction (the line if k=1) Notations xi 2 Rd zi 2 Rd x(1) x(2) u1 Xavier Bresson 46 1st principal direction: The direction u1 in R2 onto which the projection of data is minimized and the coordinate z(1) in R1 on the projected direction u1. Principal directions and components xi 2 Rd zi 2 Rd x(1) x(2) u1 z(1) 1st principal direction 1st principal component 2D representation 1D representation Compression + z 2 R1 x 2 R2 Xavier Bresson 47 1st, 2nd principal directions: The directions u1,u2 in R3 onto which the projection of data is minimized and the coordinate z(1),z(2) in R2 on the projected directions u1,u2. Principal directions and components x(1) x(2) x(3) z(1) 1st, 2nd principal directions u2 u1 1st, 2nd principal components 1st,…,kth principal directions: The directions u1,…,uk in Rd onto which the projection of data is minimized and the coordinate z(1),…,z(k) in Rk on the projected directions u1,…,uk. z(2) z 2 R2 x 2 R3 3D representation 2D representation Compression + Xavier Bresson 48 Required pre-processing: Data centering (zero mean) Training set: x1,…,xn Mean vector: Centering data: Other pre-processing: z-scoring may help or not. PCA pre-processing µ = 1 n n X i=1 xi 2 Rd xi xi −µ xi xi −µ σ Xavier Bresson 49 Unsupervised representation with PCA Formalization Outline Xavier Bresson 50 PCA technique finds a linear space (line, plane, hyper-plane) that minimizes the sum of all projection errors. How to find this linear space? Linear algebra ! Formalization Xavier Bresson 51 Matrix of all variances and co-variances: Co-variance matrix ⌃= 1 n n X i=1 xixT i = 1 nXT X dx1 1xd dxd X = 2 6 4 xT 1 . . . xT n 3 7 5 nxd with data matrix dxd ⌃= 2 6 4 ⌃11 ⌃12 . . . ⌃1d ... ⌃d1 ⌃d2 . . . ⌃dd 3 7 5 Co-variance matrix (encode all data variations) ⌃jj = n X i=1 x2 i(j) ⌃jl = n X i=1 xi(j)xi(l) data variance along j-dim data co-variance along j-dim and l-dim Xavier Bresson 52 The principal directions u1,…,uk in Rd onto which the projection of data is minimized are given by the eigenvalue decomposition (EVD) of the co-variance matrix (no proof given): Principal directions ⌃= USU T Eigenvalue decomposition is a matrix factorization technique. Eigenvector matrix: U Eigenvalue matrix: S dxd dxd dxd dxd Matrix of k principal directions u1,…,uk in Rd dxd U = 2 4 | | | u1 . . . uk . . . ud | | | 3 5 Uk dxk uT p uq = ⇢1 if p = q 0 if p 6= q with uT p up = kupk2 2 = 1, 8p (unit vector) uT p uq = 0, 8p 6= q (orthonormal vectors) up uq Xavier Bresson 53 u1 represents the direction of the largest data variance. u2 represents the direction of the second largest data variance. … S11 is the value of the data variance along the direction u1. S22 is the value of the data variance along the direction u2. … Principal directions ⌃= USU T dxd S = 2 6 6 6 4 S11 0 S22 ... 0 Sdd 3 7 7 7 5 Sll = n X i=1 |xT i ul|2 (no proof given) with and Xavier Bresson 54 S11 represents the variance of the data along the direction u1 of the largest data variance : Principal directions capture the data variance xi 2 Rd x(1) x(2) u1 S11 = n X i=1 |xT i u1|2 xT i u1 = d X j=1 xi(j)u1(j) Sum of square of projected data along u1 Xavier Bresson 55 Unsupervised representation with PCA Algorithm Outline Xavier Bresson 56 Algorithm: Pre-process data: zero-mean Construct the co-variance matrix: Compute the EVD of Σ (principal directions): Compute the principal components (or projected data): PCA algorithm ⌃= USU T ⌃= 1 nXT X xi xi −1 n n X i=1 xi Z = U T k XT kxn kxd dxn Projected data coordinates on u1,…,uk U T k = 2 6 4 uT 1 . . . uT k 3 7 5 XT = 2 4 | | x1 . . . xn | | 3 5 uT 1 x1 Z Xavier Bresson 57 Unsupervised representation with PCA PCA properties Outline Xavier Bresson 58 We can construct new data from the compressed representation: Data generation z(1) x(1) x(2) u1 Principal direction Principal component + Generation ˆ x = Ukˆ z 2 Rd ˆ z 2 Rk Generation Compression d-D representation (high-dim representation) k-D representation (low-dim representation) Compression + z 2 Rk x 2 Rd Xavier Bresson 59 Selection rule: Data variations are captured by each principal direction ⇒A natural rule is to select the first k principal directions that capture e.g. 90% of total variance: How to select k? Pk l=1 Sll Pd l=1 Sll ≥0.9 Sll represents the variance of the data along the direction u1 Structure Noise k Sll l 90% of total data variance Xavier Bresson 60 PCA in practice Applications of PCA: Visualization: Insights about data properties Compression: Reduce memory needed for data Speed up learning speed Example: Images d=256x256 ⇒k=128 Important: There is no guarantee that applying PCA will improve the performances of learning algorithm. Actually, PCA usually decreases the results, but it is fine if speed is a priority. (x1, y1), . . . , (xn, yn), xi 2 Rd (z1, y1), . . . , (zn, yn), zi 2 Rk + Xavier Bresson 61 PCA is a linear dimensionality reduction technique ⇒It means that it is guaranteed to work when the data follow a linear distribution, but no guarantee otherwise. Limitation Data xi Linear distribution Non-linear distribution Data xi Xavier Bresson 62 PCA is the most common and the oldest linear dimensionality reduction technique. There exit several improvements of PCA: sparse PCA, robust PCA, graph PCA, non-linear PCA, etc. PCA algorithm is simple, sound, linear w.r.t. number of training data and number of principal components, but does not scale w.r.t. number of features d (complexity is O(d3)). PCA is optimal for data with linear distribution. Summary Xavier Bresson 63 Conclusion Outline Xavier Bresson 64 Unsupervised learning techniques solve data analysis tasks without any label information. Unsupervised techniques can be applied to data clustering, data representation (beyond linear PCA s.a. generative adversarial networks (GAN)), data visualization(beyond PCA s.a. t-SNE), etc. Unsupervised learning problems are much more challenging to solve than supervised learning problems. The next AI revolution will be unsupervised [LeCun]. Conclusion Xavier Bresson 65 tutorial07.ipynb Coding exercise Questions? Xavier Bresson 66 "
484,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 9: Recommender Systems Semester 2 2017/18 Xavier Bresson 2 Introduction Content-based recommendation Movie features Rating prediction with linear regression Collaborative recommendation Learning movie features – sequential approach Learning movie features – parallel approach Embedding and closest movies Conclusion Outline Xavier Bresson 3 Introduction Outline Xavier Bresson 4 Recommender systems have become a central part of intelligent systems. Examples: Google search engine recommends webpages on internet, recommending movies on Netflix, friends on Facebook, products on Amazon, jobs on LinkedIn, articles on NY Times website: Introduction … … Xavier Bresson 5 Netflix Prize : 1M$ reward Netflix is the biggest online movie company worldwide: #Movies> 100K and #Users> 100M. Netflix prize was a competition in 2009 for the best algorithm that can predict user ratings for movies based on previous ratings. Data: 480,189 users 17,770 movies 100,480,507 ratings ⇒Only 0.011% available ratings Introduction Xavier Bresson 6 Predict the missing ratings (unwatched movies) of the movie-user matrix given a training set of (user,movie,rating): Introduction User 1 John User 2 David User 3 Helen User 4 Katy Movie 1 Love Story ? 0 5 ? Movie 2 Casablanca 1 1 4 ? Movie 3 Avengers 5 ? 1 0 Movie 4 Terminator 4 4 ? 1 0 1 2 3 4 5 Xavier Bresson 7 nm : Nb of movies nu : Nb of users Rim,iu : Indicator matrix of ratings Yim,iu : Rating matrix Notations Rim,iu = ⇢1 if user iu has rated movie im 0 otherwise Yim,iu = Rating value given by user iu to movie im = {0, 1, ..., 5} User 1 John User 2 David User 3 Helen User 4 Katy Movie 1 Titanic ? 0 5 ? Movie 2 Love Story 0 1 4 ? Movie 3 Avengers 5 ? 1 0 Movie 4 Terminator 4 4 ? 1 Y = 2 6 6 4 − 0 5 − 0 1 4 − 5 − 1 0 4 4 − 1 3 7 7 5 R = 2 6 6 4 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 3 7 7 5 Xavier Bresson 8 Content-based recommendation Movie features Outline Xavier Bresson 9 Content-based recommender system: Predict movie rating given a set of movie features (content): Content-based recommendation User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Romance Feature 2 Action Movie 1 Titanic ? 0 5 ? 0.7 0.2 Movie 2 Love Story 0 1 4 ? 0.9 0.05 Movie 3 Avengers 5 ? 1 0 0.1 0.95 Movie 4 Terminat or 4 4 ? 1 0.15 0.97 x(1) x(2) x(1) : Measure the degree of romance in movie x(2) : Measure the degree of action in movie Xavier Bresson 10 Movie vector features: Notations xim = 2 4 1 xim(1) xim(2) 3 5 Romance feature x(1) for movie im Add element 1 for offset parameter Action feature x(2) for movie im User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Romance Feature 2 Action Movie 1 Love Story ? 0 5 ? 0.7 0.2 Movie 2 Casa blanca 0 1 4 ? 0.9 0.05 Movie 3 Avengers 5 ? 1 0 0.1 0.95 Movie 4 Terminat or 4 4 ? 1 0.15 0.97 User vector parameter: wiu = 2 4 wiu(0) wiu(1) wiu(2) 3 5 Xavier Bresson 11 Content-based recommendation Rating prediction with linear regression Outline User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Romance Feature 2 Action Movie 1 Love Story ? 0 5 ? 0.7 0.2 Movie 2 Casablanca 0 1 4 ? 0.9 0.05 Movie 3 Avengers 5 ? 1 0 0.1 0.95 Movie 4 Terminator 4 4 ? 1 0.15 0.97 Xavier Bresson 12 We may use a linear regression model fw(x) to estimate the movie rating: Rating prediction xim = 2 4 1 xim(1) xim(2) 3 5 User vector parameter: Movie vector features: For each user, we learn a parameter vector wiu that is used to estimate the rating of any movie xim. fwiu (xim) = wT iuxim ⇡1.3 wiu = 2 4 wiu(0) wiu(1) wiu(2) 3 5 Xavier Bresson 13 Regression problem: Problem formulation Linear regression problem: Mean value Mean square error L2 regularization Nb of movies rated by user iu Regularization hyper-parameter User 1 John User 2 David User 3 Helen User 4 Katy Movie 1 Titanic ? 0 5 ? Movie 2 Love Story 0 1 4 ? Movie 3 Avengers 5 ? 1 0 Movie 4 Terminator 4 4 ? 1 X im:Rim,iu=1 im iu Sum over all movies rated by user iu min wiu 1 niu X im:Rim,iu=1 "" fwiu (xim) −Yim,iu #2 + λ d d X j=1 w2 iu(j) Linear function min wiu 1 niu X im:Rim,iu=1 "" wT iuxim −Yim,iu #2 + λ d d X j=1 w2 iu(j) Square error Xavier Bresson 14 Learn all user parameters W=(w1,…,wnu) simultaneously: Problem formulation min w1,...,wnu L(w1, ..., wnu) User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Romance Feature 2 Action Movie 1 Love Story ? 0 5 ? 0.7 0.2 Movie 2 Casablanca 0 1 4 ? 0.9 0.05 Movie 3 Avengers 5 ? 1 0 0.1 0.95 Movie 4 Terminator 4 4 ? 1 0.15 0.97 wnu w1 w2 w3 min w1,...,wnu 1 nnu nu X iu=1 1 niu X im:Rim,iu=1 "" wT iuxim −Yim,iu #2 + λ dnu nu X iu=1 d X j=1 w2 iu(j) It will not change the solution Xavier Bresson 15 Gradient descent (vectorized representation): Optimization Gradient descent (element-wise representation): For iu = 0, ..., nu For iu = 0, ..., nu For j = 0, ..., d wiu wiu −⌧@L @wiu wiu −⌧ ⇣2 niu X im:Rim,iu=1 # wT iuxim −Yim,iu $ xim + 2λ d wiu ⌘ wiu(j) wiu(j) −⌧ @L @wiu(j) wiu(j) −⌧ ⇣2 niu X im:Rim,iu=1 # wT iuxim −Yim,iu $ xim(j) + 2λ d wiu(j) ⌘ Xavier Bresson 16 When we have access to good hand-crafted movie features like genre, release year, actors/actresses, etc, then content-based recommendation has excellent performances. However, getting meaningful hand-crafted features may be hard. For example, romantic/action features may require to watch the whole movie to give a good measure. Besides, it is sometimes hard to hand-craft features for item prediction. An alternative approach is collaborative filtering when the features are learned. Limitation Xavier Bresson 17 Collaborative recommendation Learning movie features – sequential approach Outline Xavier Bresson 18 Problem formulation: Suppose we do not know the right movie features to make movie recommendation, how to predict? Collaborative recommendation User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Feature 2 Movie 1 Love Story ? 0 5 ? ? ? Movie 2 Casablanca 0 1 4 ? ? ? Movie 3 Avengers 5 ? 1 0 ? ? Movie 4 Terminator 4 4 ? 1 ? ? The collaborative approach will allow to learn the movie features and the user parameters simultaneously. Xavier Bresson 19 Sequential algorithm: Given some initial movie features X=(x1,…,xnm), learn the user parameters W=(w1,…,wnu). Repeat until convergence: Fix the user parameters W=(w1,…,wnu), learn the movie features X=(x1,…,xnm). Fix the movie features X=(x1,…,xnm), learn the user parameters W=(w1,…,wnu). Collaborative recommendation User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Feature 2 Movie 1 Love Story ? 0 5 ? ? ? Movie 2 Casablanca 0 1 4 ? ? ? Movie 3 Avengers 5 ? 1 0 ? ? Movie 4 Terminator 4 4 ? 1 ? ? x(1) x(2) x1(1) x1(2) xnm(2) xnm(1) User 1 John User 2 David User 3 Helen User 4 Katy Feature 1 Feature 2 Movie 1 Love Story ? 0 5 ? ? ? Movie 2 Casablanca 0 1 4 ? ? ? Movie 3 Avengers 5 ? 1 0 ? ? Movie 4 Terminator 4 4 ? 1 ? ? x(1) x(2) x1(1) x1(2) xnm(2) xnm(1) Fix W=(w1,…,wnu), learn X=(x1,…,xnm) Fix X=(x1,…,xnm), learn W=(w1,…,wnu) wnu w1 w2 w3 wnu w1 w2 w3 Xavier Bresson 20 Fix the user parameters W=(w1,…,wnu), learn the movie features X=(x1,…,xnm): Movie features Gradient descent (vectorized representation): Lx(x1, ..., xnm) For im = 0, ..., nm min x1,...,xnm 1 nm nm X im=1 1 nim X iu:Rim,iu=1 "" wT iuxim −Yim,iu #2 + γ dnm nm X im=1 d X j=1 x2 im(j) Nb of users who rated movie im xim xim −⌧@Lx @xim xim −⌧ ⇣2 nim X im:Rim,iu=1 # wT iuxim −Yim,iu $ wiu + 2γ d xim ⌘ Xavier Bresson 21 Fix the movie features X=(x1,…,xnm), learn the user parameters W=(w1,…,wnu): User parameters Gradient descent (vectorized representation): Lw(w1, ..., wnu) min w1,...,wnu 1 nnu nu X iu=1 1 niu X im:Rim,iu=1 "" wT iuxim −Yim,iu #2 + λ dnu nu X iu=1 d X j=1 w2 iu(j) For iu = 0, ..., nu wiu wiu −⌧@Lw @wiu wiu −⌧ ⇣2 niu X im:Rim,iu=1 # wT iuxim −Yim,iu $ xim + 2λ d wiu ⌘ Xavier Bresson 22 Alternate optimization The sequential algorithm solves the optimization problem in a sequential way: Repeat until convergence: Fix the user parameters W=(w1,…,wnu), learn the movie features X=(x1,…,xnm). Fix the movie features X=(x1,…,xnm), learn the user parameters W=(w1,…,wnu). Sequential optimization may be very long: W →X →W →X →W →X →… We can speed up the optimization process. Xavier Bresson 23 Collaborative recommendation Learning movie features – parallel approach Outline Xavier Bresson 24 Combining movie feature loss and user parameter loss: Collaborative loss min x1, ..., xnm w1, ..., wnu L(x1, ..., xnm, w1, ..., wnu) min x1, ..., xnm w1, ..., wnu 1 nmu X im,iu:Rim,iu=1 "" wT iuxim −Yim,iu #2 + γ dnm nm X im=1 d X j=1 x2 im(j) + λ dnu nu X iu=1 d X j=1 w2 iu(j) Nb of ratings Xavier Bresson 25 Collaborative algorithm Alternate learning algorithm: Given some initial random user parameters W=(w1,…,wnu). Repeat until convergence: Prediction: Select a movie im and a user iu, predict a rating with the formula: fwiu (xim) = wT iuxim ⇡1.3 For iu = 0, ..., nu and for im = 0, ..., nm xim xim −⌧ ⇣2 nmu X iu:Rim,iu=1 # wT iuxim −Yim,iu $ wiu + 2γ dnm xim ⌘ wiu wiu −⌧ ⇣2 nmu X im:Rim,iu=1 # wT iuxim −Yim,iu $ xim + 2λ dnu wiu ⌘ Xavier Bresson 26 The predicted rating matrix F has a low-rank structure: Low-rank prediction 2 4 | | x1 . . . xnu | | 3 5 = X W T = 2 6 4 wT 1 . . . wT nm 3 7 5 nm x d d x nu nm x nu F = W T X Fim,iu = wT iuxim iu im F Low-rank matrix factorization: nm x nu nm x d d x nu small value d=low-rank Xavier Bresson 27 Collaborative recommendation Embedding and closest movies Outline Xavier Bresson 28 For each movie (product), we have learned a feature vector xim in Rd. The learned features represent an embedding of all movies. An embedding is a compact and linear representation of the data. It is used in many data science tasks like natural language processing (NLP) to represent the words for translation, questions and answers, etc. The embedding can be used to find similar data. Here, we can find the movies that are the most related to a given movie im. Select the 5 most similar movies to im by finding the 5 smallest distances i : Finding related movies { i : kxim −xik2 } im i Love Story Casablanca Romeo and Juliet Xavier Bresson 29 Conclusion Outline Xavier Bresson 30 Recommender systems are the backbone of companies like Amazon, Netflix, Google, etc There exist two classes of recommender systems: Content- based and collaborative techniques. Linear prediction can be improved with polynomial capacity and neural networks (state-of-the-art performances). Performances depend on the number of available data (user data, product data and ratings): the more the better. Conclusion Questions? Xavier Bresson 31 "
485,"CE9010: Introduction to Data Science Xavier Bresson Xavier Bresson 1 School of Computer Science and Engineering Data Science and AI Research Centre Nanyang Technological University (NTU), Singapore Lecture 10: Neural Networks Semester 2 2017/18 Xavier Bresson 2 Motivation Brain inspiration Neurons and connections Neural networks with multiple layers Learned features Logical gates with neural networks Logistic regression loss Backpropagation Initialization Training neural networks Conclusion Outline Xavier Bresson 3 Motivation Outline Xavier Bresson 4 Learning complex predictive functions: High learning capacity with polynomials Logistic regression: A higher order polynomial function is required to learn the statistics of these data: How many terms for polynomials of degree 10 and (only) 2 features? 100 pw(x) = σ(w0 + w1x(1) + w2x(2) + ... + wp2xp (2)) x(1) x(2) Xavier Bresson 5 Nb of polynomial terms for d=1 features and degree p=10: p Exponential explosion Nb of polynomial terms for d=2 features and degree p=10: p2 ⇒The number of polynomial grows exponentially with the number d of data features. x(1), x2 (1), ... , xp=10 (1) p=10 terms x(1), x2 (1), ... , xp=10 (1) x(1)x(2), x2 (1)x(2), ... , xp=10 (1) x(2) . . . x(1)xp (2), x2 (1)xp (2), ... , xp (1)xp (2) p=10 terms p=10 terms 100 terms Nb of polynomial terms for d features and degree p: pd Xavier Bresson 6 Learning polynomial functions with a large number d of features is computationally expensive (intractable) as the number of parameters w to learn also grows exponentially with d. Data with high number of features are common. Example: Images have 1024 x 1024 pixels ⇒d=106 (1M) features ⇒Nb of polynomial terms for degree 10 is pd=101,000,000 terms! Potential solution: Select a smaller number of polynomial features but which ones? We would need to know before hand the geometry of the decision boundary, which is generally never the case ⇒Not a solution. Require a new approach to learn high capacity function for large number of data features ⇒Neural networks Smaller number of features Xavier Bresson 7 Brain inspiration Outline Xavier Bresson 8 Neural networks (NNs) are loosely inspired by the biological brain (very simplified). NNs started in the 50’s with the Nobel prize of medicine of Hubel and Wiesel who decoded the first two layers of the cortex visual system. From this, computer scientists dreamed to give vision to machine by simulating it on computers, and more (mimic the human brain). Preliminary results were fine, but not better, slower and less math grounded than competitive techniques. People were more interested in e.g. SVM, random forest, LDA techniques from the 90’s to 2012. After 2000, computers became more powerful (Moore’s law) and emergence of big data (e.g. smartphone). In 2012, NNs were born again (and called deep learning) and overcome by a large margin existing techniques. NNs are the state-of-the art techniques for all learning problems. Neural networks Xavier Bresson 9 Biological NNs form a powerful system to learn perceptual tasks: seeing, hearing, speaking, smelling, touching. Biological experiments were developed to demonstrate the impressive learning ability of the brain: Neuroscientists surgically cut the connection between the auditory cortex and the ear, and re-routed the connection from the eye to the auditory cortex ⇒The auditory cortex learns to see! The brain system Xavier Bresson 10 A similar experiment: The connection between the somatosensory cortex and the sensory touch, and re-routed the connection from the eye to the somatosensory cortex ⇒The somatosensory cortex learns to see. The brain system Xavier Bresson 11 The brain “learning algorithm” Data Task Biological brain Artificial brain Proof of intelligent learning system (lower bound of intelligence) The path and goal of all learning systems today Perceptual intelligence (vision is easy) vs. cognitive intelligence (math is hard) This is a potential evidence that the brain has a generic “learning algorithm” as different parts of the brain can learn the same task. Data Task Xavier Bresson 12 Neurons and connections Outline Xavier Bresson 13 A human brain has 1011 neurons and 1014 axons (connection between neurons). A basic neuron: Neurons and axons Axon Dendrites Cell body Terminal bulbs Nucleus Xavier Bresson 14 Brain = architecture/group of connected neurons = neural networks Neurons and axons Xavier Bresson 15 (Non-)linear neural model: Neuron modeling x(1) x(2) x(d) Neuron inputs Neuron outputs σ x = 2 6 4 x(1) . . . x(d) 3 7 5 w = 2 6 6 6 4 w0 w1 . . . wd 3 7 7 7 5 w1 w2 wd y = fw(x) = σ(wT 1 x "" ) Activation function sigmoid σ σ(⌘) ⌘ Xavier Bresson 16 (Non-)linear neural model: Bias unit x(1) x(2) x(d) Neuron inputs Neuron outputs σ w1 w2 wd y = fw(x) = σ(wT 1 x "" ) w0 Bias/offset/ activation threshold +1 σ(⌘) ⌘ σ(⌘) ⌘ w0 = 0 w0 < 0 Xavier Bresson 17 Neural networks with multiple layers Outline Xavier Bresson 18 Neural network learning function Neural networks learn a mapping fw from input data feature x to output vector y: NNs w x y = fw(x) x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 w1 10 w1 11 w1 12 w1 20 w1 21 w1 22 w2 10 w2 11 w2 12 w1 13 w1 23 NNs Xavier Bresson 19 One input layer and one output layer: 2-layer neural network x(1) x(2) x(3) +1 y1 y2 3 input neurons/ units Bias neuron/ unit 2 output neurons/ units Layer 1: Input layer Layer 2: Output layer x = 2 4 x(1) x(2) x(3) 3 5 y = y1 y2 "" = σ(W 1 x "" ) = fw(x) w10 w11 w12 w20 w21 w22 w13 w23 Output Predictive function Xavier Bresson 20 Activation equations for the output layer: Element/neuron-wise activation equations: Vectorized activation equations: 2-layer neural network x(1) x(2) x(3) +1 y1 y2 w10 w11 w12 w20 w21 w22 w13 w23 y1 = σ(w10 + w11x(1) + w12x(2) + w13x(3)) y2 = σ(w20 + w21x(1) + w22x(2) + w23x(3)) y = σ(W 1 x "" ) W = w10 w11 w12 w13 w20 w21 w22 w23 "" x = 2 4 x(1) x(2) x(3) 3 5 Xavier Bresson 21 One input layer, one hidden layer and one output layer: 3-layer neural network x(1) x(2) x(3) +1 3 input neurons Bias neuron 2 hidden neurons Layer 1: Input layer Layer 2: Hidden layer +1 y3 1 y1 = x = 2 4 x(1) x(2) x(3) 3 5 y2 = y2 1 y2 2 "" = σ(W 1 1 y1 "" ) y3 = ⇥y3 1 ⇤ = σ(W 2 1 y2 $ ) = fw(x) 1 output neuron Layer 3: Output layer y2 1 y2 2 Layer 1 Layer 2 Layer 3 w1 10 w1 11 w1 12 w1 20 w1 21 w1 22 w2 10 w2 11 w2 12 Notation: Super-index is for the index layer l w1 13 w1 23 Layer 1 Layer 2 Layer 3 Xavier Bresson 22 Activation equations for the hidden layer and the output layer: Element/neuron-wise activation equations: Layer 2: Layer 3: Vectorized activation equations: Layer 2: Layer 3: 3-layer neural network y2 = σ(W 1 1 y1 "" ) y3 = σ(W 2 1 y2 "" ) y2 1 = σ(w1 10 + w1 11x(1) + w1 12x(2) + w1 13x(3)) y2 2 = σ(w1 20 + w1 21x(1) + w1 22x(2) + w1 23x(3)) y3 1 = σ(w1 10 + w1 11y2 1 + w1 12y2 2) x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 w1 10 w1 11 w1 12 w1 20 w1 21 w1 22 w2 10 w2 11 w2 12 w1 13 w1 23 W 1 = w1 10 w1 11 w1 12 w1 13 w1 20 w1 21 w1 22 w1 23 "" W 2 = ⇥w2 10 w2 11 w2 12 ⇤ Xavier Bresson 23 Notation: Weight matrices yl i : Activation of the ith neuron at the lth layer : Weight parameter matrix between layer l and layer l+1. It defines the mapping from layer l to layer l+1: W l W l yl yl+1 = σ(W l 1 yl "" ) nl+1 ⇥(nl + 1) #neurons in layer l+1 #neurons in layer l nl+1 ⇥1 nl ⇥1 Bias in layer l +1 Xavier Bresson 24 3-layer neural network: Example x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 w1 10 w1 11 w1 12 w1 20 w1 21 w1 22 w2 10 w2 11 w2 12 w1 13 w1 23 W 1 2 Rn2⇥(n1+1) n3 = 1 n2 = 2 n1 = 3 W 2 2 Rn3⇥(n2+1) 2x4=8 weight parameters 1x3=3 weight parameters y1 y2 y3 Xavier Bresson 25 Neural network with multiple layers (a.k.a. deep learning): Deep neural networks x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 +1 … +1 y3 2 y3 3 yL 1 yL 2 yL−1 1 yL−1 2 y2 = σ(W 1 1 y1 "" ) y3 = σ(W 2 1 y2 "" ) yL = σ(W L−1  1 yL−1 "" ) = fw(x) y1 = x Layer activations The parameters to learn are the weight matrices: W 1, W 2, ..., W L Xavier Bresson 26 The neural network activations yl , l=1,2,…,L are computed from left to right, from input data x to output prediction fw(x). This is called the forward pass: Forward pass x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 +1 … +1 y3 2 y3 3 yL 1 yL 2 yL−1 1 yL−1 2 y2 = σ(W 1 1 y1 "" ) y3 = σ(W 2 1 y2 "" ) yL = σ(W L−1  1 yL−1 "" ) = fw(x) y1 = x Forward pass fw(x) x Xavier Bresson 27 The neural network architectures without loop connections (i.e. cycles) are called feedforward NNs. Examples are fully connected neural networks and convolutional neural networks. Feedforward neural networks The neural network architectures with loop connections (i.e. cycles) are called recurrent NNs: x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 Cycle No cycle Xavier Bresson 28 Learned features Outline Xavier Bresson 29 NNs learn new features with the hidden layers: Hidden layers Original data features x are replaced by new features y2 learned from training data. Original/ raw features Learned features Predictive function x(1) x(2) x(3) +1 +1 y3 1 y2 1 y2 2 Hidden layer Output layer Input layer Key property: The new features y2 are learned from the training data ⇒NNs learn their own data features to solve the data analysis task (e.g. regression, classification) the best possible way. Powerful paradigm called end- to-end learning systems. Xavier Bresson 30 The deeper the better: Deep neural networks are able to learn highly meaningful/abstract data features/patterns that can capture complex statistics of data. Hidden layers x(1) x(2) x(3) +1 Layer 1: Input layer Layer 2: Hidden layer Layer 6: Output layer Layer 3: Hidden layer Layer 4: Hidden layer Layer 5: Hidden layer 4 hidden layers Xavier Bresson 31 Logical gates with neural networks Outline Xavier Bresson 32 Consider the AND logic function defined the following truth table: AND function x(1) x(2) AND True True True True False False False True False False False False y = x(1) AND x(2) Xavier Bresson 33 We can define a neural network that encodes the AND logistic function: AND function with neural network x(1) x(2) +1 y1 w10 = −30 w11 = 20 w12 = 20 y1 = fw(x) = σ(w10 + w11x(1) + w12x(2)) = σ(−30 + 20x(1) + 20x(2)) σ(⌘) ⌘ -10 10 0.5 1 x(1) x(2) AND 1 1 σ(10)≈1 1 0 σ(-10)≈0 0 1 σ(-10)≈0 0 0 σ(-30)≈0 Convention: True is 1. False is 0. Xavier Bresson 34 OR function Consider the OR logic function defined the following truth table: x(1) x(2) OR True True True True False True False True True False False False y = x(1) OR x(2) Xavier Bresson 35 We can define a neural network that encodes the OR logistic function: OR function with neural network x(1) x(2) +1 y1 w11 = 20 w12 = 20 x(1) x(2) OR 1 1 σ(30)≈1 1 0 σ(10)≈1 0 1 σ(10)≈1 0 0 σ(-10)≈0 y1 = fw(x) = σ(w10 + w11x(1) + w12x(2)) = σ(−10 + 20x(1) + 20x(2)) w10 = −10 Convention: True is 1. False is 0. σ(⌘) ⌘ -10 10 0.5 1 Xavier Bresson 36 Logistic regression loss Outline Xavier Bresson 37 Reminder: Logistic regression loss Linear logistic regression Binary classification (K=2) : Neural network of linear logistic regression: pw(x) 2 R L(w) = −1 n h n X i=1 ˆ yi log pw(xi) + (1 −ˆ yi) log(1 −pw(xi)) i + λ d d X j=1 w2 j Label probability : x(1) x(2) +1 y y = pw(x) = σ(W 1 x "" ) x(d) ˆ y 2 {0, 1} Xavier Bresson 38 Multi-class logistic regression loss for NNs: Logistic regression for neural networks Multi-class classification with K clusters: Sum over all weight parameters of the NN L(w) = −1 n h K X k=1 n X i=1 ˆ yi,k log pw(xi)k + (1 −ˆ yi,k) log(1 −pw(xi)k)) i + λ L−1 X l=1 nl+1 X i=1 nl+1 X j=1 (W l ij)2 ˆ yi = 2 6 6 6 4 ˆ yi,1 ˆ yi,2 . . . ˆ yi,K 3 7 7 7 5 2 RK pw(xi) = 2 6 6 6 4 pw(xi)1 pw(xi)2 . . . pw(xi)K 3 7 7 7 5 2 RK Measure of fitness between the data and the predictive NN model Xavier Bresson 39 Output layer for the multi-class neural networks: Probability vector for multi-class x(1) x(2) x(3) +1 Output layer: The number of output neurons is the number of classes, here K=3. y1 = x y2 y3 y4 = pw(x) 2 RK y4 = pw(x) = 2 4 0.15 0.80 0.05 3 5 2 R3 Xavier Bresson 40 For example, input x is an image and output y4 is the probability for the image to belong to the class {cat,dog,car}. Probability vector for multi-class x(1) x(2) x(3) +1 y1 = x y2 y3 Output neuron encoding the probability for class cat Output neuron encoding the probability for class dog Output neuron encoding the probability for class car 80% of chance that this image is a dog. Xavier Bresson 41 One-hot/Dirac representation of classes: Probability vector for multi-class This defines a 1-vs-all classification technique ⇒ Each output neuron estimates the probability of the input to belong to one of the 3 classes. ˆ yi = 2 4 1 0 0 3 5 if xi = cat ˆ yi = 2 4 0 1 0 3 5 if xi = dog ˆ yi = 2 4 0 1 0 3 5 if xi = car Xavier Bresson 42 Classification task Training set : In practice (x1, ˆ y1), ..., (xn, ˆ yn) x 2 Rd ˆ y = {1, 2, ..., K} One-hot representation of the class + Here ˆ y2 = 2 ˆ yhot = 2 6 6 6 6 6 4 0 1 0 . . . 0 3 7 7 7 7 7 5 2 RK = ⇢1 if ˆ yk = k 0 otherwise Xavier Bresson 43 Backpropagation Outline Xavier Bresson 44 Minimizing the loss function to learn the weight parameters W1,W2,…,WL of the NNs: Gradient descent L(w) = −1 n h K X k=1 n X i=1 ˆ yi,k log pw(xi)k + (1 −ˆ yi,k) log(1 −pw(xi)k)) i + λ L−1 X l=1 nl+1 X i=1 nl+1 X j=1 (W l ij)2 Gradient descent technique: min w=(W 1,...,W L) L(w) Gradient of the loss w.r.t. weight parameter Wl W l W l −⌧@L @W l rW l = @L @W l Notation: Weight update : Xavier Bresson 45 Repeat until convergence: Forward pass (compute all activations): Backpropagation algorithm Backward pass (compute all gradients of weight parameters): For l = L −1, L −2, ..., 1 W l W l −⌧rW l Derivative of activation function σ nl ⇥1 nl ⇥1 nl+1 ⇥(nl + 1) W l = 2 4 | W l 0 ¯ W l | 3 5 Bias vector nl+1 ⇥nl nl+1 ⇥1 For l = 1, 2, ..., L nl+1 ⇥1 nl+1 ⇥(nl + 1) (nl + 1) ⇥1 yl+1 = σ ✓ W l 1 yl #◆ δl=L = yL −ˆ y δl = ( ¯ W l)T δl+1 . σ0 ! yl"" Suppose we have no regularization: λ = 0 Suppose we have only 1 data: n = 1 rW l = δl+1 1 yl ""T No proof given! Xavier Bresson 46 Three-layer neural network for classifying image data x into 10 classes: n1=d=400, n2=25, n3=K=10. Example x(1) x(2) +1 . . . . . . . . . x(d) +1 W 1 W 2 Output neuron encoding the probability for e.g. class dog. x pw(x) 2 R10 400 input neurons 25 hidden neurons 10 output neurons Xavier Bresson 47 Forward equation: Example 400 ⇥1 25 ⇥1 10 ⇥1 x(1) x(2) +1 . . . . . . . . . x(d) +1 y2 n2 y2 1 y3 1 25 ⇥401 10 ⇥26 W 1 W 2 y3 K For l = 1, 2, ..., L nl+1 ⇥1 nl+1 ⇥(nl + 1) (nl + 1) ⇥1 yl+1 = σ ✓ W l 1 yl #◆ Forward pass y2 = σ ✓ W 1 1 y1 #◆ y1 = x y3 = σ ✓ W 2 1 y2 #◆ y1 ! y2 ! y3 Forward sequence: Xavier Bresson 48 Backward equation: Example x(1) x(2) +1 . . . . . . . . . x(d) +1 y2 n2 y2 1 y3 1 25 ⇥401 10 ⇥26 W 1 W 2 y3 K Backward pass Backward sequence: δ3 = y3 −ˆ y For l = 2, 1 W l W l −⌧rW l δ3 ! rW 2 ! W 2 ! δ2 ! rW 1 ! W 1 δ3 = y3 −ˆ y 10 ⇥1 W 2 W 2 −⌧rW 2 10 ⇥26 W 1 W 1 −⌧rW 1 25 ⇥401 25 ⇥1 δ2 = ( ¯ W 2)T δ3 . σ0 ! y2"" δl = ( ¯ W l)T δl+1 . σ0(yl) rW l = δl+1 1 yl ""T rW 2 = δ3 1 y2 ""T rW 1 = δ2 1 y1 ""T Xavier Bresson 49 Iterate forward pass and backward pass until convergence: Iterate x(1) x(2) +1 . . . . . . . . . x(d) +1 y2 n2 y2 1 y3 1 W 1 W 2 y3 K Forward pass Backward pass y1 y2 y3 δ3 δ2 rW 2 rW 1 W 2 W 1 Xavier Bresson 50 Simple modification of the backpropagation algorithm: Regularization Forward pass (compute all activations): Backward pass (compute all gradient of weight parameters): For l = L −1, L −2, ..., 1 W l W l −⌧rW l For l = 1, 2, ..., L yl+1 = σ ✓ W l 1 yl #◆ δl=L = yL −ˆ y δl = ( ¯ W l)T δl+1 . σ0 ! yl"" Regularization term rW l = δl+1 1 yl ""T + 2λW l Xavier Bresson 51 Intuition Backpropagation algorithm back-propagates the prediction error from the output/last layer to the input layer: At each layer, the algorithm modifies the weight parameters to encourage or discourage (depending on the prediction error) the current values of the weights: This approach is called pattern matching. δl=L = yL −ˆ y prediction label/ground truth W l W l −⌧rW l δl = ( ¯ W l)T δl+1 . σ0 ! yl"" prediction error rW l = δl+1 1 yl ""T Xavier Bresson 52 Neural networks can also be used for regression: Regression with neural networks x(1) x(2) x(3) +1 Output layer There is only one output neuron. y1 = x y2 y3 y4 = fw(x) 2 R L(w) = 1 n n X i=1 (fw(xi) −yi)2 + λ L−1 X l=1 nl+1 X i=1 nl+1 X j=1 (W l ij)2 The loss is the standard MSE loss: Backpropagation can also be applied to minimize the MSE loss. Xavier Bresson 53 The backpropagation algorithm can be vectorized: Forward pass (compute all activations): Vectorized backpropagation Backward pass (compute all gradients of weight parameters): For l = L −1, L −2, ..., 1 W l W l −⌧rW l Derivative of activation function σ nl+1 ⇥(nl + 1) W l = 2 4 | W l 0 ¯ W l | 3 5 Bias vector For l = 1, 2, ..., L nl+1 ⇥(nl + 1) yl+1 = σ ✓ W l 1 yl #◆ δl=L = yL −ˆ y δl = ( ¯ W l)T δl+1 . σ0 ! yl"" rW l = 1 nδl+1 1 yl ""T + 2λW l nl+1 ⇥n Number of data points (nl + 1) ⇥n nl ⇥n nl ⇥n nl+1 ⇥n nl+1 ⇥nl Xavier Bresson 54 Initialization Outline Xavier Bresson 55 It is not good to initialize all weight values to Wl=0. If Wl=0 then the forward pass would give the same value for all activations yl, the same prediction error for all output neurons, and consequently the backpropagation algorithm would compute the same gradient for all weight parameters at each layer l ⇒All weight matrices Wl would be the same. The issue is that all initial weights have the same value, so the solution is to break the symmetry. Random initialization: This also guarantees unit variance (no proof). Initialization δ3 = y3 −ˆ y For l = 2, 1 rW l = δl+1 1 yl ""T W l W l −⌧rW l W l = U h − 2 pnl , 2 pnl i Uniform distribution δl = ( ¯ W l)T δl+1 . σ0 ! yl"" Xavier Bresson 56 Training neural networks Outline Xavier Bresson 57 Back to the recipe for designing learning systems (Lecture 7): Step 1: Pre-process data (zero-mean, unit variance) Step 2: Choose first a small NN and increase the size if needed. Step 3: Make sure you have enough learning capacity. Extract a sub-set of training data and over-fit them, i.e. LTrain ≈ 0 (LVal is high) by manually selecting the hyper-parameters. Step 4: Add regularization (L2, dropout) and evaluate the generalization performance on the validation set. We should have LVal ↘and LTrain ↗. The gap between LVal and LTrain should be as small as possible. Step 5: Use all training data and cross-validation (only k=1 fold) to estimate the parameters and the hyper-parameters (long running time). Ideally, LVal ≈ LTrain ≈ small value. Training neural networks Xavier Bresson 58 Backpropagation is the backbone of the learning algorithm. It is a simple algorithm (but it may not be easy to implement efficiently). Modern implementations are TensorFlow (Google) and PyTorch (Facebook) that perform automatically backpropagation and weight update - no need to implement backprog! TensorFlow and pyTorch are amazing libraries to build and train neural network architectures, and they are optimized for GPUs. Modern backpropagation Xavier Bresson 59 Conclusion Outline Xavier Bresson 60 Neural networks have become the state-of-the-art learning techniques since 2012. They provide impressive results in perceptual tasks (computer vision, speech processing, and natural language processing). This lecture has only covered fully connected neural networks, a.k.a. multi-layer perceptron (MLP). The 2 most successful neural network classes are: Convolutional neural networks Recurrent neural networks There exits a large and highly dynamic research area for designing neural network architectures. Conclusion Xavier Bresson 61 tutorial08.ipynb Coding exercise Questions? Xavier Bresson 62 "
486,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Andrew Ng Introduction to Deep Learning Welcome deeplearning.ai • AI is the new Electricity • Electricity had once transformed countless industries: transportation, manufacturing, healthcare, communications, and more • AI will now bring about an equally big transformation. Andrew Ng What you’ll learn Courses in this sequence (Specialization): 1. Neural Networks and Deep Learning 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3. Structuring your Machine Learning project 4. Convolutional Neural Networks 5. Natural Language Processing: Building sequence models Andrew Ng Introduction to Deep Learning What is a Neural Network? deeplearning.ai Housing Price Prediction size of house price Housing Price Prediction #bedrooms size Housing Price Prediction ! "" !# zip code !$ wealth !% y Introduction to Deep Learning Supervised Learning with NeuralNetworks deeplearning.ai Supervised Learning Output (y) Application Input(x) Price Click on ad? (0/1) Object (1,…,1000) Text transcript Chinese Position of other cars Real Estate Online Advertising Photo tagging Speech recognition Machine translation Autonomous driving Home features Ad, user info Image Audio English Image, Radar info Neural Network examples Standard NN Recurrent NN Convolutional NN Size #bedrooms … Price (1000$s) 2104 3 400 1600 3 330 2400 3 369 … … … 3000 4 540 Structured Data Supervised Learning User Age Ad Id … Click 41 93242 1 80 93287 0 18 87312 1 … … … 27 71244 1 Unstructured Data Image Four scores and seven years ago… Text Audio Andrew Ng Introduction to Neural Networks Why is Deep Learning takingoff? deeplearning.ai Scale drives deep learning progress Amount of data Performanc e Andrew Ng • Data • Computation • Algorithms Idea Experiment Code Scale drives deep learning progress Andrew Ng Andrew Ng Introduction to Neural Networks About thisCourse deeplearning.ai Courses in this Specialization 1. Neural Networks and DeepLearning 2. Improving Deep Neural Networks:Hyperparameter tuning, Regularization andOptimization 3. Structuring your Machine Learningproject 4. Convolutional Neural Networks 5. Natural Language Processing: Building sequencemodels Andrew Ng Andrew Ng Outline of this Course Week 1: Introduction Week 2: Basics of Neural Network programming Week 3: One hidden layer Neural Networks Week 4: Deep Neural Networks "
487,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Basics of Neural Network Programming Binary Classification deeplearning.ai Andrew Ng 1 (cat) vs 0 (non cat) 255 134 93 22 123 94 83 2 34 44 187 30 34 76 232 124 67 83 194 142 255 134 202 22 123 94 83 4 34 44 187 192 34 76 232 34 67 83 194 94 255 231 42 22 123 94 83 2 34 44 187 92 34 76 232 124 67 83 194 202 Red Green Blue Binary Classification Andrew Ng Notation Basics of Neural Network Programming Logistic Regression deeplearning.ai Andrew Ng Logistic Regression Basics of Neural Network Programming Logistic Regression cost function deeplearning.ai Andrew Ng Logistic Regression cost function ! "" = % &!' + ) , where % * = "" ""#$+, Given ('(.), !(.)),…,('(1), !(1)) , want ! ""(2) ≈! 2 . Loss (error) function: Basics of Neural Network Programming Gradient Descent deeplearning.ai Andrew Ng Gradient Descent Recap: ! "" = % &'( + * , % + = , ,-./0 1 &, * = 1 4 5 6 78,ℒ(! "" 7 , !(7)) =−1 4 5 6 78, !(7) log ! "" 7 + (1 −!(7)) log(1 −! "" 7 ) Want to find &, * that minimize 1 &, * * 1 &, * & Andrew Ng Gradient Descent ! Basics of Neural Network Programming Derivatives deeplearning.ai Andrew Ng Intuition about derivatives ! "" = 3"" "" Basics of Neural Network Programming More derivatives examples deeplearning.ai Andrew Ng Intuition about derivatives ! "" = ""$ "" Andrew Ng More derivative examples Basics of Neural Network Programming Computation Graph deeplearning.ai Andrew Ng Computation Graph Basics of Neural Network Programming Derivatives with a Computation Graph deeplearning.ai Andrew Ng Computing derivatives !=""# $ = & + ! ) = 3$ 6 11 33 & = 5 # = 2 "" = 3 Andrew Ng Computing derivatives !=""# $ = & + ! ) = 3$ 6 11 33 & = 5 # = 2 "" = 3 Basics of Neural Network Programming Logistic Regression Gradient descent deeplearning.ai Andrew Ng ! = $%& + ( ) * = + = ,(!) ℒ+, ) = −() log(+) + (1 −)) log(1 −+)) Logistic regression recap Andrew Ng Logistic regression derivatives ! = $%&% + $(&( + ) &% $% &( $( b * = +(!) ℒ(a, 1) Basics of Neural Network Programming Gradient descent on m examples deeplearning.ai Andrew Ng Logistic regression on m examples Andrew Ng Logistic regression on m examples Basics of Neural Network Programming Vectorization deeplearning.ai Andrew Ng What is vectorization? Basics of Neural Network Programming More vectorization examples deeplearning.ai Andrew Ng Neural network programming guideline Whenever possible, avoid explicit for-loops. Andrew Ng Vectors and matrix valued functions Say you need to apply the exponential operation on every element of a matrix/vector. ! = !$ ⋮ !& u[i]=math.exp(v[i]) u = np.zeros((n,1)) for i in range(n): Andrew Ng Logistic regression derivatives J = 0, dw1 = 0, dw2 = 0, db = 0 for i = 1 to n: !("") = ""$#("") + % &("") = '(!("")) * += −-("") log - 1 "" + (1 −- "" ) log(1 −- 1 "" ) d!("") = &("")(1 −&("")) d""% += #% ("")d!("") d""' += #' ("")d!("") db += d!("") J = J/m, d""% = d""%/m, d""' = d""'/m, db = db/m Basics of Neural Network Programming Vectorizing Logistic Regression deeplearning.ai Andrew Ng Vectorizing Logistic Regression !(#) = &'((#) + * +(#) = ,(!(#)) !(-) = &'((-) + * +(-) = ,(!(-)) !(.) = &'((.) + * +(.) = ,(!(.)) Basics of Neural Network Programming Vectorizing Logistic Regression’s Gradient Computation deeplearning.ai Andrew Ng Vectorizing Logistic Regression Andrew Ng Implementing Logistic Regression J = 0, d!! = 0, d!"" = 0, db = 0 for i = 1 to m: ""($) = !&#($) + % &($) = '(""($)) * += −-($) log & $ + (1 −- $ ) log(1 −& $ ) d""($) = &($) −-($) d!! += #! ($)d""($) d!"" += #"" ($)d""($) db += d""($) J = J/m, d!! = d!!/m, d!"" = d!""/m db = db/m Basics of Neural Network Programming Broadcasting in Python deeplearning.ai Broadcasting example cal = A.sum(axis = 0) percentage = 100*A/(cal.reshape(1,4)) Apples Beef Eggs Potatoes Carb Fat 56.0 0.0 4.4 68.0 1.2 104.0 52.0 8.0 1.8 135.0 99.0 0.9 Protein Calories from Carbs, Proteins, Fats in 100g of different foods: 101 102 103 204 205 206 1 2 3 4 5 6 100 200 + = 101 202 303 104 205 306 100 200 300 1 2 3 4 5 6 + = 1 2 3 4 100 101 102 103 104 + = Broadcasting example General Principle Basics of Neural Network Programming Explanation of logistic regression cost function (Optional) deeplearning.ai Andrew Ng Logistic regression cost function Andrew Ng If $ = 1: ( $ ) = $ * If $ = 0: ( $ ) = 1 −$ * Logistic regression cost function Andrew Ng Cost on m examples "
488,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai One hidden layer Neural Network Neural Networks Overview Andrew Ng What is a Neural Network? !"" !# !$ % & x w b ' = )*! + , - = .(') ℒ(-, %) x 4[""] ,[""] '[""] = 4[""]! + ,[""] -[""] = .('[""]) '[#] = 4[#]- [""] + ,[#] -[#] = .('[#]) ℒ(-[#], %) !"" !# !$ % & 4[#] ,[#] deeplearning.ai One hidden layer Neural Network Neural Network Representation Andrew Ng Neural Network Representation !"" !# !$ % & deeplearning.ai One hidden layer Neural Network Computing a Neural Network’s Output Andrew Ng Neural Network Representation !"" !# !$ % & ' = )!! + + )!! + + , !"" !# !$ -(') , = % & ' , = -(') Andrew Ng ! = #!$ + & #!$ + & ' $( $) $* +(!) ' = . / ! ' = +(!) Neural Network Representation $( $) $* . / $( $) $* . / Andrew Ng Neural Network Representation !"" !# !$ % & '"" "" '# "" '$ "" '( "" )"" "" = +"" "" , ! + /"" [""], ' "" [""] = 3()"" "" ) )# "" = +# "" , ! + /# [""], ' # [""] = 3()# "" ) )$ "" = +$ "" , ! + /$ [""], ' $ [""] = 3()$ "" ) )( "" = +( "" , ! + /( [""], ' ( [""] = 3()( "" ) Andrew Ng Neural Network Representation learning Given input x: ! "" = $ "" % + ' "" ( "" = )(! "" ) ! , = $ , ( "" + ' , ( , = )(! , ) %"" %, %- . / ("" "" (, "" (- "" (0 "" deeplearning.ai One hidden layer Neural Network Vectorizing across multiple examples Andrew Ng Vectorizing across multiple examples !"" !# !$ % & ' "" = ) "" ! + + "" , "" = -(' "" ) ' # = ) # , "" + + # , # = -(' # ) Andrew Ng ! "" ($) = ' "" (($) + * "" + "" ($) = ,(! "" $ ) ! - ($) = ' - + "" ($) + * - + - ($) = ,(! - $ ) Vectorizing across multiple examples for i = 1 to m: deeplearning.ai One hidden layer Neural Network Explanation for vectorized implementation Andrew Ng Justification for vectorized implementation Andrew Ng !"" !# !$ % & Recap of vectorizing across multiple examples for i = 1 to m ' "" ()) = , "" !()) + . "" / "" ()) = 0(' "" ) ) ' # ()) = , # / "" ()) + . # / # ()) = 0(' # ) ) … 1 = !("") !(#) !(2) /[""](#) A[""] = /[""]("") /[""](2) … 6 "" = , "" 1 + . "" 7 "" = 0(6 "" ) 6 # = , # 7 "" + . # 7 # = 0(6 # ) deeplearning.ai One hidden layer Neural Network Activation functions Andrew Ng Activation functions !"" !# !$ % & ' "" = ) "" ! + + "" , "" = -(' "" ) ' # = ) # , "" + + # , # = -(' # ) Given x: Andrew Ng Pros and cons of activation functions a z sigmoid: ! = 1 1 + &'( z a x a z a deeplearning.ai One hidden layer Neural Network Why do you need non-linear activation functions? Andrew Ng Activation function ! "" = $ "" % + ' "" ( "" = )[""](! "" ) ! . = $ . ( "" + ' . ( . = )[.](! . ) Given x: %"" %. %/ 0 1 deeplearning.ai One hidden layer Neural Network Derivatives of activation functions Andrew Ng Sigmoid activation function a z !(#) = 1 1 + )*+ Andrew Ng !(#) = tanh(#) Tanh activation function a z Andrew Ng z ReLU a z Leaky ReLU a ReLU and Leaky ReLU deeplearning.ai One hidden layer Neural Network Gradient descent for neural networks Andrew Ng Gradient descent for neural networks Andrew Ng Formulas for computing derivatives deeplearning.ai One hidden layer Neural Network Backpropagation intuition (Optional) Andrew Ng Computing gradients Logistic regression ! = #$% + ' % # ' ) = *(!) ℒ(), /) Andrew Ng ![#] = &[#]' + )[#] ' &[""] )[""] +[#] = ,(![#]) ℒ(+[0], y) ![0] = &[0]' + )[0] +[0] = ,(![0]) Neural network gradients &[$] )[$] Andrew Ng !""[$] = !'[$]( ) * !+[$] = !'[$] !'[)] = "" $ ,!'[$] ∗.[)]′(z ) ) !""[)] = !'[)]3, !+[)] = !'[)] Summary of gradient descent !'[$] = ([$] −5 Andrew Ng !""[$] = '[$] −) !*[$] = !""[$]' + , !-[$] = !""[$] !""[+] = * $ .!""[$] ∗0[+]′(z + ) !*[+] = !""[+]5. !-[+] = !""[+] !6[""] = 7[""] −8 !*[""] = 1 : !6[""]7 $ , !-[""] = 1 : ;<. >?:(!6 "" , '5A> = 1, BCC<!A:> = DE?C) !6[$] = * "" %!6[""] ∗0[$]′(Z $ ) !*[$] = 1 : !6[$]G% !-[$] = 1 : ;<. >?:(!6 $ , '5A> = 1, BCC<!A:> = DE?C) Summary of gradient descent deeplearning.ai One hidden layer Neural Network Random Initialization Andrew Ng What happens if you initialize weights to zero? !! [!] ""# !$ [!] ""$ % & !! [$] Andrew Ng Random initialization !! [!] ""# !$ [!] ""$ % & !! [$] "
489,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Deep Neural Networks Deep L-layer Neural network Andrew Ng What is a deep neural network? logistic regression 1 hidden layer 2 hidden layers 5 hidden layers Andrew Ng Deep neural network notation deeplearning.ai Deep Neural Networks Forward Propagation in a Deep Network Andrew Ng Forward propagation in a deep network deeplearning.ai Getting your matrix dimensions right Deep Neural Networks Andrew Ng Parameters ![""] and ""[""] $ % &! &"" Andrew Ng Vectorized implementation ! "" #! #"" deeplearning.ai Why deep representations? Deep Neural Networks Andrew Ng Intuition about deep representation ! "" Andrew Ng Circuit theory and deep learning Informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute. deeplearning.ai Building blocks of deep neural networks Deep Neural Networks Andrew Ng Forward and backward functions Andrew Ng Forward and backward functions deeplearning.ai Parameters vs Hyperparameters Deep Neural Networks Andrew Ng What are hyperparameters? Parameters: ! "" , % "" , ! & , % & , ! ' , % ' … Andrew Ng Applied deep learning is a very empirical process cost ! # of iterations Idea Experiment Code deeplearning.ai What does this have to do with the brain? Deep Neural Networks Andrew Ng Forward and backward propagation ![""] = #[""]$ + &[""] '[""] = ( "" (! "" ) ![$] = #[$]'[""] + &[$] '[$] = ( $ (! $ ) '[%] = ( % ! % = + , … … -![%] = '[%] −+ -#[%] = 1 0 -! % ' % & -&[%] = 1 0 12. sum(d! % , 9:;< = 1, =>>2-;0< = ?@A>) -![%'""] = -# % &-! % (( % (! %'"" ) -#[""] = 1 0 -! "" ' "" & -&[""] = 1 0 12. sum(d! "" , 9:;< = 1, =>>2-;0< = ?@A>) -![""] = -# % &-! $ (( "" (! "" ) "
490,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Setting up your ML application Train/dev/test sets deeplearning.ai Andrew Ng Applied ML is a highly iterative process Idea Experiment Code # layers # hidden units learning rates activation functions … Andrew Ng Train/dev/test sets Andrew Ng Mismatched train/test distribution Training set: Cat pictures from webpages Dev/test sets: Cat pictures from users using your app Not having a test set might be okay. (Only dev set.) Setting up your ML application Bias/Variance deeplearning.ai Andrew Ng Bias and Variance high bias “just right” high variance Andrew Ng Bias and Variance Cat classification Train set error: Dev set error: Andrew Ng High bias and high variance !"" !# Setting up your ML application Basic “recipe” for machine learning deeplearning.ai Andrew Ng Basic recipe for machine learning Regularizing your neural network Regularization deeplearning.ai Andrew Ng Logistic regression min $,& '(), *) Andrew Ng Neural network Regularizing your neural network Why regularization reduces overfitting deeplearning.ai Andrew Ng How does regularization prevent overfitting? !"" !# !$ % & high bias “just right” high variance Andrew Ng How does regularization prevent overfitting? Regularizing your neural network Dropout regularization deeplearning.ai Andrew Ng Dropout regularization !"" !# $ % !& !' !"" !# $ % !& !' Andrew Ng Implementing dropout (“Inverted dropout”) Andrew Ng Making predictions at test time Regularizing your neural network Understanding dropout deeplearning.ai Andrew Ng Why does drop-out work? Intuition: Can’t rely on any one feature, so have to spread out weights. !"" !# $ % !& Regularizing your neural network Other regularization methods deeplearning.ai Andrew Ng Data augmentation 4 Andrew Ng Early stopping # iterations Setting up your optimization problem Normalizing inputs deeplearning.ai Andrew Ng Normalizing training sets !"" !# 5 3 !# !"" !# !"" Andrew Ng Why normalize inputs? ! "", $ = 1 ' ( * +,- ℒ (0 1 + , 0(+)) "" $ "" $ "" $ ! Unnormalized: "" $ ! Normalized: Vanishing/exploding gradients deeplearning.ai Setting up your optimization problem Andrew Ng Vanishing/exploding gradients !"" !# = $ % Andrew Ng Single neuron example !"" !# !$ !% & ' ( = *(,) Numerical approximation of gradients deeplearning.ai Setting up your optimization problem Andrew Ng Checking your derivative computation ! Andrew Ng Checking your derivative computation ! Gradient Checking deeplearning.ai Setting up your optimization problem Andrew Ng Gradient check for a neural network Take ! "" , $[""], … , ! ( , $ ( and reshape into a big vector ). Take +! "" , +$[""], … , +! ( , +$ ( and reshape into a big vector d). Andrew Ng Gradient checking (Grad check) Gradient Checking implementation notes deeplearning.ai Setting up your optimization problem Andrew Ng Gradient checking implementation notes - Don’t use in training – only to debug - If algorithm fails grad check, look at components to try to identify bug. - Remember regularization. - Doesn’t work with dropout. - Run at random initialization; perhaps again after some training. "
491,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Optimization Algorithms Mini-batch gradient descent deeplearning.ai Andrew Ng Batch vs. mini-batch gradient descent Vectorization allows you to efficiently compute on m examples. Andrew Ng Mini-batch gradient descent Optimization Algorithms Understanding mini-batch gradient descent deeplearning.ai Andrew Ng Training with mini batch gradient descent # iterations cost Batch gradient descent mini batch # (t) cost Mini-batch gradient descent Andrew Ng Choosing your mini-batch size Andrew Ng Choosing your mini-batch size Andrew Ng Andrew Ng Optimization Algorithms Understanding exponentially weighted averages deeplearning.ai Andrew Ng Exponentially weighted averages days temperature !"" = $!""%& + (1 −$),"" Andrew Ng Exponentially weighted averages !""## = 0.9!(( + 0.1+""## !(( = 0.9!(, + 0.1+(( !(, = 0.9!(- + 0.1+(, … !/ = 0!/1"" + (1 −0)+/ Andrew Ng Implementing exponentially weighted averages !"" = 0 !% = &!"" + (1 −&) -% … !/ = &!% + (1 −&) -/ !0 = &!/ + (1 −&) -0 Optimization Algorithms Bias correction in exponentially weighted average deeplearning.ai Andrew Ng Bias correction days temperature !"" = $!""%& + (1 −$),"" Optimization Algorithms Gradient descent with momentum deeplearning.ai Andrew Ng Gradient descent example Andrew Ng Implementation details !""# = %!""# + 1 −% )* !""+ = %!""+ + 1 −% ), * = * −-!""#, Hyperparameters: -, % On iteration 8: Compute )*, ), on the current mini-batch , = , −-!""+ % = 0.9 Optimization Algorithms RMSprop deeplearning.ai Andrew Ng RMSprop Optimization Algorithms Adam optimization algorithm deeplearning.ai Andrew Ng Adam optimization algorithm yhat = np.array([.9, 0.2, 0.1, .4, .9]) Andrew Ng Hyperparameters choice: Adam Coates Optimization Algorithms Learning rate decay deeplearning.ai Andrew Ng Learning rate decay Andrew Ng Learning rate decay Andrew Ng Other learning rate decay methods Optimization Algorithms The problem of local optima deeplearning.ai Andrew Ng Local optima in neural networks Andrew Ng Problem of plateaus • Unlikely to get stuck in a bad local optima • Plateaus can make learning slow "
492,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Hyperparameter tuning Tuning process deeplearning.ai Andrew Ng Hyperparameters Andrew Ng Try random values: Don’t use a grid Hyperparameter 1 Hyperparameter 2 Hyperparameter 1 Hyperparameter 2 Andrew Ng Coarse to fine Hyperparameter 1 Hyperparameter 2 Hyperparameter tuning Using an appropriate scale to pick hyperparameters deeplearning.ai Andrew Ng Picking hyperparameters at random Andrew Ng Appropriate scale for hyperparameters Andrew Ng Hyperparameters for exponentially weighted averages Hyperparameters tuning Hyperparameters tuning in practice: Pandas vs. Caviar deeplearning.ai Andrew Ng Re-test hyperparameters occasionally Idea Experiment Code - NLP, Vision, Speech, Ads, logistics, …. - Intuitions do get stale. Re-evaluate occasionally. Andrew Ng Panda Babysitting one model Training many models in parallel Caviar Batch Normalization Normalizing activations in a network deeplearning.ai Andrew Ng Normalizing inputs to speed up learning !"" !# !$ % & ', ) !"" !# !$ % & Andrew Ng Implementing Batch Norm Batch Normalization Fitting Batch Norm into a neural network deeplearning.ai Andrew Ng Adding Batch Norm to a network !"" !# !$ % & Andrew Ng Working with mini-batches Andrew Ng Implementing gradient descent Batch Normalization Why does Batch Norm work? deeplearning.ai Andrew Ng Learning on shifting input distribution !"" !# !$ % & Cat % = 1 Non-Cat % = 0 % = 1 % = 0 Andrew Ng Why this is a problem with neural networks? !"" !# !$ % & Andrew Ng • Each mini-batch is scaled by the mean/variance computed on just that mini-batch. • This adds some noise to the values +[-] within that minibatch. So similar to dropout, it adds some noise to each hidden layer’s activations. • This has a slight regularization effect. Batch Norm as regularization deeplearning.ai Softmax regression Multi-class classification Andrew Ng Recognizing cats, dogs, and baby chicks X ! "" 1 2 1 3 3 0 0 2 Andrew Ng Softmax layer X ! "" Andrew Ng Softmax examples #$ #% #$ #% #$ #% #$ #% #$ #% #$ #% Programming Frameworks Deep Learning frameworks deeplearning.ai Andrew Ng Deep learning frameworks • Caffe/Caffe2 • CNTK • DL4J • Keras • Lasagne • mxnet • PaddlePaddle • TensorFlow • Theano • Torch Choosing deep learning frameworks - Ease of programming (development and deployment) - Running speed - Truly open (open source with good governance) Programming Frameworks TensorFlow deeplearning.ai Andrew Ng Motivating problem Andrew Ng Code example import numpy as np import tensorflow as tf coefficients = np.array([[1], [-20], [25]]) w = tf.Variable([0],dtype=tf.float32) x = tf.placeholder(tf.float32, [3,1]) cost = x[0][0]*w**2 + x[1][0]*w + x[2][0] # (w-5)**2 train = tf.train.GradientDescentOptimizer(0.01).minimize(cost) init = tf.global_variables_initializer() session = tf.Session() session.run(init) print(session.run(w)) for i in range(1000): session.run(train, feed_dict={x:coefficients}) print(session.run(w)) with tf.Session() as session: session.run(init) print(session.run(w)) "
493,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode Introduction to ML strategy Why ML Strategy? deeplearning.ai Andrew Ng Motivating example Ideas: • Collect more data • Collect more diverse training set • Train algorithm longer with gradient descent • Try Adam instead of gradient descent • Try bigger network • Try smaller network • Try dropout • Add !"" regularization • Network architecture • Activation functions • # hidden units • … Introduction to ML strategy Orthogonalization deeplearning.ai Andrew Ng TV tuning example Car Andrew Ng Chain of assumptions in ML Fit training set well on cost function Fit dev set well on cost function Fit test set well on cost function Performs well in real world Setting up your goal Single number evaluation metric deeplearning.ai Andrew Ng Using a single number evaluation metric Idea Experiment Code Classifier Precision Recall F1 Score A 95% 90% 92.4% B 98% 85% 91.0% Andrew Ng Another example Algorithm US China India Other Average A 3% 7% 5% 9% 6% B 5% 6% 5% 10% 6.5% C 2% 3% 4% 5% 3.5% D 5% 8% 7% 2% 5.25% E 4% 5% 2% 4% 3.75% F 7% 11% 8% 12% 9.5% Setting up your goal Satisficing and optimizing metrics deeplearning.ai Andrew Ng Another cat classification example Classifier Accuracy Running time A 90% 80ms B 92% 95ms C 95% 1,500ms Train/dev/test distributions deeplearning.ai Setting up your goal Andrew Ng Cat classification dev/test sets Idea Experiment Code Regions: • US • UK • Other Europe • South America • India • China • Other Asia • Australia Andrew Ng True story (details changed) Optimizing on dev set on loan approvals for medium income zip codes Tested on low income zip codes Andrew Ng Guideline Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on. Size of dev and test sets deeplearning.ai Setting up your goal Andrew Ng Old way of splitting data Andrew Ng Size of dev set Set your dev set to be big enough to detect differences in algorithm/models you’re trying out. Andrew Ng Size of test set Set your test set to be big enough to give high confidence in the overall performance of your system. When to change dev/test sets and metrics deeplearning.ai Setting up your goal Andrew Ng Cat dataset examples Metric: classification error Algorithm A: 3% error Algorithm B: 5% error Andrew Ng 1. So far we’ve only discussed how to define a metric to evaluate classifiers. 2. Worry separately about how to do well on this metric. Orthogonalization for cat pictures: anti-porn Andrew Ng Another example Algorithm A: 3% error Algorithm B: 5% error If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set. Dev/test User images deeplearning.ai Comparing to human- level performance Why human-level performance? Andrew Ng Comparing to human-level performance time accuracy Andrew Ng Why compare to human-level performance Humans are quite good at a lot of tasks. So long as ML is worse than humans, you can: - Get labeled data from humans. - Gain insight from manual error analysis: Why did a person get this right? - Better analysis of bias/variance. deeplearning.ai Comparing to human- level performance Avoidable bias Andrew Ng Bias and Variance high bias “just right” high variance Andrew Ng Bias and Variance Cat classification Training set error: 1% 15% 15% 0.5% Dev set error: 11% 16% 30% 1% Andrew Ng Cat classification example Training error 8% Dev error 10% 8 % 10 % deeplearning.ai Understanding human-level performance Comparing to human- level performance Andrew Ng Human-level error as a proxy for Bayes error Medical image classification example: Suppose: (a) Typical human ………………. 3 % error (b) Typical doctor ………………... 1 % error (c) Experienced doctor …………... 0.7 % error (d) Team of experienced doctors .. 0.5 % error What is “human-level” error? Andrew Ng Error analysis example Training error Dev error Andrew Ng Summary of bias/variance with human-level performance Human-level error Dev error Training error deeplearning.ai Surpassing human- level performance Comparing to human- level performance Andrew Ng Surpassing human-level performance Team of humans One human Training error Dev error Andrew Ng Problems where ML significantly surpasses human-level performance - Online advertising - Product recommendations - Logistics (predicting transit time) - Loan approvals deeplearning.ai Improving your model performance Comparing to human- level performance Andrew Ng The two fundamental assumptions of supervised learning 1. You can fit the training set pretty well. 2. The training set performance generalizes pretty well to the dev/test set. Andrew Ng Reducing (avoidable) bias and variance Human-level Dev error Train bigger model Train longer/better optimization algorithms NN architecture/hyperparameters search More data Regularization NN architecture/hyperparameters search Training error "
494,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Error Analysis Carrying out error analysis Andrew Ng Look at dev examples to evaluate ideas Should you try to make your cat classifier do better on dogs? Error analysis: • Get ~100 mislabeled dev set examples. • Count up how many are dogs. Andrew Ng Evaluate multiple ideas in parallel Ideas for cat detection: • Fix pictures of dogs being recognized as cats • Fix great cats (lions, panthers, etc..) being misrecognized • Improve performance on blurry images Image 1 2 3 % of total . . . deeplearning.ai Error Analysis Cleaning up Incorrectly labeled data Andrew Ng Incorrectly labeled examples x y 1 0 1 1 0 1 1 DL algorithms are quite robust to random errors in the training set. Andrew Ng Error analysis Overall dev set error Errors due incorrect labels Errors due to other causes Goal of dev set is to help you select between two classifiers A & B. Image Dog Great Cat Blurry Incorrectly labeled Comments … 98  Labeler missed cat in background 99  100  Drawing of a cat; Not a real cat. % of total 8% 43% 61% 6% Andrew Ng Correcting incorrect dev/test set examples • Apply same process to your dev and test sets to make sure they continue to come from the same distribution • Consider examining examples your algorithm got right as well as ones it got wrong. • Train and dev/test data may now come from slightly different distributions. deeplearning.ai Error Analysis Build your first system quickly, then iterate Andrew Ng Speech recognition example • Noisy background • Café noise • Car noise • Accented speech • Far from microphone • Young children’s speech • Stuttering • … • Set up dev/test set and metric • Build initial system quickly • Use Bias/Variance analysis & Error analysis to prioritize next steps. Guideline: Build your first system quickly, then iterate deeplearning.ai Mismatched training and dev/test data Training and testing on different distributions Andrew Ng Cat app example Data from mobile app Data from webpages Andrew Ng Speech recognition example Purchased data Smart speaker control Voice keyboard Speech activated rearview mirror … Training Dev/test deeplearning.ai Bias and Variance with mismatched data distributions Mismatched training and dev/test data Andrew Ng Cat classifier example Assume humans get ≈ 0% error. Training-dev set: Same distribution as training set, but not used for training Training error Dev error Andrew Ng Bias/variance on mismatched training and dev/test sets Andrew Ng More general formulation deeplearning.ai Addressing data mismatch Mismatched training and dev/test data Andrew Ng Addressing data mismatch • Carry out manual error analysis to try to understand difference between training and dev/test sets • Make training data more similar; or collect more data similar to dev/test sets Andrew Ng Artificial data synthesis Car noise “The quick brown fox jumps over the lazy dog.” + = Synthesized in-car audio Andrew Ng Artificial data synthesis Car recognition: deeplearning.ai Learning from multiple tasks Transfer learning Andrew Ng Transfer learning x ! "" x ! "" Andrew Ng When transfer learning makes sense • You have a lot more data for Task A than Task B. • Task A and B have the same input x. • Low level features from A could be helpful for learning B. deeplearning.ai Learning from multiple tasks Multi-task learning Andrew Ng Simplified autonomous driving example Andrew Ng Neural network architecture x ! "" Andrew Ng When multi-task learning makes sense • Training on a set of tasks that could benefit from having shared lower-level features. • Usually: Amount of data you have for each task is quite similar. • Can train a big enough neural network to do well on all the tasks. deeplearning.ai What is end-to-end deep learning End-to-end deep learning Andrew Ng What is end-to-end learning? Speech recognition example Andrew Ng Face recognition [Image courtesy of Baidu] Andrew Ng More examples Machine translation Estimating child’s age: deeplearning.ai Whether to use end-to-end learning End-to-end deep learning Andrew Ng Pros and cons of end-to-end deep learning Pros: • Let the data speak • Less hand-designing of components needed Cons: • May need large amount of data • Excludes potentially useful hand-designed components Andrew Ng Applying end-to-end deep learning Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y? "
495,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Convolutional Neural Networks Computer vision Andrew Ng Computer Vision Problems Image Classification Cat? (0/1) Neural Style Transfer Object detection 64x64 Andrew Ng Deep Learning on large images Cat? (0/1) !"" !# !$ % & ⋮ ⋮ ⋮ 64x64 deeplearning.ai Convolutional Neural Networks Edge detection example Andrew Ng Computer Vision Problem vertical edges horizontal edges Andrew Ng Vertical edge detection 3 0 1 2 7 4 1 5 8 9 3 1 2 7 2 5 1 3 0 1 3 1 7 8 4 2 1 6 2 8 2 4 5 2 3 9 ∗ = 0 -2 -4 -7 -3 -2 -3 -16 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 1 1 1 -1 -1 -1 0 0 0 Andrew Ng Vertical edge detection 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 1 0 -1 1 0 -1 1 0 -1 = 0 30 30 0 0 30 30 0 0 30 30 0 0 30 30 0 ∗ ∗ deeplearning.ai Convolutional Neural Networks More edge detection Andrew Ng Vertical edge detection examples 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 1 0 -1 1 0 -1 1 0 -1 = 0 30 30 0 0 30 30 0 0 30 30 0 0 30 30 0 ∗ ∗ 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 = 0 -30 -30 0 0 -30 -30 0 0 -30 -30 0 0 -30 -30 0 1 0 -1 1 0 -1 1 0 -1 Andrew Ng Vertical and Horizontal Edge Detection 1 0 -1 1 0 -1 1 0 -1 = ∗ 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 1 1 1 0 0 0 -1 -1 -1 0 0 0 0 30 10 -10 -30 30 10 -10 -30 0 0 0 0 Vertical 1 1 1 0 0 0 -1 -1 -1 Horizontal Andrew Ng Learning to detect edges #$ #% #& #' #( #) #* #+ #, 3 0 1 2 7 4 1 5 8 9 3 1 2 7 2 5 1 3 0 1 3 1 7 8 4 2 1 6 2 8 2 4 5 2 3 9 1 0 -1 1 0 -1 1 0 -1 deeplearning.ai Convolutional Neural Networks Padding Andrew Ng Padding ∗ = Andrew Ng Valid and Same convolutions “Valid”: “Same”: Pad so that output size is the same as the input size. deeplearning.ai Convolutional Neural Networks Strided convolutions Andrew Ng Strided convolution 3 4 4 1 0 2 -1 0 3 ∗ = 2 3 7 4 6 2 9 6 6 9 8 7 4 3 3 4 8 3 8 9 7 7 8 3 6 6 3 4 4 2 1 8 3 4 6 3 2 4 1 9 8 3 0 1 3 9 2 1 4 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 3 1 -1 4 0 0 4 2 3 Andrew Ng Summary of convolutions #× # filter & × & image padding p stride s '()* +, - + 1 × '()* +, - + 1 Andrew Ng Technical note on cross-correlation vs. convolution Convolution in math textbook: 3 4 5 1 0 2 -1 9 7 2 3 7 4 6 2 6 6 9 8 7 4 3 4 8 3 8 9 7 8 3 6 6 3 4 2 1 8 3 4 3 2 4 1 9 8 ∗ deeplearning.ai Convolutional Neural Networks Convolutions over volumes Andrew Ng Convolutions on RGB images Andrew Ng Convolutions on RGB image = ∗ 4 x 4 Andrew Ng Multiple filters = 6 x 6 x 3 4 x 4 3 x 3 x 3 ∗ ∗ 3 x 3 x 3 = 4 x 4 deeplearning.ai Convolutional Neural Networks One layer of a convolutional network Andrew Ng Example of a layer 6 x 6 x 3 ∗ 3 x 3 x 3 ∗ 3 x 3 x 3 Andrew Ng Number of parameters in one layer If you have 10 filters that are 3 x 3 x 3 in one layer of a neural network, how many parameters does that layer have? Andrew Ng Summary of notation If layer l is a convolution layer: "" # = filter size $ # = padding % # = stride &' # = number of filters Each filter is: Activations: Weights: bias: Input: Output: deeplearning.ai Convolutional Neural Networks A simple convolution network example Andrew Ng Example ConvNet Andrew Ng Types of layer in a convolutional network: - Convolution - Pooling - Fully connected deeplearning.ai Convolutional Neural Networks Pooling layers Andrew Ng Pooling layer: Max pooling 1 3 2 1 2 9 1 1 1 3 2 3 5 6 1 2 Andrew Ng Pooling layer: Max pooling 1 3 2 1 3 2 9 1 1 5 1 3 2 3 2 8 3 5 1 0 5 6 1 2 9 Andrew Ng Pooling layer: Average pooling 1 3 2 1 2 9 1 1 1 4 2 3 5 6 1 2 Andrew Ng Summary of pooling Hyperparameters: f : filter size s : stride Max or average pooling deeplearning.ai Convolutional Neural Networks Convolutional neural network example Andrew Ng Neural network example 608 3216 48120 10164 850 deeplearning.ai Convolutional Neural Networks Why convolutions? Andrew Ng Why convolutions Andrew Ng Why convolutions 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 10 10 10 0 0 0 1 0 -1 1 0 -1 1 0 -1 0 30 30 0 0 30 30 0 0 30 30 0 0 30 30 0 ∗ = Parameter sharing: A feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image. Sparsity of connections: In each layer, each output value depends only on a small number of inputs. Andrew Ng Putting it together Training set (% & , ( & ) … (% + , ( + ). Use gradient descent to optimize parameters to reduce , - + ./& Cost , = & + ℒ(( 1 . , ( . ) ( 1 "
496,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Case Studies Why look at case studies? Andrew Ng Outline Classic networks: • LeNet-5 ResNet Inception • AlexNet • VGG deeplearning.ai Case Studies Classic networks Andrew Ng LeNet - 5 ⋮ ⋮ "" # 32×32 ×1 28×28×6 14×14×6 10×10×16 5×5×16 120 84 5 × 5 s = 1 f = 2 s = 2 avg pool 5 × 5 s = 1 avg pool f = 2 s = 2 [LeCun et al., 1998. Gradient-based learning applied to document recognition] Andrew Ng AlexNet = ⋮ ⋮ 227×227 ×3 55×55 ×96 27×27 ×96 27×27 ×256 13×13 ×256 13×13 ×384 13×13 ×384 13×13 ×256 6×6 ×256 9216 4096 ⋮ 4096 11 × 11 s = 4 3 × 3 s = 2 MAX-POOL 5 × 5 same 3 × 3 s = 2 MAX-POOL 3 × 3 same 3 × 3 3 × 3 3 × 3 s = 2 MAX-POOL Softmax 1000 [Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks] Andrew Ng VGG - 16 224×224 ×3 CONV = 3×3 filter, s = 1, same MAX-POOL = 2×2 , s = 2 [CONV 64] ×2 224×224×64 POOL 112×112 ×64 [CONV 128] ×2 112×112 ×128 POOL 56×56 ×128 [CONV 256] ×3 56×56 ×256 POOL 28×28 ×256 [CONV 512] ×3 28×28 ×512 POOL 14×14×512 [CONV 512] ×3 14×14 ×512 POOL 7×7×512 FC 4096 FC 4096 Softmax 1000 [Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition] deeplearning.ai Case Studies Residual Networks (ResNets) Andrew Ng Residual block ![#] ![#%&] '[#%(] = *[#%(] ![#] + ,[#%(] ![#%(] = -('[#%(]) '[#%&] = *[#%&]![#%(] + ,[#%&] ![#%&] = -('[#%&]) ![#%(] [He et al., 2015. Deep residual networks for image recognition] Andrew Ng Residual Network x ![#] # layers training error Plain # layers training error ResNet [He et al., 2015. Deep residual networks for image recognition] deeplearning.ai Case Studies Why ResNets work Andrew Ng Why do residual networks work? Andrew Ng ResNet Plain ResNet [He et al., 2015. Deep residual networks for image recognition] deeplearning.ai Case Studies Network in Network and 1×1 convolutions Andrew Ng Why does a 1 × 1 convolution do? 1 2 3 6 5 8 3 5 5 1 3 4 2 1 3 4 9 3 4 7 8 5 7 9 1 5 3 7 4 8 5 4 9 8 3 5 2 ∗ = ∗ = 6 × 6 6 × 6 × 32 1 × 1 × 32 6 × 6 × # filters [Lin et al., 2013. Network in network] Andrew Ng Using 1×1 convolutions 28 × 28 × 192 28 × 28 × 32 ReLU CONV 1 × 1 32 [Lin et al., 2013. Network in network] deeplearning.ai Case Studies Inception network motivation Andrew Ng Motivation for inception network 28 × 28 × 192 1 × 1 3 × 3 5 × 5 MAX-POOL 128 32 32 64 28 28 [Szegedy et al. 2014. Going deeper with convolutions] Andrew Ng The problem of computational cost 28 × 28 × 192 CONV 5 × 5, same, 32 28 × 28 × 32 Andrew Ng Using 1×1 convolution 28 × 28 × 192 CONV 1 × 1, 16, 1 × 1 × 192 28 × 28 × 16 CONV 5 × 5, 32, 5 × 5 × 16 28 × 28 × 32 deeplearning.ai Case Studies Inception network Andrew Ng Inception module Previous Activation 1 × 1 CONV 1 × 1 CONV 3 × 3 CONV 1 × 1 CONV 5 × 5 CONV MAXPOOL 3 × 3,s = 1 same 1 × 1 CONV Channel Concat Andrew Ng Inception network [Szegedy et al., 2014, Going Deeper with Convolutions] Andrew Ng http://knowyourmeme.com/memes/we-need-to-go-deeper MobileNet Convolutional Neural Networks Andrew Ng Motivation for MobileNets • Low computational cost at deployment • Useful for mobile and embedded vision applications • Key idea: Normal vs. depthwise- separable convolutions [Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications] Andrew Ng Normal Convolution Computational cost = #filter params x # filter positions x # of filters 6 x 6 x 3 * 3 x 3 x 3 = 4 x 4 4 x 4 x 5 Andrew Ng Depthwise Separable Convolution * = Normal Convolution * = * Depthwise Separable Convolution Depthwise Pointwise Andrew Ng Computational cost = #filter params x # filter positions x # of filters 6 x 6 x 3 * 3 x 3 = 4 x 4 x 3 Depthwise Convolution Andrew Ng Depthwise Separable Convolution * = Depthwise Convolution Pointwise Convolution * = Andrew Ng Pointwise Convolution Computational cost = #filter params x # filter positions x # of filters * = 4 x 4 x 3 1 x 1 x 3 4 x 4 x 5 4 x 4 Andrew Ng Depthwise Separable Convolution * = Normal Convolution * = * Depthwise Separable Convolution Depthwise Pointwise Andrew Ng Cost Summary [Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications] Cost of depthwise separable convolution Cost of normal convolution Andrew Ng Depthwise Separable Convolution Depthwise Convolution * = Pointwise Convolution * = MobileNet Architecture Convolutional Neural Networks MobileNet Andrew Ng [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] MobileNet v1 MobileNet v2 Depthwise Projection Expansion Residual Connection Andrew Ng MobileNet v2 Bottleneck [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] Residual Connection Expansion Depthwise Pointwise MobileNet Andrew Ng [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] MobileNet v1 MobileNet v2 Depthwise Projection Expansion Residual Connection Andrew Ng MobileNet v2 Full Architecture [Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks] conv2d conv2d 1 x 1 avgpool 7 x 7 conv2d 1 x 1 EfficientNet Convolutional Neural Networks Andrew Ng [Tan and Le, 2019, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks] Baseline Higher Resolution Deeper Wider EfficientNet ො 𝑦 ො 𝑦 ො 𝑦 ො 𝑦 Compound scaling ො 𝑦 deeplearning.ai Practical advice for using ConvNets Transfer Learning deeplearning.ai Practical advice for using ConvNets Data augmentation Andrew Ng Common augmentation method Mirroring Random Cropping Rotation Shearing Local warping … Andrew Ng Color shifting +20,-20,+20 -20,+20,+20 +5,0,+50 Andrew Ng Implementing distortions during training deeplearning.ai Practical advice for using ConvNets The state of computer vision Andrew Ng Data vs. hand-engineering Two sources of knowledge • Labeled data • Hand engineered features/network architecture/other components Andrew Ng Tips for doing well on benchmarks/winning competitions Ensembling • Train several networks independently and average their outputs Multi-crop at test time • Run classifier on multiple versions of test images and average results Andrew Ng Use open source code • Use architectures of networks published in the literature • Use pretrained models and fine-tune on your dataset • Use open source implementations if possible "
497,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Object Detection Object localization Andrew Ng What are localization and detection? Image classification Classification with localization Detection Andrew Ng Classification with localization ⋯ ⋮ 1 - pedestrian 2 - car 3 - motorcycle 4 - background Andrew Ng Defining the target label y 1 - pedestrian 2 - car 3 - motorcycle 4 - background Need to output #$, #&, #', #(, class label (1-4) deeplearning.ai Object Detection Landmark detection Andrew Ng Landmark detection !"", !$, !%, !& ConvNet deeplearning.ai Object Detection Object detection Andrew Ng Car detection example Training set: x y 1 1 0 0 1 Andrew Ng Sliding windows detection deeplearning.ai Object Detection Convolutional implementation of sliding windows Andrew Ng Turning FC layer into convolutional layers 10 × 10 × 16 5 × 5 × 16 14 × 14 × 3 5 × 5 2 × 2 MAX POOL FC FC y softmax (4) 14 × 14 × 3 10 × 10 × 16 5 × 5 × 16 5 × 5 2 × 2 MAX POOL 1 × 1× 400 1 × 1 × 400 1 × 1 × 4 FC 5 × 5 FC 1 × 1 400 ⋮ 400 ⋮ Andrew Ng Convolution implementation of sliding windows 5×5×16 1×1×400 1×1×400 1×1×4 16×16×3 2×2×400 14×14 ×3 10×10×16 12×12×16 6×6×16 2×2×400 2×2×4 28×28×3 24×24×16 12×12×16 8×8×400 8×8×400 8×8×4 5×5 2×2 MAX POOL FC FC FC 5×5 2×2 MAX POOL 5×5 FC FC 5×5 1×1 1×1 5×5 1×1 1×1 FC 2×2 MAX POOL 5×5 1×1 [Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks] 1×1 Andrew Ng Convolution implementation of sliding windows 28×28 16×16 12×12 8×8×400 8×8×400 8×8×4 5×5 2×2 MAX POOL 5×5 1×1 1×1 deeplearning.ai Object Detection Bounding box predictions Andrew Ng Output accurate bounding boxes Andrew Ng YOLO algorithm Labels for training For each grid cell: 100 100 [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] Andrew Ng Specify the bounding boxes 100 100 [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] deeplearning.ai Object Detection Intersection over union Andrew Ng Evaluating object localization “Correct” if IoU ≥0.5 More generally, IoU is a measure of the overlap between two bounding boxes. deeplearning.ai Object Detection Non-max suppression Andrew Ng Non-max suppression example Andrew Ng Non-max suppression example 0.8 0.5 0.6 0.9 0.3 Andrew Ng Non-max suppression example 0.8 0.7 0.6 0.9 0.7 Andrew Ng Non-max suppression algorithm 19×19 $% &' &( &) &* Discard all boxes with $% ≤0.6 While there are any remaining boxes: • Pick the box with the largest $% Output that as a prediction. • Discard any remaining box with IoU ≥0.5 with the box output in the previous step Each output prediction is: deeplearning.ai Object Detection Anchor boxes Andrew Ng Overlapping objects: Anchor box 2: Anchor box 1: !"" #$ #% #& #' () (* (+ y = [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] Andrew Ng Anchor box algorithm Previously: Each object in training image is assigned to grid cell that contains that object’s midpoint. With two anchor boxes: Each object in training image is assigned to grid cell that contains object’s midpoint and anchor box for the grid cell with highest IoU. Andrew Ng Anchor box example Anchor box 2: Anchor box 1: y = !"" #$ #% #& #' () (* (+ !"" #$ #% #& #' () (* (+ deeplearning.ai Object Detection Putting it together: YOLO algorithm Andrew Ng Training y is 3×3×2×8 y 0 ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? = '( )* )+ )- ). /0 /1 /2 '( )* )+ )- ). /0 /1 /2 [Redmon et al., 2015, You Only Look Once: Unified real-time object detection] 0 ? ? ? ? ? ? ? 1 )* )+ )- ). 0 1 0 1 - pedestrian 2 - car 3 - motorcycle Andrew Ng Making predictions 4 = '( )* )+ )- ). /0 /1 /2 '( )* )+ )- ). /0 /1 /2 ⋯ 3×3×2×8 Andrew Ng Outputting the non-max supressed outputs • For each grid call, get 2 predicted bounding boxes. • Get rid of low probability predictions. • For each class (pedestrian, car, motorcycle) use non-max suppression to generate final predictions. deeplearning.ai Object Detection Region proposals (Optional) Andrew Ng Region proposal: R-CNN [Girshik et. al, 2013, Rich feature hierarchies for accurate object detection and semantic segmentation] Andrew Ng Faster algorithms R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box. [Girshik et. al, 2013. Rich feature hierarchies for accurate object detection and semantic segmentation] Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions. [Girshik, 2015. Fast R-CNN] Faster R-CNN: Use convolutional network to propose regions. [Ren et. al, 2016. Faster R-CNN: Towards real-time object detection with region proposal networks] Semantic segmentation with U-Net Convolutional Neural Networks Andrew Ng Object Detection vs. Semantic Segmentation Object Detection Semantic Segmentation Input image Andrew Ng Motivation for U-Net Chest X-Ray Brain MRI [Dong et al., 2017, Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks ] [Novikov et al., 2017, Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs] Per-pixel class labels Andrew Ng 1. Car 0. Not Car 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Per-pixel class labels Andrew Ng 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1. Car 2. Building 3. Road 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Segmentation Map Deep Learning for Semantic Segmentation Andrew Ng ො 𝑦 * = Normal Convolution Transpose Convolution * = Transpose Convolution Andrew Ng Transpose Convolution weight filter filter f x f = 3 x 3 2 x 2 4 x 4 stride s = 2 padding p = 1 2 1 3 2 0 2 4 2 1 2 2 0 1 1 0 2 1 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 +2 +0 0 1 2 1 3 3 3 3 3 3 3 3 3 +6 +3 0 3 6 3 2 2 2 2 2 2 2 2 2 +2 +4 +2 +4 0 2 4 2 +0 Andrew Ng 0 4 0 1 10 7 0 6 3 6 3 7 0 2 4 2 ො 𝑦 Deep Learning for Semantic Segmentation Andrew Ng Skip connection U-Net Andrew Ng [Ronneberger et al., 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation] Conv, RELU Max Pool Trans Conv Skip Connection Conv (1x1) U-Net Andrew Ng [Ronneberger et al., 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation] Conv, RELU Max Pool Trans Conv Skip Connection Conv (1x1) h x w x 3 h x w x # classes "
498,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Face recognition What is face recognition? Andrew Ng Face recognition [Courtesy of Baidu] Andrew Ng Face verification vs. face recognition Verification • Input image, name/ID • Output whether the input image is that of the claimed person Recognition • Has a database of K persons • Get an input image • Output ID if the image is any of the K persons (or “not recognized”) deeplearning.ai Face recognition One-shot learning Andrew Ng One-shot learning Learning from one example to recognize the person again Andrew Ng Learning a “similarity” function d(img1,img2) = degree of difference between images If d(img1,img2) ≤ - > - deeplearning.ai Face recognition Siamese network Andrew Ng Siamese network [Taigman et. al., 2014. DeepFace closing the gap to human level performance] ⋮ ⋮ ""($) ⋮ ⋮ ""(&) Andrew Ng Goal of learning ⋮ f(""($)) ⋮ Parameters of NN define an encoding (("" ) ) Learn parameters so that: If "" ) , "" + are the same person, f "" ) −f "" + & is small. If "" ) , "" + are different persons, f "" ) −f "" + &is large. deeplearning.ai Face recognition Triplet loss Andrew Ng Learning Objective [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Anchor Positive Anchor Negative Andrew Ng Loss function Training set: 10k pictures of 1k persons [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Andrew Ng Choosing the triplets A,P,N During training, if A,P,N are chosen randomly, ! "", $ + & ≤!("", )) is easily satisfied. Choose triplets that’re “hard” to train on. [Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering] Andrew Ng Training set using triplet loss Anchor Positive Negative ⋮ ⋮ ⋮ deeplearning.ai Face recognition Face verification and binary classification Andrew Ng Learning the similarity function ⋮ f($(%)) ⋮ f($(')) $(%) $(') ( ) [Taigman et. al., 2014. DeepFace closing the gap to human level performance] Andrew Ng Face verification supervised learning $ ( 1 0 0 1 [Taigman et. al., 2014. DeepFace closing the gap to human level performance] deeplearning.ai What is neural style transfer? Neural Style Transfer Andrew Ng Neural style transfer [Images generated by Justin Johnson] Content Style Style Content Generated image Generated image deeplearning.ai What are deep ConvNets learning? Neural Style Transfer Andrew Ng Visualizing what a deep network is learning 224×224×3 ⋮ ⋮ & ' 110×110×96 55×55×96 26×26×256 13×13×256 13×13×384 13×13×384 6×6×256 FC 4096 FC 4096 Pick a unit in layer 1. Find the nine image patches that maximize the unit’s activation. Repeat for other units. [Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks] Andrew Ng Visualizing deep layers Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 1 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 2 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 3 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 3 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 4 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 Andrew Ng Visualizing deep layers: Layer 5 Layer 2 Layer 4 Layer 3 Layer 5 Layer 1 deeplearning.ai Cost function Neural Style Transfer Andrew Ng Neural style transfer cost function Content C Style S Generated image G [Gatys et al., 2015. A neural algorithm of artistic style. Images on slide generated by Justin Johnson] Andrew Ng Find the generated image G 1. Initiate G randomly G: 100×100×3 2. Use gradient descent to minimize %(') [Gatys et al., 2015. A neural algorithm of artistic style] deeplearning.ai Content cost function Neural Style Transfer Andrew Ng Content cost function • Say you use hidden layer ! to compute content cost. "" # = % ""'()*+)* ,, # + / ""0*12+ (4, #) • Use pre-trained ConvNet. (E.g., VGG network) • Let 6[2](9) and 6[2](:) be the activation of layer ! on the images • If 6[2](9) and 6[2](:) are similar, both images have similar content [Gatys et al., 2015. A neural algorithm of artistic style] deeplearning.ai Style cost function Neural Style Transfer Andrew Ng Meaning of the “style” of an image ⋮ "" # 255 134 93 22 123 94 83 2 34 44 187 30 34 76 232 124 67 83 194 142 255 134 202 22 123 94 83 4 34 44 187 192 34 76 232 34 67 83 194 94 255 231 42 22 123 94 83 2 34 44 187 92 34 76 232 124 67 83 194 202 Say you are using layer $’s activation to measure “style.” Define style as correlation between activations across channels. How correlated are the activations across different channels? %& %' %( [Gatys et al., 2015. A neural algorithm of artistic style] Andrew Ng Intuition about style of an image Style image Generated Image %& %' %( %& %' %( [Gatys et al., 2015. A neural algorithm of artistic style] Andrew Ng Style matrix Let a*,,,- [/] = activation at 2, 3, 4 . 7[/] is n9 [/]×n9 [/] [Gatys et al., 2015. A neural algorithm of artistic style] Andrew Ng Style cost function ;<=>/? / (A, 7) = 1 2%' [/]%& [/]%( [/] E F F(7--G / H −7--G / J ) K -G K - [Gatys et al., 2015. A neural algorithm of artistic style] deeplearning.ai Convolutional Networks in 1D or 3D 1D and 3D generalizations of models Andrew Ng Convolutions in 2D and 1D 14×14 2D input image ∗ 2D filter 5×5 ∗ 1 3 10 3 1 1 20 15 3 18 12 4 17 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng 3D data Andrew Ng Andrew Ng 3D convolution ∗ 3D volume 3D filter "
499,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Recurrent Neural Networks Why sequence models? Andrew Ng Examples of sequence data Music generation ∅ Speech recognition “The quick brown fox jumped over the lazy dog.” Sentiment classification “There is nothing to like in this movie.” DNA sequence analysis AGCCCCTGTGAGGAACTAG AGCCCCTGTGAGGAACTAG Machine translation Voulez-vous chanter avec moi? Do you want to sing with me? Video activity recognition Running Name entity recognition Yesterday, Harry Potter met Hermione Granger. Yesterday, Harry Potter met Hermione Granger. deeplearning.ai Recurrent Neural Networks Notation Andrew Ng Motivating example x: Harry Potter and Hermione Granger invented a new spell. Andrew Ng Representing words x: Harry Potter and Hermione Granger invented a new spell. !""#$ !""%$ !""&$ ⋯ !""($ Andrew Ng Representing words x: Harry Potter and Hermione Granger invented a new spell. !""#$ !""%$ !""&$ ⋯ !""($ And = 367 Invented = 4700 A = 1 New = 5976 Spell = 8376 Harry = 4075 Potter = 6830 Hermione = 4200 Gran… = 4000 deeplearning.ai Recurrent Neural Networks Recurrent Neural Network Model Andrew Ng Why not a standard network? !""#$ !""%$ ⋮ !""' ($ ⋮ ⋮ )""#$ )""%$ ⋮ )""' *$ Problems: - Inputs, outputs can be different lengths in different examples. - Doesn’t share features learned across different positions of text. Andrew Ng Recurrent Neural Networks He said, “Teddy Roosevelt was a great President.” He said, “Teddy bears are on sale!” Andrew Ng Forward Propagation +"",$ !""#$ ) -""#$ +""#$ !""%$ ) -""%$ +""%$ !"".$ ) -"".$ +""' (/#$ !""' ($ ) -""' *$ ⋯ Andrew Ng Simplified RNN notation +""1$ = 3(5 66+""1/#$ + 5 68!""1$ + 96) ) -""1$ = 3(5 ;6+""1$ + 9;) deeplearning.ai Recurrent Neural Networks Backpropagation through time Andrew Ng Forward propagation and backpropagation !""#$ %""&$ ' (""&$ !""&$ %"")$ ' ("")$ !"")$ %""*$ ' (""*$ !""+ ,-&$ %""+ ,$ ' (""+ .$ ⋯ Andrew Ng Forward propagation and backpropagation ℒ""1$ ' (""1$, '""1$ = Backpropagation through time deeplearning.ai Recurrent Neural Networks Different types of RNNs Andrew Ng Examples of sequence data Music generation ∅ Speech recognition “The quick brown fox jumped over the lazy dog.” Sentiment classification “There is nothing to like in this movie.” DNA sequence analysis AGCCCCTGTGAGGAACTAG AGCCCCTGTGAGGAACTAG Machine translation Voulez-vous chanter avec moi? Do you want to sing with me? Video activity recognition Running Name entity recognition Yesterday, Harry Potter met Hermione Granger. Yesterday, Harry Potter met Hermione Granger. Andrew Ng Examples of RNN architectures Andrew Ng Examples of RNN architectures Andrew Ng Summary of RNN types ""#$% &#'% ( )#'% One to one One to many ""#$% & ( )#'% ( )#*% ( )#+ ,% ⋯ &#*% &#+ .% ""#$% &#'% ( ) ⋯ Many to one ""#$% &#'% ( )#+ ,% ⋯ &#*% &#+ .% ( )#'% ( )#*% Many to many Many to many ""#$% &#'% ( )#'% ⋯ &#+ .% ( )#+ ,% ⋯ ⋯ deeplearning.ai Recurrent Neural Networks Language model and sequence generation Andrew Ng What is language modelling? Speech recognition The apple and pair salad. The apple and pear salad. !(The apple and pair salad) = !(The apple and pear salad) = Andrew Ng Language modelling with an RNN Training set: large corpus of english text. Cats average 15 hours of sleep a day. The Egyptian Mau is a bread of cat. <EOS> Andrew Ng RNN model Cats average 15 hours of sleep a day. <EOS> ℒ& '()*, &()* = −- / 0 &0 ()* log & '0 ()* ℒ = - / ) ℒ()* & '()*, &()* deeplearning.ai Recurrent Neural Networks Sampling novel sequences Andrew Ng Sampling a sequence from a trained RNN !""#$ %""&$ ' ("") *$ ⋯ '""&$ '"") -.&$ ' (""&$ ' (""/$ '""/$ ' (""0$ !""&$ !""/$ !""0$ !"") *$ Andrew Ng Character-level language model Vocabulary = [a, aaron, …, zulu, <UNK>] !""#$ %""&$ ' ("") *$ ⋯ ' (""&$ ' (""/$ ' (""0$ !""&$ !""/$ !""0$ !"") *$ ' (""&$ ' (""/$ ' ("") -.&$ Andrew Ng Sequence generation President enrique peña nieto, announced sench’s sulk former coming football langston paring. “I was not at all surprised,” said hich langston. “Concussion epidemic”, to be examined. The gray football the told some and this has on the uefa icon, should money as. News Shakespeare The mortal moon hath her eclipse in love. And subject of this thou art another this fold. When besser be my love to me see sabl’s. For whose are ruse of mine eyes heaves. deeplearning.ai Recurrent Neural Networks Vanishing gradients with RNNs Andrew Ng Vanishing gradients with RNNs !""#$ %""&$ ' ("") *$ ⋯ %""-$ %"") .$ ' (""&$ ' (""-$ %""/$ ' (""/$ !""&$ !""-$ !""/$ !"") *$ Exploding gradients. % ' ( ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋯ deeplearning.ai Recurrent Neural Networks Gated Recurrent Unit (GRU) Andrew Ng RNN unit !""#$ = &(( ) !""#*+$, -""#$ + /)) Andrew Ng GRU (simplified) The cat, which already ate …, was full. [Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches] [Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling] Andrew Ng Full GRU Γ 2 = 3(( 2 5""#*+$, -""#$ + /2) 5""#$ = Γ 2∗5̃""#$ + 1 −Γ 2 + 5""#*+$ The cat, which ate already, was full. 5̃""#$ = tanh(( >[ 5""#*+$, -""#$] + />) deeplearning.ai Recurrent Neural Networks LSTM (long short term memory) unit Andrew Ng GRU and LSTM !̃#$% = tanh(, - Γ/ ∗!#$12%, 4#$% + 6-) Γ8 = 9(, 8 !#$12%, 4#$% + 68) !#$% = Γ8∗!̃#$% + 1 −Γ8 ∗!#$12% Γ/ = 9(, / !#$12%, 4#$% + 6/) GRU LSTM =#$% = !#$% [Hochreiter & Schmidhuber 1997. Long short-term memory] Andrew Ng LSTM units !̃#$% = tanh(, - Γ/ ∗!#$12%, 4#$% + 6-) Γ8 = 9(, 8 !#$12%, 4#$% + 68) !#$% = Γ8∗!̃#$% + 1 −Γ8 ∗!#$12% Γ/ = 9(, / !#$12%, 4#$% + 6/) GRU LSTM !#$% = Γ8 ∗!̃#$% + Γ > ∗!#$12% !̃#$% = tanh(, - =#$12%, 4#$% + 6-) Γ8 = 9(, 8 =#$12%, 4#$% + 68) Γ > = 9(, > =#$12%, 4#$% + 6>) Γ? = 9(, ? =#$12%, 4#$% + 6?) =#$% = Γ? ∗!#$% =#$% = !#$% [Hochreiter & Schmidhuber 1997. Long short-term memory] Andrew Ng !#$% = Γ8 ∗!̃#$% + Γ > ∗!#$12% !̃#$% = tanh(, - =#$12%, 4#$% + 6-) Γ8 = 9(, 8 =#$12%, 4#$% + 68) Γ > = 9(, > =#$12%, 4#$% + 6>) Γ? = 9(, ? =#$12%, 4#$% + 6?) =#$% = Γ? ∗!#$% LSTM in pictures !#$12% =#$12% !#$% 4#$% forget gate update gate tanh output gate ⨁ !#$% =#$% =#$% =#$% tanh softmax !̃#$% A#$% B#$% C#$% D#$% - - - - * * * !#E% =#E% !#2% 4#2% ⨁ =#2% =#2% softmax D#2% - - - - * !#2% =#2% 4#F% ⨁ =#F% =#F% softmax D#F% !#F% - - - - * !#F% =#F% 4#G% ⨁ =#G% =#G% softmax D#G% !#G% - - - - * deeplearning.ai Recurrent Neural Networks Bidirectional RNN Andrew Ng Getting information from the future He said, “Teddy bears are on sale!” He said, “Teddy Roosevelt was a great President!” He said, “Teddy bears are on sale!” ! ""#$% '#(% '#$% ! ""#)% ! ""#(% '#*% ! ""#*% +#,% '#)% +#)% +#(% +#*% +#$% '#-% ! ""#.% ! ""#-% '#/% ! ""#/% +#-% +#/% +#.% '#.% Andrew Ng Bidirectional RNN (BRNN) deeplearning.ai Recurrent Neural Networks Deep RNNs Andrew Ng Deep RNN example !""#$ !""%$ !""&$ !""'$ ([#]""+$ ([%]""&$ ([%]""'$ ([%]""#$ ([%]""%$ ([%]""+$ ,""#$ ,""%$ ,""&$ ,""'$ ([&]""&$ ([&]""'$ ([&]""#$ ([&]""%$ ([&]""+$ "
5,"School of Computer Science Probabilistic Graphical Models The Belief Propagation (Sum-Product) Algorithm Eric Xing Lecture 5, January 29, 2014 Reading: KF-chap 10 1 © Eric Xing @ CMU, 2005-2014 X1 X4 X3 X2 m32(x2) m42(x2) m21(x1) X1 X4 X3 X2 m32(x2) m42(x2) m21(x1) Pros and Cons of Procedure Elimination Algebraic elimination graphical elimination B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A 2 © Eric Xing @ CMU, 2005-2014 Complexity The overall complexity is determined by the number of the largest elimination clique  What is the largest elimination clique? – a pure graph theoretic question  Tree-width k: one less than the smallest achievable value of the cardinality of the largest elimination clique, ranging over all possible elimination ordering  “good” elimination orderings lead to small cliques and hence reduce complexity (what will happen if we eliminate ""e"" first in the above graph?)  Find the best elimination ordering of a graph --- NP-hard Inference is NP-hard  But there often exist ""obvious"" optimal or near-opt elimination ordering 3 © Eric Xing @ CMU, 2005-2014  Our algorithm so far answers only one query (e.g., on one node), do we need to do a complete elimination for every such query?  Elimination message passing on a clique tree  Messages can be reused E F H A E F B A C E G A D C E A D C B A A h m g m e m f m b m c m d m B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A  From Elimination to Message Passing   e f g e e a m e m d c e p d c a m ) , ( ) ( ) , | ( ) , , ( 4 © Eric Xing @ CMU, 2005-2014 E F H A E F B A C E G A D C E A D C B A A c m b m g m e m d m f m h m From Elimination to Message Passing  Our algorithm so far answers only one query (e.g., on one node), do we need to do a complete elimination for every such query?  Elimination message passing on a clique tree  Another query ...  Messages mf and mh are reused, others need to be recomputed 5 © Eric Xing @ CMU, 2005-2014 Undirected tree: a unique path between any pair of nodes Directed tree: all nodes except the root have exactly one parent Poly tree: can have multiple parents We will come back to this later Tree GMs 6 © Eric Xing @ CMU, 2005-2014  Any undirected tree can be converted to a directed tree by choosing a root node and directing all edges away from it  A directed tree and the corresponding undirected tree make the same conditional independence assertions  Parameterizations are essentially the same.  Undirected tree:  Directed tree:  Equivalence:  Evidence:? Equivalence of directed and undirected trees 7 © Eric Xing @ CMU, 2005-2014 From elimination to message passing  Recall ELIMINATION algorithm:  Choose an ordering Z in which query node f is the final node  Place all potentials on an active list  Eliminate node i by removing all potentials containing i, take sum/product over xi.  Place the resultant factor back on the list 8 © Eric Xing @ CMU, 2005-2014 f i j k l Elimination on a tree Let mji(xi) denote the factor resulting from eliminating variables from bellow up to i, which is a function of xi: This is reminiscent of a message sent from j to i. mij(xi) represents a ""belief"" of xi from xj! 9 © Eric Xing @ CMU, 2005-2014 Message passing on a tree Elimination on trees is equivalent to message passing along tree branches! f i j k l 10 © Eric Xing @ CMU, 2005-2014 From elimination to message passing  Recall ELIMINATION algorithm:  Choose an ordering Z in which query node f is the final node  Place all potentials on an active list  Eliminate node i by removing all potentials containing i, take sum/product over xi.  Place the resultant factor back on the list  For a TREE graph:  Choose query node f as the root of the tree  View tree as a directed tree with edges pointing towards leaves from f  Elimination ordering based on depth-first traversal  Elimination of each node can be considered as message-passing (or Belief Propagation) directly along tree branches, rather than on some transformed graphs thus, we can use the tree itself as a data-structure to do general inference!! 11 © Eric Xing @ CMU, 2005-2014 X1 X4 X3 X2 Computing P(X1) m32(x2) m42(x2) m21(x1) The message passing protocol:  A node can send a message to its neighbors when (and only when) it has received messages from all its other neighbors.  Computing node marginals:  Naïve approach: consider each node as the root and execute the message passing algorithm 12 © Eric Xing @ CMU, 2005-2014 X1 X4 X3 X2 Computing P(X2) m32(x2) m42(x2) m12(x2) The message passing protocol:  A node can send a message to its neighbors when (and only when) it has received messages from all its other neighbors.  Computing node marginals:  Naïve approach: consider each node as the root and execute the message passing algorithm 13 © Eric Xing @ CMU, 2005-2014 X1 X4 X3 X2 Computing P(X3) m23(x3) m42(x2) m12(x2) The message passing protocol:  A node can send a message to its neighbors when (and only when) it has received messages from all its other neighbors.  Computing node marginals:  Naïve approach: consider each node as the root and execute the message passing algorithm 14 © Eric Xing @ CMU, 2005-2014 Computing node marginals Naïve approach:  Complexity: NC  N is the number of nodes  C is the complexity of a complete message passing Alternative dynamic programming approach  2-Pass algorithm (next slide )  Complexity: 2C! 15 © Eric Xing @ CMU, 2005-2014 m24(X 4) X1 X2 X3 X4 The message passing protocol: A two-pass algorithm: m21(X 1) m32(X 2) m42(X 2) m12(X 2) m23(X 3) 16 © Eric Xing @ CMU, 2005-2014 Belief Propagation (SP-algorithm): Sequential implementation 17 © Eric Xing @ CMU, 2005-2014 Belief Propagation (SP-algorithm): Parallel synchronous implementation  For a node of degree d, whenever messages have arrived on any subset of d-1 node, compute the message for the remaining edge and send!  A pair of messages have been computed for each edge, one for each direction  All incoming messages are eventually computed for each node 18 © Eric Xing @ CMU, 2005-2014 Correctness of BP on tree Collollary: the synchronous implementation is ""non-blocking"" Thm: The Message Passage Guarantees obtaining all marginals in the tree What about non-tree? 19 © Eric Xing @ CMU, 2005-2014 Example 1 X1 X2 X3 X5 X4 X1 X2 X3 X5 X4 P(X1) P(X2) P(X3|X1,X2) P(X5|X1,X3) P(X4|X2,X3) fa(X1) fb(X2) fc(X3,X1,X2) fd(X5,X1,X3) fe(X4,X2,X3) fa fb fc fd fe Another view of SP: Factor Graph 20 © Eric Xing @ CMU, 2005-2014 Example 2 Example 3 X1 X2 x1,x2,x3) = fa(x1,x2)fb(x2,x3)fc(x3,x1) x1,x2,x3) = fa(x1,x2,x3) X3 fa fc fb X1 X2 X3 X1 X2 X3 fa X1 X2 X3 Factor Graphs 21 © Eric Xing @ CMU, 2005-2014 Factor Tree A Factor graph is a Factor Tree if the undirected graph obtained by ignoring the distinction between variable nodes and factor nodes is an undirected tree x1,x2,x3) = fa(x1,x2,x3) X1 X2 X3 fa X1 X2 X3 22 © Eric Xing @ CMU, 2005-2014 xi f1 fs f3 xj xi xk fs Message Passing on a Factor Tree Two kinds of messages 1. : from variables to factors 2. : from factors to variables 23 © Eric Xing @ CMU, 2005-2014 Message Passing on a Factor Tree, con'd Message passing protocol:  A node can send a message to a neighboring node only when it has received messages from all its other neighbors Marginal probability of nodes: xi f1 fs f3 xj xi xk fs P(xi) s N(i) si(xi) is(xi)si(xi) 24 © Eric Xing @ CMU, 2005-2014 X1 X2 X3 X1 X2 X3 fd fe fa fc fb a1 b2 c3 1d 3e d2 e2 2d 2e 2b d1 e3 1a 3c BP on a Factor Tree 25 © Eric Xing @ CMU, 2005-2014 Tree-like graphs to Factor trees X1 X2 X3 X4 X5 X6 X1 X2 X3 X4 X5 X6 Why factor graph? 26 © Eric Xing @ CMU, 2005-2014 X1 X2 X3 X5 X4 X1 X2 X3 X5 X4 Poly-trees to Factor trees 27 © Eric Xing @ CMU, 2005-2014 Why factor graph?  Because FG turns tree-like graphs to factor trees,  and trees are a data-structure that guarantees correctness of BP ! X1 X2 X3 X4 X5 X6 X1 X2 X3 X4 X5 X6 X1 X2 X3 X5 X4 X1 X2 X3 X5 X4 28 © Eric Xing @ CMU, 2005-2014 Max-product algorithm: computing MAP probabilities f i j k l 29 © Eric Xing @ CMU, 2005-2014 Max-product algorithm: computing MAP configurations using a final bookkeeping backward pass f i j k l 30 © Eric Xing @ CMU, 2005-2014 Sum-Product algorithm computes singleton marginal probabilities on:  Trees  Tree-like graphs  Poly-trees Maximum a posteriori configurations can be computed by replacing sum with max in the sum-product algorithm  Extra bookkeeping required Summary 31 © Eric Xing @ CMU, 2005-2014 Inference on general GM  Now, what if the GM is not a tree-like graph?  Can we still directly run message-passing protocol along its edges?  For non-trees, we do not have the guarantee that message-passing will be consistent!  Then what?  Construct a graph data-structure from P that has a tree structure, and run message-passing on it! Junction tree algorithm 32 © Eric Xing @ CMU, 2005-2014 Recall that Induced dependency during marginalization is captured in elimination cliques  Summation <-> elimination  Intermediate term <-> elimination clique  Can this lead to an generic inference algorithm? Elimination Clique E F H A E F B A C E G A D C E A D C B A A c m b m g m e m d m f m h m 33 © Eric Xing @ CMU, 2005-2014 Moral Graph  Note that for both directed GMs and undirected GMs, the joint probability is in a product form:  So let’s convert local conditional probabilities into potentials; then the second expression will be generic, but how does this operation affect the directed graph?  We can think of a conditional probability, e.g,. P(C|A,B) as a function of the three variables A, B, and C (we get a real number of each configuration):  Problem: But a node and its parent are not generally in the same clique in a BN  Solution: Marry the parents to obtain the ""moral graph""    d i i i X P P : ) | ( ) ( 1  X X    C c c c Z P ) ( ) ( X X  1 BN: MRF: A B C P(C|A,B) A B C (A,B,C) = P(C|A,B) 34 © Eric Xing @ CMU, 2005-2014 Moral Graph (cont.)  Define the potential on a clique as the product over all conditional probabilities contained within the clique  Now the product of potentials gives the right answer: ) , , ( ) , , ( ) , , ( ) , | ( ) | ( ) | ( ) , | ( ) ( ) ( ) , , , , , ( 6 5 4 5 4 3 3 2 1 5 4 6 3 5 3 4 2 1 3 2 1 6 5 4 3 2 1 X X X X X X X X X X X X P X X P X X P X X X P X P X P X X X X X X P      ) , | ( ) ( ) ( ) , , ( 2 1 3 2 1 3 2 1 X X X P X P X P X X X   ) | ( ) | ( ) , , ( 3 5 3 4 5 4 3 X X P X X P X X X   ) , | ( ) , , ( 5 4 6 6 5 4 X X X P X X X   where X1 X2 X3 X4 X5 X6 X1 X2 X3 X5 X4 X6 Note that here the interpretation of potential is ambivalent: it can be either marginals or conditionals 35 © Eric Xing @ CMU, 2005-2014 Clique trees  A clique tree is an (undirected) tree of cliques  Consider cases in which two neighboring cliques V and W have an overlap S (e.g., (X1, X2, X3) overlaps with (X3, X4, X5) ),  Now we have an alternative representation of the joint in terms of the potentials: X1 X2 X3 X5 X4 X6 X3, X4, X5 X4, X5, X6 X1, X2, X3 V W S ) (W  ) (V  ) (S  X3, X4, X5 X4, X5, X6 X1, X2, X3 X3 X4 36 © Eric Xing @ CMU, 2005-2014 Clique trees  A clique tree is an (undirected) tree of cliques  The alternative representation of the joint in terms of the potentials:  Generally: X1 X2 X3 X5 X4 X6 X3, X4, X5 X4, X5, X6 X1, X2, X3 X3 X4, X5, ) , ( ) , , ( ) ( ) , , ( ) , , ( ) , ( ) , , ( ) ( ) , , ( ) , , ( ) , | ( ) | ( ) | ( ) , | ( ) ( ) ( ) , , , , , ( 5 4 6 5 4 3 5 4 3 3 2 1 5 4 6 5 4 3 5 4 3 3 2 1 5 4 6 3 5 3 4 2 1 3 2 1 6 5 4 3 2 1 X X X X X X X X X X X X X X P X X X P X P X X X P X X X P X X X P X X P X X P X X X P X P X P X X X X X X P            S S S C C C P ) ( ) ( ) ( X X X   Now each potential is isomorphic to the cluster marginal of the attendant set of variables 37 © Eric Xing @ CMU, 2005-2014 Why this is useful? Propagation of probabilities  Now suppose that some evidence has been ""absorbed"" (i.e., certain values of some nodes have been observed). How do we propagate this effect to the rest of the graph?  What do we mean by propagate? Can we adjust all the potentials {}, {} so that they still represent the correct cluster marginals (or unnormalized equivalents) of their respective attendant variables?  Utility? X1 X2 X3 X4 X5 X6 X1 X2 X3 X5 X4 X6 X3, X4, X5 X4, X5, X6 X1, X2, X3 X3 X4 ) , , ( ) | ( , 3 2 1 6 6 1 3 2 X X X x X X P X X    ) ( ) | ( 3 6 6 3 X x X X P    Local operations! ) , , ( ) ( , 6 5 4 6 5 4 x X X x P X X   38 © Eric Xing @ CMU, 2005-2014 Local Consistency  We have two ways of obtaining p(S) and they must be the same  The following update-rule ensures this:  Forward update:  Backward update  Two important identities can be proven V W S ) (W  ) (V  ) (S  ) ( ) ( \ V S P S V    ) ( ) ( \ W S P S W      S V V S \ * *   W S S W     * *    S W W S \ * * *   * * * * * * V S S V      * * \ * \ * * S S W W S V V       S W V S W V S W V            * * * * * * * * * Local Consistency Invariant Joint 39 © Eric Xing @ CMU, 2005-2014 Message Passing Algorithm This simple local message-passing algorithm on a clique tree defines the general probability propagation algorithm for directed graphs!  Many interesting algorithms are special cases:  Forward-backward algorithm for hidden Markov models,  Kalman filter updates  Pealing algorithms for probabilistic trees  The algorithm seems reasonable. Is it correct? V W S ) (W  ) (V  ) (S    S V S V \ * *   W S S W     * *    S W W S \ * * *   * * * * * * V S S V      40 © Eric Xing @ CMU, 2005-2014 A problem Consider the following graph and a corresponding clique tree  Note that C appears in two non-neighboring cliques Question: with the previous message passage, can we ensure that the probability associated with C in these two (non- neighboring) cliques consistent? Answer: No. It is not true that in general local consistency implies global consistency What else do we need to get such a guarantee? A B C D A,B B,D A,C C,D 41 © Eric Xing @ CMU, 2005-2014 Triangulation  A triangulated graph is one in which no cycles with four or more nodes exist in which there is no chord  We triangulate a graph by adding chords:  Now we no longer have our global inconsistency problem.  A clique tree for a triangulated graph has the running intersection property: If a node appears in two cliques, it appears everywhere on the path between the cliques  Thus local consistency implies global consistency A B C D A B C D A,B,C B,C,D 42 © Eric Xing @ CMU, 2005-2014 Junction trees  A clique tree for a triangulated graph is referred to as a junction tree  In junction trees, local consistency implies global consistency. Thus the local message-passing algorithms is (provably) correct  It is also possible to show that only triangulated graphs have the property that their clique trees are junction trees. Thus if we want local algorithms, we must triangulate  Are we now all set?  How to triangulate?  The complexity of building a JT depends on how we triangulate!!  Consider this network: it turns out that we will need to pay an O(24) or O(26) cost depending on how we triangulate! B A D C E F G H 43 © Eric Xing @ CMU, 2005-2014 moralization B A D C E F G H B A D C E F G H B A D C B A D C E F G B A D C E F B A D C E B A C B A A graph elimination How to triangulate  A graph elimination algorithm  Intermediate terms correspond to the cliques resulted from elimination  “good” elimination orderings lead to small cliques and hence reduce complexity (what will happen if we eliminate ""e"" first in the above graph?)  finding the optimum ordering is NP-hard, but for many graph optimum or near- optimum can often be heuristically found 44 © Eric Xing @ CMU, 2005-2014 E F H A E F B A C E G A D C E A D C B A A c m b m g m e m d m f m h m A junction tree 45 © Eric Xing @ CMU, 2005-2014 Message-passing algorithms Message update  The Hugin update  The Shafer-Shenoy update collect distribute   S V V S \ *   W S S W     * *        ij i i S C j k ki i k C ij j i S m S m \ ) ( ) (  46 © Eric Xing @ CMU, 2005-2014 A Sketch of the Junction Tree Algorithm  The algorithm 1. Moralize the graph (trivial) 2. Triangulate the graph (good heuristic exist, but actually NP hard) 3. Build a clique tree (e.g., using a maximum spanning tree algorithm 4. Propagation of probabilities --- a local message-passing protocol  Results in marginal probabilities of all cliques --- solves all queries in a single run  A generic exact inference algorithm for any GM  Complexity: exponential in the size of the maximal clique --- a good elimination order often leads to small maximal clique, and hence a good (i.e., thin) JT 47 © Eric Xing @ CMU, 2005-2014  Elimination message passing on a clique tree E F H A E F B A C E G A D C E A D C B A A h m g m e m f m b m c m d m Recall the Elimination and Message Passing Algorithm A A A A x2 x3 x1 xT y2 y3 y1 yT ... ...     i k i i t k t t k t a y x p , ) | ( 1 1     k k T P  ) (x   e f g e e a m e m d c e p d c a m ) , ( ) ( ) , | ( ) , , ( 48 © Eric Xing @ CMU, 2005-2014 Shafer Shenoy for HMMs Recap: Shafer-Shenoy algorithm  Message from clique i to clique j :  Clique marginal      ij i i S C j k ki i k C j i S \ ) (       k ki i k C i S C p i ) ( ) (   49 © Eric Xing @ CMU, 2005-2014 Message Passing for HMMs (cont.) A junction tree for the HMM Rightward pass  This is exactly the forward algorithm! Leftward pass …  This is exactly the backward algorithm! A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... ) , ( 1 1 x y  ) , ( 2 1 y y  ) , ( 3 2 y y  ) , ( T T y y 1   ) , ( 2 2 x y  ) , ( 3 3 x y  ) , ( T T x y  ) ( 2 y  ) ( 3 y  ) ( T y  ) ( 1 y  ) ( 2 y   ) , ( 1  t t y y  ) ( t t t y  1  ) , ( 1 1   t t x y  ) ( 1 1    t t t y  ) ( 1   t t y             1 1 1 1 1 1 t y t t t t t t t t t t y y y y y ) ( ) ( ) , ( ) (                   t t t t y t t t y y t t y t t t t t t t y a y x p y x p y y y p ) ( ) | ( ) | ( ) ( ) | ( , 1 1 1 1 1 1 1 1            t y t t t t t t t t t t y y y y y ) ( ) ( ) , ( ) ( 1 1 1 1 1              1 1 1 1 1 1 t y t t t t t t t y x p y y y p ) | ( ) ( ) | (  ) , ( 1  t t y y  ) ( t t t y  1  ) ( 1 1    t t t y  ) , ( 1 1   t t x y  ) ( 1   t t y  50 © Eric Xing @ CMU, 2005-2014 Summary Junction tree data-structure for exact inference on general graphs Two methods  Shafer-Shenoy  Belief-update or Lauritzen-Speigelhalter Constructing Junction tree from chordal graphs  Maximum spanning tree approach 51 © Eric Xing @ CMU, 2005-2014 "
500,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai NLP and Word Embeddings Word representation Andrew Ng Word representation V = [a, aaron, …, zulu, <UNK>] 1-hot representation Apple (456) Orange (6257) I want a glass of orange ______. I want a glass of apple______. King (4914) 0 0 0 ⋮ 1 ⋮ 0 0 0 Woman (9853) 0 0 0 0 0 ⋮ 1 ⋮ 0 Man (5391) 0 0 0 0 ⋮ 1 ⋮ 0 0 Queen (7157) 0 0 0 0 0 ⋮ 1 ⋮ 0 0 ⋮ 1 ⋮ 0 0 0 0 0 0 0 0 0 0 ⋮ 1 ⋮ 0 Andrew Ng Featurized representation: word embedding Apple (456) Orange (6257) King (4914) Woman (9853) Man (5391) Queen (7157) I want a glass of orange ______. I want a glass of apple______. -0.95 0.97 0.00 0.01 0.93 0.95 -0.01 0.00 0.7 0.69 0.03 -0.02 0.02 0.01 0.95 0.97 Andrew Ng Visualizing word embeddings fish dog cat apple grape orange one three two four king man queen woman [van der Maaten and Hinton., 2008. Visualizing data using t-SNE] deeplearning.ai NLP and Word Embeddings Using word embeddings Andrew Ng Named entity recognition example Sally Johnson is an orange farmer 1 1 0 0 0 0 Robert Lin is an apple farmer Andrew Ng Transfer learning and word embeddings 1. Learn word embeddings from large text corpus. (1-100B words) (Or download pre-trained embedding online.) 2. Transfer embedding to new task with smaller training set. (say, 100k words) 3. Optional: Continue to finetune the word embeddings with new data. Andrew Ng Relation to face encoding ⋮ $(&) ⋮ ⋮ $(() ) * [Taigman et. al., 2014. DeepFace: Closing the gap to human level performance] f($(&)) f($(()) deeplearning.ai NLP and Word Embeddings Properties of word embeddings Andrew Ng Analogies Apple (456) Orange (6257) King (4914) Woman (9853) Man (5391) Queen (7157) Gender Royal Age Food −1 0.01 0.03 0.09 1 0.02 0.02 0.01 -0.95 0.93 0.70 0.02 0.97 0.95 0.69 0.01 0.00 -0.01 0.03 0.95 0.01 0.00 -0.02 0.97 [Mikolov et. al., 2013, Linguistic regularities in continuous space word representations] Andrew Ng Analogies using word vectors fish dog cat apple grape orange one three two four king man queen woman ()*+ −(,-)*+ ≈(/0+1 −(? Andrew Ng Cosine similarity 345((,, (/0+1 −()*+ + (,-)*+) Man:Woman as Boy:Girl Ottawa:Canada as Nairobi:Kenya Big:Bigger as Tall:Taller Yen:Japan as Ruble:Russia deeplearning.ai NLP and Word Embeddings Embedding matrix Andrew Ng Embedding matrix In practice, use specialized function to look up an embedding. deeplearning.ai NLP and Word Embeddings Learning word embeddings Andrew Ng Neural language model I want a glass of orange ______. 4343 9665 1 3852 6163 6257 I want a glass of orange *+,+, *-../ *0 *,1/2 *.0., *.2/3 4 4 4 4 4 4 5+,+, 5-../ 50 5,1/2 5.0., 5.2/3 [Bengio et. al., 2003, A neural probabilistic language model] Andrew Ng Other context/target pairs I want a glass of orange juice to go along with my cereal. Context: Last 4 words. 4 words on left & right Last 1 word Nearby 1 word deeplearning.ai NLP and Word Embeddings Word2Vec Andrew Ng Skip-grams I want a glass of orange juice to go along with my cereal. [Mikolov et. al., 2013. Efficient estimation of word representations in vector space.] Andrew Ng Model Vocab size = 10,000k Andrew Ng Problems with softmax classification ! "" # = %&' ()* ∑ %&, ()* -.,... 01- How to sample the context #? deeplearning.ai NLP and Word Embeddings Negative sampling Andrew Ng Defining a new learning problem I want a glass of orange juice to go along with my cereal. [Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality] Andrew Ng Model Softmax: ! "" # = %&' ()* ∑ %&, ()* -.,... 01- context word orange orange orange juice king book target? the of orange orange 1 0 0 0 0 Andrew Ng Selecting negative examples context word orange orange orange juice king book target? the of orange orange 1 0 0 0 0 deeplearning.ai NLP and Word Embeddings GloVe word vectors Andrew Ng GloVe (global vectors for word representation) I want a glass of orange juice to go along with my cereal. [Pennington et. al., 2014. GloVe: Global vectors for word representation] Andrew Ng Model Andrew Ng A note on the featurization view of word embeddings minimize ∑ ∑ ( )*+ ,* -. + + 0* −0 + 2 −log )*+ 6 78,888 +:7 78,888 *:7 King (4914) Woman (9853) Man (5391) Queen (7157) -0.95 0.93 0.70 0.02 0.97 0.95 0.69 0.01 −1 0.01 0.03 0.09 1 0.02 0.02 0.01 Gender Royal Age Food deeplearning.ai NLP and Word Embeddings Sentiment classification Andrew Ng Sentiment classification problem ! "" The dessert is excellent. Service was quite slow. Good for a quick meal, but nothing special. Completely lacking in good taste, good service, and good ambience. Andrew Ng Simple sentiment classification model The desert is excellent #$%&$ #&'($ #'(%' #)*$+ , , , , -$%&$ -&'($ -'(%' -)*$+ 8928 2468 4694 3180 The dessert is excellent “Completely lacking in good taste, good service, and good ambience.” Andrew Ng RNN for sentiment classification Completely lacking in good …. ambience , , , , , -*$6& -'%(( -''&7 -)$$& -))+ "" 8 softmax ⋯ :;+< :;*< :;&< :;)< :;'< :;*+< deeplearning.ai NLP and Word Embeddings Debiasing word embeddings Andrew Ng The problem of bias in word embeddings [Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings] Man:Woman as King:Queen Man:Computer_Programmer as Woman: Father:Doctor as Mother: Word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. Homemaker Nurse Andrew Ng Addressing bias in word embeddings 1. Identify bias direction. 2. Neutralize: For every word that is not definitional, project to get rid of bias. 3. Equalize pairs. [Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings] "
501,"Copyright Notice These slides are distributed under the Creative Commons License. DeepLearning.AI makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite DeepLearning.AI as the source of the slides. For the rest of the details of the license, see https://creativecommons.org/licenses/by-sa/2.0/legalcode deeplearning.ai Sequence to sequence models Basic models Andrew Ng Sequence to sequence model Jane visite l’Afrique en septembre Jane is visiting Africa in September. !""#$ %""&$ %""' ($ ⋯ [Cho et al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation] %""&$ %""*$ %""+$ %"",$ %""-$ .""&$ .""*$ .""+$ ."",$ .""-$ .""/$ [Sutskever et al., 2014. Sequence to sequence learning with neural networks] Andrew Ng Image captioning A cat sitting on a chair .""&$ .""*$ .""+$ ."",$ .""-$ 55×55 ×96 27×27 ×96 27×27 ×256 13×13 ×256 11 × 11 s = 4 3 × 3 s = 2 MAX-POOL 5 × 5 same 3 × 3 s = 2 MAX-POOL 13×13 ×384 3 × 3 same 3 × 3 = 13×13 ×384 13×13 ×256 6×6 ×256 3 × 3 3 × 3 s = 2 MAX-POOL ⋮ 9216 Softmax 1000 ⋮ 4096 ⋮ 4096 [Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks] [Vinyals et. al., 2014. Show and tell: Neural image caption generator] [Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions] .""/$ . 9""' :$ % . 9""&$ . 9""*$ ⋯ deeplearning.ai Sequence to sequence models Picking the most likely sentence Andrew Ng Machine translation as building a conditional language model Language model: Machine translation: !""#$ %""&$ ' (""&$ %""* +$ ' (""* ,$ ⋯ ⋯ !""#$ %""&$ ' (""&$ ' ("".$ ' (""* ,$ ⋯ %"".$ Andrew Ng Finding the most likely translation Jane visite l’Afrique en septembre. /('""&$, … , '""* ,$| %) Jane is visiting Africa in September. Jane is going to be visiting Africa in September. In September, Jane will visit Africa. Her African friend welcomed Jane in September. arg max :;<=,…,:;>,= /('""&$, … , '""* ,$| %) Andrew Ng Why not a greedy search? Jane is visiting Africa in September. Jane is going to be visiting Africa in September. !""#$ %""&$ ' (""&$ %""* +$ ' (""* ,$ ⋯ ⋯ deeplearning.ai Sequence to sequence models Beam search Andrew Ng Beam search algorithm !""#$ %""&$ ' (""&$ %""* +$ ⋯ a in jane september zulu ⋮ ⋮ ⋮ ⋮ 10000 0('""&$ | %) Step 1 Andrew Ng Beam search algorithm a in jane september zulu ⋮ ⋮ ⋮ ⋮ Step 1 10000 !""#$ %""&$ %""* +$ ⋯ !""#$ %""&$ %""* +$ ⋯ !""#$ %""&$ %""* +$ ⋯ Step 2 Andrew Ng Beam search (4 = 3) in september jane is jane visits !""#$ %""&$ %""* +$ ' (""7$ ⋯ september in 0('""&$, '""9$| %) jane visits africa in september. <EOS> !""#$ %""&$ %""* +$ ' (""7$ ⋯ is jane !""#$ %""&$ %""* +$ ' (""7$ ⋯ visits jane deeplearning.ai Sequence to sequence models Refinements to beam search Andrew Ng Length normalization arg max ' ( ) *+,- ., *+0-, … , *+,20-) 4 5 ,60 arg max ' 7 log ) *+,- ., *+0-, … , *+,20-) 4 5 ,60 7 log ) *+,- ., *+0-, … , *+,20-) 4 5 ,60 Andrew Ng Beam search discussion Beam width B? Unlike exact search algorithms like BFS (Breadth First Search) or DFS (Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for arg max ' )(*|.). deeplearning.ai Sequence to sequence models Error analysis on beam search Andrew Ng Example Jane visite l’Afrique en septembre. Human: Jane visits Africa in September. Algorithm: Jane visited Africa last September. !""#$ %""&$ %""' ($ ⋯ Andrew Ng Error analysis on beam search Human: Jane visits Africa in September. (+∗) Algorithm: Jane visited Africa last September. (+ .) Case 1: Beam search chose + .. But +∗attains higher / + % . Conclusion: Beam search is at fault. Case 2: +∗ is a better translation than + .. But RNN predicted / +∗% < / + . % . Conclusion: RNN model is at fault. Andrew Ng Error analysis process Jane visits Africa in September. Jane visited Africa last September. Human Algorithm / +∗% / + . % At fault? Figures out what faction of errors are “due to” beam search vs. RNN model deeplearning.ai Sequence to sequence models Bleu score (optional) Andrew Ng Evaluating machine translation French: Le chat est sur le tapis. Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: the the the the the the the. Precision: Modified precision: [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Andrew Ng Bleu score on bigrams Example: Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: The cat the cat on the mat. the cat cat the cat on on the the mat [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] Andrew Ng Bleu score on unigrams Example: Reference 1: The cat is on the mat. Reference 2: There is a cat on the mat. MT output: The cat the cat on the mat. [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] !"" = % &'()*+,∈. / 0123456(7 (239:;<=) % &'()*+,∈. / 01234 (239:;<=) !' = % ')*+,∈. / 0123456(7 (3:;<=) % ')*+,∈. / 01234 (3:;<=) Andrew Ng Bleu details !' = Bleu score on n-grams only Combined Bleu score: BP = 1 if MT_output_length > reference_output_length exp (1 −MT_output_length/reference_output_length) otherwise [Papineni et. al., 2002. Bleu: A method for automatic evaluation of machine translation] deeplearning.ai Sequence to sequence models Attention model intuition Andrew Ng The problem of long sequences Jane s'est rendue en Afrique en septembre dernier, a apprécié la culture et a rencontré beaucoup de gens merveilleux; elle est revenue en parlant comment son voyage était merveilleux, et elle me tente d'y aller aussi. Jane went to Africa last September, and enjoyed the culture and met many wonderful people; she came back raving about how wonderful her trip was, and is tempting me to go too. 10 20 30 40 50 Sentence length Bleu score !""#$ %""&$ ' (""&$ %""* +$ ' (""* ,$ ⋯ ⋯ Andrew Ng Attention model intuition %""&$ %"".$ %""/$ %""0$ %""1$ jane visite l’Afrique en septembre [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] ' (""&$ ' ("".$ ' (""0$ ' (""/$ ' (""1$ !""#$ deeplearning.ai Sequence to sequence models Attention model Andrew Ng Attention model !""#$ !""%$ !""&$ !""'$ !""($ jane visite l’Afrique en septembre [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] )""*$ Andrew Ng [Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate] Computing attention +"",,,.$ [Xu et. al., 2015. Show, attend and tell: Neural image caption generation with visual attention] +"",,,.$ = amount of attention /"", $ should pay to )"",.$ +"",,,.$ = 123 (567,7.8) ∑7.;< => 123 (567,7.8) ?"",@#$ )"",.$ A"",,,.$ + / C"",@#$ / C"",$ ?"",@#$ ?"",$ )""*$ !""#$ ⋯ !""%$ !""E >$ !""E >@#$ Andrew Ng Attention examples July 20th 1969 1969 −07 −20 23 April, 1564 1564 −04 −23 Visualization of +"",,,.$: deeplearning.ai Audio data Speech recognition Andrew Ng Speech recognition problem ! audio clip # transcript “the quick brown fox” Andrew Ng Attention model for speech recognition + “h” %&'( %&)( “T” ⋯ +&,( !&'( ⋯ !&)( !&',,,( !&---( ⋯ Andrew Ng CTC cost for speech recognition [Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks] (Connectionist temporal classification) “the quick brown fox” Basic rule: collapse repeated characters not separated by “blank” +&,( !&'( # .&',,,( ⋯ !&)( !&',,,( # .&'( # .&)( deeplearning.ai Audio data Trigger word detection Andrew Ng What is trigger word detection? Amazon Echo (Alexa) Baidu DuerOS (xiaodunihao) Apple Siri (Hey Siri) Google Home (Okay Google) Andrew Ng Trigger word detection algorithm !""#$ %""&$ %""'$ %""($ deeplearning.ai Conclusion Summary and thank you Andrew Ng Specialization outline 1. Neural Networks and Deep Learning 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization 3. Structuring Machine Learning Projects 4. Convolutional Neural Networks 5. Sequence Models Andrew Ng Deep learning is a super power Please buy this from shutterstock and replace in final video. Andrew Ng -Andrew Ng Thank you. "
502,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230 Winter 2019 Andrew Ng & Kian Katanforoosh Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Teaching Team Sagar Honnungar (remote) Younes Bensouda Mourri Teaching Assistants Swati Dube Course advisor Course coordinator Andrew Ng Instructor Kian Katanforoosh Instructor Hoormazd Rezaei Ahmad-reza Momeni Abhijeet Shenoi Cristian Aramburu Head TAs Weini Yu Sarah Najmark Daniel Kunin Shervine Amidi Ishan Patil (remote) Hojat Ghorbani (remote) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. What is deep learning? (25min) II. Course Logistics (15min) III. Introduction to Deep Learning Applications (20min) IV. Examples of student projects (10min) Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [AI Index Report 2018] What is deep learning? • The growth in annually published papers in AI has outpaced that of CS. • A growing number of AI publications by researchers from other scientiﬁc ﬁelds (Physics, Chemistry, Astronomy, Material Science, etc.) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What is deep learning? • The number of Scopus papers on Neural Networks had a compound annual growth rate of 37% from 2014 to 2017. • It has notably driven the growth of #papers published in ML and CV. Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. What is deep learning? (25min) II. Course Logistics (15min) III. Introduction to Deep Learning Applications (20min) IV. Examples of student projects (10min) Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Course Logistics We are going to use the Coursera Platform: www.coursera.org 5 “courses”: Schedule is on http://cs230.stanford.edu/syllabus/ Example: C2M3: Course 2 Module 3 C1: Neural Networks and Deep Learning C2: Improving Deep Neural Networks C3: Strategy for Machine Learning Projects C4: Convolutional Neural Networks C5: Sequence Models The class forum is on Piazza: piazza.com/stanford/winter2019/cs230 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Watch videos on Coursera ≈1h 1 module 1 week of class ≈ 2 modules + + 15min project mentorship with TA One week in the life of a CS230 student Assignments and Quizzes are due every Tuesday at 10am Do not follow the deadlines displayed on Coursera!!! Go to in-class lecture ≈1h20 + TA sections on Fridays ≈ 1 hour Complete programming assignments ≈1-3h Solve quiz ≈20min Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Grading Formula Grade = 0.02A + 0.08Q + 0.25Pa + 0.25M + 0.40Pr A = Attendance Q = Quizzes Pa = (Programming) assignments M = Midterm Pr = Final-project Active Piazza participation = 1% bonus Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Late days Example: For next Tuesday at 10am you have to complete the following assignments: - 2 Quizzes: ★Introduction to deep learning ★Neural Network Basics - 2 Programming assignments: ★Python Basics with Numpy ★Logistic Regression with a neural network mindset At 9am on Tuesday: you submit 1 quiz and the 1 PA. At 3pm on Tuesday: you submit the second quiz. At 2pm on Wednesday: you submit the second PA. How many late days did you use? 3 late days Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. What is deep learning? (25min) II. Course Logistics (15min) III. Introduction to Deep Learning Applications (20min) IV. Examples of student projects (10min) Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri y = 0 y = 1 y = 2 y = 3 y = 4 y = 5 1 0 0 0 0 0 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 0 1 0 0 0 0 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 0 0 1 0 0 0 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 0 0 0 1 0 0 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 0 0 0 0 1 0 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 0 0 0 0 0 1 ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ Projects: SIGN language detection Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Assignment: The Happy House y = 0 y = 1 y = 0 y = 1 can’t enter the Happy House can enter the Happy House! Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Assignment: Object detection [Joseph Redmon, Ali Farhadi: YOLO9000: Better, Faster, Stronger, 2016] [Another fun video generated with YOLOv2 by J. Redmon: https://youtu.be/VOC3huqHrss] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others Face recognition 85% Ramtin And many more… Car detection Music generation Text generation Trigger word detection Machine translation Optimal goalkeeper shoot prediction “I love you” Emojiﬁer Art generation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Deep Learning Specialization] Assignment: Car detection for autonomous driving Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others Face recognition 85% Ramtin And many more… Car detection Music generation Text generation Trigger word detection Machine translation Optimal goalkeeper shoot prediction “I love you” Emojiﬁer Art generation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [L. Gatys et al.: Image Style Transfer Using Convolutional Neural Networks , 2015] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others Face recognition 85% Ramtin And many more… Car detection Music generation Text generation Trigger word detection Machine translation Optimal goalkeeper shoot prediction “I love you” Emojiﬁer Art generation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. What is deep learning? (25min) II. Course Logistics (15min) III. Introduction to Deep Learning Applications (20min) IV. Examples of student projects (10min) Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others Coloring Black&White pictures with Deep Learning Predicting price of an object from a picture Neural Network 300$ Neural Network Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others Image-to-Image translation with Conditional-GAN [Hu, Yu & Yu, Spring 2018: http://cs230.stanford.edu/projects_spring_2018/reports/8289557.pdf] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Projects: others LeafNet: A Deep Learning Solution to Tree Species Identiﬁcation [Galbally, Rao & Pacalin: Spring 2018, http://cs230.stanford.edu/projects_spring_2018/posters/8285741.pdf] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri And many more… Predicting atom energy based on atomic-structure Visual Question Answering Cancer/Parkinson/Alzheimer detection Activity recognition in video … Accent transfer in a speech Music genre classiﬁcation / Music Compression Generating images based on a given legend Detecting earthquake precursor signals Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri To sum up 1. You will learn about wide range of deep learning topics 2. The course is very applied, you will code these applications 3. You have access to mentorship to build an outstanding project in 10 weeks For next Tuesday (01/15) 10am: - Create Coursera account and join the private session using the invitation - Finish C1M1 & C1M2 - 2 Quizzes: ★Introduction to deep learning ★Neural Network Basics - 2 Programming assignments: ★Python Basics with Numpy ★Logistic Regression with a neural network mindset - Find project team-mates and ﬁll-in the Google form that will be posted on Piazza. This Friday (01/11): - TA section “Deep Learning Applications” Download your notebooks after you ﬁnished them! Follow only the website deadlines! "
503,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230: Lecture 2 Deep Learning Intuition Kian Katanforoosh Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Model = Architecture + Learning Process Input Output 0 Loss Gradients Things that can change - Activation function - Optimizer - Hyperparameters Parameters - … Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Logistic Regression as a Neural Network image2vector 255 231 ... 94 142 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ … /255 /255 /255 /255 … σ 0.73 x1 (i) wT x(i) + b x2 (i) xn−1 (i) xn (i) “it’s a cat” 0.73 > 0.5 … Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri image2vector 255 231 ... 94 142 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ … /255 /255 /255 /255 … σ 0.73 x1 (i) wT x(i) + b x2 (i) xn−1 (i) xn (i) σ wT x(i) + b σ wT x(i) + b Multi-class 0.04 0.12 Cat? 0.73 > 0.5 Giraffe? 0.04 < 0.5 Dog? 0.12 < 0.5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri image2vector 255 231 ... 94 142 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ … /255 /255 /255 /255 … σ x1 (i) wT x(i) + b x2 (i) xn−1 (i) xn (i) σ wT x(i) + b σ wT x(i) + b Neural Network (Multi-class) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri image2vector 255 231 ... 94 142 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ … /255 /255 /255 /255 … x1 (i) x2 (i) xn−1 (i) xn (i) Hidden layer a1 [2] output layer 0.73 Cat 0.73 > 0.5 a1 [1] a2 [1] a3 [1] Neural Network (1 hidden layer) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Deeper network: Encoding output layer a1 [1] a2 [1] a3 [1] a4 [1] a1 [2] a2 [2] a3 [2] a1 [3] x1 (i) x2 (i) x3 (i) x4 (i) ˆ y(i) Technique called “encoding” Hidden layer Hidden layer Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Let’s build intuition on concrete applications Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Day’n’Night classiﬁcation II. Face Recognition III. Art generation IV. Keyword Spotting V. Shipping model Today’s outline We will learn how to: - Analyze a problem from a deep learning approach - Choose an architecture - Choose a loss and a training strategy Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Day’n’Night classiﬁcation (warm-up) Goal: Given an image, classify as taken “during the day” (0) or “during the night” (1) 1. Data? 2. Input? 3. Output? 4. Architecture ? 5. Loss? 10,000 images Resolution? y = 0 or y = 1 Last Activation? Split? Bias? Shallow network should do the job pretty well L = −[ylog( ˆ y) + (1−y)log(1−ˆ y)] Easy (64, 64, 3) sigmoid Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Face Veriﬁcation 2. Input? Resolution? (412, 412, 3) 1. Data? Picture of every student labelled with their name 3. Output? y = 1 (it’s you) or y = 0 (it’s not you) Bertrand Goal: A school wants to use Face Veriﬁcation for validating student IDs in facilities (dinning halls, gym, pool …) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Goal: A school wants to use Face Veriﬁcation for validating student IDs in facilities (dinning halls, gym, pool …) 4. What architecture? Simple solution: database image input image compute distance pixel per pixel if less than threshold then y=1 - Background lighting differences - A person can wear make-up, grow a beard… - ID photo can be outdated Issues: Face Veriﬁcation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Goal: A school wants to use Face Veriﬁcation for validating student IDs in facilities (dinning halls, gym, pool …) 4. What architecture? Our solution: encode information about a picture in a vector Deep Network 0.931 0.433 0.331 ! 0.942 0.158 0.039 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ 128-d Deep Network 0.922 0.343 0.312 ! 0.892 0.142 0.024 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ distance 0.4 y=1 0.4 < threshold We gather all student faces encoding in a database. Given a new picture, we compute its distance with the encoding of card holder Face Veriﬁcation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Face Recognition Goal: A school wants to use Face Veriﬁcation for validating student IDs in facilities (dinning hall, gym, pool …) 4. Loss? Training? We need more data so that our model understands how to encode: Use public face datasets What we really want: similar encoding different encoding So let’s generate triplets: anchor positive negative minimize encoding distance maximize encoding distance Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Model = Architecture + Recap: Learning Process Input Output Loss Gradients Parameters anchor positive negative L = Enc(A)−Enc(P) 2 2 −Enc(A)−Enc(N) 2 2 +α 0.13 0.42 .. 0.10 0.31 0.73 .. 0.43 0.33 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ 0.95 0.45 .. 0.20 0.41 0.89 .. 0.31 0.34 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ 0.01 0.54 .. 0.45 0.11 0.49 .. 0.12 0.01 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Enc(A) Enc(P) Enc(N) 0 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Face Recognition Goal: A school wants to use Face Identiﬁcation for recognize students in facilities (dinning hall, gym, pool …) Goal: You want to use Face Clustering to group pictures of the same people on your smartphone K-Nearest Neighbors K-Means Algorithm Maybe we need to detect the faces ﬁrst? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Art generation (Neural Style Transfer) Goal: Given a picture, make it look beautiful 1. Data? Let’s say we have any data 2. Input? 3. Output? content image style image generated image Leon A. Gatys, Alexander S. Ecker, Matthias Bethge: A Neural Algorithm of Artistic Style, 2015 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Art generation (Neural Style Transfer) 4. Architecture? 5. Loss? We want a model that understands images very well We load an existing model trained on ImageNet for example Deep Network classiﬁcation When this image forward propagates, we can get information about its content & its style by inspecting the layers. ContentC StyleS Leon A. Gatys, Alexander S. Ecker, Matthias Bethge: A Neural Algorithm of Artistic Style, 2015 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Correct Approach Art generation (Neural Style Transfer) Deep Network (pretrained) After 2000 iterations compute loss update pixels using gradients L = ContentC −ContentG 2 2 + StyleS −StyleG 2 2 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Leon A. Gatys, Alexander S. Ecker, Matthias Bethge: A Neural Algorithm of Artistic Style, 2015 We are not learning parameters by minimizing L. We are learning an image! Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Speech recognition: Keyword Spotting Goal: Given an audio speech, detect the word “lion”. 1. Input? 3. Data? 2. Output? y = (0,0,0,0,0,0,0,0,0,0,….,0,0,0,0,0,0,1,0,0, ….,0,0,0,0,0,0,0,0,0,0) Many audio recordings (“words”) y = 0 (there is “lion”) or y = 1 (there isn’t “lion”) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Speech recognition: Keyword Spotting Goal: Given an audio speech, detect the word “lion”. 4. What architecture? 0.12 0.01 0.27 … … … … … … … 0.21 0.92 0.43 … … … … … … Threshold: 0.6 Deep Network 0.931 0.433 0.331 ! 0.942 0.158 0.039 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Deep Network 0.922 0.343 0.312 ! 0.892 0.142 0.024 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ distance 0.4 y=1 0.4 < threshold L = Enc(A)−Enc(P) 2 2 −Enc(A)−Enc(N) 2 2 +α y = (0,0,0,0,0,0,0,0,0,0,0,0,0,….,0,0,0,0,0,0,1,0,0, ….,0,0,0,0,0,0,0,0,0,0) App implementation Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Server-based or on-device? On-device Server-based Model Architecture + Learned Parameters y = 0 Model Architecture + Learnt Parameters y=0 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Server-based or on-device? On-device Server-based Model Architecture + Learned Parameters y = 0 Model Architecture + Learnt Parameters y=0 App is light-weight Faster predictions App is easy to update Works ofﬂine Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Duties for next week For Tuesday 04/17, 9am: C1M3 • Quiz: Shallow Neural Networks • Programming Assignment: Planar data classiﬁcation with one-hidden layer C1M4 • Quiz: Deep Neural Networks • Programming Assignment: Building a deep neural network - Step by Step • Programming Assignment: Deep Neural Network Application Project • For this Friday (04/13): ﬁnd teammate and submit the Team-members form with your project category • Fill-in AWS Form to get GPU credits This Friday (04/13): • (optional) Project section: How to get started with your projects? "
504,"CS230: Lecture 3 The mathematics of deep learning Backpropagation, Initializations, Regularization Kian Katanforoosh I – Backpropagation II – Initializations III – Regularization I - Backpropagation Problem statement A – Logistic Regression backpropagation for one training example B – Logistic Regression backpropagation for a batch of m examples Question: You have trained an animal classifier. Can you tell what part of the input led to this prediction? II - Initializations Problem statement In class, you’ve seen that: The goal: Let’s prove that: !""# ""[%&'] = !""# ""[%] → !""# +[%] = 1 -[%&'] Let’s prove that: !""# ""[%&'] = !""# ""[%] → !""# +[%] = 1 -[%&'] !""# ""[%] = !""# .[%] = !""# / + ',1 % ""1 %&' 2 345 16' Checkpoint: We’ve shown that for every layer l: !""# ""[%] = -[%&']!""# +[%] !""# ""[%&'] "
505,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230: Lecture 5 Attacking Networks with Adversarial Examples - Generative Adversarial Networks Kian Katanforoosh Go to www.menti.com and use the code 53 39 35 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Attacking NNs with Adversarial Examples II. Generative Adversarial Networks Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Adversarial examples Discovery (2014): several machine learning models, including state-of-the-art neural networks, are vulnerable to adversarial examples [Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy (2015): Explaining and harnessing adversarial examples] A. How to build adversarial examples and attack a network? B. Examples C. How to defend against adversarial examples? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri 0.04 ! 0.02 0.07 0.81 ! 0.07 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ “bike” “tomato” “car” “crab” “cat” Neural network (pretrained on ImageNet) [Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy (2015): Explaining and harnessing adversarial examples] I. A. How to build adversarial examples and attack a network? Goal: Given a pretrained network on ImageNet, ﬁnd an example that is not a iguana but will be classify as an iguana. x 0.04 0.85 ! 0.07 0.01 ! 0.02 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ “bike” “iguana “car” “crab” “cat” 1. Rephrasing what we want: ˆ y(x) = yiguana = 0 1 ! 0 0 ! 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Find x such that: 2. Deﬁning the loss function L( ˆ y, y) = 1 2 ˆ y(W,b,x) −yiguana 2 2 Network (pretrained on ImageNet) x L( ˆ y, y) 3. Optimize the image ∂L ∂x After many iterations x = x −α ∂L ∂x ? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Question: Will the learned image x look like an iguana? Space of possible input images 25632×32×3 ≈107400 Space of real images I. A. How to build adversarial examples and attack a network? ? Space of images classiﬁed as iguanas Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri ? 0.04 ! 0.02 0.07 0.81 ! 0.07 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ “bike” “tomato” “car” “crab” “cat” Neural network (pretrained on ImageNet) [Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy (2015): Explaining and harnessing adversarial examples] I. A. How to build adversarial examples and attack a network? Goal: Given a pretrained network on ImageNet, ﬁnd an example that is a cat but will be classify as an iguana. 0.04 0.85 ! 0.07 0.01 ! 0.02 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ “bike” “iguana “car” “crab” “cat” 1. Rephrasing what we want: ˆ y(x) = yiguana = 0 1 ! 0 0 ! 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Find x such that: 2. Deﬁning the loss function L( ˆ y, y) = 1 2 ˆ y(W,b,x) −yiguana 2 2 Network (pretrained on ImageNet) L( ˆ y, y) 3. Optimize the image ∂L ∂x After many iterations x = x −α ∂L ∂x x x = xcat And: +λ x −xcat 2 2 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri 92% Cat 94% Iguana I. A. How to build adversarial examples and attack a network? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Alexey Kurakin, Ian J. Goodfellow, Samy Bengio (2017): Adversarial examples in the physical world] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri • Train on correctly labelled adversarial examples y = cat Lnew = L(W,b,x, y) + λL(W,b,xadv, y) • Adversarial training I. B. How to defend against adversarial examples? Solution 1 Solution 2 Do neural networks actually understand the data? x = • Adversarial logit pairing Lnew = L(W,b,x, y) + λ f (x;W,b) −f (xadv;W,b) 2 2 [Harini Kannan et al. (2018): Adversarial Logit Pairing] Types of attacks: • Non-targeted attacks • Targeted attacks Knowledge of the attacker: • White-box • Black-box Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II. Generative Adversarial Networks (GANs) A. Motivation B. G/D Game C. Practical tips to train/evaluate GANs D. Interesting results [Ian J. Goodfellow, Jonathon Shlens & Christian Szegedy (2015): Explaining and harnessing adversarial examples] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.A - Motivation Motivation: endowing computers with an understanding of our world. Goal: collect a lot of data, use it to train a model to generate similar data from scratch. Intuition: number of parameters of the model << amount of data [The Gan Zoo (2017)] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.A - Motivation Probability distributions: Samples from the “true data distribution” [Andrej Karpathy et al. (2016): Generative Models, OpenAI blog] “true data distribution” Image space “generated distribution” Image space Matching distributions Image space Goal Samples from the “generated distribution” Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.B - G/D Game How can we train G to generate images from the true data distributions? ≠ [Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas (2017): StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks] Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ 100-d random code (64,64,3) generated image z Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ 100-d random code (64,64,3) generated image Real images (database) Discriminator “D” (Neural Network) x z y = 0 if x = G(z) y = 1 otherwise ⎧ ⎨ ⎪ ⎩ ⎪ Binary classiﬁcation Gradients Run Adam simultaneously on two minibatches (true data / generated data) II.B - G/D Game Probability distributions Image space Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.B - G/D Game Discriminator “D” (Neural Network) Binary classiﬁcation y = 0 if x = G(z) y = 1 otherwise ⎧ ⎨ ⎪ ⎩ ⎪ Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ 100-d random code (64,64,3) generated image Real images (database) x z Gradients End goal: G is outputting images that are indistinguishable from real images for D Probability distribution Image space Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Training procedure, we want to minimize: J ( D) = − 1 mreal yreal (i) .log(D(x(i))) i=1 mreal ∑ ! "" #### $ #### −1 mgen (1−ygen (i) ).log(1−D(G(z(i)))) i=1 mgen ∑ ! "" ###### # $ ###### # cross-entropy 1: “D should correctly label real data as 1” cross-entropy 2: “D should correctly label generated data as 0” J (G) = −J ( D) = 1 mgen log(1−D(G(z(i)))) i=1 mgen ∑ “G should try to fool D: by minimizing the opposite of what D is trying to minimize” II.B - G/D Game • The loss of the discriminator • The loss of the generator yreal ygen ⎧ ⎨ ⎪ ⎩ ⎪ Labels: is always 1 is always 0 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.C - Training GANs Saturating cost for the generator: Non-saturating cost Saturating cost -20 J (G) 5 0 1 D(G(z)) 0 J (G) = 1 mgen log(1−D(G(z(i)))) i=1 mgen ∑ J (G) = −1 mgen log(D(G(z(i)))) i=1 mgen ∑ [Ian Goodfellow (2014): NIPS Tutorial: GANs] min 1 mgen log(1−D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⇔max 1 mgen log(D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⇔min −1 mgen log(D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Note that: New training procedure, we want to minimize: J (G) = −1 mg log(D(G(z(i)))) i=1 mg ∑ “G should try to fool D: by minimizing this” II.C - Training GANs min 1 mgen log(1−D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⇔max 1 mgen log(D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ ⇔min −1 mgen log(D(G(z(i)))) i=1 mgen ∑ ⎡ ⎣ ⎢ ⎢ ⎤ ⎦ ⎥ ⎥ J ( D) = − 1 mreal yreal (i) .log(D(x(i))) i=1 mreal ∑ ! "" #### $ #### −1 mgen (1−ygen (i) ).log(1−D(G(z(i)))) i=1 mgen ∑ ! "" ###### # $ ###### # cross-entropy 1: “D should correctly label real data as 1” cross-entropy 2: “D should correctly label generated data as 0” Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Simultaneously training G/D? [Ian Goodfellow (2014): NIPS Tutorial: GANs] Non-saturating cost Saturating cost -20 J (G) 5 0 1 D(G(z)) 0 J (G) = 1 mg log(1−D(G(z(i)))) i=1 mg ∑ J (G) = −1 mg log(D(G(z(i)))) i=1 mg ∑ for num_iterations: for k iterations: update D update G II.C - Training GANs Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri BatchNorm with GANs: Generated images (batch 1) Generated images (batch 2) II.C - Training GANs Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri BatchNorm with GANs: Generator “G” (Neural Network) 0.12 0.92 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 1 (64,64,3) generated image 1 Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 2 (64,64,3) generated image 2 Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 2 (64,64,3) generated images 2 0.12 0.92 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 1 1 Assume no batchnorm Assume batchnorm II.C - Training GANs Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri µB = 1 m+1 z(k ) + r(i) i=1 m ∑ ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ σ B 2 = 1 m+1 (z(k ) −µB)2 + (r(i) − i=1 m ∑ µB)2 ⎛ ⎝ ⎜ ⎞ ⎠ ⎟ znorm (k ) = z(k ) −µB σ B 2 + ε ! z(k ) = γ znorm (k ) + β BatchNorm with GANs: BatchNorm Reference BatchNorm Virtual BatchNorm Z ={z(1),....,z(m)} µB = 1 m z(i) i=1 m ∑ σ B 2 = 1 m (z(i) − i=1 m ∑ µB)2 znorm (i) = z(i) −µB σ B 2 + ε ! z(i) = γ znorm (i) + β R ={r(1),....,r(m)} Z ={z(1),....,z(m)} µB = 1 m r(i) i=1 m ∑ σ B 2 = 1 m (r(i) − i=1 m ∑ µB)2 znorm (i) = z(i) −µB σ B 2 + ε ! z(i) = γ znorm (i) + β R ={r(1),....,r(m)} Z ={z(1),....,z(m)} For k = 1….m II.C - Training GANs Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap: GANs’ training tips • Use the non-saturated cost function • Keep D up-to-date with respect to G (k update for D / 1 update for G) • Use Virtual Batchnorm • (not presented but important) One-sided label smoothing And a lot more, GANs are hard to train! II.C - Training GANs Non- Satura - J (G) 5 0 1 D(G(z)) 0 J (G) = 1 mg log(1−D(G(z(i)))) i=1 mg ∑ J (G) = −1 mg log(D(G(z(i)))) i=1 mg ∑ [Soumith et al. (2016): GanHacks] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II.D - Interesting results Operation on codes [Radford et al. (2015): UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS] Generator “G” (Neural Network) 0.12 0.92 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 1 (64,64,3) generated image 1 Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 2 (64,64,3) generated image 2 Generator “G” (Neural Network) 0.42 0.07 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 3 (64,64,3) generated image 2 0.12 0.92 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 1 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 2 0.42 0.07 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Code 3 Generator “G” (Neural Network) - + Man with glasses - man + woman = woman with glasses Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Image Generation: II.D - Interesting results Samples from the “generated distribution” [Zhang et al. (2017): StackGAN++] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Pix2Pix: https://afﬁnelayer.com/pixsrv/ II.D - Interesting results [Isola et al. (2017): Image-to-Image Translation with Conditional Adversarial Networks] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Super-resolution image: II.D - Interesting results [Ledig et al. (2016): Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CycleGANs: [Jun-Yan Zhu et al. (2017): Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks] II.D - Interesting results Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CycleGANs: Face2ramen https://hardikbansal.github.io/CycleGANBlog/ [Jun-Yan Zhu et al. (2017): Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks] II.D - Interesting results Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Announcements For Tuesday 05/01, 9am: C2M3 • Quiz: Hyperparameter tuning, Batch Normalization, Programming Frameworks • Programming assignment: Tensorﬂow C3M1 and C3M2 • Quiz: Bird recognition in the city of Peacetopia (case study) • Quiz: Autonomous driving (case study) For Friday 02/16, 9am: • Hands-on session this Friday Meet with your mentor (TA), you’ll receive a Calendly invite. Check out the project example code! (cs230-stanford.github.io) "
506,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230: Lecture 5 Case Study Kian Katanforoosh Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Problem statement: Live-Cell Detection Goal: determining which parts of a microscope image corresponds to which individual cells. Data: Doctors have collected 100,000 images from microscopes and gave them to you. Images have been taken from three types of microscopes: Type A 50,000 images Type B 25,000 images Type C 25,000 images Question: The doctors who hired you would like to use your algorithm on images from microscope C. How you would split this dataset into train, dev and test sets? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Question: The doctors who hired you would like to use your algorithm on images from microscope C. How you would split this dataset into train, dev and test sets? Answer: i) Split has to be roughly 90,5,5. Not 60,20,20. ii) Distribution of dev and test set have to be the same (contain images from C ). iii) There should be C images in the training as well, more than in the test/dev set. Question: Can you augment this dataset? If yes, give only 3 distinct methods you would use. If no, explain why (give only 2 reasons). Answer: Many augmentation methods would work in this case: – cropping – adding random noise – changing contrast, blurring. – flip – rotate Data Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Question: - What is the mathematical relation between nx and ny? - What’s the last activation of your network? - What loss function should you use? Answer: i) nx = 3 × ny ii) Sigmoid activation iii) Summation over all pixel value with cross entropy loss. Architecture and Loss Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Transfer Learning First try: You have coded your neural network (model M1) and have trained it for 1000 epochs. It doesn’t perform well. Transfer Learning: One of your friends suggested to use transfer learning using another labeled dataset made of 1,000,000 microscope images for skin disease classification (very similar images). A model (M2) has been trained on this dataset on a 10-class classification. Here is an example of input/output of the model M2. Question: You perform transfer learning from M2 to M1, what are the new hyperparameters that you’ll have to tune? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Question: You perform transfer learning from M2 to M1, what are the new hyperparameters that you’ll have to tune? Transfer Learning Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Network modification Question: How can you correct your model and/or dataset to satisfy the doctors’ request? Answer: Modify the dataset in order to label the boundaries between cells. On top of that, change the loss function to give more weight to boundaries or penalize false positives. Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Network modification Question: Given an image classified as 1 (cancer present), how can you figure out based on which cell(s) the model predicted 1? Answer: Gradient of output w.r.t. input X New goal: They give you a dataset containing images similar to the previous ones. The difference is that each image is labeled as 0 (there are no cancer cells on the image) or 1 (there are cancer cells on the image). You easily build a state-of-the-art model to classify these images with 99% accuracy. The doctors are astonished and surprised, they ask you to explain your network’s predictions. Question: Your model detects cancer on cells (test set) images with 99% accuracy, while a doctor would on average perform 97% accuracy on the same task. Is this possible? Explain. Answer: If the dataset was entirely labeled by this one doctor with 97% accuracy, it is unlikely that the model can perform at 99% accuracy. However if annotated by multiple doctors, the network will learn from these several doctors and be able to outperform the one doctor with 97% accuracy. In this case, a panel composed of the doctors who labeled the data would likely perform at 99% accuracy or higher. Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Duties for next week For next Tuesday 05/08, 9am: C4M1 • Quiz: The basics of ConvNets • Programming Assignment: Convolutional Neural Network - Step by Step • Programming Assignment: Convolutional Neural Network - Application C4M2 • Quiz: Convolutional models • Programming Assignment: Keras Tutorial (optional, but highly recommended) • Programming Assignment: Residual Networks Midterm, on 05/11: everything up to C4M2 (included) and next Tuesday’s in-class lecture can be expected. This Friday (05/04): • (optional) Hands-on TA session: GPU / Practical project advice "
507,Lecture 6: ML Project Strategy Menti Code: 12 07 68         
508,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230: Lecture 7 Interpretability of Neural Networks Kian Katanforoosh Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - What is the role of a given neuron/ﬁlter/layer? - Can we check what the network focuses on given an input image? - How does a neural network see our world? Does it actually understand it? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Image segmentation II. The deconvolution III. Interpreting and visualizing Neural Networks A. With deconvolutions B. With occlusion sensitivity C. With gradient ascent (class model visualization) D. With saliency maps E. With class activation maps (Global Average Pooling) IV. Deep Dream: going deeper in NNs Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Image segmentation Goal: Segment cells on a picture. In other words, every pixel of the image, indicate if it is part of a cell (1) or not (0). 1. Data? image label 2. Input? 3. Output? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Architecture? Encoding Convolutions (reduces volume height and width) De-convolutions (increases volume height and width) Information Encoded Input image (400, 400, 3) Per-Pixel Classiﬁcation (400, 400, 1) I. Image segmentation Loss? Logistic loss per pixel L (i) = y(i) log( ˆ y(i)) + (1−y(i))log(1−ˆ y(i)) pixels ∑ Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Stanford Drone Dataset: A. Robicquet, A. Sadeghian, A. Alahi, S. Savarese, Learning Social Etiquette: Human Trajectory Prediction In Crowded Scenes in European Conference on Computer Vision (ECCV), 2016 If the problem is multi-class Encoding Convolutions (reduces volume height and width) De-convolutions (increases volume height and width) Per-Pixel Classiﬁcation (600, 400, 1) Information Encoded Input image (600, 400, 3) I. Image segmentation 0 1 0 ! 0 0 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ L (i) = y(i) log( ˆ y(i)) pixels ∑ Cross entropy per pixel 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ p(“building”) p(“grass”) p(“road”) … Let’s understand deconvolutions Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Wenzhe Shi, et al. : Is the deconvolution layer the same as a convolutional layer?] Create sub-pixel image 255 134 202 22 123 94 2 4 11 3 22 192 12 4 23 34 Input (4x4) 0 0 0 0 0 0 0 0 0 0 255 0 134 0 202 0 22 0 0 0 0 0 0 0 0 0 0 0 123 0 94 0 2 0 4 0 0 0 0 0 0 0 0 0 0 0 11 0 3 0 22 0 192 0 0 0 0 0 0 0 0 0 0 0 12 0 4 0 23 0 34 0 0 0 0 0 0 0 0 0 0 Sub-pixel image (9x9) Output (6x6) Convolve Stride = 1 * This allows us to upsample an encoding into an image. Encoding CONV DECONV [Vincent Dumoulin and Francesco Visin : A guide to convolution arithmetic for deep learning] II. The deconvolution [Matthew Zeiler et al.: Deconvolutional Networks] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II. The deconvolution [Vincent Dumoulin and Francesco Visin : A guide to convolution arithmetic for deep learning] http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#no-zero-padding-unit-strides-transposed Convolution “Deconvolution” w 11 w 12 w 13 0 w21 w22 w23 0 w31 w32 w33 0 0 0 0 0 0 w 11 w 12 w 13 0 w21 w22 w23 0 w31 w32 w33 0 0 0 0 0 0 0 0 w 11 w 12 w 13 0 w21 w22 w23 0 w31 w32 w33 0 0 0 0 0 0 w 11 w 12 w 13 0 w21 w22 w23 0 w31 w32 w33 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ A Convolution can be seen as a simple dot product. Flattened 4x4 input Matrix “faking” the 3x3 window convolved around the 4x4 input. 4x4 input 2x2 output 2x2 input (+pad) 4x4 output Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Generator “G” (Neural Network) 0.47 0.19 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ 100-d random code (64,64,3) generated image Real images (database) Discriminator “D” (Neural Network) x z y = 0 if x = G(z) y = 1 otherwise ⎧ ⎨ ⎪ ⎩ ⎪ Binary classiﬁcation DECONV [Ian Goodfellow (2014): NIPS Tutorial: GANs] II. The deconvolution These deconvolutions will help us better understand and interpret neural networks. Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Image segmentation II. The deconvolution III. Interpreting and visualizing Neural Networks A. With deconvolutions B. With occlusion sensitivity C. With gradient ascent (class model visualization) D. With saliency maps E. With class activation maps (Global Average Pooling) IV. Deep Dream: going deeper in NNs Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - What is the role of a given neuron/ﬁlter/layer? - Can we check what the network focuses on given an input image? - How does a neural network see our world? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Interpretability of Neural Networks a1 [1] a2 [1] a3 [1] a4 [1] a1 [2] a2 [2] a3 [2] a1 [3] x1 (i) x2 (i) x3 (i) x4 (i) ˆ y(i) First problem: Fully-connected layers’ neurons lack spatial information convolve stack ﬁlter outputs input volume (n_H_prev, n_W_prev, n_C_prev) First ﬁlter output (n_H, n_W) Second ﬁlter output (n_H, n_W) n_C = 2 = #ﬁlters Filter 1 (f, f, n_C_prev) Filter 2 (f, f, n_C_prev) output volume (n_H, n_W, n_C) III. Interpretability of Neural Networks convolve stack ﬁlter outputs input volume (n_H_prev, n_W_prev, n_C_prev) First ﬁlter output (n_H, n_W) Second ﬁlter output (n_H, n_W) n_C = 2 = #ﬁlters Filter 1 (f, f, n_C_prev) Filter 2 (f, f, n_C_prev) output volume (n_H, n_W, n_C) III. Interpretability of Neural Networks stack ﬁlter outputs convolve input volume (n_H_prev, n_W_prev, n_C_prev) First ﬁlter output (n_H, n_W) Second ﬁlter output (n_H, n_W) n_C = 2 = #ﬁlters Filter 1 (f, f, n_C_prev) Filter 2 (f, f, n_C_prev) output volume (n_H, n_W, n_C) III. Interpretability of Neural Networks stack ﬁlter outputs convolve input volume (n_H_prev, n_W_prev, n_C_prev) First ﬁlter output (n_H, n_W) Second ﬁlter output (n_H, n_W) n_C = 2 = #ﬁlters Filter 1 (f, f, n_C_prev) Filter 2 (f, f, n_C_prev) output volume (n_H, n_W, n_C) III. Interpretability of Neural Networks Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CONV (5 layers) Input image (64,64,3) Encoding volume (13,13,256) 13 13 256 values Consider a CNN trained on ImageNet (classiﬁcation), we look at the output of the 5th CONV layer III. Interpretability of Neural Networks Conclusion: the deeper the activation, the more it “sees” from the image Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - What is the role of a given neuron/ﬁlter/layer? - Can we check what the network focuses on given an input image? - How does a neural network see our world? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Given a ﬁlter, what examples in the dataset lead to a strongly activated feature map? III. Interpretability of Neural Networks ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ (5,5,256) Input image (256,256,3) Only one feature maps (among 256) is displayed here : Top 5 images It seems that the ﬁlter has learned to detect shirts Here is our CNN, trained on ImageNet (1.3m images, 1000 classes), we’re trying to interpret it. Top 5 images It seems that the ﬁlter has learned to detect edges Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - What is the role of a given neuron/ﬁlter/layer? - Search dataset images maximizing the activation - Can we check what the network focuses on given an input image? - How does a neural network see our world? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX Here is our CNN, trained on ImageNet (1.3m images, 1000 classes), we’re trying to interpret it. 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ UNPOOL ReLU DECONV UNPOOL ReLU DECONV UNPOOL ReLU DECONV • Keep the max activation of a feature map • Set all other activations of the layer to 0 (5,5,256) Input image (256,256,3) [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] III. A. Interpreting NNs using deconvolutions Reconstruction Only one feature maps (among 256) is displayed here : Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri 0 1 12 1 6 -1 2 3 42 3 0 1 8 1 3 7 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 6 12 42 7 UNPOOL Switches 6 12 42 7 MAXPOOL III. A. Interpreting NNs using deconvolutions 0 0 12 0 6 0 0 0 42 0 0 0 0 0 0 7 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX Here is our CNN, trained on ImageNet (1.3m images, 1000 classes) 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Input image (256,256,3) UNPOOL ReLU DECONV UNPOOL ReLU DECONV UNPOOL ReLU DECONV Switches Switches Filters Switches Filters Filters • Keep the max activation of a feature map • Set all other activations of the layer to 0 (5,5,256) Reconstruction [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] III. A. Interpreting NNs using deconvolutions Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX Other CONV layers can be visualized the same way 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ (13,13,128) (5,5,256) (20,20,128) UNPOOL ReLU DECONV UNPOOL ReLU DECONV … … UNPOOL ReLU DECONV … Input image (256,256,3) Reconstruction [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] III. A. Interpreting NNs using deconvolutions Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Results on a validation set of 50,000 images Filters • Top-9 strongest activations per ﬁlter in the 1st layer • Because we know the position of the activation and all the pooling switches we can crop the part of the image that ﬁred the activation Patches [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] Layer 1 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Filters Layer 2 reconstructions • Learning a more complex set of patterns than 1st layer edges • Covers a much larger space of the image because of the pooling layer before. • Top-1 strongest activation per feature map in the 2nd layer (256 feature maps) [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] Results on a validation set of 50,000 images Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Filters • Learning a more complex set of patterns than 1st layer edges • Covers a much larger space of the image probably because of the pooling layer before. • Features are more invariant to small changes. Ex: A dot, spiral, circle all ﬁre the same 2nd layer feature very strongly [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] Results on a validation set of 50,000 images Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Filters • 3rd layer: increased complexity • An activated neuron is seeing ≈80x80 part of a 256x256 image • Learning objects, faces etc.. [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] Results on a validation set of 50,000 images Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri • 3rd layer: increased complexity • An activated neuron is seeing ≈80x80 part of a 256x256 image • Learning objects, faces etc.. • Patches: Semantic grouping, not structural Faces Clouds [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] Results on a validation set of 50,000 images Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Top 5 images ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ (5,5,256) Input image (256,256,3) Only one feature maps (among 256) is displayed here : Here is our CNN, trained on ImageNet (1.3m images, 1000 classes), we’re trying to interpret it. III. C. Interpreting NNs using gradient ascent (class model visualization) The ﬁlter might be detecting wrinkles, not shirts! Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - What is the role of a given neuron/ﬁlter/layer? - Search dataset images maximizing the activation - Deconvolutions can help visualize the role of a neuron - Can we check what the network focuses on given an input image? - How does a neural network see our world? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri In classiﬁcation, is the model identifying the location of an object in the image, or just the surrounding context? Indicates low conﬁdence on the true class for the corresponding position of the grey square Indicates high conﬁdence on the true class for the corresponding position of the grey square Probability map of the true class for different positions of the grey square [Matthew D. Zeiler and Rob Fergus (2013): Visualizing and Understanding Convolutional Networks] III. B. Interpreting NNs using occlusion sensitivity Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - Occlusion sensitivity - What is the role of a given neuron/ﬁlter/layer? - Deconvolutions can help visualize the role of a neuron - Search dataset images maximizing the activation - Can we check what the network focuses on given an input image? - Occlusion sensitivity - How does a neural network see our world? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. C. Interpreting NNs using gradient ascent (class model visualization) [Karen Simonyan et al. (2014): Deep Inside Convolutional Networks: Visualising Image Classiﬁcation Models and Saliency Maps] ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL CONV ReLU MAX POOL SOFTMAX Given this trained ConvNet and a class of interest (dog), generate an image which is representative of the class according to the ConvNet 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Input image x (256,256,3) Keep the weights ﬁxed and use gradient ascent on the input image to maximize this loss : L = Sdog(x) −λ x 2 2 x = x +α ∂L ∂x Gradient ascent: “x should look natural” Repeat this process: 1. Forward propagate image x 2. Compute the objective L 3. Backpropagate to get dL/dx 4. Update x’s pixels with gradient ascent Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Karen Simonyan et al. (2014): Deep Inside Convolutional Networks: Visualising Image Classiﬁcation Models and Saliency Maps] We can do this for all classes: Looks better with additional regularization methods. [Jason Yosinski et al. (2015): Understanding Neural Networks Through Deep Visualization] Regularizer: L2 III. C. Interpreting NNs using gradient ascent (class model visualization) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri This method can be applied to any activation in the network in order to interpret what a neuron is detecting On the class score: On any activation: L = Sdog(x) −R(x) L = a j [l](x) −R(x) [Jason Yosinski et al. (2015): Understanding Neural Networks Through Deep Visualization] change to III. C. Interpreting NNs using gradient ascent (class model visualization) [Jason Yosinski et al. (2015): Understanding Neural Networks Through Deep Visualization] [Link to video: https://www.youtube.com/watch?v=AgkfIQ4IGaM] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - Occlusion sensitivity - What is the role of a given neuron/ﬁlter/layer? - Deconvolutions can help visualize the role of a neuron - Search dataset images maximizing the activation - Gradient ascent (class model visualization) - Can we check what the network focuses on given an input image? - Occlusion sensitivity - How does a neural network see our world? - Gradient ascent (class model visualization) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. D. Interpreting NNs using saliency maps Rank the pixels of x based on their inﬂuence on the score S of the class. indicates which pixels need to be changed the least to affect the class score the most. ∂Sdog(x) ∂x ∂Sdog(x) ∂x Can be used for segmentation? Given: 1. Input image x CNN 3. Convolutional Network (Trained) 2. A class c “dog” CNN Sdog(x) backpropagation [Karen Simonyan et al. (2014): Deep Inside Convolutional Networks: Visualising Image Classiﬁcation Models and Saliency Maps] Yes Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - Occlusion sensitivity - What is the role of a given neuron/ﬁlter/layer? - Deconvolutions can help visualize the role of a neuron - Search dataset images maximizing the activation - Gradient ascent (class model visualization) - Can we check what the network focuses on given an input image? - Occlusion sensitivity - Saliency maps (one-time gradient ascent) - How does a neural network see our world? - Gradient ascent (class model visualization) - Do these visualization have use cases? - Segmentation (saliency maps) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri ZERO PAD CONV ReLU MAX POOL Flatten FC (x3) input output CONV ReLU MAX POOL SOFTMAX 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ … III. E. Interpreting NNs using class activation maps Using a classiﬁcation network for localization Converted to: [Bolei Zhou et al. (2016): Learning Deep Features for Discriminative Localization] ZERO PAD CONV ReLU MAX POOL Global Average Pooling FC input output CONV ReLU MAX POOL SOFTMAX 0.02 0.93 0.04 ! 0.07 0.11 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ … Why this? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri 83 9 93 11 123 94 83 83 34 44 187 93 34 76 232 92 55 33 0 22 123 94 83 1 34 44 187 30 34 76 232 1 255 134 93 18 123 94 83 29 34 44 187 22 34 76 232 192 III. E. Interpreting NNs using class activation maps 255 134 93 22 123 94 83 2 34 44 187 30 34 76 232 124 255 134 202 22 123 94 83 4 34 44 187 192 34 76 232 34 44 94 42 22 123 231 83 2 34 255 232 92 34 76 187 124 Volume (4,4,6) 93 1 124 22 134 Volume (1,1,6) 104.7 GAP Last CONV layer = a1 [1] a2 [1] a4 [1] a5 [1] a6 [1] a3 [1] [Bolei Zhou et al. (2016): Learning Deep Features for Discriminative Localization] wny−1,1 ∗ wny−1,5 ∗ wny−1,6 ∗ + + + … Input image wny−1,1 wny−1,2 wny−1,3 wny−1,4 wny−1,5 wny−1,6 a1 [1] a2 [1] any−1 [1] any [1] Softmax = Class activation map for “dog"" 0.02 0.03 0.04 ! 0.07 0.91 0.09 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ “grass” “car” “human” “bed” “dog” “sky” Probas Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. B. Deconvolutional layer [Bolei Zhou et al. (2016): Learning Deep Features for Discriminative Localization] Source video: Kyle McDonald Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - Occlusion sensitivity - Class Activation Maps - What is the role of a given neuron/ﬁlter/layer? - Deconvolutions can help visualize the role of a neuron - Search dataset images maximizing the activation - Gradient ascent (class model visualization) - Can we check what the network focuses on given an input image? - Occlusion sensitivity - Saliency maps (one-time gradient ascent) - Class Activation Maps - How does a neural network see our world? - Gradient ascent (class model visualization) - Do these visualization have use cases? - Segmentation (saliency maps) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Deep Dream input How to boost the activation of a neuron? [Alexander Mordvintsev et al. (2015): Inceptionism: Going Deeper into Neural Networks] ZERO PAD CONV ReLU MAX POOL CONV ReLU MAX POOL … Activations (5,5,256) Dreaming process (to repeat): 1. Forward propagate image until dreaming layer 2. Set gradients of dreaming layer to be equal to its activations 3. Backpropagate gradients to input image 4. Update Pixels of the image Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Alexander Mordvintsev et al. (2015): Inceptionism: Going Deeper into Neural Networks] III. Deep Dream Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri [Alexander Mordvintsev et al. (2015): Inceptionism: Going Deeper into Neural Networks] If you dream in lower layers: [Github repo (deepdream): https://github.com/google/deepdream/] III. Deep Dream Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Deep Dream [Alexander Mordvintsev et al. (2015): Inceptionism: Going Deeper into Neural Networks] [Github repo (deepdream): https://github.com/google/deepdream/] [Link to video: https://www.youtube.com/watch?v=DgPaCWJL7XI] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Did the neural network learned the right features to detect an object? III. Deep Dream Dumbbells the network failed to understand the essence of a dumbbell [Alexander Mordvintsev et al. (2015): Inceptionism: Going Deeper into Neural Networks] [Github repo (deepdream): https://github.com/google/deepdream/] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Questions we will try to answer: - What part of the input is responsible for the output? - Occlusion sensitivity - Class Activation Maps - What is the role of a given neuron/ﬁlter/layer? - Deconvolutions can help visualize the role of a neuron - Search dataset images maximizing the activation - Gradient ascent (class model visualization) - Can we check what the network focuses on given an input image? - Occlusion sensitivity - Saliency maps (one-time gradient ascent) - Class Activation Maps - How does a neural network see our world? - Gradient ascent (class model visualization) - Deep Dream - Do these visualization have use cases? - Segmentation (saliency maps) - Art (Deep Dream) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Announcements For Tuesday 05/15, 9am: C5M1 • Quiz: Recurrent Neural Networks • Programming Assignment: • Building a Recurrent Neural Network - Step by Step • Dinosaur Land -- Character-level Language Modeling • Jazz improvisation with LSTM For this Friday (05/18): • Project Milestone due. (we expect running code.) • TA Section this Friday Check out the project example code (cs230-stanford.github.io) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Question for me: - What visualization methods to use and why? My answer: CAMs and Saliency mast are the quickest and most common, DeconvNets is hard to implement, and gradient ascent is used for adversarial examples and art mostly. - What are the next steps in interpreting a neural network? - Class activation maps vs. Saliency maps? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. B. Deconvolutional layer [Wenzhe Shi, et al. : Is the deconvolution layer the same as a convolutional layer?] 0 0 0 0 0 0 0 0 0 0 255 0 134 0 202 0 22 0 0 0 0 0 0 0 0 0 0 0 123 0 94 0 2 0 4 0 0 0 0 0 0 0 0 0 0 0 11 0 3 0 22 0 192 0 0 0 0 0 0 0 0 0 0 0 12 0 4 0 23 0 34 0 0 0 0 0 0 0 0 0 0 255 134 202 22 123 94 2 4 11 3 22 192 12 4 23 34 Create sub-pixel image Input (4x4) Sub-pixel image (9x9) Output (6x6) Convolve Stride = 1 * Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri http://cnnlocalization.csail.mit.edu/ VIDEO to motivate We have seen convNets, how do you think we can do this visualization in practice? Start with AlexNet, NetinNet, GoogLeNet. Add GAP + SOFTMAX FC Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri It should be noted that we used the (unnormalised) class scores Sc, rather than the class posteriors, returned by the soft-max layer: Pc = Pexp Sc c exp Sc . The reason is that the maximisation of the class posterior can be achieved by minimising the scores of other classes. Therefore, we optimise Sc to ensure that the optimisation concentrates only on the class in question c. We also experimented with optimising the posterior Pc, but the results were not visually prominent, thus conﬁrming our intuition. Saliency maps https://arxiv.org/pdf/1312.6034.pdf You can now convert a classiﬁcation algorithm in a localization also using saliency maps (weak supervision). Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Despite this encouraging progress, there is still little insight into the internal operation and behavior of these complex models, or how they achieve such good performance. From a scientiﬁc standpoint, this is deeply unsatisfactory. Without clear understanding of how and why they work, the development of better models is reduced to trial-and-error https://arxiv.org/pdf/1311.2901.pdf reveals the input stimuli that excite individual feature maps at any layer in the model. It also allows us to observe the evolution of features during training and to diagnose potential problems with the model. to project the feature activations back to the input pixel space. Our approach, by contrast, provides a non-parametric view of invariance, showing which patterns from the training set activate the feature map. To examine a convnet, a deconvnet is attached to each of its layers, as illustrated in Fig. 1(top), providing a continuous path back to image pixels Motivation: Earlier in the course we talk about the fact that early layers detect edges while later layers detect more complex patterns. We did not talk about how to visualize these. The number of activations is often not equal to the number of pixels in the input image. How can we project a small volume back to the input image? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Deep Dream We can dream at any layer in the convnet We forwarded the image in the network up to a certain layer, Now we set the gradients of this to be equal to its activations, then we do backward from that point back to the image. Then update the image pixels. You always dream after Relu units so these activations are zero cropped. Your are amplifying the features that maximally activated the network. We will ﬁnd the image update that will boost the strong activations of the network "
509,"Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri CS230: Lecture 9 Deep Reinforcement Learning Kian Katanforoosh Menti code: 80 24 08 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation II. Recycling is good: an introduction to RL III. Deep Q-Networks IV. Application of Deep Q-Network: Breakout (Atari) V. Tips to train Deep Q-Network VI. Advanced topics Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation Human Level Control through Deep Reinforcement Learning AlphaGo [Mnih et al. (2015): Human Level Control through Deep Reinforcement Learning] [Silver et al. (2017): Mastering the game of Go without human knowledge] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation Why RL? • Delayed labels • Making sequences of decisions What is RL? • Automatically learn to make good sequences of decision Examples of RL applications Robotics Advertisement Games Source: https://deepmind.com/blog/alphago- zero-learning-scratch/ Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation II. Recycling is good: an introduction to RL III. Deep Q-Networks IV. Application of Deep Q-Network: Breakout (Atari) V. Tips to train Deep Q-Network VI. Advanced topics Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II. Recycling is good: an introduction to RL Problem statement START Goal: maximize the return (rewards) Agent’s Possible actions: Deﬁne reward “r” in every state +2 0 0 +1 +10 Number of states: 5 Types of states: initial normal terminal State 1 State 2 (initial) State 3 State 4 State 5 How to deﬁne the long-term return? Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... Best strategy to follow if γ = 1 Additional rule: garbage collector coming in 3min, it takes 1min to move between states Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II. Recycling is good: an introduction to RL What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Q-table how good is it to take action 1 in state 2 S2 S1 S3 S2 S4 S3 S5 +2 +0 How? +1 +0 +10 +0 Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 Problem statement Deﬁne reward “r” in every state Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri II. Recycling is good: an introduction to RL What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ S2 S1 S3 S2 S4 S3 S5 +2 +0 +0 +0 + 10 How? +10 (= 1 + 10 x 0.9) Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 how good is it to take action 1 in state 2 Problem statement Deﬁne reward “r” in every state Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... Q-table S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ S2 S1 S3 S2 S4 S3 S5 +0 +0 How? +10 + 9 (= 0 + 0.9 x 10) Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 how good is it to take action 1 in state 2 + 10 (= 1 + 10 x 0.9) +2 Problem statement Deﬁne reward “r” in every state II. Recycling is good: an introduction to RL Q-table Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ S2 S1 S3 S2 S4 S3 S5 +2 +0 How? +10 Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 how good is it to take action 1 in state 2 + 9 (= 0 + 0.9 x 10) + 9 (= 0 + 0.9 x 10) + 10 (= 1 + 10 x 0.9) Problem statement Deﬁne reward “r” in every state II. Recycling is good: an introduction to RL Q-table Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ S2 S1 S3 S2 S4 S3 S5 +2 +0 How? +10 Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 how good is it to take action 1 in state 2 + 9 (= 0 + 0.9 x 10) + 9 (= 0 + 0.9 x 10) + 10 (= 1 + 10 x 0.9) + 8.1 (= 0 + 0.9 x 9) Problem statement Deﬁne reward “r” in every state II. Recycling is good: an introduction to RL Q-table Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What do we want to learn? #actions #states Q = Q11 Q21 Q31 Q41 Q51 Q12 Q22 Q32 Q42 Q52 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ S2 S1 S3 S2 S4 S3 S5 +2 How? +10 Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 how good is it to take action 1 in state 2 + 9 (= 0 + 0.9 x 10) + 9 (= 0 + 0.9 x 10) + 10 (= 1 + 10 x 0.9) + 8.1 (= 0 + 0.9 x 9) + 8.1 (= 0 + 0.9 x 9) Problem statement Deﬁne reward “r” in every state II. Recycling is good: an introduction to RL Q-table Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What do we want to learn? S2 S1 S3 S2 S4 S3 S5 +2 How? +10 Assuming γ = 0.9 START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 + 9 (= 0 + 0.9 x 10) + 9 (= 0 + 0.9 x 10) + 10 (= 1 + 10 x 0.9) + 8.1 (= 0 + 0.9 x 9) + 8.1 (= 0 + 0.9 x 9) Problem statement Deﬁne reward “r” in every state II. Recycling is good: an introduction to RL Discounted return R = γ tr t t=0 ∑ = r 0 +γ r 1 +γ 2r 2 +... S1 S2 S3 S4 S5 #actions #states Q = 0 2 8.1 9 0 0 9 10 10 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ how good is it to take action 1 in state 2 Q-table Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Best strategy to follow if γ = 0.9 What do we want to learn? #actions #states Q = 0 2 8.1 9 0 0 9 10 10 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Bellman equation (optimality equation) Q*(s,a) = r +γ max a' (Q*(s',a')) how good is it to take action 1 in state 2 When state and actions space are too big, this method has huge memory cost Policy π(s) = argmax a (Q*(s,a)) Function telling us our best strategy II. Recycling is good: an introduction to RL Q-table START +2 0 0 +1 +10 State 1 State 2 (initial) State 3 State 4 State 5 Problem statement Deﬁne reward “r” in every state Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri What we’ve learned so far: - Vocabulary: environment, agent, state, action, reward, total return, discount factor. - Q-table: matrix of entries representing “how good is it to take action a in state s” - Policy: function telling us what’s the best strategy to adopt - Bellman equation satisﬁed by the optimal Q-table Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation II. Recycling is good: an introduction to RL III. Deep Q-Networks IV. Application of Deep Q-Network: Breakout (Atari) V. Tips to train Deep Q-Network VI. Advanced topics Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Deep Q-Networks Main idea: ﬁnd a Q-function to replace the Q-table Neural Network Problem statement START State 1 State 2 (initial) State 3 State 4 State 5 #actions #states Q = 0 2 8.1 9 0 0 9 10 10 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Q-table s = 0 1 0 0 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ a1 [1] a2 [1] a3 [1] a4 [1] a3 [2] a1 [3] a1 [2] a2 [2] a1 [3] Q(s,→) Q(s,←) Then compute loss, backpropagate. How to compute the loss? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Deep Q-Networks s = 0 1 0 0 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ a1 [1] a2 [1] a3 [1] a4 [1] a3 [2] a1 [3] a1 [2] a2 [2] a1 [3] Q(s,→) Q(s,←) Loss function y = r →+γ max a' (Q(s→ next,a')) y = r ←+γ max a' (Q(s← next,a')) Target value Immediate reward for taking action in state s Immediate Reward for taking action in state s Discounted maximum future reward when you are in state s← next Discounted maximum future reward when you are in state s→ next Hold ﬁxed for backprop Hold ﬁxed for backprop L = (y −Q(s,←))2 Q(s,←) > Q(s,→) Case: Q(s,←) < Q(s,→) Case: Q*(s,a) = r +γ max a' (Q*(s',a')) [Francisco S. Melo: Convergence of Q-learning: a simple proof] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri III. Deep Q-Networks s = 0 1 0 0 0 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ a1 [1] a2 [1] a3 [1] a4 [1] a3 [2] a1 [3] a1 [2] a2 [2] a1 [3] Q(s,→) Q(s,←) Loss function (regression) y = r →+γ max a' (Q(s→ next,a')) y = r ←+γ max a' (Q(s← next,a')) Target value L = (y −Q(s,→))2 Q(s,←) > Q(s,→) Q(s,←) < Q(s,→) Case: Case: Backpropagation Compute and update W using stochastic gradient descent ∂L ∂W Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ DQN Implementation: - Initialize your Q-network parameters - Loop over episodes: - Start from initial state s - Loop over time-steps: - Forward propagate s in the Q-network - Execute action a (that has the maximum Q(s,a) output of Q-network) - Observe rewards r and next state s’ - Compute targets y by forward propagating state s’ in the Q-network, then compute loss. - Update parameters with gradient descent START State 1 State 2 (initial) State 3 State 4 State 5 Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation II. Recycling is good: an introduction to RL III. Deep Q-Networks IV. Application of Deep Q-Network: Breakout (Atari) V. Tips to train Deep Q-Network VI. Advanced topics Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri IV. Deep Q-Networks application: Breakout (Atari) Goal: play breakout, i.e. destroy all the bricks. input of Q-network Output of Q-network Q(s,←) Q(s,→) Q(s,−) ⎛ ⎝ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ Q-values Demo s = Would that work? https://www.youtube.com/watch?v=V1eYniJ0Rnk Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri IV. Deep Q-Networks application: Breakout (Atari) Goal: play breakout, i.e. destroy all the bricks. input of Q-network Demo Output of Q-network Q(s,←) Q(s,→) Q(s,−) ⎛ ⎝ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ Q-values https://www.youtube.com/watch?v=V1eYniJ0Rnk Preprocessing φ(s) s = What is done in preprocessing? - Convert to grayscale - Reduce dimensions (h,w) - History (4 frames) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri IV. Deep Q-Networks application: Breakout (Atari) input of Q-network φ(s) = Deep Q-network architecture? CONV ReLU CONV ReLU FC (RELU) FC (LINEAR) φ(s) Q(s,←) Q(s,→) Q(s,−) ⎛ ⎝ ⎜ ⎜ ⎜ ⎞ ⎠ ⎟ ⎟ ⎟ CONV ReLU Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ (+ preprocessing + terminal state) DQN Implementation: - Initialize your Q-network parameters - Loop over episodes: - Start from initial state s - Loop over time-steps: - Forward propagate s in the Q-network - Execute action a (that has the maximum Q(s,a) output of Q-network) - Observe rewards r and next state s’ - - Compute targets y by forward propagating state s’ in the Q-network, then compute loss. - Update parameters with gradient descent Some training challenges: - Keep track of terminal step - Experience replay - Epsilon greedy action choice (Exploration / Exploitation tradeoff) φ(s) φ(s) φ(s') φ(s) φ(s') - Use s’ to create Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ (+ preprocessing + terminal state) DQN Implementation: - Initialize your Q-network parameters - Loop over episodes: - Start from initial state s - Loop over time-steps: - Forward propagate s in the Q-network - Execute action a (that has the maximum Q(s,a) output of Q-network) - Observe rewards r and next state s’ - - Compute targets y by forward propagating state s’ in the Q-network, then compute loss. - Update parameters with gradient descent Some training challenges: - Keep track of terminal step - Experience replay - Epsilon greedy action choice (Exploration / Exploitation tradeoff) φ(s) φ(s) φ(s') φ(s) φ(s') - Use s’ to create Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ (+ preprocessing + terminal state) DQN Implementation: - Initialize your Q-network parameters - Loop over episodes: - Start from initial state s - Create a boolean to detect terminal states: terminal = False - Loop over time-steps: - Forward propagate s in the Q-network - Execute action a (that has the maximum Q(s,a) output of Q-network) - Observe rewards r and next state s’ - Use s’ to create - Check if s’ is a terminal state. Compute targets y by forward propagating state s’ in the Q-network, then compute loss. - Update parameters with gradient descent Some training challenges: - Keep track of terminal step - Experience replay - Epsilon greedy action choice (Exploration / Exploitation tradeoff) φ(s) φ(s) φ(s') φ(s') φ(s) if terminal = False : y = r +γ max a' (Q(s',a')) if terminal = True : y = r (break) ⎧ ⎨ ⎪ ⎩ ⎪ Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Replay memory (D) IV - DQN training challenges Experience replay Current method is to start from initial state s and follow: 1 experience (leads to one iteration of gradient descent) Experience Replay E1 E2 E3 E1 E1 Training: E1 E2 E3 Training: E1 sample(E1, E2) sample(E1, E2, E3) sample(E1, E2, E3, E4) … E2 E3 … E2 E3 Can be used with mini batch gradient descent φ(s) →a →r →φ(s') φ(s') →a' →r' →φ(s'') φ(s'') →a'' →r'' →φ(s''') ... Advantages of experience replay? Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ (+ experience replay) DQN Implementation: - Initialize your Q-network parameters - Initialize replay memory D - Loop over episodes: - Start from initial state - Create a boolean to detect terminal states: terminal = False - Loop over time-steps: - Forward propagate in the Q-network - Execute action a (that has the maximum Q( ,a) output of Q-network) - Observe rewards r and next state s’ - Use s’ to create - Add experience to replay memory (D) - Sample random mini-batch of transitions from D - Check if s’ is a terminal state. Compute targets y by forward propagating state in the Q-network, then compute loss. - Update parameters with gradient descent φ(s) φ(s) φ(s') φ(s') φ(s) (φ(s),a,r,φ(s')) The transition resulting from this is added to D, and will not always be used in this iteration’s update! Update using sampled transitions Some training challenges: - Keep track of terminal step - Experience replay - Epsilon greedy action choice (Exploration / Exploitation tradeoff) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Exploration vs. Exploitation S1 R = +0 R = +1 R = +1000 Q(S1,a1) = 0.5 Q(S1,a2) = 0.4 Q(S1,a3) = 0.3 Just after initializing the Q-network, we get: Initial state S2 a1 Terminal state S3 a2 Terminal state S4 a3 Terminal state Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Exploration vs. Exploitation S1 R = +0 S2 S3 S4 R = +1 R = +1000 Q(S1,a1) = 0.5 Q(S1,a2) = 0.4 Q(S1,a3) = 0.3 a1 a2 a3 Just after initializing the Q-network, we get: 0 Initial state Terminal state Terminal state Terminal state Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Exploration vs. Exploitation S1 R = +0 S2 S3 S4 R = +1 R = +1000 Q(S1,a1) = 0.5 Q(S1,a2) = 0.4 Q(S1,a3) = 0.3 a1 a2 a3 Just after initializing the Q-network, we get: 0 1 Initial state Terminal state Terminal state Terminal state Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Exploration vs. Exploitation S1 R = +0 S2 S3 S4 R = +1 R = +1000 Q(S1,a1) = 0.5 Q(S1,a2) = 0.4 Q(S1,a3) = 0.3 a1 a2 a3 Just after initializing the Q-network, we get: 0 1 Initial state Will never be visited, because Q(S1,a3) < Q(S1,a2) Terminal state Terminal state Terminal state Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Recap’ (+ epsilon greedy action) DQN Implementation: - Initialize your Q-network parameters - Initialize replay memory D - Loop over episodes: - Start from initial state - Create a boolean to detect terminal states: terminal = False - Loop over time-steps: - With probability epsilon, take random action a. - Otherwise: - Forward propagate in the Q-network - Execute action a (that has the maximum Q( ,a) output of Q-network). - Observe rewards r and next state s’ - Use s’ to create - Add experience to replay memory (D) - Sample random mini-batch of transitions from D - Check if s’ is a terminal state. Compute targets y by forward propagating state in the Q-network, then compute loss. - Update parameters with gradient descent φ(s) φ(s) φ(s') φ(s') φ(s) (φ(s),a,r,φ(s')) Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Overall recap’ DQN Implementation: - Initialize your Q-network parameters - Initialize replay memory D - Loop over episodes: - Start from initial state - Create a boolean to detect terminal states: terminal = False - Loop over time-steps: - With probability epsilon, take random action a. - Otherwise: - Forward propagate in the Q-network - Execute action a (that has the maximum Q( ,a) output of Q-network). - Observe rewards r and next state s’ - Use s’ to create - Add experience to replay memory (D) - Sample random mini-batch of transitions from D - Check if s’ is a terminal state. Compute targets y by forward propagating state in the Q-network, then compute loss. - Update parameters with gradient descent φ(s) φ(s) φ(s') φ(s') φ(s) (φ(s),a,r,φ(s')) - Preprocessing - Detect terminal state - Experience replay - Epsilon greedy action Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Results [https://www.youtube.com/watch?v=TmPfTpjtdgg] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Other Atari games Pong SeaQuest Space Invaders [https://www.youtube.com/watch?v=p88R2_3yWPA] [https://www.youtube.com/watch?v=NirMkC5uvWU] [https://www.youtube.com/watch?v=W2CAghUiofY&t=2s] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri I. Motivation II. Recycling is good: an introduction to RL III. Deep Q-Networks IV. Application of Deep Q-Network: Breakout (Atari) V. Tips to train Deep Q-Network VI. Advanced topics Today’s outline Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri VI - Advanced topics Alpha Go [DeepMind Blog] [Silver et al. (2017): Mastering the game of Go without human knowledge] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri VI - Advanced topics Competitive self-play [OpenAI Blog: Competitive self-play] [Bansal et al. (2017): Emergent Complexity via multi-agent competition] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri VI - Advanced topics Meta learning [Finn et al. (2017): Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri VI - Advanced topics Imitation learning [Ho et al. (2016): Generative Adversarial Imitation Learning] [Source: Bellemare et al. (2016): Unifying Count-Based Exploration and Intrinsic Motivation] Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri VI - Advanced topics Auxiliary task Kian Katanforoosh, Andrew Ng, Younes Bensouda Mourri Announcements For Tuesday 06/05, 9am: This Friday: • TA Sections: • How to have a great ﬁnal project write-up. • Advices on: How to write a great report. • Advices on: How to build a super poster. • Advices on: Final project grading criteria. • Going through examples of great projects and why they were great. • Small competitive quiz in section. "
511,"CS 188: Artificial Intelligence Introduction Pieter Abbeel & Dan Klein University of California, Berkeley Course Staff Dan Klein GSIs Professors Alex Li Daniel Ho Aditya Baradwaj Pieter Abbeel David Gaddy Ignasi Clavera Jasmine Deng Jonathan Ho Katie Luo Laura Smith Mitchell Stern Nikhil Sharma Nikita Kitaev Noah Golmant Ronghang Hu Wilson Yu Yi Wu Thanard Kurutach (Head GSI) Course Information § Communication: § Announcements on Piazza § Questions? Discussion on Piazza § Staff email: cs188@berkeley.edu § Course technology: § Website § Piazza § Gradescope § This course is webcast ( = Fa18 videos) + edited videos from past semester http://inst.cs.berkeley.edu/~cs188 Course Information § Prerequisites: § (CS 61A or CS 61B) and (CS 70 or Math 55) § Recommended: CS 61A and CS 61B and CS 70 § There will be a lot of math (and programming) § Work and Grading: § 5 programming projects: Python, groups of 1 or 2 § 5 late days for semester, maximum 2 per project § 11 homework assignments: § Electronic component: Online, interactive, solve alone/together, submit alone § Written component: On paper, solve alone/together, submit alone, self-asses § Two midterms, One final § Fixed scale § Participation can help on margins § Academic integrity policy § Contests! Exam Dates § Midterm 1: October 9th, 7:30-9:30pm § Midterm 2: November 15th, 7:30-9:30pm § Final Exam: December 11th, 8-11am § There will be no alternate exams Discussion Section (Optional Attendance) § Topic: review / warm-up exercises § Currently, none of you are assigned to sections § You are welcome to attend any section of your preference § Piazza survey later this week to help keep sections balanced § From past semesters’ experience we know sections will be (over)crowded the first two weeks of section, but then onwards section attendance will be lower and things will sort themselves out § There will be a webcast of section § There is no section in the current week (8/20-8/24). Textbook § Not required, but for students who want to read more we recommend § Russell & Norvig, AI: A Modern Approach, 3rd Ed. § Warning: Not a course textbook, so our presentation does not necessarily follow the presentation in the book. Laptops in Lecture § Laptops can easily distract students behind you Please consider sitting towards the back if using your laptop in lecture Announcements This Week • Important this week: • Check out website: https://inst.eecs.berkeley.edu/~cs188/fa18 • Register on Gradescope and Piazza (check your email for links) • HW0: Math self-diagnostic is online now (due on Monday 8/27 at 11:59pm) • P0: Python tutorial is online now (due on Monday 8/27 at 11:59pm) • One-time (optional) P0 lab hours (Friday 3-6pm, 330 Soda Hall) • Inst accounts: not needed, but if you want one, check instructions on Piazza • Also important: • Sections will be loosely assigned via Piazza poll (check the cs188 Piazza page) • Sections start next week. You may go to any section that has space. • The waitlist might take a while to sort out. We don’t control enrollment. Please see https://eecs.berkeley.edu/resources/undergrads/cs/degree-reqs/enrollment-policy for information regarding enrollment into CS classes, including email contact for staff if you have additional enrollment-related questions. Instruction vs. Assessment Our experience: these two goals don’t mix § Lecture / Section / OH / Piazza / Homework / Projects are instruction § collaborative, work until success (but please no spoilers) § Exams are assessment § on your own Instruction Grow knowledge, collaborate, work until success Assessment Measure knowledge, each student on their own, stopped before success § Homework and projects: work alone/together, iterate/learn till you nailed it § Exams: assessment § New this year: written component to homework ( = old exam questions) § Suggestion: assess yourself by first spending some time working alone Some Historical Statistics Today § What is artificial intelligence? § What can AI do? § What is this course? Sci-Fi AI? News AI? What is AI? The science of making machines that: Think like people Act like people Think rationally Act rationally Rational Decisions We’ll use the term rational in a very specific, technical way: § Rational: maximally achieving pre-defined goals § Rationality only concerns what decisions are made (not the thought process behind them) § Goals are expressed in terms of the utility of outcomes § Being rational means maximizing your expected utility A better title for this course would be: Computational Rationality Maximize Your Expected Utility What About the Brain? § Brains (human minds) are very good at making rational decisions, but not perfect § Brains aren’t as modular as software, so hard to reverse engineer! § “Brains are to intelligence as wings are to flight” § Lessons learned from the brain: memory (data) and simulation (computation) are key to decision making Course Topics § Part I: Intelligence from Computation § Fast search / planning § Constraint satisfaction § Adversarial and uncertain search § Part II: Intelligence from Data § Bayes’ nets § Decision theory § Machine learning § Throughout: Applications § Natural language, vision, robotics, games, … A (Short) History of AI A (Short) History of AI § 1940-1950: Early days § 1943: McCulloch & Pitts: Boolean circuit model of brain § 1950: Turing's “Computing Machinery and Intelligence” § 1950—70: Excitement: Look, Ma, no hands! § 1950s: Early AI programs, including Samuel's checkers program, Newell & Simon's Logic Theorist, Gelernter's Geometry Engine § 1956: Dartmouth meeting: “Artificial Intelligence” adopted § 1965: Robinson's complete algorithm for logical reasoning § 1970—90: Knowledge-based approaches § 1969—79: Early development of knowledge-based systems § 1980—88: Expert systems industry booms § 1988—93: Expert systems industry busts: “AI Winter” § 1990— 2012: Statistical approaches + subfield expertise § Resurgence of probability, focus on uncertainty § General increase in technical depth § Agents and learning systems… “AI Spring”? § 2012— ___: Excitement: Look, Ma, no hands again? § Big data, big compute, neural networks § Some re-unification of sub-fields § AI used in many industries What Can AI Do? Quiz: Which of the following can be done at present? § Play a decent game of table tennis? § Play a decent game of Jeopardy? § Drive safely along a curving mountain road? § Drive safely along Telegraph Avenue? § Buy a week's worth of groceries on the web? § Buy a week's worth of groceries at Berkeley Bowl? § Discover and prove a new mathematical theorem? § Converse successfully with another person for an hour? § Perform a surgical operation? § Translate spoken Chinese into spoken English in real time? § Fold the laundry and put away the dishes? § Write an intentionally funny story? Unintentionally Funny Stories § One day Joe Bear was hungry. He asked his friend Irving Bird where some honey was. Irving told him there was a beehive in the oak tree. Joe walked to the oak tree. He ate the beehive. The End. § Henry Squirrel was thirsty. He walked over to the river bank where his good friend Bill Bird was sitting. Henryslipped and fell in the river. Gravity drowned. The End. § Once upon a time there was a dishonest fox and a vain crow. One day the crow was sitting in his tree, holding a piece of cheese in his mouth. He noticed that he was holding the piece of cheese. He became hungry, and swallowed the cheese. The fox walked over to the crow. The End. [Shank, Tale-Spin System, 1984] Natural Language § Speech technologies (e.g. Siri) § Automatic speech recognition (ASR) § Text-to-speech synthesis (TTS) § Dialog systems Natural Language § Speech technologies § Automatic speech recognition (ASR) § Text-to-speech synthesis (TTS) § Dialog systems § Language processing technologies § Question answering § Machine translation § Web search § Text classification, spam filtering, etc… Vision (Perception) PIXELS -> INFO/DECISION E.g.: Source: TechCrunch [Caesar et al, ECCV 2017] Face detection and recognition Semantic Scene Segmentation 3-D Understanding [DensePose] Robotics § Robotics § Part mech. eng. § Part AI § Reality much harder than simulations! § Technologies § Vehicles § Rescue § Soccer! § Lots of automation… § In this class: § We ignore mechanical aspects § Methods for planning § Methods for control Images from UC Berkeley, RoboCup, Google/Waymo, Boston Dynamics Game Playing § Classic Moment: May, '97: Deep Blue vs. Kasparov § First match won against world champion § “Intelligent creative” play § 200 million board positions per second § Humans understood 99.9 of Deep Blue's moves § Can do about the same now with commodity parts § 1996: Kasparov beats Deep Blue: “I could feel --- I could smell --- a new kind of intelligence across the table.” § 1997: Deep Blue beats Kasparaov: “Deep Blue hasn’t proven anything.” § Open question: § How does human cognition deal with the search space explosion of chess? § Or: how can humans compete with computers at all?? § 2016: AlphaGo beats Lee Sedol – huge advance: sparse rollouts and self-play § Right now: OpenAI Five vs Team paiN (human pros) -- some caveats! § “[The AI play] was just something like completely different.” Austin Walsh Text from Bart Selman, image from IBM’s Deep Blue pages Logic § Logical systems § Theorem provers § NASA fault diagnosis § Question answering § Methods: § Deduction systems § Constraint satisfaction § Satisfiability solvers (huge advances!) Image from Bart Selman AI is starting to be everywhere… § Applied AI automates all kinds of things § Search engines § Route planning, e.g. maps, traffic § Logistics, e.g. packages, inventory § Medical diagnosis § Automated help desks § Spam / fraud detection § Smarter devices, e.g. cameras § Product recommendations § … Lots more! Designing Rational Agents § An agent is an entity that perceives and acts. § A rational agent selects actions that maximize its (expected) utility. § Characteristics of the percepts, environment, and action space dictate techniques for selecting rational actions § This course is about: § General AI techniques for a variety of problem types § Learning to recognize when and how a new problem can be solved with an existing technique Agent ? Sensors Actuators Environment Percepts Actions Pac-Man as an Agent Agent ? Sensors Actuators Environment Percepts Actions Pac-Man is a registered trademark of Namco-Bandai Games, used here for educational purposes "
512,"Announcements § Project 0: Python Tutorial § Due yesterday / Monday at 11:59pm (0 points in class, but pulse check to see you are in + get to know submission system) § Homework 0: Math self-diagnostic § Optional, but important to check your preparedness for second half § Project 1: Search § Will go out this week § Longer than most, and best way to test your programming preparedness § Sections § Start this week, can go to any but priority in the one you signed up for on piazza § Instructional accounts: online (see our Welcome post on piazza) § Pinned posts on piazza § Reminder: We don’t use bCourses [we use: class website, piazza, gradescope] How about AI Research? https://bair.berkeley.edu CS 188: Artificial Intelligence Search Instructors: Pieter Abbeel & Dan Klein University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley (ai.berkeley.edu).] Today § Agents that Plan Ahead § Search Problems § Uninformed Search Methods § Depth-First Search § Breadth-First Search § Uniform-Cost Search Agents that Plan Reflex Agents § Reflex agents: § Choose action based on current percept (and maybe memory) § May have memory or a model of the world’s current state § Do not consider the future consequences of their actions § Consider how the world IS § Can a reflex agent be rational? [Demo: reflex optimal (L2D1)] [Demo: reflex optimal (L2D2)] Video of Demo Reflex Optimal Video of Demo Reflex Odd Planning Agents § Planning agents: § Ask “what if” § Decisions based on (hypothesized) consequences of actions § Must have a model of how the world evolves in response to actions § Must formulate a goal (test) § Consider how the world WOULD BE § Optimal vs. complete planning § Planning vs. replanning [Demo: re-planning (L2D3)] [Demo: mastermind (L2D4)] Video of Demo Replanning Video of Demo Mastermind Search Problems Search Problems § A search problem consists of: § A state space § A successor function (with actions, costs) § A start state and a goal test § A solution is a sequence of actions (a plan) which transforms the start state to a goal state “N”, 1.0 “E”, 1.0 Search Problems Are Models Example: Traveling in Romania § State space: § Cities § Successor function: § Roads: Go to adjacent city with cost = distance § Start state: § Arad § Goal test: § Is state == Bucharest? § Solution? What’s in a State Space? § Problem: Pathing § States: (x,y) location § Actions: NSEW § Successor: update location only § Goal test: is (x,y)=END § Problem: Eat-All-Dots § States: {(x,y), dot booleans} § Actions: NSEW § Successor: update location and possibly a dot boolean § Goal test: dots all false The world state includes every last detail of the environment A search state keeps only the details needed for planning (abstraction) State Space Sizes? § World state: § Agent positions: 120 § Food count: 30 § Ghost positions: 12 § Agent facing: NSEW § How many § World states? 120x(230)x(122)x4 § States for pathing? 120 § States for eat-all-dots? 120x(230) Quiz: Safe Passage § Problem: eat all dots while keeping the ghosts perma-scared § What does the state space have to specify? § (agent position, dot booleans, power pellet booleans, remaining scared time) State Space Graphs and Search Trees State Space Graphs § State space graph: A mathematical representation of a search problem § Nodes are (abstracted) world configurations § Arcs represent successors (action results) § The goal test is a set of goal nodes (maybe only one) § In a state space graph, each state occurs only once! § We can rarely build this full graph in memory (it’s too big), but it’s a useful idea State Space Graphs § State space graph: A mathematical representation of a search problem § Nodes are (abstracted) world configurations § Arcs represent successors (action results) § The goal test is a set of goal nodes (maybe only one) § In a state space graph, each state occurs only once! § We can rarely build this full graph in memory (it’s too big), but it’s a useful idea S G d b p q c e h a f r Tiny state space graph for a tiny search problem Search Trees § A search tree: § A “what if” tree of plans and their outcomes § The start state is the root node § Children correspond to successors § Nodes show states, but correspond to PLANS that achieve those states § For most problems, we can never actually build the whole tree “E”, 1.0 “N”, 1.0 This is now / start Possible futures State Space Graphs vs. Search Trees S a b d p a c e p h f r q q c G a q e p h f r q q c G a S G d b p q c e h a f r We construct both on demand – and we construct as little as possible. Each NODE in in the search tree is an entire PATH in the state space graph. Search Tree State Space Graph Quiz: State Space Graphs vs. Search Trees S G b a Consider this 4-state graph: How big is its search tree (from S)? Quiz: State Space Graphs vs. Search Trees S G b a Consider this 4-state graph: Important: Lots of repeated structure in the search tree! How big is its search tree (from S)? s b b G a a G a G b G … … Tree Search Search Example: Romania Searching with a Search Tree § Search: § Expand out potential plans (tree nodes) § Maintain a fringe of partial plans under consideration § Try to expand as few tree nodes as possible General Tree Search § Important ideas: § Fringe § Expansion § Exploration strategy § Main question: which fringe nodes to explore? Example: Tree Search S G d b p q c e h a f r Example: Tree Search a a p q h f r q c G a q q p q a S G d b p q c e h a f r f d e r S d e p e h r f c G b c s s à d s à e s à p s à d à b s à d à c s à d à e s à d à e à h s à d à e à r s à d à e à r à f s à d à e à r à f à c s à d à e à r à f à G Depth-First Search Depth-First Search S a b d p a c e p h f r q q c G a q e p h f r q q c G a S G d b p q c e h a f r q p h f d b a c e r Strategy: expand a deepest node first Implementation: Fringe is a LIFO stack Search Algorithm Properties Search Algorithm Properties § Complete: Guaranteed to find a solution if one exists? § Optimal: Guaranteed to find the least cost path? § Time complexity? § Space complexity? § Cartoon of search tree: § b is the branching factor § m is the maximum depth § solutions at various depths § Number of nodes in entire tree? § 1 + b + b2 + …. bm = O(bm) … b 1 node b nodes b2 nodes bm nodes m tiers Depth-First Search (DFS) Properties … b 1 node b nodes b2 nodes bm nodes m tiers § What nodes DFS expand? § Some left prefix of the tree. § Could process the whole tree! § If m is finite, takes time O(bm) § How much space does the fringe take? § Only has siblings on path to root, so O(bm) § Is it complete? § m could be infinite, so only if we prevent cycles (more later) § Is it optimal? § No, it finds the “leftmost” solution, regardless of depth or cost Breadth-First Search Breadth-First Search S a b d p a c e p h f r q q c G a q e p h f r q q c G a S G d b p q c e h a f r Search Tiers Strategy: expand a shallowest node first Implementation: Fringe is a FIFO queue Breadth-First Search (BFS) Properties § What nodes does BFS expand? § Processes all nodes above shallowest solution § Let depth of shallowest solution be s § Search takes time O(bs) § How much space does the fringe take? § Has roughly the last tier, so O(bs) § Is it complete? § s must be finite if a solution exists, so yes! § Is it optimal? § Only if costs are all 1 (more on costs later) … b 1 node b nodes b2 nodes bm nodes s tiers bs nodes Quiz: DFS vs BFS Quiz: DFS vs BFS § When will BFS outperform DFS? § When will DFS outperform BFS? [Demo: dfs/bfs maze water (L2D6)] Video of Demo Maze Water DFS/BFS (part 1) Video of Demo Maze Water DFS/BFS (part 2) Iterative Deepening … b § Idea: get DFS’s space advantage with BFS’s time / shallow-solution advantages § Run a DFS with depth limit 1. If no solution… § Run a DFS with depth limit 2. If no solution… § Run a DFS with depth limit 3. ….. § Isn’t that wastefully redundant? § Generally most work happens in the lowest level searched, so not so bad! Cost-Sensitive Search BFS finds the shortest path in terms of number of actions. It does not find the least-cost path. We will now cover a similar algorithm which does find the least-cost path. START GOAL d b p q c e h a f r 2 9 2 8 1 8 2 3 2 4 4 15 1 3 2 2 Uniform Cost Search Uniform Cost Search S a b d p a c e p h f r q q c G a q e p h f r q q c G a Strategy: expand a cheapest node first: Fringe is a priority queue (priority: cumulative cost) S G d b p q c e h a f r 3 9 1 16 4 11 5 7 13 8 10 11 17 11 0 6 3 9 1 1 2 8 8 2 15 1 2 Cost contours 2 … Uniform Cost Search (UCS) Properties § What nodes does UCS expand? § Processes all nodes with cost less than cheapest solution! § If that solution costs C* and arcs cost at least e , then the “effective depth” is roughly C*/e § Takes time O(bC*/e) (exponential in effective depth) § How much space does the fringe take? § Has roughly the last tier, so O(bC*/e) § Is it complete? § Assuming best solution has a finite cost and minimum arc cost is positive, yes! § Is it optimal? § Yes! (Proof next lecture via A*) b C*/e “tiers” c £ 3 c £ 2 c £ 1 Uniform Cost Issues § Remember: UCS explores increasing cost contours § The good: UCS is complete and optimal! § The bad: § Explores options in every “direction” § No information about goal location § We’ll fix that soon! Start Goal … c £ 3 c £ 2 c £ 1 [Demo: empty grid UCS (L2D5)] [Demo: maze with deep/shallow water DFS/BFS/UCS (L2D7)] Video of Demo Empty UCS Video of Demo Maze with Deep/Shallow Water --- DFS, BFS, or UCS? (part 1) Video of Demo Maze with Deep/Shallow Water --- DFS, BFS, or UCS? (part 2) Video of Demo Maze with Deep/Shallow Water --- DFS, BFS, or UCS? (part 3) The One Queue § All these search algorithms are the same except for fringe strategies § Conceptually, all fringes are priority queues (i.e. collections of nodes with attached priorities) § Practically, for DFS and BFS, you can avoid the log(n) overhead from an actual priority queue, by using stacks and queues § Can even code one implementation that takes a variable queuing object Search and Models § Search operates over models of the world § The agent doesn’t actually try all the plans out in the real world! § Planning is all “in simulation” § Your search is only as good as your models… Search Gone Wrong? "
513,"Announcements § Homework 1: Search § Has been released! Due Tuesday, Sep 4th, at 11:59pm. § Electronic component: on Gradescope, instant grading, submit as often as you like. § Written component: exam-style template to be completed (we recommend on paper) and to be submitted into Gradescope (graded on effort/completion) § Project 1: Search § Has been released! Due Friday, Sep 7th, at 4pm. § Start early and ask questions. It’s longer than most! § Sections § Started this week § You can go to any, but have priority in your own § Section webcasts CS 188: Artificial Intelligence Informed Search Instructors: Pieter Abbeel & Dan Klein University of California, Berkeley Today § Informed Search § Heuristics § Greedy Search § A* Search § Graph Search Recap: Search Recap: Search § Search problem: § States (configurations of the world) § Actions and costs § Successor function (world dynamics) § Start state and goal test § Search tree: § Nodes: represent plans for reaching states § Plans have costs (sum of action costs) § Search algorithm: § Systematically builds a search tree § Chooses an ordering of the fringe (unexplored nodes) § Optimal: finds least-cost plans Example: Pancake Problem Cost: Number of pancakes flipped Example: Pancake Problem Example: Pancake Problem 3 2 4 3 3 2 2 2 4 State space graph with costs as weights 3 4 3 4 2 General Tree Search Action: flip top two Cost: 2 Action: flip all four Cost: 4 Path to reach goal: Flip four, flip three Total cost: 7 The One Queue § All these search algorithms are the same except for fringe strategies § Conceptually, all fringes are priority queues (i.e. collections of nodes with attached priorities) § Practically, for DFS and BFS, you can avoid the log(n) overhead from an actual priority queue, by using stacks and queues § Can even code one implementation that takes a variable queuing object Uninformed Search Uniform Cost Search § Strategy: expand lowest path cost § The good: UCS is complete and optimal! § The bad: § Explores options in every “direction” § No information about goal location Start Goal … c £ 3 c £ 2 c £ 1 [Demo: contours UCS empty (L3D1)] [Demo: contours UCS pacman small maze (L3D3)] Video of Demo Contours UCS Empty Video of Demo Contours UCS Pacman Small Maze Informed Search Search Heuristics § A heuristic is: § A function that estimates how close a state is to a goal § Designed for a particular search problem § Examples: Manhattan distance, Euclidean distance for pathing 10 5 11.2 Example: Heuristic Function h(x) Example: Heuristic Function Heuristic: the number of the largest pancake that is still out of place 4 3 0 2 3 3 3 4 4 3 4 4 4 h(x) Greedy Search Example: Heuristic Function h(x) Greedy Search § Expand the node that seems closest… § What can go wrong? Greedy Search § Strategy: expand a node that you think is closest to a goal state § Heuristic: estimate of distance to nearest goal for each state § A common case: § Best-first takes you straight to the (wrong) goal § Worst-case: like a badly-guided DFS … b … b [Demo: contours greedy empty (L3D1)] [Demo: contours greedy pacman small maze (L3D4)] Video of Demo Contours Greedy (Empty) Video of Demo Contours Greedy (Pacman Small Maze) A* Search A* Search UCS Greedy A* Combining UCS and Greedy § Uniform-cost orders by path cost, or backward cost g(n) § Greedy orders by goal proximity, or forward cost h(n) § A* Search orders by the sum: f(n) = g(n) + h(n) S a d b G h=5 h=6 h=2 1 8 1 1 2 h=6 h=0 c h=7 3 e h=1 1 Example: Teg Grenager S a b c e d d G G g = 0 h=6 g = 1 h=5 g = 2 h=6 g = 3 h=7 g = 4 h=2 g = 6 h=0 g = 9 h=1 g = 10 h=2 g = 12 h=0 When should A* terminate? § Should we stop when we enqueue a goal? § No: only stop when we dequeue a goal S B A G 2 3 2 2 h = 1 h = 2 h = 0 h = 3 Is A* Optimal? § What went wrong? § Actual bad goal cost < estimated good goal cost § We need estimates to be less than actual costs! A G S 1 3 h = 6 h = 0 5 h = 7 Admissible Heuristics Idea: Admissibility Inadmissible (pessimistic) heuristics break optimality by trapping good plans on the fringe Admissible (optimistic) heuristics slow down bad plans but never outweigh true costs Admissible Heuristics § A heuristic h is admissible (optimistic) if: where is the true cost to a nearest goal § Examples: § Coming up with admissible heuristics is most of what’s involved in using A* in practice. 4 15 Optimality of A* Tree Search Optimality of A* Tree Search Assume: § A is an optimal goal node § B is a suboptimal goal node § h is admissible Claim: § A will exit the fringe before B … Optimality of A* Tree Search: Blocking Proof: § Imagine B is on the fringe § Some ancestor n of A is on the fringe, too (maybe A!) § Claim: n will be expanded before B 1. f(n) is less or equal to f(A) Definition of f-cost Admissibility of h … h = 0 at a goal Optimality of A* Tree Search: Blocking Proof: § Imagine B is on the fringe § Some ancestor n of A is on the fringe, too (maybe A!) § Claim: n will be expanded before B 1. f(n) is less or equal to f(A) 2. f(A) is less than f(B) B is suboptimal h = 0 at a goal … Optimality of A* Tree Search: Blocking Proof: § Imagine B is on the fringe § Some ancestor n of A is on the fringe, too (maybe A!) § Claim: n will be expanded before B 1. f(n) is less or equal to f(A) 2. f(A) is less than f(B) 3. n expands before B § All ancestors of A expand before B § A expands before B § A* search is optimal … Properties of A* Properties of A* … b … b Uniform-Cost A* UCS vs A* Contours § Uniform-cost expands equally in all “directions” § A* expands mainly toward the goal, but does hedge its bets to ensure optimality Start Goal Start Goal [Demo: contours UCS / greedy / A* empty (L3D1)] [Demo: contours A* pacman small maze (L3D5)] Video of Demo Contours (Empty) -- UCS Video of Demo Contours (Empty) -- Greedy Video of Demo Contours (Empty) – A* Video of Demo Contours (Pacman Small Maze) – A* Comparison Greedy Uniform Cost A* A* Applications A* Applications § Video games § Pathing / routing problems § Resource planning problems § Robot motion planning § Language analysis § Machine translation § Speech recognition § … [Demo: UCS / A* pacman tiny maze (L3D6,L3D7)] [Demo: guess algorithm Empty Shallow/Deep (L3D8)] Video of Demo Pacman (Tiny Maze) – UCS / A* Video of Demo Empty Water Shallow/Deep – Guess Algorithm Creating Heuristics Creating Admissible Heuristics § Most of the work in solving hard search problems optimally is in coming up with admissible heuristics § Often, admissible heuristics are solutions to relaxed problems, where new actions are available § Inadmissible heuristics are often useful too 15 366 Example: 8 Puzzle § What are the states? § How many states? § What are the actions? § How many successors from the start state? § What should the costs be? Start State Goal State Actions 8 Puzzle I § Heuristic: Number of tiles misplaced § Why is it admissible? § h(start) = § This is a relaxed-problem heuristic 8 Average nodes expanded when the optimal path has… …4 steps …8 steps …12 steps UCS 112 6,300 3.6 x 106 TILES 13 39 227 Start State Goal State Statistics from Andrew Moore 8 Puzzle II § What if we had an easier 8-puzzle where any tile could slide any direction at any time, ignoring other tiles? § Total Manhattan distance § Why is it admissible? § h(start) = 3 + 1 + 2 + … = 18 Average nodes expanded when the optimal path has… …4 steps …8 steps …12 steps TILES 13 39 227 MANHATTAN 12 25 73 Start State Goal State 8 Puzzle III § How about using the actual cost as a heuristic? § Would it be admissible? § Would we save on nodes expanded? § What’s wrong with it? § With A*: a trade-off between quality of estimate and work per node § As heuristics get closer to the true cost, you will expand fewer nodes but usually do more work per node to compute the heuristic itself Semi-Lattice of Heuristics Trivial Heuristics, Dominance § Dominance: ha ≥hc if § Heuristics form a semi-lattice: § Max of admissible heuristics is admissible § Trivial heuristics § Bottom of lattice is the zero heuristic (what does this give us?) § Top of lattice is the exact heuristic Graph Search § Failure to detect repeated states can cause exponentially more work. Search Tree State Graph Tree Search: Extra Work! Graph Search § In BFS, for example, we shouldn’t bother expanding the circled nodes (why?) S a b d p a c e p h f r q q c G a q e p h f r q q c G a Graph Search § Idea: never expand a state twice § How to implement: § Tree search + set of expanded states (“closed set”) § Expand the search tree node-by-node, but… § Before expanding a node, check to make sure its state has never been expanded before § If not new, skip it, if new add to closed set § Important: store the closed set as a set, not a list § Can graph search wreck completeness? Why/why not? § How about optimality? A* Graph Search Gone Wrong? S A B C G 1 1 1 2 3 h=2 h=1 h=4 h=1 h=0 S (0+2) A (1+4) B (1+1) C (2+1) G (5+0) C (3+1) G (6+0) State space graph Search tree Consistency of Heuristics § Main idea: estimated heuristic costs ≤ actual costs § Admissibility: heuristic cost ≤ actual cost to goal h(A) ≤actual cost from A to G § Consistency: heuristic “arc” cost ≤ actual cost for each arc h(A) – h(C) ≤ cost(A to C) § Consequences of consistency: § The f value along a path never decreases h(A) ≤ cost(A to C) + h(C) § A* graph search is optimal 3 A C G h=4 h=1 1 h=2 Optimality of A* Graph Search Optimality of A* Graph Search § Sketch: consider what A* does with a consistent heuristic: § Fact 1: In tree search, A* expands nodes in increasing total f value (f-contours) § Fact 2: For every state s, nodes that reach s optimally are expanded before nodes that reach s suboptimally § Result: A* graph search is optimal … f £ 3 f £ 2 f £ 1 Optimality § Tree search: § A* is optimal if heuristic is admissible § UCS is a special case (h = 0) § Graph search: § A* optimal if heuristic is consistent § UCS optimal (h = 0 is consistent) § Consistency implies admissibility § In general, most natural admissible heuristics tend to be consistent, especially if from relaxed problems A*: Summary A*: Summary § A* uses both backward costs and (estimates of) forward costs § A* is optimal with admissible / consistent heuristics § Heuristic design is key: often use relaxed problems Tree Search Pseudo-Code Graph Search Pseudo-Code Optimality of A* Graph Search § Consider what A* does: § Expands nodes in increasing total f value (f-contours) Reminder: f(n) = g(n) + h(n) = cost to n + heuristic § Proof idea: the optimal goal(s) have the lowest f value, so it must get expanded first … f £ 3 f £ 2 f £ 1 There’s a problem with this argument. What are we assuming is true? Optimality of A* Graph Search Proof: § New possible problem: some n on path to G* isn’t in queue when we need it, because some worse n’ for the same state dequeued and expanded first (disaster!) § Take the highest such n in tree § Let p be the ancestor of n that was on the queue when n’ was popped § f(p) < f(n) because of consistency § f(n) < f(n’) because n’ is suboptimal § p would have been expanded before n’ § Contradiction! "
514,"CS 188: Artificial Intelligence Constraint Satisfaction Problems Dan Klein, Pieter Abbeel University of California, Berkeley What is Search For? Assumptions about the world: a single agent, deterministic actions, fully observed state, discrete state space Planning: sequences of actions The path to the goal is the important thing Paths have various costs, depths Heuristics give problem-specific guidance Identification: assignments to variables The goal itself is important, not the path All paths at the same depth (for some formulations) CSPs are a specialized class of identification problems Constraint Satisfaction Problems Constraint Satisfaction Problems  Standard search problems: State is a “black box”: arbitrary data structure Goal test can be any function over states Successor function can also be anything  Constraint satisfaction problems (CSPs): A special subset of search problems State is defined by variables Xi with values from a domain D (sometimes D depends on i) Goal test is a set of constraints specifying allowable combinations of values for subsets of variables  Simple example of a formal representation language  Allows useful general-purpose algorithms with more power than standard search algorithms CSP Examples Example: Map Coloring Variables: Domains: Constraints: adjacent regions must have different colors Solutions are assignments satisfying all constraints, e.g.: Implicit: Explicit: Example: N-Queens Formulation 1: Variables: Domains: Constraints Example: N-Queens Formulation 2: Variables: Domains: Constraints: Implicit: Explicit: Constraint Graphs Constraint Graphs Binary CSP: each constraint relates (at most) two variables Binary constraint graph: nodes are variables, arcs show constraints General-purpose CSP algorithms use the graph structure to speed up search. E.g., Tasmania is an independent subproblem! [Demo: CSP applet (made available by aispace.org) -- n-queens] Example: Cryptarithmetic Variables: Domains: Constraints: Example: Sudoku Variables: Each (open) square Domains: {1,2,…,9} Constraints: 9-way alldiff for each row 9-way alldiff for each column 9-way alldiff for each region (or can have a bunch of pairwise inequality constraints) Example: The Waltz Algorithm The Waltz algorithm is for interpreting line drawings of solid polyhedra as 3D objects An early example of an AI computation posed as a CSP  Approach:  Each intersection is a variable  Adjacent intersections impose constraints on each other  Solutions are physically realizable 3D interpretations ? Varieties of CSPs and Constraints Varieties of CSPs Discrete Variables Finite domains Size d means O(dn) complete assignments E.g., Boolean CSPs, including Boolean satisfiability (NP- complete) Infinite domains (integers, strings, etc.) E.g., job scheduling, variables are start/end times for each job Linear constraints solvable, nonlinear undecidable Continuous variables E.g., start/end times for Hubble Telescope observations Linear constraints solvable in polynomial time by LP methods (see cs170 for a bit of this theory) Varieties of Constraints  Varieties of Constraints Unary constraints involve a single variable (equivalent to reducing domains), e.g.: Binary constraints involve pairs of variables, e.g.: Higher-order constraints involve 3 or more variables: e.g., cryptarithmetic column constraints  Preferences (soft constraints): E.g., red is better than green Often representable by a cost for each variable assignment Gives constrained optimization problems (We’ll ignore these until we get to Bayes’ nets) Real-World CSPs Scheduling problems: e.g., when can we all meet? Timetabling problems: e.g., which class is offered when and where? Assignment problems: e.g., who teaches what class Hardware configuration Transportation scheduling Factory scheduling Circuit layout Fault diagnosis … lots more! Many real-world problems involve real-valued variables… Solving CSPs Standard Search Formulation Standard search formulation of CSPs States defined by the values assigned so far (partial assignments) Initial state: the empty assignment, {} Successor function: assign a value to an unassigned variable Goal test: the current assignment is complete and satisfies all constraints We’ll start with the straightforward, naïve approach, then improve it Search Methods What would BFS do? What would DFS do? What problems does naïve search have? [Demo: coloring -- dfs] Backtracking Search Backtracking Search Backtracking search is the basic uninformed algorithm for solving CSPs Idea 1: One variable at a time Variable assignments are commutative, so fix ordering I.e., [WA = red then NT = green] same as [NT = green then WA = red] Only need to consider assignments to a single variable at each step Idea 2: Check constraints as you go I.e. consider only values which do not conflict with previous assignments Might have to do some computation to check the constraints “Incremental goal test” Depth-first search with these two improvements is called backtracking search (not the best name) Can solve n-queens for n ≈25 Backtracking Example Backtracking Search Backtracking = DFS + variable-ordering + fail-on-violation What are the choice points? [Demo: coloring -- backtracking] Improving Backtracking General-purpose ideas give huge gains in speed Ordering: Which variable should be assigned next? In what order should its values be tried? Filtering: Can we detect inevitable failure early? Structure: Can we exploit the problem structure? Filtering Filtering: Keep track of domains for unassigned variables and cross off bad options Forward checking: Cross off values that violate a constraint when added to the existing assignment Filtering: Forward Checking WA SA NT Q NSW V [Demo: coloring -- forward checking] Filtering: Constraint Propagation Forward checking propagates information from assigned to unassigned variables, but doesn't provide early detection for all failures: NT and SA cannot both be blue! Why didn’t we detect this yet? Constraint propagation: reason from constraint to constraint WA SA NT Q NSW V Consistency of A Single Arc An arc X →Y is consistent iff for every x in the tail there is some y in the head which could be assigned without violating a constraint Forward checking: Enforcing consistency of arcs pointing to each new assignment Delete from the tail! WA SA NT Q NSW V Arc Consistency of an Entire CSP A simple form of propagation makes sure all arcs are consistent: Important: If X loses a value, neighbors of X need to be rechecked! Arc consistency detects failure earlier than forward checking Can be run as a preprocessor or after each assignment What’s the downside of enforcing arc consistency? Remember: Delete from the tail! WA SA NT Q NSW V Enforcing Arc Consistency in a CSP  Runtime: O(n2d3), can be reduced to O(n2d2)  … but detecting all possible future problems is NP-hard – why? [Demo: CSP applet (made available by aispace.org) -- n-queens] Limitations of Arc Consistency After enforcing arc consistency: Can have one solution left Can have multiple solutions left Can have no solutions left (and not know it) Arc consistency still runs inside a backtracking search! What went wrong here? [Demo: coloring -- arc consistency] [Demo: coloring -- forward checking] Ordering Ordering: Minimum Remaining Values Variable Ordering: Minimum remaining values (MRV): Choose the variable with the fewest legal left values in its domain Why min rather than max? Also called “most constrained variable” “Fail-fast” ordering Ordering: Least Constraining Value Value Ordering: Least Constraining Value Given a choice of variable, choose the least constraining value I.e., the one that rules out the fewest values in the remaining variables Note that it may take some computation to determine this! (E.g., rerunning filtering) Why least rather than most? Combining these ordering ideas makes 1000 queens feasible [Demo: coloring – backtracking + AC + ordering] "
515,"CS 188: Artificial Intelligence Constraint Satisfaction Problems II Instructors: Dan Klein and Pieter Abbeel University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Today Efficient Solution of CSPs Local Search Reminder: CSPs CSPs: Variables Domains Constraints Implicit (provide code to compute) Explicit (provide a list of the legal tuples) Unary / Binary / N-ary Goals: Here: find any solution Also: find all, find best, etc. Backtracking Search Improving Backtracking General-purpose ideas give huge gains in speed … but it’s all still NP-hard Filtering: Can we detect inevitable failure early? Ordering: Which variable should be assigned next? (MRV) In what order should its values be tried? (LCV) Structure: Can we exploit the problem structure? Arc Consistency and Beyond Arc Consistency of an Entire CSP A simple form of propagation makes sure all arcs are simultaneously consistent: Arc consistency detects failure earlier than forward checking Important: If X loses a value, neighbors of X need to be rechecked! Must rerun after each assignment! Remember: Delete from the tail! WA SA NT Q NSW V Limitations of Arc Consistency After enforcing arc consistency: Can have one solution left Can have multiple solutions left Can have no solutions left (and not know it) Arc consistency still runs inside a backtracking search! What went wrong here? K-Consistency K-Consistency Increasing degrees of consistency 1-Consistency (Node Consistency): Each single node’s domain has a value which meets that node’s unary constraints 2-Consistency (Arc Consistency): For each pair of nodes, any consistent assignment to one can be extended to the other K-Consistency: For each k nodes, any consistent assignment to k-1 can be extended to the kth node. Higher k more expensive to compute (You need to know the k=2 case: arc consistency) Strong K-Consistency Strong k-consistency: also k-1, k-2, … 1 consistent Claim: strong n-consistency means we can solve without backtracking! Why? Choose any assignment to any variable Choose a new variable By 2-consistency, there is a choice consistent with the first Choose a new variable By 3-consistency, there is a choice consistent with the first 2 … Lots of middle ground between arc consistency and n-consistency! (e.g. k=3, called path consistency) Structure Problem Structure Extreme case: independent subproblems Example: Tasmania and mainland do not interact Independent subproblems are identifiable as connected components of constraint graph Suppose a graph of n variables can be broken into subproblems of only c variables: Worst-case solution cost is O((n/c)(dc)), linear in n E.g., n = 80, d = 2, c =20 280 = 4 billion years at 10 million nodes/sec (4)(220) = 0.4 seconds at 10 million nodes/sec Tree-Structured CSPs Theorem: if the constraint graph has no loops, the CSP can be solved in O(n d2) time Compare to general CSPs, where worst-case time is O(dn) This property also applies to probabilistic reasoning (later): an example of the relation between syntactic restrictions and the complexity of reasoning Tree-Structured CSPs Algorithm for tree-structured CSPs: Order: Choose a root variable, order variables so that parents precede children Remove backward: For i = n : 2, apply RemoveInconsistent(Parent(Xi),Xi) Assign forward: For i = 1 : n, assign Xi consistently with Parent(Xi) Runtime: O(n d2) (why?) Tree-Structured CSPs Claim 1: After backward pass, all root-to-leaf arcs are consistent Proof: Each X→Y was made consistent at one point and Y’s domain could not have been reduced thereafter (because Y’s children were processed before Y) Claim 2: If root-to-leaf arcs are consistent, forward assignment will not backtrack Proof: Induction on position Why doesn’t this algorithm work with cycles in the constraint graph? Note: we’ll see this basic idea again with Bayes’ nets Improving Structure Nearly Tree-Structured CSPs Conditioning: instantiate a variable, prune its neighbors' domains Cutset conditioning: instantiate (in all ways) a set of variables such that the remaining constraint graph is a tree Cutset size c gives runtime O( (dc) (n-c) d2 ), very fast for small c Cutset Conditioning SA SA SA SA Instantiate the cutset (all possible ways) Compute residual CSP for each assignment Solve the residual CSPs (tree structured) Choose a cutset Cutset Quiz Find the smallest cutset for the graph below. Tree Decomposition*  Idea: create a tree-structured graph of mega-variables  Each mega-variable encodes part of the original CSP  Subproblems overlap to ensure consistent solutions M1 M2 M3 M4 {(WA=r,SA=g,NT=b), (WA=b,SA=r,NT=g), …} {(NT=r,SA=g,Q=b), (NT=b,SA=g,Q=r), …} Agree: (M1,M2) ∈ {((WA=g,SA=g,NT=g), (NT=g,SA=g,Q=g)), …} Agree on shared vars NT SA ≠ WA ≠ ≠ Q SA ≠ NT ≠ ≠ Agree on shared vars NS W SA ≠ Q ≠ ≠ Agree on shared vars V SA ≠ NS W ≠ ≠ Iterative Improvement Iterative Algorithms for CSPs Local search methods typically work with “complete” states, i.e., all variables assigned To apply to CSPs: Take an assignment with unsatisfied constraints Operators reassign variable values No fringe! Live on the edge. Algorithm: While not solved, Variable selection: randomly select any conflicted variable Value selection: min-conflicts heuristic: Choose a value that violates the fewest constraints I.e., hill climb with h(n) = total number of violated constraints Example: 4-Queens States: 4 queens in 4 columns (44 = 256 states) Operators: move queen in column Goal test: no attacks Evaluation: c(n) = number of attacks [Demo: n-queens – iterative improvement (L5D1)] [Demo: coloring – iterative improvement] Video of Demo Iterative Improvement – n Queens Video of Demo Iterative Improvement – Coloring Performance of Min-Conflicts Given random initial state, can solve n-queens in almost constant time for arbitrary n with high probability (e.g., n = 10,000,000)! The same appears to be true for any randomly-generated CSP except in a narrow range of the ratio Summary: CSPs CSPs are a special kind of search problem: States are partial assignments Goal test defined by constraints Basic solution: backtracking search Speed-ups: Ordering Filtering Structure Iterative min-conflicts is often effective in practice Local Search Local Search Tree search keeps unexplored alternatives on the fringe (ensures completeness) Local search: improve a single option until you can’t make it better (no fringe!) New successor function: local changes Generally much faster and more memory efficient (but incomplete and suboptimal) Hill Climbing Simple, general idea: Start wherever Repeat: move to the best neighboring state If no neighbors better than current, quit What’s bad about this approach? Complete? Optimal? What’s good about it? Hill Climbing Diagram Hill Climbing Quiz Starting from X, where do you end up ? Starting from Y, where do you end up ? Starting from Z, where do you end up ? Simulated Annealing Idea: Escape local maxima by allowing downhill moves But make them rarer as time goes on 34 Simulated Annealing Theoretical guarantee: Stationary distribution: If T decreased slowly enough, will converge to optimal state! Is this an interesting guarantee? Sounds like magic, but reality is reality: The more downhill steps you need to escape a local optimum, the less likely you are to ever make them all in a row People think hard about ridge operators which let you jump around the space in better ways Genetic Algorithms Genetic algorithms use a natural selection metaphor Keep best N hypotheses at each step (selection) based on a fitness function Also have pairwise crossover operators, with optional mutation to give variety Possibly the most misunderstood, misapplied (and even maligned) technique around Example: N-Queens Why does crossover make sense here? When wouldn’t it make sense? What would mutation be? What would a good fitness function be? Next Time: Adversarial Search! "
516,"CS 188: Artificial Intelligence Adversarial Search Instructors: Pieter Abbeel & Dan Klein University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley (ai.berkeley.edu).] Game Playing State-of-the-Art § Checkers: 1950: First computer player. 1994: First computer champion: Chinook ended 40-year-reign of human champion Marion Tinsley using complete 8-piece endgame. 2007: Checkers solved! § Chess: 1997: Deep Blue defeats human champion Gary Kasparov in a six-game match. Deep Blue examined 200M positions per second, used very sophisticated evaluation and undisclosed methods for extending some lines of search up to 40 ply. Current programs are even better, if less historic. § Go: Human champions are now starting to be challenged by machines. In go, b > 300! Classic programs use pattern knowledge bases, but big recent advances use Monte Carlo (randomized) expansion methods. Game Playing State-of-the-Art § Checkers: 1950: First computer player. 1994: First computer champion: Chinook ended 40-year-reign of human champion Marion Tinsley using complete 8-piece endgame. 2007: Checkers solved! § Chess: 1997: Deep Blue defeats human champion Gary Kasparov in a six-game match. Deep Blue examined 200M positions per second, used very sophisticated evaluation and undisclosed methods for extending some lines of search up to 40 ply. Current programs are even better, if less historic. § Go: 2016: Alpha GO defeats human champion. Uses Monte Carlo Tree Search, learned evaluation function. § Pacman Behavior from Computation [Demo: mystery pacman (L6D1)] Video of Demo Mystery Pacman Adversarial Games § Many different kinds of games! § Axes: § Deterministic or stochastic? § One, two, or more players? § Zero sum? § Perfect information (can you see the state)? § Want algorithms for calculating a strategy (policy) which recommends a move from each state Types of Games Deterministic Games § Many possible formalizations, one is: § States: S (start at s0) § Players: P={1...N} (usually take turns) § Actions: A (may depend on player / state) § Transition Function: SxA ® S § Terminal Test: S ® {t,f} § Terminal Utilities: SxP ® R § Solution for a player is a policy: S ® A Zero-Sum Games § Zero-Sum Games § Agents have opposite utilities (values on outcomes) § Lets us think of a single value that one maximizes and the other minimizes § Adversarial, pure competition § General Games § Agents have independent utilities (values on outcomes) § Cooperation, indifference, competition, and more are all possible § More later on non-zero-sum games Adversarial Search Single-Agent Trees 8 2 0 2 6 4 6 … … Value of a State Non-Terminal States: 8 2 0 2 6 4 6 … … Terminal States: Value of a state: The best achievable outcome (utility) from that state Adversarial Game Trees -20 -8 -18 -5 -10 +4 … … -20 +8 Minimax Values +8 -10 -5 -8 States Under Agent’s Control: Terminal States: States Under Opponent’s Control: Tic-Tac-Toe Game Tree Adversarial Search (Minimax) § Deterministic, zero-sum games: § Tic-tac-toe, chess, checkers § One player maximizes result § The other minimizes result § Minimax search: § A state-space search tree § Players alternate turns § Compute each node’s minimax value: the best achievable utility against a rational (optimal) adversary 8 2 5 6 max min 2 5 5 Terminal values: part of the game Minimax values: computed recursively Minimax Implementation def min-value(state): initialize v = +∞ for each successor of state: v = min(v, max-value(successor)) return v def max-value(state): initialize v = -∞ for each successor of state: v = max(v, min-value(successor)) return v Minimax Implementation (Dispatch) def value(state): if the state is a terminal state: return the state’s utility if the next agent is MAX: return max-value(state) if the next agent is MIN: return min-value(state) def min-value(state): initialize v = +∞ for each successor of state: v = min(v, value(successor)) return v def max-value(state): initialize v = -∞ for each successor of state: v = max(v, value(successor)) return v Minimax Example 12 8 5 2 3 2 14 4 6 Minimax Properties Optimal against a perfect player. Otherwise? 10 10 9 100 max min [Demo: min vs exp (L6D2, L6D3)] Video of Demo Min vs. Exp (Min) Video of Demo Min vs. Exp (Exp) Minimax Efficiency § How efficient is minimax? § Just like (exhaustive) DFS § Time: O(bm) § Space: O(bm) § Example: For chess, b » 35, m » 100 § Exact solution is completely infeasible § But, do we need to explore the whole tree? Resource Limits Game Tree Pruning Minimax Example 12 8 5 2 3 2 14 4 6 Minimax Pruning 12 8 5 2 3 2 14 Alpha-Beta Pruning § General configuration (MIN version) § We’re computing the MIN-VALUE at some node n § We’re looping over n’s children § n’s estimate of the childrens’ min is dropping § Who cares about n’s value? MAX § Let a be the best value that MAX can get at any choice point along the current path from the root § If n becomes worse than a, MAX will avoid it, so we can stop considering n’s other children (it’s already bad enough that it won’t be played) § MAX version is symmetric MAX MIN MAX MIN a n Alpha-Beta Implementation def min-value(state , α, β): initialize v = +∞ for each successor of state: v = min(v, value(successor, α, β)) if v ≤ α return v β = min(β, v) return v def max-value(state, α, β): initialize v = -∞ for each successor of state: v = max(v, value(successor, α, β)) if v ≥ β return v α = max(α, v) return v α: MAX’s best option on path to root β: MIN’s best option on path to root Alpha-Beta Pruning Properties § This pruning has no effect on minimax value computed for the root! § Values of intermediate nodes might be wrong § Important: children of the root may have the wrong value § So the most naïve version won’t let you do action selection § Good child ordering improves effectiveness of pruning § With “perfect ordering”: § Time complexity drops to O(bm/2) § Doubles solvable depth! § Full search of, e.g. chess, is still hopeless… § This is a simple example of metareasoning (computing about what to compute) 10 10 0 max min Alpha-Beta Quiz Alpha-Beta Quiz 2 Resource Limits Resource Limits § Problem: In realistic games, cannot search to leaves! § Solution: Depth-limited search § Instead, search only to a limited depth in the tree § Replace terminal utilities with an evaluation function for non-terminal positions § Example: § Suppose we have 100 seconds, can explore 10K nodes / sec § So can check 1M nodes per move § a-b reaches about depth 8 – decent chess program § Guarantee of optimal play is gone § More plies makes a BIG difference § Use iterative deepening for an anytime algorithm ? ? ? ? -1 -2 4 9 4 min max -2 4 Video of Demo Thrashing (d=2) [Demo: thrashing d=2, thrashing d=2 (fixed evaluation function) (L6D6)] Why Pacman Starves § A danger of replanning agents! § He knows his score will go up by eating the dot now (west, east) § He knows his score will go up just as much by eating the dot later (east, west) § There are no point-scoring opportunities after eating the dot (within the horizon, two here) § Therefore, waiting seems just as good as eating: he may go east, then back west in the next round of replanning! Video of Demo Thrashing -- Fixed (d=2) [Demo: thrashing d=2, thrashing d=2 (fixed evaluation function) (L6D7)] Evaluation Functions Evaluation Functions § Evaluation functions score non-terminals in depth-limited search § Ideal function: returns the actual minimax value of the position § In practice: typically weighted linear sum of features: § e.g. f1(s) = (num white queens – num black queens), etc. Evaluation for Pacman [Demo: thrashing d=2, thrashing d=2 (fixed evaluation function), smart ghosts coordinate (L6D6,7,8,10)] Video of Demo Smart Ghosts (Coordination) Video of Demo Smart Ghosts (Coordination) – Zoomed In Depth Matters § Evaluation functions are always imperfect § The deeper in the tree the evaluation function is buried, the less the quality of the evaluation function matters § An important example of the tradeoff between complexity of features and complexity of computation [Demo: depth limited (L6D4, L6D5)] Video of Demo Limited Depth (2) Video of Demo Limited Depth (10) Synergies between Evaluation Function and Alpha-Beta? § Alpha-Beta: amount of pruning depends on expansion ordering § Evaluation function can provide guidance to expand most promising nodes first (which later makes it more likely there is already a good alternative on the path to the root) § (somewhat similar to role of A* heuristic, CSPs filtering) § Alpha-Beta: (similar for roles of min-max swapped) § Value at a min-node will only keep going down § Once value of min-node lower than better option for max along path to root, can prune § Hence: IF evaluation function provides upper-bound on value at min-node, and upper-bound already lower than better option for max along path to root THEN can prune Next Time: Uncertainty! "
517,"Announcements § Homework 3: Games § Has been released, due Monday 9/17 at 11:59pm § Electronic HW3 § Written HW3 § Self-assessment HW2 § Project 2: Games § Released, due Friday 9/21 at 4:00pm § Homework Policy Update § Drop 2 lowest CS 188: Artificial Intelligence Uncertainty and Utilities Instructors: Pieter Abbeel & Dan Klein University of California, Berkeley [These slides were created by Dan Klein, Pieter Abbeel for CS188 Intro to AI at UC Berkeley (ai.berkeley.edu).] Uncertain Outcomes Worst-Case vs. Average Case 10 10 9 100 max min Idea: Uncertain outcomes controlled by chance, not an adversary! Expectimax Search § Why wouldn’t we know what the result of an action will be? § Explicit randomness: rolling dice § Unpredictable opponents: the ghosts respond randomly § Actions can fail: when moving a robot, wheels might slip § Values should now reflect average-case (expectimax) outcomes, not worst-case (minimax) outcomes § Expectimax search: compute the average score under optimal play § Max nodes as in minimax search § Chance nodes are like min nodes but the outcome is uncertain § Calculate their expected utilities § I.e. take weighted average (expectation) of children § Later, we’ll learn how to formalize the underlying uncertain- result problems as Markov Decision Processes 10 4 5 7 max chance 10 10 9 100 [Demo: min vs exp (L7D1,2)] Video of Demo Minimax vs Expectimax (Min) Video of Demo Minimax vs Expectimax (Exp) Expectimax Pseudocode def value(state): if the state is a terminal state: return the state’s utility if the next agent is MAX: return max-value(state) if the next agent is EXP: return exp-value(state) def exp-value(state): initialize v = 0 for each successor of state: p = probability(successor) v += p * value(successor) return v def max-value(state): initialize v = -∞ for each successor of state: v = max(v, value(successor)) return v Expectimax Pseudocode def exp-value(state): initialize v = 0 for each successor of state: p = probability(successor) v += p * value(successor) return v 5 7 8 24 -12 1/2 1/3 1/6 v = (1/2) (8) + (1/3) (24) + (1/6) (-12) = 10 Expectimax Example 12 9 6 0 3 2 15 4 6 Expectimax Pruning? 12 9 3 2 Depth-Limited Expectimax … … 492 362 … 400 300 Estimate of true expectimax value (which would require a lot of work to compute) Probabilities Reminder: Probabilities § A random variable represents an event whose outcome is unknown § A probability distribution is an assignment of weights to outcomes § Example: Traffic on freeway § Random variable: T = whether there’s traffic § Outcomes: T in {none, light, heavy} § Distribution: P(T=none) = 0.25, P(T=light) = 0.50, P(T=heavy) = 0.25 § Some laws of probability (more later): § Probabilities are always non-negative § Probabilities over all possible outcomes sum to one § As we get more evidence, probabilities may change: § P(T=heavy) = 0.25, P(T=heavy | Hour=8am) = 0.60 § We’ll talk about methods for reasoning and updating probabilities later 0.25 0.50 0.25 § The expected value of a function of a random variable is the average, weighted by the probability distribution over outcomes § Example: How long to get to the airport? Reminder: Expectations 0.25 0.50 0.25 Probability: 20 min 30 min 60 min Time: 35 min x x x + + § In expectimax search, we have a probabilistic model of how the opponent (or environment) will behave in any state § Model could be a simple uniform distribution (roll a die) § Model could be sophisticated and require a great deal of computation § We have a chance node for any outcome out of our control: opponent or environment § The model might say that adversarial actions are likely! § For now, assume each chance node magically comes along with probabilities that specify the distribution over its outcomes What Probabilities to Use? Having a probabilistic belief about another agent’s action does not mean that the agent is flipping any coins! Quiz: Informed Probabilities § Let’s say you know that your opponent is actually running a depth 2 minimax, using the result 80% of the time, and moving randomly otherwise § Question: What tree search should you use? 0.1 0.9 § Answer: Expectimax! § To figure out EACH chance node’s probabilities, you have to run a simulation of your opponent § This kind of thing gets very slow very quickly § Even worse if you have to simulate your opponent simulating you… § … except for minimax, which has the nice property that it all collapses into one game tree Modeling Assumptions The Dangers of Optimism and Pessimism Dangerous Optimism Assuming chance when the world is adversarial Dangerous Pessimism Assuming the worst case when it’s not likely Assumptions vs. Reality Adversarial Ghost Random Ghost Minimax Pacman Won 5/5 Avg. Score: 483 Won 5/5 Avg. Score: 493 Expectimax Pacman Won 1/5 Avg. Score: -303 Won 5/5 Avg. Score: 503 [Demos: world assumptions (L7D3,4,5,6)] Results from playing 5 games Pacman used depth 4 search with an eval function that avoids trouble Ghost used depth 2 search with an eval function that seeks Pacman Assumptions vs. Reality Adversarial Ghost Random Ghost Minimax Pacman Won 5/5 Avg. Score: 483 Won 5/5 Avg. Score: 493 Expectimax Pacman Won 1/5 Avg. Score: -303 Won 5/5 Avg. Score: 503 [Demos: world assumptions (L7D3,4,5,6)] Results from playing 5 games Pacman used depth 4 search with an eval function that avoids trouble Ghost used depth 2 search with an eval function that seeks Pacman Video of Demo World Assumptions Random Ghost – Expectimax Pacman Video of Demo World Assumptions Adversarial Ghost – Minimax Pacman Video of Demo World Assumptions Adversarial Ghost – Expectimax Pacman Video of Demo World Assumptions Random Ghost – Minimax Pacman Other Game Types Mixed Layer Types § E.g. Backgammon § Expectiminimax § Environment is an extra “random agent” player that moves after each min/max agent § Each node computes the appropriate combination of its children Example: Backgammon § Dice rolls increase b: 21 possible rolls with 2 dice § Backgammon » 20 legal moves § Depth 2 = 20 x (21 x 20)3 = 1.2 x 109 § As depth increases, probability of reaching a given search node shrinks § So usefulness of search is diminished § So limiting depth is less damaging § But pruning is trickier… § Historic AI: TDGammon uses depth-2 search + very good evaluation function + reinforcement learning: world-champion level play § 1st AI world champion in any game! Image: Wikipedia Multi-Agent Utilities § What if the game is not zero-sum, or has multiple players? § Generalization of minimax: § Terminals have utility tuples § Node values are also utility tuples § Each player maximizes its own component § Can give rise to cooperation and competition dynamically… 1,6,6 7,1,2 6,1,2 7,2,1 5,1,7 1,5,2 7,7,1 5,2,5 Utilities Maximum Expected Utility § Why should we average utilities? Why not minimax? § Principle of maximum expected utility: § A rational agent should chose the action that maximizes its expected utility, given its knowledge § Questions: § Where do utilities come from? § How do we know such utilities even exist? § How do we know that averaging even makes sense? § What if our behavior (preferences) can’t be described by utilities? What Utilities to Use? § For worst-case minimax reasoning, terminal function scale doesn’t matter § We just want better states to have higher evaluations (get the ordering right) § We call this insensitivity to monotonic transformations § For average-case expectimax reasoning, we need magnitudes to be meaningful 0 40 20 30 x2 0 1600 400 900 Utilities § Utilities are functions from outcomes (states of the world) to real numbers that describe an agent’s preferences § Where do utilities come from? § In a game, may be simple (+1/-1) § Utilities summarize the agent’s goals § Theorem: any “rational” preferences can be summarized as a utility function § We hard-wire utilities and let behaviors emerge § Why don’t we let agents pick utilities? § Why don’t we prescribe behaviors? Utilities: Uncertain Outcomes Getting ice cream Get Single Get Double Oops Whew! Preferences § An agent must have preferences among: § Prizes: A, B, etc. § Lotteries: situations with uncertain prizes § Notation: § Preference: § Indifference: A B p 1-p A Lottery A Prize A Rationality § We want some constraints on preferences before we call them rational, such as: § For example: an agent with intransitive preferences can be induced to give away all of its money § If B > C, then an agent with C would pay (say) 1 cent to get B § If A > B, then an agent with B would pay (say) 1 cent to get A § If C > A, then an agent with A would pay (say) 1 cent to get C Rational Preferences ) ( ) ( ) ( C A C B B A ! ! ! Þ Ù Axiom of Transitivity: Rational Preferences Theorem: Rational preferences imply behavior describable as maximization of expected utility The Axioms of Rationality § Theorem [Ramsey, 1931; von Neumann & Morgenstern, 1944] § Given any preferences satisfying these constraints, there exists a real-valued function U such that: § I.e. values assigned by U preserve preferences of both prizes and lotteries! § Maximum expected utility (MEU) principle: § Choose the action that maximizes expected utility § Note: an agent can be entirely rational (consistent with MEU) without ever representing or manipulating utilities and probabilities § E.g., a lookup table for perfect tic-tac-toe, a reflex vacuum cleaner MEU Principle Human Utilities Utility Scales § Normalized utilities: u+ = 1.0, u- = 0.0 § Micromorts: one-millionth chance of death, useful for paying to reduce product risks, etc. § QALYs: quality-adjusted life years, useful for medical decisions involving substantial risk § Note: behavior is invariant under positive linear transformation § With deterministic prizes only (no lottery choices), only ordinal utility can be determined, i.e., total order on prizes § Utilities map states to real numbers. Which numbers? § Standard approach to assessment (elicitation) of human utilities: § Compare a prize A to a standard lottery Lp between § “best possible prize” u+ with probability p § “worst possible catastrophe” u- with probability 1-p § Adjust lottery probability p until indifference: A ~ Lp § Resulting p is a utility in [0,1] Human Utilities 0.999999 0.000001 No change Pay $30 Instant death Money § Money does not behave as a utility function, but we can talk about the utility of having money (or being in debt) § Given a lottery L = [p, $X; (1-p), $Y] § The expected monetary value EMV(L) is p*X + (1-p)*Y § U(L) = p*U($X) + (1-p)*U($Y) § Typically, U(L) < U( EMV(L) ) § In this sense, people are risk-averse § When deep in debt, people are risk-prone Example: Insurance § Consider the lottery [0.5, $1000; 0.5, $0] § What is its expected monetary value? ($500) § What is its certainty equivalent? § Monetary value acceptable in lieu of lottery § $400 for most people § Difference of $100 is the insurance premium § There’s an insurance industry because people will pay to reduce their risk § If everyone were risk-neutral, no insurance needed! § It’s win-win: you’d rather have the $400 and the insurance company would rather have the lottery (their utility curve is flat and they have many lotteries) Example: Human Rationality? § Famous example of Allais (1953) § A: [0.8, $4k; 0.2, $0] § B: [1.0, $3k; 0.0, $0] § C: [0.2, $4k; 0.8, $0] § D: [0.25, $3k; 0.75, $0] § Most people prefer B > A, C > D § But if U($0) = 0, then § B > A Þ U($3k) > 0.8 U($4k) § C > D Þ 0.8 U($4k) > U($3k) Next Time: MDPs! "
518,"CS 188: Artificial Intelligence Markov Decision Processes Instructors: Dan Klein and Pieter Abbeel University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Non-Deterministic Search Example: Grid World  A maze-like problem  The agent lives in a grid  Walls block the agent’s path  Noisy movement: actions do not always go as planned  80% of the time, the action North takes the agent North (if there is no wall there)  10% of the time, North takes the agent West; 10% East  If there is a wall in the direction the agent would have been taken, the agent stays put  The agent receives rewards each time step  Small “living” reward each step (can be negative)  Big rewards come at the end (good or bad)  Goal: maximize sum of rewards Grid World Actions Deterministic Grid World Stochastic Grid World Markov Decision Processes An MDP is defined by: A set of states s ∈ S A set of actions a ∈ A A transition function T(s, a, s’) Probability that a from s leads to s’, i.e., P(s’| s, a) Also called the model or the dynamics A reward function R(s, a, s’) Sometimes just R(s) or R(s’) A start state Maybe a terminal state MDPs are non-deterministic search problems One way to solve them is with expectimax search We’ll have a new tool soon [Demo – gridworld manual intro (L8D1)] Video of Demo Gridworld Manual Intro What is Markov about MDPs? “Markov” generally means that given the present state, the future and the past are independent For Markov decision processes, “Markov” means action outcomes depend only on the current state This is just like search, where the successor function could only depend on the current state (not the history) Andrey Markov (1856-1922) Policies Optimal policy when R(s, a, s’) = -0.03 for all non-terminals s In deterministic single-agent search problems, we wanted an optimal plan, or sequence of actions, from start to a goal For MDPs, we want an optimal policy π*: S → A A policy π gives an action for each state An optimal policy is one that maximizes expected utility if followed An explicit policy defines a reflex agent Expectimax didn’t compute entire policies It computed the action for a single state only Optimal Policies R(s) = -2.0 R(s) = -0.4 R(s) = -0.03 R(s) = -0.01 Example: Racing Example: Racing  A robot car wants to travel far, quickly  Three states: Cool, Warm, Overheated  Two actions: Slow, Fast  Going faster gets double reward Cool Warm Overheated Fast Fast Slow Slow 0.5 0.5 0.5 0.5 1.0 1.0 +1 +1 +1 +2 +2 -10 Racing Search Tree MDP Search Trees Each MDP state projects an expectimax-like search tree a s s’ s, a (s,a,s’) called a transition T(s,a,s’) = P(s’|s,a) R(s,a,s’) s,a,s’ s is a state (s, a) is a q-state Utilities of Sequences Utilities of Sequences What preferences should an agent have over reward sequences? More or less? Now or later? [1, 2, 2] [2, 3, 4] or [0, 0, 1] [1, 0, 0] or Discounting It’s reasonable to maximize the sum of rewards It’s also reasonable to prefer rewards now to rewards later One solution: values of rewards decay exponentially Worth Now Worth Next Step Worth In Two Steps Discounting How to discount? Each time we descend a level, we multiply in the discount once Why discount? Sooner rewards probably do have higher utility than later rewards Also helps our algorithms converge Example: discount of 0.5 U([1,2,3]) = 1*1 + 0.5*2 + 0.25*3 U([1,2,3]) < U([3,2,1]) Stationary Preferences Theorem: if we assume stationary preferences: Then: there are only two ways to define utilities Additive utility: Discounted utility: Quiz: Discounting Given: Actions: East, West, and Exit (only available in exit states a, e) Transitions: deterministic Quiz 1: For γ = 1, what is the optimal policy? Quiz 2: For γ = 0.1, what is the optimal policy? Quiz 3: For which γ are West and East equally good when in state d? Infinite Utilities?! Problem: What if the game lasts forever? Do we get infinite rewards? Solutions: Finite horizon: (similar to depth-limited search) Terminate episodes after a fixed T steps (e.g. life) Gives nonstationary policies (π depends on time left) Discounting: use 0 < γ < 1 Smaller γ means smaller “horizon” – shorter term focus Absorbing state: guarantee that for every policy, a terminal state will eventually be reached (like “overheated” for racing) Recap: Defining MDPs Markov decision processes: Set of states S Start state s0 Set of actions A Transitions P(s’|s,a) (or T(s,a,s’)) Rewards R(s,a,s’) (and discount γ) MDP quantities so far: Policy = Choice of action for each state Utility = sum of (discounted) rewards a s s, a s,a,s’ s’ Solving MDPs Optimal Quantities The value (utility) of a state s: V*(s) = expected utility starting in s and acting optimally The value (utility) of a q-state (s,a): Q*(s,a) = expected utility starting out having taken action a from state s and (thereafter) acting optimally The optimal policy: π*(s) = optimal action from state s a s s’ s, a (s,a,s’) is a transition s,a,s’ s is a state (s, a) is a q-state [Demo – gridworld values (L8D4)] Snapshot of Demo – Gridworld V Values Noise = 0.2 Discount = 0.9 Living reward = 0 Snapshot of Demo – Gridworld Q Values Noise = 0.2 Discount = 0.9 Living reward = 0 Values of States Fundamental operation: compute the (expectimax) value of a state Expected utility under optimal action Average sum of (discounted) rewards This is just what expectimax computed! Recursive definition of value: a s s, a s,a,s’ s’ Racing Search Tree Racing Search Tree Racing Search Tree We’re doing way too much work with expectimax! Problem: States are repeated Idea: Only compute needed quantities once Problem: Tree goes on forever Idea: Do a depth-limited computation, but with increasing depths until change is small Note: deep parts of the tree eventually don’t matter if γ < 1 Time-Limited Values Key idea: time-limited values Define Vk(s) to be the optimal value of s if the game ends in k more time steps Equivalently, it’s what a depth-k expectimax would give from s [Demo – time-limited values (L8D6)] k=0 Noise = 0.2 Discount = 0.9 Living reward = 0 k=1 Noise = 0.2 Discount = 0.9 Living reward = 0 k=2 Noise = 0.2 Discount = 0.9 Living reward = 0 k=3 Noise = 0.2 Discount = 0.9 Living reward = 0 k=4 Noise = 0.2 Discount = 0.9 Living reward = 0 k=5 Noise = 0.2 Discount = 0.9 Living reward = 0 k=6 Noise = 0.2 Discount = 0.9 Living reward = 0 k=7 Noise = 0.2 Discount = 0.9 Living reward = 0 k=8 Noise = 0.2 Discount = 0.9 Living reward = 0 k=9 Noise = 0.2 Discount = 0.9 Living reward = 0 k=10 Noise = 0.2 Discount = 0.9 Living reward = 0 k=11 Noise = 0.2 Discount = 0.9 Living reward = 0 k=12 Noise = 0.2 Discount = 0.9 Living reward = 0 k=100 Noise = 0.2 Discount = 0.9 Living reward = 0 Computing Time-Limited Values Value Iteration Value Iteration Start with V0(s) = 0: no time steps left means an expected reward sum of zero Given vector of Vk(s) values, do one ply of expectimax from each state: Repeat until convergence Complexity of each iteration: O(S2A) Theorem: will converge to unique optimal values Basic idea: approximations get refined towards optimal values Policy may converge long before values do a Vk+1(s) s, a s,a,s’ Vk(s’) Example: Value Iteration 0 0 0 2 1 0 3.5 2.5 0 Assume no discount! Convergence*  How do we know the Vk vectors are going to converge?  Case 1: If the tree has maximum depth M, then VM holds the actual untruncated values  Case 2: If the discount is less than 1 Sketch: For any state Vk and Vk+1 can be viewed as depth k+1 expectimax results in nearly identical search trees The difference is that on the bottom layer, Vk+1 has actual rewards while Vk has zeros That last layer is at best all RMAX It is at worst RMIN But everything is discounted by γk that far out So Vk and Vk+1 are at most γk max|R| different So as k increases, the values converge Next Time: Policy-Based Methods "
519,"CS 188: Artificial Intelligence Markov Decision Processes II Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Example: Grid World  A maze-like problem  The agent lives in a grid  Walls block the agent’s path  Noisy movement: actions do not always go as planned  80% of the time, the action North takes the agent North  10% of the time, North takes the agent West; 10% East  If there is a wall in the direction the agent would have been taken, the agent stays put  The agent receives rewards each time step  Small “living” reward each step (can be negative)  Big rewards come at the end (good or bad)  Goal: maximize sum of (discounted) rewards Recap: MDPs Markov decision processes: States S Actions A Transitions P(s’|s,a) (or T(s,a,s’)) Rewards R(s,a,s’) (and discount γ) Start state s0 Quantities: Policy = map of states to actions Utility = sum of discounted rewards Values = expected future utility from a state (max node) Q-Values = expected future utility from a q-state (chance node) a s s, a s,a,s’ s’ Optimal Quantities The value (utility) of a state s: V*(s) = expected utility starting in s and acting optimally The value (utility) of a q-state (s,a): Q*(s,a) = expected utility starting out having taken action a from state s and (thereafter) acting optimally The optimal policy: π*(s) = optimal action from state s a s s’ s, a (s,a,s’) is a transition s,a,s’ s is a state (s, a) is a q-state [Demo: gridworld values (L9D1)] Gridworld Values V* Gridworld: Q* The Bellman Equations How to be optimal: Step 1: Take correct first action Step 2: Keep being optimal The Bellman Equations Definition of “optimal utility” via expectimax recurrence gives a simple one-step lookahead relationship amongst optimal utility values These are the Bellman equations, and they characterize optimal values in a way we’ll use over and over a s s, a s,a,s’ s’ Value Iteration Bellman equations characterize the optimal values: Value iteration computes them: Value iteration is just a fixed point solution method … though the Vk vectors are also interpretable as time-limited values a V(s) s, a s,a,s’ V(s’) Convergence*  How do we know the Vk vectors are going to converge?  Case 1: If the tree has maximum depth M, then VM holds the actual untruncated values  Case 2: If the discount is less than 1 Sketch: For any state Vk and Vk+1 can be viewed as depth k+1 expectimax results in nearly identical search trees The difference is that on the bottom layer, Vk+1 has actual rewards while Vk has zeros That last layer is at best all RMAX It is at worst RMIN But everything is discounted by γk that far out So Vk and Vk+1 are at most γk max|R| different So as k increases, the values converge Policy Methods Policy Evaluation Fixed Policies Expectimax trees max over all actions to compute the optimal values If we fixed some policy π(s), then the tree would be simpler – only one action per state … though the tree’s value would depend on which policy we fixed a s s, a s,a,s’ s’ π(s) s s, π(s) s, π(s),s’ s’ Do the optimal action Do what π says to do Utilities for a Fixed Policy Another basic operation: compute the utility of a state s under a fixed (generally non-optimal) policy Define the utility of a state s, under a fixed policy π: Vπ(s) = expected total discounted rewards starting in s and following π Recursive relation (one-step look-ahead / Bellman equation): π(s) s s, π(s) s, π(s),s’ s’ Example: Policy Evaluation Always Go Right Always Go Forward Example: Policy Evaluation Always Go Right Always Go Forward Policy Evaluation How do we calculate the V’s for a fixed policy π? Idea 1: Turn recursive Bellman equations into updates (like value iteration) Efficiency: O(S2) per iteration Idea 2: Without the maxes, the Bellman equations are just a linear system Solve with Matlab (or your favorite linear system solver) π(s) s s, π(s) s, π(s),s’ s’ Policy Extraction Computing Actions from Values Let’s imagine we have the optimal values V*(s) How should we act? It’s not obvious! We need to do a mini-expectimax (one step) This is called policy extraction, since it gets the policy implied by the values Computing Actions from Q-Values Let’s imagine we have the optimal q-values: How should we act? Completely trivial to decide! Important lesson: actions are easier to select from q-values than values! Policy Iteration Problems with Value Iteration Value iteration repeats the Bellman updates: Problem 1: It’s slow – O(S2A) per iteration Problem 2: The “max” at each state rarely changes Problem 3: The policy often converges long before the values a s s, a s,a,s’ s’ [Demo: value iteration (L9D2)] k=0 Noise = 0.2 Discount = 0.9 Living reward = 0 k=1 Noise = 0.2 Discount = 0.9 Living reward = 0 k=2 Noise = 0.2 Discount = 0.9 Living reward = 0 k=3 Noise = 0.2 Discount = 0.9 Living reward = 0 k=4 Noise = 0.2 Discount = 0.9 Living reward = 0 k=5 Noise = 0.2 Discount = 0.9 Living reward = 0 k=6 Noise = 0.2 Discount = 0.9 Living reward = 0 k=7 Noise = 0.2 Discount = 0.9 Living reward = 0 k=8 Noise = 0.2 Discount = 0.9 Living reward = 0 k=9 Noise = 0.2 Discount = 0.9 Living reward = 0 k=10 Noise = 0.2 Discount = 0.9 Living reward = 0 k=11 Noise = 0.2 Discount = 0.9 Living reward = 0 k=12 Noise = 0.2 Discount = 0.9 Living reward = 0 k=100 Noise = 0.2 Discount = 0.9 Living reward = 0 Policy Iteration Alternative approach for optimal values: Step 1: Policy evaluation: calculate utilities for some fixed policy (not optimal utilities!) until convergence Step 2: Policy improvement: update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values Repeat steps until policy converges This is policy iteration It’s still optimal! Can converge (much) faster under some conditions Policy Iteration Evaluation: For fixed current policy π, find values with policy evaluation: Iterate until values converge: Improvement: For fixed values, get a better policy using policy extraction One-step look-ahead: Comparison Both value iteration and policy iteration compute the same thing (all optimal values) In value iteration: Every iteration updates both the values and (implicitly) the policy We don’t track the policy, but taking the max over actions implicitly recomputes it In policy iteration: We do several passes that update utilities with fixed policy (each pass is fast because we consider only one action, not all of them) After the policy is evaluated, a new policy is chosen (slow like a value iteration pass) The new policy will be better (or we’re done) Both are dynamic programs for solving MDPs Summary: MDP Algorithms So you want to…. Compute optimal values: use value iteration or policy iteration Compute values for a particular policy: use policy evaluation Turn your values into a policy: use policy extraction (one-step lookahead) These all look the same! They basically are – they are all variations of Bellman updates They all use one-step lookahead expectimax fragments They differ only in whether we plug in a fixed policy or max over actions Double Bandits Double-Bandit MDP Actions: Blue, Red States: Win, Lose W L $1 1.0 $1 1.0 0.25 $0 0.75 $2 0.75 $2 0.25 $0 No discount 100 time steps Both states have the same value Offline Planning Solving MDPs is offline planning You determine all quantities through computation You need to know the details of the MDP You do not actually play the game! Play Red Play Blue Value No discount 100 time steps Both states have the same value 150 100 W L $1 1.0 $1 1.0 0.25 $0 0.75 $2 0.75 $2 0.25 $0 Let’s Play! $2 $2 $0 $2 $2 $2 $2 $0 $0 $0 Online Planning Rules changed! Red’s win chance is different. W L $1 1.0 $1 1.0 ?? $0 ?? $2 ?? $2 ?? $0 Let’s Play! $0 $0 $0 $2 $0 $2 $0 $0 $0 $0 What Just Happened? That wasn’t planning, it was learning! Specifically, reinforcement learning There was an MDP, but you couldn’t solve it with just computation You needed to actually act to figure it out Important ideas in reinforcement learning that came up Exploration: you have to try unknown actions to get information Exploitation: eventually, you have to use what you know Regret: even if you learn intelligently, you make mistakes Sampling: because of chance, you have to try things repeatedly Difficulty: learning can be much harder than solving a known MDP Next Time: Reinforcement Learning! "
520,"CS 188: Artificial Intelligence Reinforcement Learning Instructors: Pieter Abbeel and Dan Klein University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Reinforcement Learning Reinforcement Learning § Basic idea: § Receive feedback in the form of rewards § Agent’s utility is defined by the reward function § Must (learn to) act so as to maximize expected rewards § All learning is based on observed samples of outcomes! Environment Agent Actions: a State: s Reward: r Example: Learning to Walk Initial A Learning Trial After Learning [1K Trials] [Kohl and Stone, ICRA 2004] Example: Learning to Walk Initial [Video: AIBO WALK – initial] [Kohl and Stone, ICRA 2004] Example: Learning to Walk Training [Video: AIBO WALK – training] [Kohl and Stone, ICRA 2004] Example: Learning to Walk Finished [Video: AIBO WALK – finished] [Kohl and Stone, ICRA 2004] Example: Sidewinding [Andrew Ng] [Video: SNAKE – climbStep+sidewinding] Example: Toddler Robot [Tedrake, Zhang and Seung, 2005] [Video: TODDLER – 40s] The Crawler! [Demo: Crawler Bot (L10D1)] [You, in Project 3] Video of Demo Crawler Bot Reinforcement Learning § Still assume a Markov decision process (MDP): § A set of states s Î S § A set of actions (per state) A § A model T(s,a,s’) § A reward function R(s,a,s’) § Still looking for a policy p(s) § New twist: don’t know T or R § I.e. we don’t know which states are good or what the actions do § Must actually try out actions and states to learn Offline (MDPs) vs. Online (RL) Offline Solution Online Learning Model-Based Learning Model-Based Learning § Model-Based Idea: § Learn an approximate model based on experiences § Solve for values as if the learned model were correct § Step 1: Learn empirical MDP model § Count outcomes s’ for each s, a § Normalize to give an estimate of § Discover each when we experience (s, a, s’) § Step 2: Solve the learned MDP § For example, use value iteration, as before Example: Model-Based Learning Input Policy p Assume: g = 1 Observed Episodes (Training) Learned Model A B C D E B, east, C, -1 C, east, D, -1 D, exit, x, +10 B, east, C, -1 C, east, D, -1 D, exit, x, +10 E, north, C, -1 C, east, A, -1 A, exit, x, -10 Episode 1 Episode 2 Episode 3 Episode 4 E, north, C, -1 C, east, D, -1 D, exit, x, +10 T(s,a,s’). T(B, east, C) = 1.00 T(C, east, D) = 0.75 T(C, east, A) = 0.25 … R(s,a,s’). R(B, east, C) = -1 R(C, east, D) = -1 R(D, exit, x) = +10 … Example: Expected Age Goal: Compute expected age of cs188 students Unknown P(A): “Model Based” Unknown P(A): “Model Free” Without P(A), instead collect samples [a1, a2, … aN] Known P(A) Why does this work? Because samples appear with the right frequencies. Why does this work? Because eventually you learn the right model. Model-Free Learning Passive Reinforcement Learning Passive Reinforcement Learning § Simplified task: policy evaluation § Input: a fixed policy p(s) § You don’t know the transitions T(s,a,s’) § You don’t know the rewards R(s,a,s’) § Goal: learn the state values § In this case: § Learner is “along for the ride” § No choice about what actions to take § Just execute the policy and learn from experience § This is NOT offline planning! You actually take actions in the world. Direct Evaluation § Goal: Compute values for each state under p § Idea: Average together observed sample values § Act according to p § Every time you visit a state, write down what the sum of discounted rewards turned out to be § Average those samples § This is called direct evaluation Example: Direct Evaluation Input Policy p Assume: g = 1 Observed Episodes (Training) Output Values A B C D E B, east, C, -1 C, east, D, -1 D, exit, x, +10 B, east, C, -1 C, east, D, -1 D, exit, x, +10 E, north, C, -1 C, east, A, -1 A, exit, x, -10 Episode 1 Episode 2 Episode 3 Episode 4 E, north, C, -1 C, east, D, -1 D, exit, x, +10 A B C D E +8 +4 +10 -10 -2 Problems with Direct Evaluation § What’s good about direct evaluation? § It’s easy to understand § It doesn’t require any knowledge of T, R § It eventually computes the correct average values, using just sample transitions § What bad about it? § It wastes information about state connections § Each state must be learned separately § So, it takes a long time to learn Output Values A B C D E +8 +4 +10 -10 -2 If B and E both go to C under this policy, how can their values be different? Why Not Use Policy Evaluation? § Simplified Bellman updates calculate V for a fixed policy: § Each round, replace V with a one-step-look-ahead layer over V § This approach fully exploited the connections between the states § Unfortunately, we need T and R to do it! § Key question: how can we do this update to V without knowing T and R? § In other words, how to we take a weighted average without knowing the weights? p(s) s s, p(s) s, p(s),s’ s’ Sample-Based Policy Evaluation? § We want to improve our estimate of V by computing these averages: § Idea: Take samples of outcomes s’ (by doing the action!) and average p(s) s s, p(s) s1' s2' s3' s, p(s),s’ s' Almost! But we can’t rewind time to get sample after sample from state s. Temporal Difference Learning Temporal Difference Learning § Big idea: learn from every experience! § Update V(s) each time we experience a transition (s, a, s’, r) § Likely outcomes s’ will contribute updates more often § Temporal difference learning of values § Policy still fixed, still doing evaluation! § Move values toward value of whatever successor occurs: running average p(s) s s, p(s) s’ Sample of V(s): Update to V(s): Same update: Exponential Moving Average § Exponential moving average § The running interpolation update: § Makes recent samples more important: § Forgets about the past (distant past values were wrong anyway) § Decreasing learning rate (alpha) can give converging averages Example: Temporal Difference Learning Assume: g = 1, α = 1/2 Observed Transitions B, east, C, -2 0 0 0 8 0 0 -1 0 8 0 0 -1 3 8 0 C, east, D, -2 A B C D E States Problems with TD Value Learning § TD value leaning is a model-free way to do policy evaluation, mimicking Bellman updates with running sample averages § However, if we want to turn values into a (new) policy, we’re sunk: § Idea: learn Q-values, not values § Makes action selection model-free too! a s s, a s,a,s’ s’ Active Reinforcement Learning Active Reinforcement Learning § Full reinforcement learning: optimal policies (like value iteration) § You don’t know the transitions T(s,a,s’) § You don’t know the rewards R(s,a,s’) § You choose the actions now § Goal: learn the optimal policy / values § In this case: § Learner makes choices! § Fundamental tradeoff: exploration vs. exploitation § This is NOT offline planning! You actually take actions in the world and find out what happens… Detour: Q-Value Iteration § Value iteration: find successive (depth-limited) values § Start with V0(s) = 0, which we know is right § Given Vk, calculate the depth k+1 values for all states: § But Q-values are more useful, so compute them instead § Start with Q0(s,a) = 0, which we know is right § Given Qk, calculate the depth k+1 q-values for all q-states: Q-Learning § Q-Learning: sample-based Q-value iteration § Learn Q(s,a) values as you go § Receive a sample (s,a,s’,r) § Consider your old estimate: § Consider your new sample estimate: § Incorporate the new estimate into a running average: [Demo: Q-learning – gridworld (L10D2)] [Demo: Q-learning – crawler (L10D3)] Video of Demo Q-Learning -- Gridworld Video of Demo Q-Learning -- Crawler Q-Learning Properties § Amazing result: Q-learning converges to optimal policy -- even if you’re acting suboptimally! § This is called off-policy learning § Caveats: § You have to explore enough § You have to eventually make the learning rate small enough § … but not decrease it too quickly § Basically, in the limit, it doesn’t matter how you select actions (!) "
521,"Announcements § Project 2 Mini-Contest (Optional) § Ends Sunday 9/30 § Homework 5 § Released, due Monday 10/1 at 11:59pm. § Project 3: RL § Released, due Friday 10/5 at 4:00pm. CS 188: Artificial Intelligence Reinforcement Learning II Instructors: Pieter Abbeel & Dan Klein --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Reinforcement Learning § We still assume an MDP: § A set of states s Î S § A set of actions (per state) A § A model T(s,a,s’) § A reward function R(s,a,s’) § Still looking for a policy p(s) § New twist: don’t know T or R, so must try out actions § Big idea: Compute all averages over T using sample outcomes The Story So Far: MDPs and RL Known MDP: Offline Solution Goal Technique Compute V*, Q*, p* Value / policy iteration Evaluate a fixed policy p Policy evaluation Unknown MDP: Model-Based Unknown MDP: Model-Free Goal Technique Compute V*, Q*, p* VI/PI on approx. MDP Evaluate a fixed policy p PE on approx. MDP Goal Technique Compute V*, Q*, p* Q-learning Evaluate a fixed policy p Value Learning Model-Free Learning § Model-free (temporal difference) learning § Experience world through episodes § Update estimates each transition § Over time, updates will mimic Bellman updates r a s s, a s’ a’ s’, a’ s’’ Q-Learning § We’d like to do Q-value updates to each Q-state: § But can’t compute this update without knowing T, R § Instead, compute average as we go § Receive a sample transition (s,a,r,s’) § This sample suggests § But we want to average over results from (s,a) (Why?) § So keep a running average Q-Learning Properties § Amazing result: Q-learning converges to optimal policy -- even if you’re acting suboptimally! § This is called off-policy learning § Caveats: § You have to explore enough § You have to eventually make the learning rate small enough § … but not decrease it too quickly § Basically, in the limit, it doesn’t matter how you select actions (!) [Demo: Q-learning – auto – cliff grid (L11D1)] Video of Demo Q-Learning Auto Cliff Grid Exploration vs. Exploitation How to Explore? § Several schemes for forcing exploration § Simplest: random actions (e-greedy) § Every time step, flip a coin § With (small) probability e, act randomly § With (large) probability 1-e, act on current policy § Problems with random actions? § You do eventually explore the space, but keep thrashing around once learning is done § One solution: lower e over time § Another solution: exploration functions [Demo: Q-learning – manual exploration – bridge grid (L11D2)] [Demo: Q-learning – epsilon-greedy -- crawler (L11D3)] Video of Demo Q-learning – Manual Exploration – Bridge Grid Video of Demo Q-learning – Epsilon-Greedy – Crawler Exploration Functions § When to explore? § Random actions: explore a fixed amount § Better idea: explore areas whose badness is not (yet) established, eventually stop exploring § Exploration function § Takes a value estimate u and a visit count n, and returns an optimistic utility, e.g. § Note: this propagates the “bonus” back to states that lead to unknown states as well! Modified Q-Update: Regular Q-Update: [Demo: exploration – Q-learning – crawler – exploration function (L11D4)] Video of Demo Q-learning – Exploration Function – Crawler Regret § Even if you learn the optimal policy, you still make mistakes along the way! § Regret is a measure of your total mistake cost: the difference between your (expected) rewards, including youthful suboptimality, and optimal (expected) rewards § Minimizing regret goes beyond learning to be optimal – it requires optimally learning to be optimal § Example: random exploration and exploration functions both end up optimal, but random exploration has higher regret Approximate Q-Learning Generalizing Across States § Basic Q-Learning keeps a table of all q-values § In realistic situations, we cannot possibly learn about every single state! § Too many states to visit them all in training § Too many states to hold the q-tables in memory § Instead, we want to generalize: § Learn about some small number of training states from experience § Generalize that experience to new, similar situations § This is a fundamental idea in machine learning, and we’ll see it over and over again [demo – RL pacman] Example: Pacman [Demo: Q-learning – pacman – tiny – watch all (L11D5)],[Demo: Q-learning – pacman – tiny – silent train (L11D6)], [Demo: Q-learning – pacman – tricky – watch all (L11D7)] Let’s say we discover through experience that this state is bad: In naïve q-learning, we know nothing about this state: Or even this one! Video of Demo Q-Learning Pacman – Tiny – Watch All Video of Demo Q-Learning Pacman – Tiny – Silent Train Video of Demo Q-Learning Pacman – Tricky – Watch All Feature-Based Representations § Solution: describe a state using a vector of features (properties) § Features are functions from states to real numbers (often 0/1) that capture important properties of the state § Example features: § Distance to closest ghost § Distance to closest dot § Number of ghosts § 1 / (dist to dot)2 § Is Pacman in a tunnel? (0/1) § …… etc. § Is it the exact state on this slide? § Can also describe a q-state (s, a) with features (e.g. action moves closer to food) Linear Value Functions § Using a feature representation, we can write a q function (or value function) for any state using a few weights: § Advantage: our experience is summed up in a few powerful numbers § Disadvantage: states may share features but actually be very different in value! Approximate Q-Learning § Q-learning with linear Q-functions: § Intuitive interpretation: § Adjust weights of active features § E.g., if something unexpectedly bad happens, blame the features that were on: disprefer all states with that state’s features § Formal justification: online least squares Exact Q’s Approximate Q’s Example: Q-Pacman [Demo: approximate Q- learning pacman (L11D10)] Video of Demo Approximate Q-Learning -- Pacman Q-Learning and Least Squares 0 20 0 20 40 0 10 20 30 40 0 10 20 30 20 22 24 26 Linear Approximation: Regression* Prediction: Prediction: Optimization: Least Squares* 0 20 0 Error or “residual” Prediction Observation Minimizing Error* Approximate q update explained: Imagine we had only one point x, with features f(x), target value y, and weights w: “target” “prediction” 0 2 4 6 8 10 12 14 16 18 20 -15 -10 -5 0 5 10 15 20 25 30 Degree 15 polynomial Overfitting: Why Limiting Capacity Can Help* Policy Search Policy Search § Problem: often the feature-based policies that work well (win games, maximize utilities) aren’t the ones that approximate V / Q best § E.g. your value functions from project 2 were probably horrible estimates of future rewards, but they still produced good decisions § Q-learning’s priority: get Q-values close (modeling) § Action selection priority: get ordering of Q-values right (prediction) § We’ll see this distinction between modeling and prediction again later in the course § Solution: learn policies that maximize rewards, not the values that predict them § Policy search: start with an ok solution (e.g. Q-learning) then fine-tune by hill climbing on feature weights Policy Search § Simplest policy search: § Start with an initial linear value function or Q-function § Nudge each feature weight up and down and see if your policy is better than before § Problems: § How do we tell the policy got better? § Need to run many sample episodes! § If there are a lot of features, this can be impractical § Better methods exploit lookahead structure, sample wisely, change multiple parameters… RL: Helicopter Flight [Andrew Ng] [Video: HELICOPTER] RL: Learning Locomotion [Video: GAE] [Schulman, Moritz, Levine, Jordan, Abbeel, ICLR 2016] RL: Learning Soccer [Bansal et al, 2017] RL: Learning Manipulation [Levine*, Finn*, Darrell, Abbeel, JMLR 2016] RL: NASA SUPERball [Geng*, Zhang*, Bruce*, Caluwaerts, Vespignani, Sunspiral, Abbeel, Levine, ICRA 2017] Pieter Abbeel -- UC Berkeley | Gradescope | Covariant.AI RL: In-Hand Manipulation Pieter Abbeel -- UC Berkeley | Gradescope | Covariant.AI OpenAI: Dactyl Trained with domain randomization [OpenAI] Conclusion § We’re done with Part I: Search and Planning! § We’ve seen how AI methods can solve problems in: § Search § Constraint Satisfaction Problems § Games § Markov Decision Problems § Reinforcement Learning § Next up: Part II: Uncertainty and Learning! "
522,"CS188 Outline We’re done with Part I: Search and Planning! Part II: Probabilistic Reasoning Diagnosis Speech recognition Tracking objects Robot mapping Genetics Error correcting codes … lots more! Part III: Machine Learning CS 188: Artificial Intelligence Probability Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Today Probability Random Variables Joint and Marginal Distributions Conditional Distribution Product Rule, Chain Rule, Bayes’ Rule Inference Independence You’ll need all this stuff A LOT for the next few weeks, so make sure you go over it now! Inference in Ghostbusters A ghost is in the grid somewhere Sensor readings tell how close a square is to the ghost On the ghost: red 1 or 2 away: orange 3 or 4 away: yellow 5+ away: green P(red | 3) P(orange | 3) P(yellow | 3) P(green | 3) 0.05 0.15 0.5 0.3  Sensors are noisy, but we know P(Color | Distance) [Demo: Ghostbuster – no probability (L12D1) ] Uncertainty General situation: Observed variables (evidence): Agent knows certain things about the state of the world (e.g., sensor readings or symptoms) Unobserved variables: Agent needs to reason about other aspects (e.g. where an object is or what disease is present) Model: Agent knows something about how the known variables relate to the unknown variables Probabilistic reasoning gives us a framework for managing our beliefs and knowledge Random Variables A random variable is some aspect of the world about which we (may) have uncertainty R = Is it raining? T = Is it hot or cold? D = How long will it take to drive to work? L = Where is the ghost? We denote random variables with capital letters Like variables in a CSP, random variables have domains R in {true, false} (often write as {+r, -r}) T in {hot, cold} D in [0, ∞) L in possible locations, maybe {(0,0), (0,1), …} Probability Distributions Associate a probability with each value Temperature: T P hot 0.5 cold 0.5 W P sun 0.6 rain 0.1 fog 0.3 meteor 0.0 Weather:  Shorthand notation: OK if all domain entries are unique Probability Distributions Unobserved random variables have distributions A distribution is a TABLE of probabilities of values A probability (lower case value) is a single number Must have: and T P hot 0.5 cold 0.5 W P sun 0.6 rain 0.1 fog 0.3 meteor 0.0 Joint Distributions A joint distribution over a set of random variables: specifies a real number for each assignment (or outcome): Must obey: Size of distribution if n variables with domain sizes d? For all but the smallest distributions, impractical to write out! T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 Probabilistic Models  A probabilistic model is a joint distribution over a set of random variables  Probabilistic models: (Random) variables with domains Assignments are called outcomes Joint distributions: say whether assignments (outcomes) are likely Normalized: sum to 1.0 Ideally: only certain variables directly interact  Constraint satisfaction problems: Variables with domains Constraints: state whether assignments are possible Ideally: only certain variables directly interact T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 T W P hot sun T hot rain F cold sun F cold rain T Distribution over T,W Constraint over T,W Events An event is a set E of outcomes From a joint distribution, we can calculate the probability of any event Probability that it’s hot AND sunny? Probability that it’s hot? Probability that it’s hot OR sunny? Typically, the events we care about are partial assignments, like P(T=hot) T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 Quiz: Events P(+x, +y) ? P(+x) ? P(-y OR +x) ? X Y P +x +y 0.2 +x -y 0.3 -x +y 0.4 -x -y 0.1 Marginal Distributions  Marginal distributions are sub-tables which eliminate variables  Marginalization (summing out): Combine collapsed rows by adding T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 T P hot 0.5 cold 0.5 W P sun 0.6 rain 0.4 Quiz: Marginal Distributions X Y P +x +y 0.2 +x -y 0.3 -x +y 0.4 -x -y 0.1 X P +x -x Y P +y -y Conditional Probabilities A simple relation between joint and conditional probabilities In fact, this is taken as the definition of a conditional probability T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 P(b) P(a) P(a,b) Quiz: Conditional Probabilities X Y P +x +y 0.2 +x -y 0.3 -x +y 0.4 -x -y 0.1 P(+x | +y) ? P(-x | +y) ? P(-y | +x) ? Conditional Distributions Conditional distributions are probability distributions over some variables given fixed values of others T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 W P sun 0.8 rain 0.2 W P sun 0.4 rain 0.6 Conditional Distributions Joint Distribution Normalization Trick T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 W P sun 0.4 rain 0.6 SELECT the joint probabilities matching the evidence Normalization Trick T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 W P sun 0.4 rain 0.6 T W P cold sun 0.2 cold rain 0.3 NORMALIZE the selection (make it sum to one) Normalization Trick T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 W P sun 0.4 rain 0.6 T W P cold sun 0.2 cold rain 0.3 SELECT the joint probabilities matching the evidence NORMALIZE the selection (make it sum to one) Why does this work? Sum of selection is P(evidence)! (P(T=c), here) Quiz: Normalization Trick X Y P +x +y 0.2 +x -y 0.3 -x +y 0.4 -x -y 0.1 SELECT the joint probabilities matching the evidence NORMALIZE the selection (make it sum to one) P(X | Y=-y) ? (Dictionary) To bring or restore to a normal condition Procedure: Step 1: Compute Z = sum over all entries Step 2: Divide every entry by Z Example 1 To Normalize All entries sum to ONE W P sun 0.2 rain 0.3 Z = 0.5 W P sun 0.4 rain 0.6 Example 2 T W P hot sun 20 hot rain 5 cold sun 10 cold rain 15 Normalize Z = 50 Normalize T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 Probabilistic Inference Probabilistic inference: compute a desired probability from other known probabilities (e.g. conditional from joint) We generally compute conditional probabilities P(on time | no reported accidents) = 0.90 These represent the agent’s beliefs given the evidence Probabilities change with new evidence: P(on time | no accidents, 5 a.m.) = 0.95 P(on time | no accidents, 5 a.m., raining) = 0.80 Observing new evidence causes beliefs to be updated Inference by Enumeration  General case: Evidence variables: Query* variable: Hidden variables: All variables * Works fine with multiple query variables, too  We want:  Step 1: Select the entries consistent with the evidence  Step 2: Sum out H to get joint of Query and evidence  Step 3: Normalize Inference by Enumeration P(W)? P(W | winter)? P(W | winter, hot)? S T W P summer hot sun 0.30 summer hot rain 0.05 summer cold sun 0.10 summer cold rain 0.05 winter hot sun 0.10 winter hot rain 0.05 winter cold sun 0.15 winter cold rain 0.20  Obvious problems: Worst-case time complexity O(dn) Space complexity O(dn) to store the joint distribution Inference by Enumeration The Product Rule Sometimes have conditional distributions but want the joint The Product Rule Example: R P sun 0.8 rain 0.2 D W P wet sun 0.1 dry sun 0.9 wet rain 0.7 dry rain 0.3 D W P wet sun 0.08 dry sun 0.72 wet rain 0.14 dry rain 0.06 The Chain Rule More generally, can always write any joint distribution as an incremental product of conditional distributions Why is this always true? Bayes Rule Bayes’ Rule Two ways to factor a joint distribution over two variables: Dividing, we get: Why is this at all helpful? Lets us build one conditional from its reverse Often one conditional is tricky but the other one is simple Foundation of many systems we’ll see later (e.g. ASR, MT) In the running for most important AI equation! That’s my rule! Inference with Bayes’ Rule Example: Diagnostic probability from causal probability: Example: M: meningitis, S: stiff neck Note: posterior probability of meningitis still very small Note: you should still get stiff necks checked out! Why? Example givens Quiz: Bayes’ Rule Given: What is P(W | dry) ? R P sun 0.8 rain 0.2 D W P wet sun 0.1 dry sun 0.9 wet rain 0.7 dry rain 0.3 Ghostbusters, Revisited Let’s say we have two distributions: Prior distribution over ghost location: P(G) Let’s say this is uniform Sensor reading model: P(R | G) Given: we know what our sensors do R = reading color measured at (1,1) E.g. P(R = yellow | G=(1,1)) = 0.1 We can calculate the posterior distribution P(G|r) over ghost locations given a reading using Bayes’ rule: [Demo: Ghostbuster – with probability (L12D2) ] Next Time: Markov Models "
523,"CS 188: Artificial Intelligence Bayes’ Nets Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Probabilistic Models Models describe how (a portion of) the world works Models are always simplifications May not account for every variable May not account for all interactions between variables “All models are wrong; but some are useful.” – George E. P. Box What do we do with probabilistic models? We (or our agents) need to reason about unknown variables, given evidence Example: explanation (diagnostic reasoning) Example: prediction (causal reasoning) Example: value of information Independence Two variables are independent if: This says that their joint distribution factors into a product two simpler distributions Another form: We write: Independence is a simplifying modeling assumption Empirical joint distributions: at best “close” to independent What could we assume for {Weather, Traffic, Cavity, Toothache}? Independence Example: Independence? T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 T W P hot sun 0.3 hot rain 0.2 cold sun 0.3 cold rain 0.2 T P hot 0.5 cold 0.5 W P sun 0.6 rain 0.4 Example: Independence N fair, independent coin flips: H 0.5 T 0.5 H 0.5 T 0.5 H 0.5 T 0.5 Conditional Independence  P(Toothache, Cavity, Catch)  If I have a cavity, the probability that the probe catches in it doesn't depend on whether I have a toothache: P(+catch | +toothache, +cavity) = P(+catch | +cavity)  The same independence holds if I don’t have a cavity: P(+catch | +toothache, -cavity) = P(+catch| -cavity)  Catch is conditionally independent of Toothache given Cavity: P(Catch | Toothache, Cavity) = P(Catch | Cavity)  Equivalent statements: P(Toothache | Catch , Cavity) = P(Toothache | Cavity) P(Toothache, Catch | Cavity) = P(Toothache | Cavity) P(Catch | Cavity) One can be derived from the other easily Conditional Independence Unconditional (absolute) independence very rare (why?) Conditional independence is our most basic and robust form of knowledge about uncertain environments. X is conditionally independent of Y given Z if and only if: or, equivalently, if and only if Conditional Independence What about this domain: Traffic Umbrella Raining Conditional Independence What about this domain: Fire Smoke Alarm Conditional Independence and the Chain Rule Chain rule: Trivial decomposition: With assumption of conditional independence: Bayes’nets / graphical models help us express conditional independence assumptions Ghostbusters Chain Rule  Each sensor depends only on where the ghost is  That means, the two sensors are conditionally independent, given the ghost position  T: Top square is red B: Bottom square is red G: Ghost is in the top  Givens: P( +g ) = 0.5 P( -g ) = 0.5 P( +t | +g ) = 0.8 P( +t | -g ) = 0.4 P( +b | +g ) = 0.4 P( +b | -g ) = 0.8 P(T,B,G) = P(G) P(T|G) P(B|G) T B G P(T,B,G) +t +b +g 0.16 +t +b -g 0.16 +t -b +g 0.24 +t -b -g 0.04 -t +b +g 0.04 -t +b -g 0.24 -t -b +g 0.06 -t -b -g 0.06 Bayes’Nets: Big Picture Bayes’ Nets: Big Picture Two problems with using full joint distribution tables as our probabilistic models: Unless there are only a few variables, the joint is WAY too big to represent explicitly Hard to learn (estimate) anything empirically about more than a few variables at a time Bayes’ nets: a technique for describing complex joint distributions (models) using simple, local distributions (conditional probabilities) More properly called graphical models We describe how variables locally interact Local interactions chain together to give global, indirect interactions For about 10 min, we’ll be vague about how these interactions are specified Example Bayes’ Net: Insurance Example Bayes’ Net: Car Graphical Model Notation Nodes: variables (with domains) Can be assigned (observed) or unassigned (unobserved) Arcs: interactions Similar to CSP constraints Indicate “direct influence” between variables Formally: encode conditional independence (more later) For now: imagine that arrows mean direct causation (in general, they don’t!) Example: Coin Flips N independent coin flips No interactions between variables: absolute independence X1 X2 Xn Example: Traffic Variables: R: It rains T: There is traffic Model 1: independence Why is an agent using model 2 better? R T R T Model 2: rain causes traffic Let’s build a causal graphical model! Variables T: Traffic R: It rains L: Low pressure D: Roof drips B: Ballgame C: Cavity Example: Traffic II Example: Alarm Network Variables B: Burglary A: Alarm goes off M: Mary calls J: John calls E: Earthquake! Bayes’ Net Semantics Bayes’ Net Semantics A set of nodes, one per variable X A directed, acyclic graph A conditional distribution for each node A collection of distributions over X, one for each combination of parents’ values CPT: conditional probability table Description of a noisy “causal” process A1 X An A Bayes net = Topology (graph) + Local Conditional Probabilities Probabilities in BNs Bayes’ nets implicitly encode joint distributions As a product of local conditional distributions To see what probability a BN gives to a full assignment, multiply all the relevant conditionals together: Example: Probabilities in BNs Why are we guaranteed that setting results in a proper joint distribution? Chain rule (valid for all distributions): Assume conditional independences: Consequence: Not every BN can represent every joint distribution The topology enforces certain conditional independencies Only distributions whose variables are absolutely independent can be represented by a Bayes’ net with no arcs. Example: Coin Flips h 0.5 t 0.5 h 0.5 t 0.5 h 0.5 t 0.5 X1 X2 Xn Example: Traffic R T +r 1/4 -r 3/4 +r +t 3/4 -t 1/4 -r +t 1/2 -t 1/2 Example: Alarm Network Burglary Earthqk Alarm John calls Mary calls B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 Example: Traffic Causal direction R T +r 1/4 -r 3/4 +r +t 3/4 -t 1/4 -r +t 1/2 -t 1/2 +r +t 3/16 +r -t 1/16 -r +t 6/16 -r -t 6/16 Example: Reverse Traffic Reverse causality? T R +t 9/16 -t 7/16 +t +r 1/3 -r 2/3 -t +r 1/7 -r 6/7 +r +t 3/16 +r -t 1/16 -r +t 6/16 -r -t 6/16 Causality? When Bayes’ nets reflect the true causal patterns: Often simpler (nodes have fewer parents) Often easier to think about Often easier to elicit from experts BNs need not actually be causal Sometimes no causal net exists over the domain (especially if variables are missing) E.g. consider the variables Traffic and Drips End up with arrows that reflect correlation, not causation What do the arrows really mean? Topology may happen to encode causal structure Topology really encodes conditional independence Bayes’ Nets So far: how a Bayes’ net encodes a joint distribution Next: how to answer queries about that distribution Today: First assembled BNs using an intuitive notion of conditional independence as causality Then saw that key property is conditional independence Main goal: answer queries about conditional independence and influence After that: how to answer numerical queries (inference) "
524,"CS 188: Artificial Intelligence Bayes’ Nets: Independence Instructors: Pieter Abbeel & Dan Klein --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Probability Recap § Conditional probability § Product rule § Chain rule § X, Y independent if and only if: § X and Y are conditionally independent given Z if and only if: Bayes’ Nets § A Bayes’ net is an efficient encoding of a probabilistic model of a domain § Questions we can ask: § Inference: given a fixed BN, what is P(X | e)? § Representation: given a BN graph, what kinds of distributions can it encode? § Modeling: what BN is most appropriate for a given domain? Bayes’ Net Semantics § A directed, acyclic graph, one node per random variable § A conditional probability table (CPT) for each node § A collection of distributions over X, one for each combination of parents values § Bayes nets implicitly encode joint distributions § As a product of local conditional distributions § To see what probability a BN gives to a full assignment, multiply all the relevant conditionals together: Example: Alarm Network B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 B E A M J Example: Alarm Network B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 B E A M J Size of a Bayes Net § How big is a joint distribution over N Boolean variables? 2N § How big is an N-node net if nodes have up to k parents? O(N * 2k+1) § Both give you the power to calculate § BNs: Huge space savings! § Also easier to elicit local CPTs § Also faster to answer queries (coming) Bayes’ Nets § Representation § Conditional Independences § Probabilistic Inference § Learning Bayes’ Nets from Data Conditional Independence § X and Y are independent if § X and Y are conditionally independent given Z § (Conditional) independence is a property of a distribution § Example: Bayes Nets: Assumptions § Assumptions we are required to make to define the Bayes net when given the graph: § Beyond above “chain rule à Bayes net” conditional independence assumptions § Often additional conditional independences § They can be read off the graph § Important for modeling: understand assumptions made when choosing a Bayes net graph P(xi|x1 · · · xi−1) = P(xi|parents(Xi)) Example § Conditional independence assumptions directly from simplifications in chain rule: § Additional implied conditional independence assumptions? X Y Z W Independence in a BN § Important question about a BN: § Are two nodes independent given certain evidence? § If yes, can prove using algebra (tedious in general) § If no, can prove with a counter example § Example: § Question: are X and Z necessarily independent? § Answer: no. Example: low pressure causes rain, which causes traffic. § X can influence Z, Z can influence X (via Y) § Addendum: they could be independent: how? X Y Z D-separation: Outline D-separation: Outline § Study independence properties for triples § Analyze complex cases in terms of member triples § D-separation: a condition / algorithm for answering such queries Causal Chains § This configuration is a causal chain X: Low pressure Y: Rain Z: Traffic § Guaranteed X independent of Z ? No! § One example set of CPTs for which X is not independent of Z is sufficient to show this independence is not guaranteed. § Example: § Low pressure causes rain causes traffic, high pressure causes no rain causes no traffic § In numbers: P( +y | +x ) = 1, P( -y | - x ) = 1, P( +z | +y ) = 1, P( -z | -y ) = 1 Causal Chains § This configuration is a causal chain § Guaranteed X independent of Z given Y? § Evidence along the chain blocks the influence Yes! X: Low pressure Y: Rain Z: Traffic Common Cause § This configuration is a common cause § Guaranteed X independent of Z ? No! § One example set of CPTs for which X is not independent of Z is sufficient to show this independence is not guaranteed. § Example: § Project due causes both forums busy and lab full § In numbers: P( +x | +y ) = 1, P( -x | -y ) = 1, P( +z | +y ) = 1, P( -z | -y ) = 1 Y: Project due X: Forums busy Z: Lab full Common Cause § This configuration is a common cause § Guaranteed X and Z independent given Y? § Observing the cause blocks influence between effects. Yes! Y: Project due X: Forums busy Z: Lab full Common Effect § Last configuration: two causes of one effect (v-structures) Z: Traffic § Are X and Y independent? § Yes: the ballgame and the rain cause traffic, but they are not correlated § Still need to prove they must be (try it!) § Are X and Y independent given Z? § No: seeing traffic puts the rain and the ballgame in competition as explanation. § This is backwards from the other cases § Observing an effect activates influence between possible causes. X: Raining Y: Ballgame The General Case The General Case § General question: in a given BN, are two variables independent (given evidence)? § Solution: analyze the graph § Any complex example can be broken into repetitions of the three canonical cases Reachability § Recipe: shade evidence nodes, look for paths in the resulting graph § Attempt 1: if two nodes are connected by an undirected path not blocked by a shaded node, they are conditionally independent § Almost works, but not quite § Where does it break? § Answer: the v-structure at T doesn’t count as a link in a path unless “active” R T B D L Active / Inactive Paths § Question: Are X and Y conditionally independent given evidence variables {Z}? § Yes, if X and Y d-separated by Z § Consider all (undirected) paths from X to Y § No active paths = independence! § A path is active if each triple is active: § Causal chain A ® B ® C where B is unobserved (either direction) § Common cause A ¬ B ® C where B is unobserved § Common effect (aka v-structure) A ® B ¬ C where B or one of its descendents is observed § All it takes to block a path is a single inactive segment Active Triples Inactive Triples § Query: § Check all (undirected!) paths between and § If one or more active, then independence not guaranteed § Otherwise (i.e. if all paths are inactive), then independence is guaranteed D-Separation Xi ⊥ ⊥Xj|{Xk1, ..., Xkn} Xi ⊥ ⊥Xj|{Xk1, ..., Xkn} ? Xi ⊥ ⊥Xj|{Xk1, ..., Xkn} Example Yes R T B T Example R T B D L T Yes Yes Yes Example § Variables: § R: Raining § T: Traffic § D: Roof drips § S: I’m sad § Questions: T S D R Yes Structure Implications § Given a Bayes net structure, can run d- separation algorithm to build a complete list of conditional independences that are necessarily true of the form § This list determines the set of probability distributions that can be represented Xi ⊥ ⊥Xj|{Xk1, ..., Xkn} Computing All Independences X Y Z X Y Z X Y Z X Y Z X Y Z {X ⊥ ⊥Y, X ⊥ ⊥Z, Y ⊥ ⊥Z, X ⊥ ⊥Z | Y, X ⊥ ⊥Y | Z, Y ⊥ ⊥Z | X} Topology Limits Distributions § Given some graph topology G, only certain joint distributions can be encoded § The graph structure guarantees certain (conditional) independences § (There might be more independence) § Adding arcs increases the set of distributions, but has several costs § Full conditioning can encode any distribution X Y Z X Y Z X Y Z {X ⊥ ⊥Z | Y } X Y Z X Y Z X Y Z X Y Z X Y Z X Y Z {} Bayes Nets Representation Summary § Bayes nets compactly encode joint distributions § Guaranteed independencies of distributions can be deduced from BN graph structure § D-separation gives precise conditional independence guarantees from graph alone § A Bayes nets joint distribution may have further (conditional) independence that is not detectable until you inspect its specific distribution Bayes’ Nets § Representation § Conditional Independences § Probabilistic Inference § Enumeration (exact, exponential complexity) § Variable elimination (exact, worst-case exponential complexity, often better) § Probabilistic inference is NP-complete § Sampling (approximate) § Learning Bayes’ Nets from Data "
525,"CS 188: Artificial Intelligence Bayes’ Nets: Inference Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Bayes’ Net Representation A directed, acyclic graph, one node per random variable A conditional probability table (CPT) for each node A collection of distributions over X, one for each combination of parents’ values Bayes’ nets implicitly encode joint distributions As a product of local conditional distributions To see what probability a BN gives to a full assignment, multiply all the relevant conditionals together: Example: Alarm Network Burglary Earthqk Alarm John calls Mary calls B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 [Demo: BN Applet] Example: Alarm Network B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 B E A M J Example: Alarm Network B P(B) +b 0.001 -b 0.999 E P(E) +e 0.002 -e 0.998 B E A P(A|B,E) +b +e +a 0.95 +b +e -a 0.05 +b -e +a 0.94 +b -e -a 0.06 -b +e +a 0.29 -b +e -a 0.71 -b -e +a 0.001 -b -e -a 0.999 A J P(J|A) +a +j 0.9 +a -j 0.1 -a +j 0.05 -a -j 0.95 A M P(M|A) +a +m 0.7 +a -m 0.3 -a +m 0.01 -a -m 0.99 B E A M J Bayes’ Nets Representation Conditional Independences Probabilistic Inference Enumeration (exact, exponential complexity) Variable elimination (exact, worst-case exponential complexity, often better) Inference is NP-complete Sampling (approximate) Learning Bayes’ Nets from Data Examples: Posterior probability Most likely explanation: Inference Inference: calculating some useful quantity from a joint probability distribution Inference by Enumeration  General case: Evidence variables: Query* variable: Hidden variables: All variables * Works fine with multiple query variables, too  We want:  Step 1: Select the entries consistent with the evidence  Step 2: Sum out H to get joint of Query and evidence  Step 3: Normalize Inference by Enumeration in Bayes’ Net Given unlimited time, inference in BNs is easy Reminder of inference by enumeration by example: B E A M J Inference by Enumeration? Inference by Enumeration vs. Variable Elimination Why is inference by enumeration so slow? You join up the whole joint distribution before you sum out the hidden variables Idea: interleave joining and marginalizing! Called “Variable Elimination” Still NP-hard, but usually much faster than inference by enumeration First we’ll need some new notation: factors Factor Zoo Factor Zoo I Joint distribution: P(X,Y) Entries P(x,y) for all x, y Sums to 1 Selected joint: P(x,Y) A slice of the joint distribution Entries P(x,y) for fixed x, all y Sums to P(x) Number of capitals = dimensionality of the table T W P hot sun 0.4 hot rain 0.1 cold sun 0.2 cold rain 0.3 T W P cold sun 0.2 cold rain 0.3 Factor Zoo II Single conditional: P(Y | x) Entries P(y | x) for fixed x, all y Sums to 1 Family of conditionals: P(Y | X) Multiple conditionals Entries P(y | x) for all x, y Sums to |X| T W P hot sun 0.8 hot rain 0.2 cold sun 0.4 cold rain 0.6 T W P cold sun 0.4 cold rain 0.6 Factor Zoo III Specified family: P( y | X ) Entries P(y | x) for fixed y, but for all x Sums to … who knows! T W P hot rain 0.2 cold rain 0.6 Factor Zoo Summary In general, when we write P(Y1 … YN | X1 … XM) It is a “factor,” a multi-dimensional array Its values are P(y1 … yN | x1 … xM) Any assigned (=lower-case) X or Y is a dimension missing (selected) from the array Example: Traffic Domain Random Variables R: Raining T: Traffic L: Late for class! T L R +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 Inference by Enumeration: Procedural Outline Track objects called factors Initial factors are local CPTs (one per node) Any known values are selected E.g. if we know , the initial factors are Procedure: Join all factors, eliminate all hidden variables, normalize +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 +t +l 0.3 -t +l 0.1 +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 Operation 1: Join Factors  First basic operation: joining factors  Combining factors: Just like a database join Get all factors over the joining variable Build a new factor over the union of the variables involved  Example: Join on R Computation for each entry: pointwise products +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +r +t 0.08 +r -t 0.02 -r +t 0.09 -r -t 0.81 T R R,T Example: Multiple Joins Example: Multiple Joins T R Join R L R, T L +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 +r +t 0.08 +r -t 0.02 -r +t 0.09 -r -t 0.81 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 R, T, L +r +t +l 0.024 +r +t -l 0.056 +r -t +l 0.002 +r -t -l 0.018 -r +t +l 0.027 -r +t -l 0.063 -r -t +l 0.081 -r -t -l 0.729 Join T Operation 2: Eliminate Second basic operation: marginalization Take a factor and sum out a variable Shrinks a factor to a smaller one A projection operation Example: +r +t 0.08 +r -t 0.02 -r +t 0.09 -r -t 0.81 +t 0.17 -t 0.83 Multiple Elimination Sum out R Sum out T T, L L R, T, L +r +t +l 0.024 +r +t -l 0.056 +r -t +l 0.002 +r -t -l 0.018 -r +t +l 0.027 -r +t -l 0.063 -r -t +l 0.081 -r -t -l 0.729 +t +l 0.051 +t -l 0.119 -t +l 0.083 -t -l 0.747 +l 0.134 -l 0.886 Thus Far: Multiple Join, Multiple Eliminate (= Inference by Enumeration) Marginalizing Early (= Variable Elimination) Traffic Domain Inference by Enumeration T L R Variable Elimination Join on r Join on r Join on t Join on t Eliminate r Eliminate t Eliminate r Eliminate t Marginalizing Early! (aka VE) Sum out R T L +r +t 0.08 +r -t 0.02 -r +t 0.09 -r -t 0.81 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 +t 0.17 -t 0.83 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 T R L +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 Join R R, T L T, L L +t +l 0.051 +t -l 0.119 -t +l 0.083 -t -l 0.747 +l 0.134 -l 0.866 Join T Sum out T Evidence If evidence, start with factors that select that evidence No evidence uses these initial factors: Computing , the initial factors become: We eliminate all vars other than query + evidence +r 0.1 -r 0.9 +r +t 0.8 +r -t 0.2 -r +t 0.1 -r -t 0.9 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 +r 0.1 +r +t 0.8 +r -t 0.2 +t +l 0.3 +t -l 0.7 -t +l 0.1 -t -l 0.9 Evidence II Result will be a selected joint of query and evidence E.g. for P(L | +r), we would end up with: To get our answer, just normalize this! That’s it! +l 0.26 -l 0.74 +r +l 0.026 +r -l 0.074 Normalize General Variable Elimination Query: Start with initial factors: Local CPTs (but instantiated by evidence) While there are still hidden variables (not Q or evidence): Pick a hidden variable H Join all factors mentioning H Eliminate (sum out) H Join all remaining factors and normalize Example Choose A Example Choose E Finish with B Normalize Same Example in Equations marginal obtained from joint by summing out use Bayes’ net joint distribution expression use x*(y+z) = xy + xz joining on a, and then summing out gives f1 use x*(y+z) = xy + xz joining on e, and then summing out gives f2 All we are doing is exploiting uwy + uwz + uxy + uxz + vwy + vwz + vxy +vxz = (u+v)(w+x)(y+z) to improve computational efficiency! Another Variable Elimination Example Computational complexity critically depends on the largest factor being generated in this process. Size of factor = number of entries in table. In example above (assuming binary) all factors generated are of size 2 --- as they all only have one variable (Z, Z, and X3 respectively). Variable Elimination Ordering  For the query P(Xn|y1,…,yn) work through the following two different orderings as done in previous slide: Z, X1, …, Xn-1 and X1, …, Xn-1, Z. What is the size of the maximum factor generated for each of the orderings?  Answer: 2n+1 versus 22 (assuming binary)  In general: the ordering can greatly affect efficiency. … … VE: Computational and Space Complexity The computational and space complexity of variable elimination is determined by the largest factor The elimination ordering can greatly affect the size of the largest factor. E.g., previous slide’s example 2n vs. 2 Does there always exist an ordering that only results in small factors? No! Worst Case Complexity? CSP:  If we can answer P(z) equal to zero or not, we answered whether the 3-SAT problem has a solution.  Hence inference in Bayes’ nets is NP-hard. No known efficient probabilistic inference in general. … … Polytrees A polytree is a directed graph with no undirected cycles For poly-trees you can always find an ordering that is efficient Try it!! Cut-set conditioning for Bayes’ net inference Choose set of variables such that if removed only a polytree remains Exercise: Think about how the specifics would work out! Bayes’ Nets Representation Conditional Independences Probabilistic Inference Enumeration (exact, exponential complexity) Variable elimination (exact, worst-case exponential complexity, often better) Inference is NP-complete Sampling (approximate) Learning Bayes’ Nets from Data "
526,"CS 188: Artificial Intelligence Bayes’ Nets: Sampling Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Bayes’ Net Representation A directed, acyclic graph, one node per random variable A conditional probability table (CPT) for each node A collection of distributions over X, one for each combination of parents’ values Bayes’ nets implicitly encode joint distributions As a product of local conditional distributions To see what probability a BN gives to a full assignment, multiply all the relevant conditionals together: Variable Elimination Interleave joining and marginalizing dk entries computed for a factor over k variables with domain sizes d Ordering of elimination of hidden variables can affect size of factors generated Worst case: running time exponential in the size of the Bayes’ net … … Approximate Inference: Sampling Sampling Sampling is a lot like repeated simulation Predicting the weather, basketball games, … Basic idea Draw N samples from a sampling distribution S Compute an approximate posterior probability Show this converges to the true probability P Why sample? Learning: get samples from a distribution you don’t know Inference: getting a sample is faster than computing the right answer (e.g. with variable elimination) Sampling Sampling from given distribution Step 1: Get sample u from uniform distribution over [0, 1) E.g. random() in python Step 2: Convert this sample u into an outcome for the given distribution by having each target outcome associated with a sub-interval of [0,1) with sub-interval size equal to probability of the outcome Example If random() returns u = 0.83, then our sample is C = blue E.g, after sampling 8 times: C P(C) red 0.6 green 0.1 blue 0.3 Sampling in Bayes’ Nets Prior Sampling Rejection Sampling Likelihood Weighting Gibbs Sampling Prior Sampling Prior Sampling Cloudy Sprinkler Rain WetGrass Cloudy Sprinkler Rain WetGrass +c 0.5 -c 0.5 +c +s 0.1 -s 0.9 -c +s 0.5 -s 0.5 +c +r 0.8 -r 0.2 -c +r 0.2 -r 0.8 +s +r +w 0.99 -w 0.01 -r +w 0.90 -w 0.10 -s +r +w 0.90 -w 0.10 -r +w 0.01 -w 0.99 Samples: +c, -s, +r, +w -c, +s, -r, +w … Prior Sampling For i = 1, 2, …, n Sample xi from P(Xi | Parents(Xi)) Return (x1, x2, …, xn) Prior Sampling This process generates samples with probability: …i.e. the BN’s joint probability Let the number of samples of an event be Then I.e., the sampling procedure is consistent Example We’ll get a bunch of samples from the BN: +c, -s, +r, +w +c, +s, +r, +w -c, +s, +r, -w +c, -s, +r, +w -c, -s, -r, +w If we want to know P(W) We have counts <+w:4, -w:1> Normalize to get P(W) = <+w:0.8, -w:0.2> This will get closer to the true distribution with more samples Can estimate anything else, too What about P(C | +w)? P(C | +r, +w)? P(C | -r, -w)? Fast: can use fewer samples if less time (what’s the drawback?) S R W C Rejection Sampling  +c, -s, +r, +w +c, +s, +r, +w -c, +s, +r, -w +c, -s, +r, +w -c, -s, -r, +w Rejection Sampling Let’s say we want P(C) No point keeping all samples around Just tally counts of C as we go Let’s say we want P(C | +s) Same thing: tally C outcomes, but ignore (reject) samples which don’t have S=+s This is called rejection sampling It is also consistent for conditional probabilities (i.e., correct in the limit) S R W C Rejection Sampling  Input: evidence instantiation  For i = 1, 2, …, n Sample xi from P(Xi | Parents(Xi)) If xi not consistent with evidence Reject: return – no sample is generated in this cycle  Return (x1, x2, …, xn) Likelihood Weighting Idea: fix evidence variables and sample the rest Problem: sample distribution not consistent! Solution: weight by probability of evidence given parents Likelihood Weighting Problem with rejection sampling: If evidence is unlikely, rejects lots of samples Evidence not exploited as you sample Consider P( Shape | blue ) Shape Color Shape Color pyramid, green pyramid, red sphere, blue cube, red sphere, green pyramid, blue pyramid, blue sphere, blue cube, blue sphere, blue Likelihood Weighting +c 0.5 -c 0.5 +c +s 0.1 -s 0.9 -c +s 0.5 -s 0.5 +c +r 0.8 -r 0.2 -c +r 0.2 -r 0.8 +s +r +w 0.99 -w 0.01 -r +w 0.90 -w 0.10 -s +r +w 0.90 -w 0.10 -r +w 0.01 -w 0.99 Samples: +c, +s, +r, +w … Cloudy Sprinkler Rain WetGrass Cloudy Sprinkler Rain WetGrass Likelihood Weighting  Input: evidence instantiation  w = 1.0  for i = 1, 2, …, n if Xi is an evidence variable Xi = observation xi for Xi Set w = w * P(xi | Parents(Xi)) else Sample xi from P(Xi | Parents(Xi))  return (x1, x2, …, xn), w Likelihood Weighting  Sampling distribution if z sampled and e fixed evidence  Now, samples have weights  Together, weighted sampling distribution is consistent Cloudy R C S W Likelihood Weighting  Likelihood weighting is good We have taken evidence into account as we generate the sample E.g. here, W’s value will get picked based on the evidence values of S, R More of our samples will reflect the state of the world suggested by the evidence  Likelihood weighting doesn’t solve all our problems Evidence influences the choice of downstream variables, but not upstream ones (C isn’t more likely to get a value matching the evidence)  We would like to consider evidence when we sample every variable (leads to Gibbs sampling) S R W C Gibbs Sampling Gibbs Sampling  Procedure: keep track of a full instantiation x1, x2, …, xn. Start with an arbitrary instantiation consistent with the evidence. Sample one variable at a time, conditioned on all the rest, but keep evidence fixed. Keep repeating this for a long time.  Property: in the limit of repeating this infinitely many times the resulting samples come from the correct distribution (i.e. conditioned on evidence).  Rationale: both upstream and downstream variables condition on evidence.  In contrast: likelihood weighting only conditions on upstream evidence, and hence weights obtained in likelihood weighting can sometimes be very small. Sum of weights over all samples is indicative of how many “effective” samples were obtained, so we want high weight. Step 2: Initialize other variables Randomly Gibbs Sampling Example: P( S | +r) Step 1: Fix evidence R = +r Steps 3: Repeat Choose a non-evidence variable X Resample X from P( X | all other variables) S +r W C S +r W C S +r W C S +r W C S +r W C S +r W C S +r W C S +r W C Efficient Resampling of One Variable  Sample from P(S | +c, +r, -w) Many things cancel out – only CPTs with S remain! More generally: only CPTs that have resampled variable need to be considered, and joined together S +r W C Bayes’ Net Sampling Summary Prior Sampling P( Q ) Likelihood Weighting P( Q | e) Rejection Sampling P( Q | e ) Gibbs Sampling P( Q | e ) Further Reading on Gibbs Sampling* Gibbs sampling produces sample from the query distribution P( Q | e ) in limit of re-sampling infinitely often Gibbs sampling is a special case of more general methods called Markov chain Monte Carlo (MCMC) methods Metropolis-Hastings is one of the more famous MCMC methods (in fact, Gibbs sampling is a special case of Metropolis-Hastings) You may read about Monte Carlo methods – they’re just sampling "
527,"CS 188: Artificial Intelligence Decision Networks and Value of Information Instructors: Dan Klein and Pieter Abbeel University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Decision Networks Decision Networks Weather Forecast Umbrella U Decision Networks  MEU: choose the action which maximizes the expected utility given the evidence Weather Forecast Umbrella U  Can directly operationalize this with decision networks Bayes nets with nodes for utility and actions Lets us calculate the expected utility for each action  New node types: Chance nodes (just like BNs) Actions (rectangles, cannot have parents, act as observed evidence) Utility node (diamond, depends on action and chance nodes) Decision Networks Weather Forecast Umbrella U Action selection Instantiate all evidence Set action node(s) each possible way Calculate posterior for all parents of utility node, given the evidence Calculate expected utility for each action Choose maximizing action Decision Networks Weather Umbrella U W P(W) sun 0.7 rain 0.3 Umbrella = leave Umbrella = take Optimal decision = leave A W U(A,W) leave sun 100 leave rain 0 take sun 20 take rain 70 Decisions as Outcome Trees Almost exactly like expectimax / MDPs What’s changed? U(t,s) Weather | {} Weather | {} {} U(t,r) U(l,s) U(l,r) Weather Umbrella U Example: Decision Networks Weather Forecast =bad Umbrella U A W U(A,W) leave sun 100 leave rain 0 take sun 20 take rain 70 W P(W|F=bad) sun 0.34 rain 0.66 Umbrella = leave Umbrella = take Optimal decision = take Decisions as Outcome Trees U(t,s) W | {b} W | {b} U(t,r) U(l,s) U(l,r) {b} Weather Forecast =bad Umbrella U Ghostbusters Decision Network Ghost Location Sensor (1,1) Bust U Sensor (1,2) Sensor (1,3) Sensor (1,n) Sensor (2,1) Sensor (m,1) Sensor (m,n) … … … … Demo: Ghostbusters with probability Video of Demo Ghostbusters with Probability Value of Information Value of Information  Idea: compute value of acquiring evidence Can be done directly from decision network  Example: buying oil drilling rights Two blocks A and B, exactly one has oil, worth k You can drill in one location Prior probabilities 0.5 each, & mutually exclusive Drilling in either A or B has EU = k/2, MEU = k/2  Question: what’s the value of information of O? Value of knowing which of A or B has oil Value is expected gain in MEU from new info Survey may say “oil in a” or “oil in b”, prob 0.5 each If we know OilLoc, MEU is k (either way) Gain in MEU from knowing OilLoc? VPI(OilLoc) = k/2 Fair price of information: k/2 OilLoc DrillLoc U D O U a a k a b 0 b a 0 b b k O P a 1/2 b 1/2 VPI Example: Weather Weather Forecast Umbrella U A W U leave sun 100 leave rain 0 take sun 20 take rain 70 MEU with no evidence MEU if forecast is bad MEU if forecast is good F P(F) good 0.59 bad 0.41 Forecast distribution Value of Information  Assume we have evidence E=e. Value if we act now:  Assume we see that E’ = e’. Value if we act then:  BUT E’ is a random variable whose value is unknown, so we don’t know what e’ will be  Expected value if E’ is revealed and then we act:  Value of information: how much MEU goes up by revealing E’ first then acting, over acting now: P(s | +e) {+e} a U {+e, +e’} a P(s | +e, +e’) U {+e} P(+e’ | +e) {+e, +e’} P(-e’ | +e) {+e, -e’} a VPI Properties Nonnegative Nonadditive (think of observing Ej twice) Order-independent Quick VPI Questions The soup of the day is either clam chowder or split pea, but you wouldn’t order either one. What’s the value of knowing which it is? There are two kinds of plastic forks at a picnic. One kind is slightly sturdier. What’s the value of knowing which? You’re playing the lottery. The prize will be $0 or $100. You can play any number between 1 and 100 (chance of winning is 1%). What is the value of knowing the winning number? Value of Imperfect Information? No such thing (as we formulate it) Information corresponds to the observation of a node in the decision network If data is “noisy” that just means we don’t observe the original variable, but another variable which is a noisy version of the original one VPI Question VPI(OilLoc) ? VPI(ScoutingReport) ? VPI(Scout) ? VPI(Scout | ScoutingReport) ? Generally: If Parents(U) Z | CurrentEvidence Then VPI( Z | CurrentEvidence) = 0 OilLoc DrillLoc U Scouting Report Scout POMDPs POMDPs MDPs have: States S Actions A Transition function P(s’|s,a) (or T(s,a,s’)) Rewards R(s,a,s’) POMDPs add: Observations O Observation function P(o|s) (or O(s,o)) POMDPs are MDPs over belief states b (distributions over S) We’ll be able to say more in a few lectures a s s, a s,a,s’ s' a b b, a o b' Example: Ghostbusters In (static) Ghostbusters: Belief state determined by evidence to date {e} Tree really over evidence sets Probabilistic reasoning needed to predict new evidence given past evidence Solving POMDPs One way: use truncated expectimax to compute approximate value of actions What if you only considered busting or one sense followed by a bust? You get a VPI-based agent! a {e} e, a e’ {e, e’} a b b, a b’ abust {e} {e}, asense e’ {e, e’} asense U(abust, {e}) abust U(abust, {e, e’}) Demo: Ghostbusters with VPI e’ Video of Demo Ghostbusters with VPI More Generally* General solutions map belief functions to actions Can divide regions of belief space (set of belief functions) into policy regions (gets complex quickly) Can build approximate policies using discretization methods Can factor belief functions in various ways Overall, POMDPs are very (actually PSPACE-) hard Most real problems are POMDPs, and we can rarely solve then in their full generality Next Time: Dynamic Models "
528,"CS 188: Artificial Intelligence Hidden Markov Models Instructors: Pieter Abbeel and Dan Klein --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Probability Recap § Conditional probability § Product rule § Chain rule § X, Y independent if and only if: § X and Y are conditionally independent given Z if and only if: Reasoning over Time or Space § Often, we want to reason about a sequence of observations § Speech recognition § Robot localization § User attention § Medical monitoring § Need to introduce time (or space) into our models Markov Models § Value of X at a given time is called the state § Parameters: called transition probabilities or dynamics, specify how the state evolves over time (also, initial state probabilities) § Stationarity assumption: transition probabilities the same at all times § Same as MDP transition model, but no choice of action X2 X1 X3 X4 Conditional Independence § Basic conditional independence: § Past and future independent given the present § Each time step only depends on the previous § This is called the (first order) Markov property § Note that the chain is just a (growable) BN § We can always use generic BN reasoning on it if we truncate the chain at a fixed length Example Markov Chain: Weather § States: X = {rain, sun} rain sun 0.9 0.7 0.3 0.1 Two new ways of representing the same CPT sun rain sun rain 0.1 0.9 0.7 0.3 Xt-1 Xt P(Xt|Xt-1) sun sun 0.9 sun rain 0.1 rain sun 0.3 rain rain 0.7 § Initial distribution: 1.0 sun § CPT P(Xt | Xt-1): Example Markov Chain: Weather § Initial distribution: 1.0 sun § What is the probability distribution after one step? rain sun 0.9 0.7 0.3 0.1 Mini-Forward Algorithm § Question: What’s P(X) on some day t? Forward simulation X2 X1 X3 X4 P(xt) = X xt−1 P(xt−1, xt) = X xt−1 P(xt | xt−1)P(xt−1) Example Run of Mini-Forward Algorithm § From initial observation of sun § From initial observation of rain § From yet another initial distribution P(X1): P(X1) P(X2) P(X3) P(X¥) P(X4) P(X1) P(X2) P(X3) P(X¥) P(X4) P(X1) P(X¥) … [Demo: L13D1,2,3] Video of Demo Ghostbusters Basic Dynamics Video of Demo Ghostbusters Circular Dynamics Video of Demo Ghostbusters Whirlpool Dynamics § Stationary distribution: § The distribution we end up with is called the stationary distribution of the chain § It satisfies Stationary Distributions § For most chains: § Influence of the initial distribution gets less and less over time. § The distribution we end up in is independent of the initial distribution P1(X) = P1+1(X) = X x P(X|x)P1(x) P1 Example: Stationary Distributions § Question: What’s P(X) at time t = infinity? X2 X1 X3 X4 Xt-1 Xt P(Xt|Xt-1) sun sun 0.9 sun rain 0.1 rain sun 0.3 rain rain 0.7 P1(sun) = P(sun|sun)P1(sun) + P(sun|rain)P1(rain) P1(rain) = P(rain|sun)P1(sun) + P(rain|rain)P1(rain) P1(sun) = 0.9P1(sun) + 0.3P1(rain) P1(rain) = 0.1P1(sun) + 0.7P1(rain) P1(sun) = 3P1(rain) P1(rain) = 1/3P1(sun) P1(sun) + P1(rain) = 1 P1(sun) = 3/4 P1(rain) = 1/4 Also: Application of Stationary Distribution: Web Link Analysis § PageRank over a web graph § Each web page is a state § Initial distribution: uniform over pages § Transitions: § With prob. c, uniform jump to a random page (dotted lines, not all shown) § With prob. 1-c, follow a random outlink (solid lines) § Stationary distribution § Will spend more time on highly reachable pages § E.g. many ways to get to the Acrobat Reader download page § Somewhat robust to link spam § Google 1.0 returned the set of pages containing all your keywords in decreasing rank, now all search engines use link analysis along with many other factors (rank actually getting less important over time) Application of Stationary Distributions: Gibbs Sampling* § Each joint instantiation over all hidden and query variables is a state: {X1, …, Xn} = H U Q § Transitions: § With probability 1/n resample variable Xj according to P(Xj | x1, x2, …, xj-1, xj+1, …, xn, e1, …, em) § Stationary distribution: § Conditional distribution P(X1, X2 , … , Xn|e1, …, em) § Means that when running Gibbs sampling long enough we get a sample from the desired distribution § Requires some proof to show this is true! Hidden Markov Models Pacman – Sonar (P4) [Demo: Pacman – Sonar – No Beliefs(L14D1)] Video of Demo Pacman – Sonar (no beliefs) Hidden Markov Models § Markov chains not so useful for most agents § Need observations to update your beliefs § Hidden Markov models (HMMs) § Underlying Markov chain over states X § You observe outputs (effects) at each time step X5 X2 E1 X1 X3 X4 E2 E3 E4 E5 Example: Weather HMM Rt-1 Rt P(Rt|Rt-1) +r +r 0.7 +r -r 0.3 -r +r 0.3 -r -r 0.7 Umbrellat-1 Rt Ut P(Ut|Rt) +r +u 0.9 +r -u 0.1 -r +u 0.2 -r -u 0.8 Umbrellat Umbrellat+1 Raint-1 Raint Raint+1 § An HMM is defined by: § Initial distribution: § Transitions: § Emissions: P(Xt | Xt−1) P(Et | Xt) P(Xt | Xt−1) P(Et | Xt) Example: Ghostbusters HMM § P(X1) = uniform § P(X|X) = usually move clockwise, but sometimes move in a random direction or stay in place § P(Rij|X) = same sensor model as before: red means close, green means far away. 1/9 1/9 1/9 1/9 1/9 1/9 1/9 1/9 1/9 P(X1) P(X|X=<1,2>) 1/6 1/6 0 1/6 1/2 0 0 0 0 X5 X2 Ri,j X1 X3 X4 Ri,j Ri,j Ri,j [Demo: Ghostbusters – Circular Dynamics – HMM (L14D2)] Video of Demo Ghostbusters – Circular Dynamics -- HMM Conditional Independence § HMMs have two important independence properties: § Markov hidden process: future depends on past via the present § Current observation independent of all else given current state § Quiz: does this mean that evidence variables are guaranteed to be independent? § [No, they tend to correlated by the hidden state] X5 X2 E1 X1 X3 X4 E2 E3 E4 E5 Real HMM Examples § Speech recognition HMMs: § Observations are acoustic signals (continuous valued) § States are specific positions in specific words (so, tens of thousands) § Machine translation HMMs: § Observations are words (tens of thousands) § States are translation options § Robot tracking: § Observations are range readings (continuous) § States are positions on a map (continuous) Filtering / Monitoring § Filtering, or monitoring, is the task of tracking the distribution Bt(X) = Pt(Xt | e1, …, et) (the belief state) over time § We start with B1(X) in an initial setting, usually uniform § As time passes, or we get observations, we update B(X) § The Kalman filter was invented in the 60’s and first implemented as a method of trajectory estimation for the Apollo program Example: Robot Localization t=0 Sensor model: can read in which directions there is a wall, never more than 1 mistake Motion model: may not execute action with small prob. 1 0 Prob Example from Michael Pfeiffer Example: Robot Localization t=1 Lighter grey: was possible to get the reading, but less likely b/c required 1 mistake 1 0 Prob Example: Robot Localization t=2 1 0 Prob Example: Robot Localization t=3 1 0 Prob Example: Robot Localization t=4 1 0 Prob Example: Robot Localization t=5 1 0 Prob Inference: Base Cases E1 X1 X2 X1 Passage of Time § Assume we have current belief P(X | evidence to date) § Then, after one time step passes: § Basic idea: beliefs get “pushed” through the transitions § With the “B” notation, we have to be careful about what time step t the belief is about, and what evidence it includes X2 X1 = X xt P(Xt+1, xt|e1:t) = X xt P(Xt+1|xt, e1:t)P(xt|e1:t) = X xt P(Xt+1|xt)P(xt|e1:t) § Or compactly: B0(Xt+1) = X xt P(X0|xt)B(xt) P(Xt+1|e1:t) Example: Passage of Time § As time passes, uncertainty accumulates T = 1 T = 2 T = 5 (Transition model: ghosts usually go clockwise) Observation § Assume we have current belief P(X | previous evidence): § Then, after evidence comes in: § Or, compactly: E1 X1 B0(Xt+1) = P(Xt+1|e1:t) P(Xt+1|e1:t+1) = P(Xt+1, et+1|e1:t)/P(et+1|e1:t) /Xt+1 P(Xt+1, et+1|e1:t) = P(et+1|Xt+1)P(Xt+1|e1:t) = P(et+1|e1:t, Xt+1)P(Xt+1|e1:t) B(Xt+1) /Xt+1 P(et+1|Xt+1)B0(Xt+1) § Basic idea: beliefs “reweighted” by likelihood of evidence § Unlike passage of time, we have to renormalize Example: Observation § As we get observations, beliefs get reweighted, uncertainty decreases Before observation After observation Example: Weather HMM Rt Rt+1 P(Rt+1|Rt) +r +r 0.7 +r -r 0.3 -r +r 0.3 -r -r 0.7 Rt Ut P(Ut|Rt) +r +u 0.9 +r -u 0.1 -r +u 0.2 -r -u 0.8 Umbrella1 Umbrella2 Rain0 Rain1 Rain2 B(+r) = 0.5 B(-r) = 0.5 B’(+r) = 0.5 B’(-r) = 0.5 B(+r) = 0.818 B(-r) = 0.182 B’(+r) = 0.627 B’(-r) = 0.373 B(+r) = 0.883 B(-r) = 0.117 The Forward Algorithm § We are given evidence at each time and want to know § We can derive the following updates We can normalize as we go if we want to have P(x|e) at each time step, or just once at the end… Online Belief Updates § Every time step, we start with current P(X | evidence) § We update for time: § We update for evidence: § The forward algorithm does both at once (and doesn’t normalize) X2 X1 X2 E2 Pacman – Sonar (P4) [Demo: Pacman – Sonar – No Beliefs(L14D1)] Video of Demo Pacman – Sonar (with beliefs) Next Time: Particle Filtering and Applications of HMMs "
529,"CS 188: Artificial Intelligence HMMs, Particle Filters, and Applications Instructors: Dan Klein and Pieter Abbeel University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Today HMMs Particle filters Demos! Most‐likely‐explanation queries Applications: Robot localization / mapping Speech recognition (later) Recap: Reasoning Over Time Markov models Hidden Markov models X2 X1 X3 X4 rain sun 0.7 0.7 0.3 0.3 X5 X2 E1 X1 X3 X4 E2 E3 E4 E5 X E P rain umbrella 0.9 rain no umbrella 0.1 sun umbrella 0.2 sun no umbrella 0.8 [Demo: Ghostbusters Markov Model (L15D1)] Inference: Base Cases E1 X1 X2 X1 Inference: Base Cases X2 X1 Passage of Time Assume we have current belief P(X | evidence to date) Then, after one time step passes: Basic idea: beliefs get “pushed” through the transitions With the “B” notation, we have to be careful about what time step t the belief is about, and what evidence it includes X2 X1 Or compactly: Example: Passage of Time As time passes, uncertainty “accumulates” T = 1 T = 2 T = 5 (Transition model: ghosts usually go clockwise) Inference: Base Cases E1 X1 Observation Assume we have current belief P(X | previous evidence): Then, after evidence comes in: Or, compactly: E1 X1 Basic idea: beliefs “reweighted” by likelihood of evidence Unlike passage of time, we have to renormalize Example: Observation As we get observations, beliefs get reweighted, uncertainty “decreases” Before observation After observation Filtering Elapse time: compute P( Xt | e1:t‐1 ) Observe: compute P( Xt | e1:t ) X2 E1 X1 E2 <0.5, 0.5> Belief: <P(rain), P(sun)> <0.82, 0.18> <0.63, 0.37> <0.88, 0.12> Prior on X1 Observe Elapse time Observe [Demo: Ghostbusters Exact Filtering (L15D2)] Particle Filtering Particle Filtering 0.0 0.1 0.0 0.0 0.0 0.2 0.0 0.2 0.5 Filtering: approximate solution Sometimes |X| is too big to use exact inference |X| may be too big to even store B(X) E.g. X is continuous Solution: approximate inference Track samples of X, not all values Samples are called particles Time per step is linear in the number of samples But: number needed may be large In memory: list of particles, not states This is how robot localization works in practice Particle is just new name for sample Representation: Particles Our representation of P(X) is now a list of N particles (samples) Generally, N << |X| Storing map from X to counts would defeat the point P(x) approximated by number of particles with value x So, many x may have P(x) = 0! More particles, more accuracy For now, all particles have a weight of 1 Particles: (3,3) (2,3) (3,3) (3,2) (3,3) (3,2) (1,2) (3,3) (3,3) (2,3) Particle Filtering: Elapse Time Each particle is moved by sampling its next position from the transition model This is like prior sampling – samples’ frequencies reflect the transition probabilities Here, most samples move clockwise, but some move in another direction or stay in place This captures the passage of time If enough samples, close to exact values before and after (consistent) Particles: (3,3) (2,3) (3,3) (3,2) (3,3) (3,2) (1,2) (3,3) (3,3) (2,3) Particles: (3,2) (2,3) (3,2) (3,1) (3,3) (3,2) (1,3) (2,3) (3,2) (2,2) Slightly trickier: Don’t sample observation, fix it Similar to likelihood weighting, downweight samples based on the evidence As before, the probabilities don’t sum to one, since all have been downweighted (in fact they now sum to (N times) an approximation of P(e)) Particle Filtering: Observe Particles: (3,2) w=.9 (2,3) w=.2 (3,2) w=.9 (3,1) w=.4 (3,3) w=.4 (3,2) w=.9 (1,3) w=.1 (2,3) w=.2 (3,2) w=.9 (2,2) w=.4 Particles: (3,2) (2,3) (3,2) (3,1) (3,3) (3,2) (1,3) (2,3) (3,2) (2,2) Particle Filtering: Resample Rather than tracking weighted samples, we resample N times, we choose from our weighted sample distribution (i.e. draw with replacement) This is equivalent to renormalizing the distribution Now the update is complete for this time step, continue with the next one Particles: (3,2) w=.9 (2,3) w=.2 (3,2) w=.9 (3,1) w=.4 (3,3) w=.4 (3,2) w=.9 (1,3) w=.1 (2,3) w=.2 (3,2) w=.9 (2,2) w=.4 (New) Particles: (3,2) (2,2) (3,2) (2,3) (3,3) (3,2) (1,3) (2,3) (3,2) (3,2) Recap: Particle Filtering Particles: track samples of states rather than an explicit distribution Particles: (3,3) (2,3) (3,3) (3,2) (3,3) (3,2) (1,2) (3,3) (3,3) (2,3) Elapse Weight Resample Particles: (3,2) (2,3) (3,2) (3,1) (3,3) (3,2) (1,3) (2,3) (3,2) (2,2) Particles: (3,2) w=.9 (2,3) w=.2 (3,2) w=.9 (3,1) w=.4 (3,3) w=.4 (3,2) w=.9 (1,3) w=.1 (2,3) w=.2 (3,2) w=.9 (2,2) w=.4 (New) Particles: (3,2) (2,2) (3,2) (2,3) (3,3) (3,2) (1,3) (2,3) (3,2) (3,2) [Demos: ghostbusters particle filtering (L15D3,4,5)] Robot Localization In robot localization: We know the map, but not the robot’s position Observations may be vectors of range finder readings State space and readings are typically continuous (works basically like a very fine grid) and so we cannot store B(X) Particle filtering is a main technique Particle Filter Localization (Sonar) [Video: global‐sonar‐uw‐annotated.avi] Particle Filter Localization (Laser) [Video: global‐floor.gif] Robot Mapping SLAM: Simultaneous Localization And Mapping We do not know the map or our location State consists of position AND map! Main techniques: Kalman filtering (Gaussian HMMs) and particle methods DP‐SLAM, Ron Parr [Demo: PARTICLES‐SLAM‐mapping1‐new.avi] Particle Filter SLAM – Video 1 [Demo: PARTICLES‐SLAM‐mapping1‐new.avi] Particle Filter SLAM – Video 2 [Demo: PARTICLES‐SLAM‐fastslam.avi] Dynamic Bayes Nets Dynamic Bayes Nets (DBNs) We want to track multiple variables over time, using multiple sources of evidence Idea: Repeat a fixed Bayes net structure at each time Variables from time t can condition on those from t‐1 Dynamic Bayes nets are a generalization of HMMs G1 a E1a E1b G1 b G2 a E2a E2b G2 b t =1 t =2 G3 a E3a E3b G3 b t =3 [Demo: pacman sonar ghost DBN model (L15D6)] Pacman – Sonar (P4) [Demo: Pacman – Sonar – No Beliefs(L14D1)] Exact Inference in DBNs Variable elimination applies to dynamic Bayes nets Procedure: “unroll” the network for T time steps, then eliminate variables until P(XT|e1:T) is computed Online belief updates: Eliminate all variables from the previous time step; store factors for current time only G1 a E1a E1b G1 b G2 a E2a E2b G2 b G3 a E3a E3b G3 b t =1 t =2 t =3 G3 b DBN Particle Filters A particle is a complete sample for a time step Initialize: Generate prior samples for the t=1 Bayes net Example particle: G1 a = (3,3) G1 b = (5,3) Elapse time: Sample a successor for each particle Example successor: G2 a = (2,3) G2 b = (6,3) Observe: Weight each entire sample by the likelihood of the evidence conditioned on the sample Likelihood: P(E1 a |G1 a ) * P(E1 b |G1 b ) Resample: Select prior samples (tuples of values) in proportion to their likelihood Most Likely Explanation HMMs: MLE Queries HMMs defined by States X Observations E Initial distribution: Transitions: Emissions: New query: most likely explanation: New method: the Viterbi algorithm X5 X2 E1 X1 X3 X4 E2 E3 E4 E5 State Trellis State trellis: graph of states and transitions over time Each arc represents some transition Each arc has weight Each path is a sequence of states The product of weights on a path is that sequence’s probability along with the evidence Forward algorithm computes sums of paths, Viterbi computes best paths sun rain sun rain sun rain sun rain Forward / Viterbi Algorithms sun rain sun rain sun rain sun rain Forward Algorithm (Sum) Viterbi Algorithm (Max) "
530,"CS 188: Artificial Intelligence Naïve Bayes Instructors: Dan Klein and Pieter Abbeel --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Machine Learning Up until now: how use a model to make optimal decisions Machine learning: how to acquire a model from data / experience Learning parameters (e.g. probabilities) Learning structure (e.g. BN graphs) Learning hidden concepts (e.g. clustering, neural nets) Today: model-based classification with Naive Bayes Classification Example: Spam Filter Input: an email Output: spam/ham Setup: Get a large collection of example emails, each labeled “spam” or “ham” Note: someone has to hand label all this data! Want to learn to predict labels of new, future emails Features: The attributes used to make the ham / spam decision Words: FREE! Text Patterns: $dd, CAPS Non-text: SenderInContacts, WidelyBroadcast … Dear Sir. First, I must solicit your confidence in this transaction, this is by virture of its nature as being utterly confidencial and top secret. … TO BE REMOVED FROM FUTURE MAILINGS, SIMPLY REPLY TO THIS MESSAGE AND PUT ""REMOVE"" IN THE SUBJECT. 99 MILLION EMAIL ADDRESSES FOR ONLY $99 Ok, Iknow this is blatantly OT but I'm beginning to go insane. Had an old Dell Dimension XPS sitting in the corner and decided to put it to use, I know it was working pre being stuck in the corner, but when I plugged it in, hit the power nothing happened. Example: Digit Recognition Input: images / pixel grids Output: a digit 0-9 Setup: Get a large collection of example images, each labeled with a digit Note: someone has to hand label all this data! Want to learn to predict labels of new, future digit images Features: The attributes used to make the digit decision Pixels: (6,8)=ON Shape Patterns: NumComponents, AspectRatio, NumLoops … Features are increasingly induced rather than crafted 0 1 2 1 ?? Other Classification Tasks Classification: given inputs x, predict labels (classes) y Examples: Medical diagnosis (input: symptoms, classes: diseases) Fraud detection (input: account activity, classes: fraud / no fraud) Automatic essay grading (input: document, classes: grades) Customer service email routing Review sentiment Language ID … many more Classification is an important commercial technology! Model-Based Classification Model-Based Classification Model-based approach Build a model (e.g. Bayes’ net) where both the output label and input features are random variables Instantiate any observed features Query for the distribution of the label conditioned on the features Challenges What structure should the BN have? How should we learn its parameters? Naïve Bayes for Digits Naïve Bayes: Assume all features are independent effects of the label Simple digit recognition version: One feature (variable) Fij for each grid position <i,j> Feature values are on / off, based on whether intensity is more or less than 0.5 in underlying image Each input maps to a feature vector, e.g. Here: lots of features, each is binary valued Naïve Bayes model: What do we need to learn? Y F1 Fn F2 General Naïve Bayes A general Naive Bayes model: We only have to specify how each feature depends on the class Total number of parameters is linear in n Model is very simplistic, but often works anyway Y F1 Fn F2 |Y| parameters n x |F| x |Y| parameters |Y| x |F|n values Inference for Naïve Bayes Goal: compute posterior distribution over label variable Y Step 1: get joint probability of label and evidence for each label Step 2: sum to get probability of evidence Step 3: normalize by dividing Step 1 by Step 2 + General Naïve Bayes What do we need in order to use Naïve Bayes? Inference method (we just saw this part) Start with a bunch of probabilities: P(Y) and the P(Fi|Y) tables Use standard inference to compute P(Y|F1…Fn) Nothing new here Estimates of local conditional probability tables P(Y), the prior over labels P(Fi|Y) for each feature (evidence variable) These probabilities are collectively called the parameters of the model and denoted by θ Up until now, we assumed these appeared by magic, but… …they typically come from training data counts: we’ll look at this soon Example: Conditional Probabilities 1 0.1 2 0.1 3 0.1 4 0.1 5 0.1 6 0.1 7 0.1 8 0.1 9 0.1 0 0.1 1 0.01 2 0.05 3 0.05 4 0.30 5 0.80 6 0.90 7 0.05 8 0.60 9 0.50 0 0.80 1 0.05 2 0.01 3 0.90 4 0.80 5 0.90 6 0.90 7 0.25 8 0.85 9 0.60 0 0.80 Naïve Bayes for Text Bag-of-words Naïve Bayes: Features: Wi is the word at position i As before: predict label conditioned on feature variables (spam vs. ham) As before: assume features are conditionally independent given label New: each Wi is identically distributed Generative model: “Tied” distributions and bag-of-words Usually, each variable gets its own conditional probability distribution P(F|Y) In a bag-of-words model Each position is identically distributed All positions share the same conditional probs P(W|Y) Why make this assumption? Called “bag-of-words” because model is insensitive to word order or reordering Word at position i, not ith word in the dictionary! Example: Spam Filtering Model: What are the parameters? Where do these tables come from? the : 0.0156 to : 0.0153 and : 0.0115 of : 0.0095 you : 0.0093 a : 0.0086 with: 0.0080 from: 0.0075 ... the : 0.0210 to : 0.0133 of : 0.0119 2002: 0.0110 with: 0.0108 from: 0.0107 and : 0.0105 a : 0.0100 ... ham : 0.66 spam: 0.33 Spam Example Word P(w|spam) P(w|ham) Tot Spam Tot Ham (prior) 0.33333 0.66666 -1.1 -0.4 Gary 0.00002 0.00021 -11.8 -8.9 would 0.00069 0.00084 -19.1 -16.0 you 0.00881 0.00304 -23.8 -21.8 like 0.00086 0.00083 -30.9 -28.9 to 0.01517 0.01339 -35.1 -33.2 lose 0.00008 0.00002 -44.5 -44.0 weight 0.00016 0.00002 -53.3 -55.0 while 0.00027 0.00027 -61.5 -63.2 you 0.00881 0.00304 -66.2 -69.0 sleep 0.00006 0.00001 -76.0 -80.5 P(spam | w) = 98.9 Training and Testing Empirical Risk Minimization Empirical risk minimization Basic principle of machine learning We want the model (classifier, etc) that does best on the true test distribution Don’t know the true distribution so pick the best model on our actual training set Finding “the best” model on the training set is phrased as an optimization problem Main worry: overfitting to the training set Better with more training data (less sampling variance, training more like test) Better if we limit the complexity of our hypotheses (regularization and/or small hypothesis spaces) Important Concepts  Data: labeled instances (e.g. emails marked spam/ham)  Training set  Held out set  Test set  Features: attribute-value pairs which characterize each x  Experimentation cycle  Learn parameters (e.g. model probabilities) on training set  (Tune hyperparameters on held-out set)  Compute accuracy of test set  Very important: never “peek” at the test set!  Evaluation (many metrics possible, e.g. accuracy)  Accuracy: fraction of instances predicted correctly  Overfitting and generalization  Want a classifier which does well on test data  Overfitting: fitting the training data very closely, but not generalizing well  We’ll investigate overfitting and generalization formally in a few lectures Training Data Held-Out Data Test Data Generalization and Overfitting 0 2 4 6 8 10 12 14 16 18 20 -15 -10 -5 0 5 10 15 20 25 30 Degree 15 polynomial Overfitting Example: Overfitting 2 wins!! Example: Overfitting  Posteriors determined by relative probabilities (odds ratios): south-west : inf nation : inf morally : inf nicely : inf extent : inf seriously : inf ... What went wrong here? screens : inf minute : inf guaranteed : inf $205.00 : inf delivery : inf signature : inf ... Generalization and Overfitting  Relative frequency parameters will overfit the training data! Just because we never saw a 3 with pixel (15,15) on during training doesn’t mean we won’t see it at test time Unlikely that every occurrence of “minute” is 100% spam Unlikely that every occurrence of “seriously” is 100% ham What about all the words that don’t occur in the training set at all? In general, we can’t go around giving unseen events zero probability  As an extreme case, imagine using the entire email as the only feature (e.g. document ID) Would get the training data perfect (if deterministic labeling) Wouldn’t generalize at all Just making the bag-of-words assumption gives us some generalization, but isn’t enough  To generalize better: we need to smooth or regularize the estimates Parameter Estimation Parameter Estimation Estimating the distribution of a random variable Elicitation: ask a human (why is this hard?) Empirically: use training data (learning!) E.g.: for each outcome x, look at the empirical rate of that value: This is the estimate that maximizes the likelihood of the data r r b r b b r b b r b b r b b r b b Smoothing Maximum Likelihood? Relative frequencies are the maximum likelihood estimates Another option is to consider the most likely parameter value given the data ???? Unseen Events Laplace Smoothing Laplace’s estimate: Pretend you saw every outcome once more than you actually did Can derive this estimate with Dirichlet priors (see cs281a) r r b Laplace Smoothing Laplace’s estimate (extended): Pretend you saw every outcome k extra times What’s Laplace with k = 0? k is the strength of the prior Laplace for conditionals: Smooth each condition independently: r r b Estimation: Linear Interpolation* In practice, Laplace often performs poorly for P(X|Y): When |X| is very large When |Y| is very large Another option: linear interpolation Also get the empirical P(X) from the data Make sure the estimate of P(X|Y) isn’t too different from the empirical P(X) What if α is 0? 1? For even better ways to estimate parameters, as well as details of the math, see cs281a, cs288 Real NB: Smoothing For real classification problems, smoothing is critical New odds ratios: helvetica : 11.4 seems : 10.8 group : 10.2 ago : 8.4 areas : 8.3 ... verdana : 28.8 Credit : 28.4 ORDER : 27.2 <FONT> : 26.9 money : 26.5 ... Do these make more sense? Tuning Tuning on Held-Out Data Now we’ve got two kinds of unknowns Parameters: the probabilities P(X|Y), P(Y) Hyperparameters: e.g. the amount / type of smoothing to do, k, α What should we learn where? Learn parameters from training data Tune hyperparameters on different data Why? For each value of the hyperparameters, train and test on the held-out data Choose the best value and do a final test on the test data Features Errors, and What to Do Examples of errors Dear GlobalSCAPE Customer, GlobalSCAPE has partnered with ScanSoft to offer you the latest version of OmniPage Pro, for just $99.99* - the regular list price is $499! The most common question we've received about this offer is - Is this genuine? We would like to assure you that this offer is authorized by ScanSoft, is genuine and valid. You can get the . . . . . . To receive your $30 Amazon.com promotional certificate, click through to http://www.amazon.com/apparel and see the prominent link for the $30 offer. All details are there. We hope you enjoyed receiving this message. However, if you'd rather not receive future e-mails announcing new store launches, please click . . . What to Do About Errors? Need more features– words aren’t enough! Have you emailed the sender before? Have 1K other people just gotten the same email? Is the sending information consistent? Is the email in ALL CAPS? Do inline URLs point where they say they point? Does the email address you by (your) name? Can add these information sources as new variables in the NB model Next class we’ll talk about classifiers which let you easily add arbitrary features more easily, and, later, how to induce new features Baselines First step: get a baseline Baselines are very simple “straw man” procedures Help determine how hard the task is Help know what a “good” accuracy is Weak baseline: most frequent label classifier Gives all test instances whatever label was most common in the training set E.g. for spam filtering, might label everything as ham Accuracy might be very high if the problem is skewed E.g. calling everything “ham” gets 66%, so a classifier that gets 70% isn’t very good… For real research, usually use previous work as a (strong) baseline Confidences from a Classifier The confidence of a probabilistic classifier: Posterior probability of the top label Represents how sure the classifier is of the classification Any probabilistic model will have confidences No guarantee confidence is correct Calibration Weak calibration: higher confidences mean higher accuracy Strong calibration: confidence predicts accuracy rate What’s the value of calibration? Summary Bayes rule lets us do diagnostic queries with causal probabilities The naïve Bayes assumption takes all features to be independent given the class label We can build classifiers out of a naïve Bayes model using training data Smoothing estimates is important in real systems Classifier confidences are useful, when you can get them Next Time: Discriminative Learning "
531,"CS 188: Artificial Intelligence Perceptrons and Logistic Regression Pieter Abbeel & Dan Klein University of California, Berkeley Linear Classifiers Feature Vectors Hello, Do you want free printr cartriges? Why pay more when you can get them ABSOLUTELY FREE! Just # free : 2 YOUR_NAME : 0 MISSPELLED : 2 FROM_FRIEND : 0 ... SPAM or + PIXEL-7,12 : 1 PIXEL-7,13 : 0 ... NUM_LOOPS : 1 ... “2” Some (Simplified) Biology § Very loose inspiration: human neurons Linear Classifiers § Inputs are feature values § Each feature has a weight § Sum is the activation § If the activation is: § Positive, output +1 § Negative, output -1 S f1 f2 f3 w1 w2 w3 >0? Weights § Binary case: compare features to a weight vector § Learning: figure out the weight vector from examples # free : 2 YOUR_NAME : 0 MISSPELLED : 2 FROM_FRIEND : 0 ... # free : 4 YOUR_NAME :-1 MISSPELLED : 1 FROM_FRIEND :-3 ... # free : 0 YOUR_NAME : 1 MISSPELLED : 1 FROM_FRIEND : 1 ... Dot product positive means the positive class Decision Rules Binary Decision Rule § In the space of feature vectors § Examples are points § Any weight vector is a hyperplane § One side corresponds to Y=+1 § Other corresponds to Y=-1 BIAS : -3 free : 4 money : 2 ... 0 1 0 1 2 free money +1 = SPAM -1 = HAM Weight Updates Learning: Binary Perceptron § Start with weights = 0 § For each training instance: § Classify with current weights § If correct (i.e., y=y*), no change! § If wrong: adjust the weight vector Learning: Binary Perceptron § Start with weights = 0 § For each training instance: § Classify with current weights § If correct (i.e., y=y*), no change! § If wrong: adjust the weight vector by adding or subtracting the feature vector. Subtract if y* is -1. Examples: Perceptron § Separable Case Multiclass Decision Rule § If we have multiple classes: § A weight vector for each class: § Score (activation) of a class y: § Prediction highest score wins Binary = multiclass where the negative class has weight zero Learning: Multiclass Perceptron § Start with all weights = 0 § Pick up training examples one by one § Predict with current weights § If correct, no change! § If wrong: lower score of wrong answer, raise score of right answer Example: Multiclass Perceptron BIAS : 1 win : 0 game : 0 vote : 0 the : 0 ... BIAS : 0 win : 0 game : 0 vote : 0 the : 0 ... BIAS : 0 win : 0 game : 0 vote : 0 the : 0 ... “win the vote” “win the election” “win the game” Properties of Perceptrons § Separability: true if some parameters get the training set perfectly correct § Convergence: if the training is separable, perceptron will eventually converge (binary case) § Mistake Bound: the maximum number of mistakes (binary case) related to the margin or degree of separability Separable Non-Separable Problems with the Perceptron § Noise: if the data isn’t separable, weights might thrash § Averaging weight vectors over time can help (averaged perceptron) § Mediocre generalization: finds a “barely” separating solution § Overtraining: test / held-out accuracy usually rises, then falls § Overtraining is a kind of overfitting Improving the Perceptron Non-Separable Case: Deterministic Decision Even the best linear boundary makes at least one mistake Non-Separable Case: Probabilistic Decision 0.5 | 0.5 0.3 | 0.7 0.1 | 0.9 0.7 | 0.3 0.9 | 0.1 How to get probabilistic decisions? § Perceptron scoring: § If very positive à want probability going to 1 § If very negative à want probability going to 0 § Sigmoid function Best w? § Maximum likelihood estimation: with: = Logistic Regression Separable Case: Deterministic Decision – Many Options Separable Case: Probabilistic Decision – Clear Preference 0.5 | 0.5 0.3 | 0.7 0.7 | 0.3 0.5 | 0.5 0.3 | 0.7 0.7 | 0.3 Multiclass Logistic Regression § Recall Perceptron: § A weight vector for each class: § Score (activation) of a class y: § Prediction highest score wins § How to make the scores into probabilities? original activations softmax activations Best w? § Maximum likelihood estimation: with: = Multi-Class Logistic Regression Next Lecture § Optimization § i.e., how do we solve: "
532,"CS 188: Artificial Intelligence Optimization and Neural Nets Instructors: Pieter Abbeel and Dan Klein --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Reminder: Linear Classifiers § Inputs are feature values § Each feature has a weight § Sum is the activation § If the activation is: § Positive, output +1 § Negative, output -1 S f1 f2 f3 w1 w2 w3 >0? How to get probabilistic decisions? § Activation: § If very positive à want probability going to 1 § If very negative à want probability going to 0 § Sigmoid function z = w · f(x) z = w · f(x) z = w · f(x) φ(z) = 1 1 + e−z Best w? § Maximum likelihood estimation: with: max w ll(w) = max w X i log P(y(i)|x(i); w) P(y(i) = +1|x(i); w) = 1 1 + e−w·f(x(i)) P(y(i) = −1|x(i); w) = 1 − 1 1 + e−w·f(x(i)) = Logistic Regression Multiclass Logistic Regression § Multi-class linear classification § A weight vector for each class: § Score (activation) of a class y: § Prediction w/highest score wins: § How to make the scores into probabilities? z1, z2, z3 ! ez1 ez1 + ez2 + ez3 , ez2 ez1 + ez2 + ez3 , ez3 ez1 + ez2 + ez3 original activations softmax activations Best w? § Maximum likelihood estimation: with: max w ll(w) = max w X i log P(y(i)|x(i); w) P(y(i)|x(i); w) = ewy(i)·f(x(i)) P y ewy·f(x(i)) = Multi-Class Logistic Regression This Lecture § Optimization § i.e., how do we solve: max w ll(w) = max w X i log P(y(i)|x(i); w) Hill Climbing § Recall from CSPs lecture: simple, general idea § Start wherever § Repeat: move to the best neighboring state § If no neighbors better than current, quit § What’s particularly tricky when hill-climbing for multiclass logistic regression? • Optimization over a continuous space • Infinitely many neighbors! • How to do this efficiently? 1-D Optimization § Could evaluate and § Then step in best direction § Or, evaluate derivative: § Tells which direction to step into w g(w) w0 g(w0) g(w0 + h) g(w0 −h) @g(w0) @w = lim h!0 g(w0 + h) −g(w0 −h) 2h 2-D Optimization Source: offconvex.org Gradient Ascent § Perform update in uphill direction for each coordinate § The steeper the slope (i.e. the higher the derivative) the bigger the step for that coordinate § E.g., consider: § Updates: § Updates in vector notation: with: = gradient § Idea: § Start somewhere § Repeat: Take a step in the gradient direction Gradient Ascent Figure source: Mathworks What is the Steepest Direction? § First-Order Taylor Expansion: § Steepest Descent Direction: § Recall: à § Hence, solution: g(w + ∆) ⇡g(w) + @g @w1 ∆1 + @g @w2 ∆2 rg = "" @g @w1 @g @w2 # Gradient direction = steepest direction! Gradient in n dimensions rg = 2 6 6 6 4 @g @w1 @g @w2 · · · @g @wn 3 7 7 7 5 Optimization Procedure: Gradient Ascent § init § for iter = 1, 2, … w § : learning rate --- tweaking parameter that needs to be chosen carefully § How? Try multiple choices § Crude rule of thumb: update changes about 0.1 – 1 % ↵ w Batch Gradient Ascent on the Log Likelihood Objective max w ll(w) = max w X i log P(y(i)|x(i); w) § init § for iter = 1, 2, … w Stochastic Gradient Ascent on the Log Likelihood Objective max w ll(w) = max w X i log P(y(i)|x(i); w) § init § for iter = 1, 2, … § pick random j w Observation: once gradient on one training example has been computed, might as well incorporate before computing next one Mini-Batch Gradient Ascent on the Log Likelihood Objective max w ll(w) = max w X i log P(y(i)|x(i); w) § init § for iter = 1, 2, … § pick random subset of training examples J w Observation: gradient over small set of training examples (=mini-batch) can be computed in parallel, might as well do that instead of a single one § We’ll talk about that once we covered neural networks, which are a generalization of logistic regression How about computing all the derivatives? Neural Networks Multi-class Logistic Regression § = special case of neural network z1 z2 z3 f1(x) f2(x) f3(x) fK(x) s o f t m a x … Deep Neural Network = Also learn the features! z1 z2 z3 f1(x) f2(x) f3(x) fK(x) s o f t m a x … Deep Neural Network = Also learn the features! f1(x) f2(x) f3(x) fK(x) s o f t m a x … x1 x2 x3 xL … … … … … g = nonlinear activation function Deep Neural Network = Also learn the features! s o f t m a x … x1 x2 x3 xL … … … … … g = nonlinear activation function Common Activation Functions [source: MIT 6.S191 introtodeeplearning.com] Deep Neural Network: Also Learn the Features! § Training the deep neural network is just like logistic regression: just w tends to be a much, much larger vector J àjust run gradient ascent + stop when log likelihood of hold-out data starts to decrease Neural Networks Properties § Theorem (Universal Function Approximators). A two-layer neural network with a sufficient number of neurons can approximate any continuous function to any desired accuracy. § Practical considerations § Can be seen as learning the features § Large number of neurons § Danger for overfitting § (hence early stopping!) Universal Function Approximation Theorem* § In words: Given any continuous function f(x), if a 2-layer neural network has enough hidden units, then there is a choice of weights that allow it to closely approximate f(x). Cybenko (1989) “Approximations by superpositions of sigmoidal functions” Hornik (1991) “Approximation Capabilities of Multilayer Feedforward Networks” Leshno and Schocken (1991) ”Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function” Universal Function Approximation Theorem* Cybenko (1989) “Approximations by superpositions of sigmoidal functions” Hornik (1991) “Approximation Capabilities of Multilayer Feedforward Networks” Leshno and Schocken (1991) ”Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function” Fun Neural Net Demo Site § Demo-site: § http://playground.tensorflow.org/ § Derivatives tables: How about computing all the derivatives? [source: http://hyperphysics.phy-astr.gsu.edu/hbase/Math/derfunc.html How about computing all the derivatives? n But neural net f is never one of those? n No problem: CHAIN RULE: If Then à Derivatives can be computed by following well-defined procedures f(x) = g(h(x)) f 0(x) = g0(h(x))h0(x) § Automatic differentiation software § e.g. Theano, TensorFlow, PyTorch, Chainer § Only need to program the function g(x,y,w) § Can automatically compute all derivatives w.r.t. all entries in w § This is typically done by caching info during forward computation pass of f, and then doing a backward pass = “backpropagation” § Autodiff / Backpropagation can often be done at computational cost comparable to the forward pass § Need to know this exists § How this is done? -- outside of scope of CS188 Automatic Differentiation Summary of Key Ideas § Optimize probability of label given input § Continuous optimization § Gradient ascent: § Compute steepest uphill direction = gradient (= just vector of partial derivatives) § Take step in the gradient direction § Repeat (until held-out data accuracy starts to drop = “early stopping”) § Deep neural nets § Last layer = still logistic regression § Now also many more layers before this last layer § = computing the features § à the features are learned rather than hand-designed § Universal function approximation theorem § If neural net is large enough § Then neural net can represent any continuous mapping from input to output with arbitrary accuracy § But remember: need to avoid overfitting / memorizing the training data à early stopping! § Automatic differentiation gives the derivatives efficiently (how? = outside of scope of 188) How well does it work? Computer Vision Object Detection Manual Feature Design Features and Generalization [HoG: Dalal and Triggs, 2005] Features and Generalization Image HoG Performance graph credit Matt Zeiler, Clarifai Performance graph credit Matt Zeiler, Clarifai Performance graph credit Matt Zeiler, Clarifai AlexNet Performance graph credit Matt Zeiler, Clarifai AlexNet Performance graph credit Matt Zeiler, Clarifai AlexNet MS COCO Image Captioning Challenge Karpathy & Fei-Fei, 2015; Donahue et al., 2015; Xu et al, 2015; many more Visual QA Challenge Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh Speech Recognition graph credit Matt Zeiler, Clarifai Machine Translation Google Neural Machine Translation (in production) Next: More Neural Net Applications! "
533,"CS 188: Artificial Intelligence Neural Nets (wrap-up) and Decision Trees Instructors: Pieter Abbeel and Dan Klein --- University of California, Berkeley [These slides were created by Dan Klein and Pieter Abbeel for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu.] Today § Neural Nets -- wrap § Formalizing Learning § Consistency § Simplicity § Decision Trees § Expressiveness § Information Gain § Overfitting Deep Neural Network s o f t m a x P(y1|x; w) = P(y2|x; w) = P(y3|x; w) … x1 x2 x3 xL … … … … z(1) 1 z(1) 2 z(1) 3 z(1) K(1) z(n) K(n) z(2) K(2) z(2) 1 z(2) 2 z(2) 3 z(n) 3 z(n) 2 z(n) 1 z(OUT ) 1 z(OUT ) 2 z(OUT ) 3 z(n−1) 3 z(n−1) 2 z(n−1) 1 z(n−1) K(n−1) … z(k) i = g( X j W (k−1,k) i,j z(k−1) j ) g = nonlinear activation function Deep Neural Network: Also Learn the Features! § Training the deep neural network is just like logistic regression: just w tends to be a much, much larger vector J àjust run gradient ascent + stop when log likelihood of hold-out data starts to decrease max w ll(w) = max w X i log P(y(i)|x(i); w) Neural Networks Properties § Theorem (Universal Function Approximators). A two-layer neural network with a sufficient number of neurons can approximate any continuous function to any desired accuracy. § Practical considerations § Can be seen as learning the features § Large number of neurons § Danger for overfitting § (hence early stopping!) How well does it work? Computer Vision Object Detection Manual Feature Design Features and Generalization [HoG: Dalal and Triggs, 2005] Features and Generalization Image HoG Performance graph credit Matt Zeiler, Clarifai Performance graph credit Matt Zeiler, Clarifai Performance graph credit Matt Zeiler, Clarifai AlexNet Performance graph credit Matt Zeiler, Clarifai AlexNet Performance graph credit Matt Zeiler, Clarifai AlexNet MS COCO Image Captioning Challenge Karpathy & Fei-Fei, 2015; Donahue et al., 2015; Xu et al, 2015; many more Visual QA Challenge Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh Speech Recognition graph credit Matt Zeiler, Clarifai Machine Translation Google Neural Machine Translation (in production) Today § Neural Nets -- wrap § Formalizing Learning § Consistency § Simplicity § Decision Trees § Expressiveness § Information Gain § Overfitting § Clustering Inductive Learning Inductive Learning (Science) § Simplest form: learn a function from examples § A target function: g § Examples: input-output pairs (x, g(x)) § E.g. x is an email and g(x) is spam / ham § E.g. x is a house and g(x) is its selling price § Problem: § Given a hypothesis space H § Given a training set of examples xi § Find a hypothesis h(x) such that h ~ g § Includes: § Classification (outputs = class labels) § Regression (outputs = real numbers) § How do perceptron and naïve Bayes fit in? (H, h, g, etc.) Inductive Learning § Curve fitting (regression, function approximation): § Consistency vs. simplicity § Ockham’s razor Consistency vs. Simplicity § Fundamental tradeoff: bias vs. variance § Usually algorithms prefer consistency by default (why?) § Several ways to operationalize “simplicity” § Reduce the hypothesis space § Assume more: e.g. independence assumptions, as in naïve Bayes § Have fewer, better features / attributes: feature selection § Other structural limitations (decision lists vs trees) § Regularization § Smoothing: cautious use of small counts § Many other generalization parameters (pruning cutoffs today) § Hypothesis space stays big, but harder to get to the outskirts Decision Trees Reminder: Features § Features, aka attributes § Sometimes: TYPE=French § Sometimes: fTYPE=French(x) = 1 Decision Trees § Compact representation of a function: § Truth table § Conditional probability table § Regression values § True function § Realizable: in H Expressiveness of DTs § Can express any function of the features § However, we hope for compact trees Comparison: Perceptrons § What is the expressiveness of a perceptron over these features? § For a perceptron, a feature’s contribution is either positive or negative § If you want one feature’s effect to depend on another, you have to add a new conjunction feature § E.g. adding “PATRONS=full Ù WAIT = 60” allows a perceptron to model the interaction between the two atomic features § DTs automatically conjoin features / attributes § Features can have different effects in different branches of the tree! § Difference between modeling relative evidence weighting (NB) and complex evidence interaction (DTs) § Though if the interactions are too complex, may not find the DT greedily Hypothesis Spaces § How many distinct decision trees with n Boolean attributes? = number of Boolean functions over n attributes = number of distinct truth tables with 2n rows = 2^(2n) § E.g., with 6 Boolean attributes, there are 18,446,744,073,709,551,616 trees § How many trees of depth 1 (decision stumps)? = number of Boolean functions over 1 attribute = number of truth tables with 2 rows, times n = 4n § E.g. with 6 Boolean attributes, there are 24 decision stumps § More expressive hypothesis space: § Increases chance that target function can be expressed (good) § Increases number of hypotheses consistent with training set (bad, why?) § Means we can get better predictions (lower bias) § But we may get worse predictions (higher variance) Decision Tree Learning § Aim: find a small tree consistent with the training examples § Idea: (recursively) choose “most significant” attribute as root of (sub)tree Choosing an Attribute § Idea: a good attribute splits the examples into subsets that are (ideally) “all positive” or “all negative” § So: we need a measure of how “good” a split is, even if the results aren’t perfectly separated out Entropy and Information § Information answers questions § The more uncertain about the answer initially, the more information in the answer § Scale: bits § Answer to Boolean question with prior <1/2, 1/2>? § Answer to 4-way question with prior <1/4, 1/4, 1/4, 1/4>? § Answer to 4-way question with prior <0, 0, 0, 1>? § Answer to 3-way question with prior <1/2, 1/4, 1/4>? § A probability p is typical of: § A uniform distribution of size 1/p § A code of length log 1/p Entropy § General answer: if prior is <p1,…,pn>: § Information is the expected code length § Also called the entropy of the distribution § More uniform = higher entropy § More values = higher entropy § More peaked = lower entropy § Rare values almost “don’t count” 1 bit 0 bits 0.5 bit Information Gain § Back to decision trees! § For each split, compare entropy before and after § Difference is the information gain § Problem: there’s more than one distribution after split! § Solution: use expected entropy, weighted by the number of examples Next Step: Recurse § Now we need to keep growing the tree! § Two branches are done (why?) § What to do under “full”? § See what examples are there… Example: Learned Tree § Decision tree learned from these 12 examples: § Substantially simpler than “true” tree § A more complex hypothesis isn't justified by data § Also: it’s reasonable, but wrong Example: Miles Per Gallon 40 Examples mpg cylinders displacement horsepower weight acceleration modelyear maker good 4 low low low high 75to78 asia bad 6 medium medium medium medium 70to74 america bad 4 medium medium medium low 75to78 europe bad 8 high high high low 70to74 america bad 6 medium medium medium medium 70to74 america bad 4 low medium low medium 70to74 asia bad 4 low medium low low 70to74 asia bad 8 high high high low 75to78 america : : : : : : : : : : : : : : : : : : : : : : : : bad 8 high high high low 70to74 america good 8 high medium high high 79to83 america bad 8 high high high low 75to78 america good 4 low low low low 79to83 america bad 6 medium medium medium high 75to78 america good 4 medium low low low 79to83 america good 4 low low medium high 79to83 america bad 8 high high high low 70to74 america good 4 low medium low medium 75to78 europe bad 5 medium medium medium medium 75to78 europe Find the First Split § Look at information gain for each attribute § Note that each attribute is correlated with the target! § What do we split on? Result: Decision Stump Second Level Final Tree Reminder: Overfitting § Overfitting: § When you stop modeling the patterns in the training data (which generalize) § And start modeling the noise (which doesn’t) § We had this before: § Naïve Bayes: needed to smooth § Perceptron: early stopping MPG Training Error The test set error is much worse than the training set error… …why? Consider this split Significance of a Split § Starting with: § Three cars with 4 cylinders, from Asia, with medium HP § 2 bad MPG § 1 good MPG § What do we expect from a three-way split? § Maybe each example in its own subset? § Maybe just what we saw in the last slide? § Probably shouldn’t split if the counts are so small they could be due to chance § A chi-squared test can tell us how likely it is that deviations from a perfect split are due to chance* § Each split will have a significance value, pCHANCE Keeping it General § Pruning: § Build the full decision tree § Begin at the bottom of the tree § Delete splits in which pCHANCE > MaxPCHANCE § Continue working upward until there are no more prunable nodes § Note: some chance nodes may not get pruned because they were “redeemed” later a b y 0 0 0 0 1 1 1 0 1 1 1 0 y = a XOR b Pruning example § With MaxPCHANCE = 0.1: Note the improved test set accuracy compared with the unpruned tree Regularization § MaxPCHANCE is a regularization parameter § Generally, set it using held-out data (as usual) Small Trees Large Trees MaxPCHANCE Increasing Decreasing Accuracy High Bias High Variance Held-out / Test Training Two Ways of Controlling Overfitting § Limit the hypothesis space § E.g. limit the max depth of trees § Easier to analyze § Regularize the hypothesis selection § E.g. chance cutoff § Disprefer most of the hypotheses unless data is clear § Usually done in practice Next Lecture: Applications! "
534,"CS 188: Artificial Intelligence Advanced Applications: Robotics** Instructors: Pieter Abbeel & Dan Klein --- University of California, Berkeley These slides were created by Dan Klein, Pieter Abbeel and Anca Dragan for CS188 Intro to AI at UC Berkeley. All CS188 materials are available at http://ai.berkeley.edu So Far: Foundational Methods Now: Advanced Applications How would you make an AI for Go? MiniMax! Why is it hard? In particular, why is it harder than chess? Exhaustive search Reducing depth with value network Reducing breadth with policy network Neural network training pipeline Human expert positions Supervised Learning policy network Self-play data Value network Reinforcement Learning policy network One more thing: Monte-Carlo rollouts Robotic Helicopters Motivating Example n How do we execute a task like this? [VIDEO: tictoc_results.wmv] Autonomous Helicopter Flight § Key challenges: § Track helicopter position and orientation during flight § Decide on control inputs to send to helicopter Autonomous Helicopter Setup On-board inertial measurement unit (IMU) Send out controls to helicopter Position HMM for Tracking the Helicopter § State: § Measurements: [observation update] § 3-D coordinates from vision, 3-axis magnetometer, 3-axis gyro, 3-axis accelerometer § Transitions (dynamics): [time elapse update] § st+1 = f (st, at) + wt f: encodes helicopter dynamics, w: noise s = (x, y, z, φ, ✓, , ˙ x, ˙ y, ˙ z, ˙ φ, ˙ ✓, ˙ ) Helicopter MDP § State: § Actions (control inputs): § alon : Main rotor longitudinal cyclic pitch control (affects pitch rate) § alat : Main rotor latitudinal cyclic pitch control (affects roll rate) § acoll : Main rotor collective pitch (affects main rotor thrust) § arud : Tail rotor collective pitch (affects tail rotor thrust) § Transitions (dynamics): § st+1 = f (st, at) + wt [f encodes helicopter dynamics] [w is a probabilistic noise model] § Can we solve the MDP yet? s = (x, y, z, φ, ✓, , ˙ x, ˙ y, ˙ z, ˙ φ, ˙ ✓, ˙ ) Problem: What’s the Reward? § Reward for hovering: RL: Helicopter Flight [Andrew Ng] [Video: HELICOPTER] Problem for More General Case: What’s the Reward? § Rewards for “Flip”? § Problem: what’s the target trajectory? § Just write it down by hand? Flips (?) [VIDEO: 20061204---bad.wmv] Helicopter Apprenticeship? 30 Demonstrations [VIDEO: airshow_unaligned.wmv] Learning a Trajectory • HMM-like generative model – Dynamics model used as HMM transition model – Demos are observations of hidden trajectory • Problem: how do we align observations to hidden trajectory? Demo 1 Demo 2 Hidden Abbeel, Coates, Ng, IJRR 2010 Probabilistic Alignment using a Bayes’ Net § Dynamic Time Warping (Needleman&Wunsch 1970, Sakoe&Chiba, 1978) § Extended Kalman filter / smoother Demo 1 Demo 2 Hidden Abbeel, Coates, Ng, IJRR 2010 Aligned Demonstrations [VIDEO: airshow_unaligned.wmv] Alignment of Samples § Result: inferred sequence is much cleaner! Learned Behavior [Abbeel, Coates, Quigley, Ng, 2010] [VIDEO: airshow_trimmed.wmv] Legged Locomotion For Perspective: Darpa Robotics Challenge (2015) How About Continuous Control, e.g., Locomotion? 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 [22] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 5026–5033. IEEE, 2012. [23] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pages 745–750. ACM, 2007. [24] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difﬁcult learning control problems. Systems, Man and Cybernetics, IEEE Transactions on, (5):834–846, 1983. [25] David Asher Levin, Yuval Peres, and Elizabeth Lee Wilmer. Markov chains and mixing times. American Mathematical Society, 2009. [26] Stephen J Wright and Jorge Nocedal. Numerical optimization, volume 2. Springer New York, 1999. [27] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013. [28] James Bergstra et al. Theano: a CPU and GPU math expression compiler. A Approximating policies with neural networks Joint angles and kinematics Control Standard deviations Fully connected layer 30 units Input layer Mean parameters Sampling Figure 4: Neural network architecture for the locomotion domain: Two fully connected hidden lay- ers transform the input to the mean µ of a Normal distribution from which the controls are sampled Neural network architecture: Input: joint angles and velocities Output: joint torques 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 Figure 1: Robot models used for locomotion experiments, instantiated in MuJoCo physics simulator. The three models on the left are constrained to two dimensions and are called the swimmer, hopper, and walker—these models were used for the main experimental comparisons. two other approaches: reward-weighted regression (RWR) [23]4 and the cross entropy method. All of the methods were used to optimize the same neural-network parameterization of the policy.5 A detailed listing of parameters used in the experiment is provided in Appendix E. We include the classic cart-pole balancing task in additional to the more challenging locomotion domains, based on the formulation from Barto et al. [24]. Learning curves of the policy optimization methods are shown in Figure 2. Our vine algorithm was able to solve all of the tasks, learning a stable and naturalistic gait for the 2D hopper and walker. The single path algorithm exhibits the best looking learning curves (the most reliably monotonic improvement of total expected cost), but it did not yield a locomotion controller for the 2D walker; instead, it yielded a controller that stood up but did not bother to move. The cross-entropy method was not able to solve any of the tasks other than cart-pole, presumably because it does not perform well for optimization problems with more than a couple dozen parame- ters. Reward-weighted regression also performed reasonably well on the tasks, which is consistent Robot models in physics simulator (MuJoCo, from Emo Todorov) Learning Locomotion [Schulman, Moritz, Levine, Jordan, Abbeel, 2015] Deep RL: Virtual Stuntman [Peng, Abbeel, Levine, van de Panne, 2018] Pieter Abbeel -- UC Berkeley | Gradescope | Covariant.AI Quadruped § Low-level control problem: moving a foot into a new location à search with successor function ~ moving the motors § High-level control problem: where should we place the feet? § Reward function R(x) = w .f(s) [25 features] [Kolter, Abbeel & Ng, 2008] § Demonstrate path across the “training terrain” § Learn the reward function § Receive “testing terrain”---height map. § Find the optimal policy with respect to the learned reward function for crossing the testing terrain. Reward Learning + Reinforcement Learning [Kolter, Abbeel & Ng, 2008] Without reward learning With reward learning Autonomous Driving § 150 mile off-road robot race across the Mojave desert § Natural and manmade hazards § No driver, no remote control § No dynamic passing Grand Challenge 2005: Barstow, CA, to Primm, NV Autonomous Vehicles Autonomous vehicle slides adapted from Sebastian Thrun Grand Challenge 2005 Nova Video [VIDEO: nova-race-supershort.mp4] Grand Challenge 2005 – Bad [VIDEO: grand challenge – bad.wmv] An Autonomous Car 5 Lasers Camera Radar E-stop GPS GPS compass 6 Computers IMU Steering motor Control Screen Actions: Steering Control Reference Trajectory Error Velocity Steering Angle (with respect to trajectory) Laser Readings for Flat / Empty Road 1 2 3 Laser Readings for Road with Obstacle DZ Raw Measurements: 12.6% false positives Obstacle Detection Trigger if |Zi-Zj| > 15cm for nearby zi, zj xt+2 xt xt+1 zt+2 zt zt+1 Probabilistic Error Model GPS IMU GPS IMU GPS IMU HMM Inference: 0.02% false positives Raw Measurements: 12.6% false positives HMMs for Detection Sensors: Camera Vision for a Car Vision for a Car [VIDEO: lidar vision for a car] Self-Supervised Vision [VIDEO: self-supervised vision] Urban Environments Google Self-Driving Car (2013) [VIDEO: ROBOTICS – gcar.m4v] (mostly lidar) Recent Progress: NN Semantic Scene Segmentation PSPNet50 ~ neural net classifies every pixel Self-Driving Cars -- Stats Pieter Abbeel -- UC Berkeley | Gradescope | Covariant.AI Self-Driving Cars -- Stats Energy-Inference-Accuracy Landscape on the Squeezelator 68 ImageNet energy-accuracy for different NNs SqueezeNext vs SqueezeNet/AlexNet • 8% more accurate • 2.25x better than SqueezeNet • 7.5x better than AlexNet * MobileNet v1 [slide credit: Kurt Keutzer] Personal Robotics [Wyrobek, Berger, van der Loos, Salisbury, ICRA 2008] PR-1 Pieter Abbeel -- UC Berkeley | Gradescope | Covariant.AI Challenge Task: Robotic Laundry Sock Sorting How about a range of skills? [Levine*, Finn*, Darrell, Abbeel, JMLR 2016] Pieter Abbeel -- UC Berkeley / OpenAI / Gradescope Reinforcement Learning [Levine*, Finn*, Darrell, Abbeel, JMLR 2016] Learned Skills [Levine*, Finn*, Darrell, Abbeel, JMLR 2016 Pieter Abbeel -- UC Berkeley / OpenAI / Gradescope Unsupervised Learning for Interaction? [Levine et al, 2016] Next Time: § Natural language processing § Final contest § Course wrap-up § Where to go next 77 "
59,"Deep Learning for Natural Language Processing Stephen Clark University of Cambridge and DeepMind 1. Introduction to Neural Networks for NLP Stephen Clark University of Cambridge and DeepMind What’s all the fuss about? (Let’s look at Machine Translation) • MT started in the 1950s • Much harder problem than people originally thought • Back translation output of an original system (allegedly): The spirit is willing but the ﬂesh is weak The vodka is good but the steak is lousy Rule-Based MT and the MT Pyramid • Rule-based MT (70s and 80s) • Linguist writes analysis, transfer and generation rules • Resulted in some working systems, e.g. SYSTRAN Statistical Machine Translation (SMT) • Started late 1990s (IBM, Jelinek) “every time I ﬁre a linguist, my recognition rates go up” • Requires parallel corpora • Typically no linguistic knowledge encoded arg max e p(e|f) / p(f|e)p(e) SMT not Science? Taken from cs.jhu.edu/~post/bitext (1988) Phrase-Based SMT • 2003 onwards • Count-based phrasal translation • Used in e.g. the popular Moses SMT system, and previously Google translate Neural MT • 2013 onwards • Based on continuous, distributed representations • Whole system learned end-to- end using gradient descent • Now used by Google (for many languages) Is This a Revolution? • Move from symbolic to statistical NLP (1990s) certainly was a paradigm shift • Neural models certainly dominant in 2018 • Will neural models prove as successful for text as they have for vision and speech? Remember NNs and AI/NLP have been around for decades • Many current papers written as if NLP started in 2013 • See Jürgen Schmidhuber for a similar (highly critical, sometimes amusing) take on current NNs research 1 Mary moved to the bathroom. 2 John went to the hallway. 3 Where is Mary? bathroom 4 Daniel went back to the hallway. 5 Sandra moved to the garden. 6 Where is Daniel? hallway 7 John moved to the office. 8 Sandra journeyed to the bathroom. 9 Where is Daniel? hallway 10 Mary moved to the hallway. 11 Daniel travelled to the office. 12 Where is Daniel? office FAIR bAbi task Lectures* 1. Introduction to neural networks for NLP (Stephen Clark) 2. Feedforward neural networks (Clark) 3. Training and optimization (Clark) 4. Word embeddings (Felix Hill, DeepMind) 5. Recurrent neural networks (Hill) 6. Long short-term memory networks (Hill) 7. Convolutional neural networks (Hill) 8. Tensorﬂow I (Clark) 9. Conditional language models (Chris Dyer, DeepMind and CMU) 10. Conditional language models with attention (Dyer) 11. Machine comprehension (Ed Grefenstette, DeepMind) 12. Tensorﬂow II (Clark) 13. Sentence representations and inference (Grefenstette) 14. Image captioning (Clark) 15. Grounded language learning (Hill) 16. Language in Labyrinth (Hill) *subject to change What this course is not about • Not about building tools for intermediate representations (parsers, pos taggers) • More focus on “end-to-end” task-based training AI-hard Tasks/Applications thanks to Phil Blunsom for slide AI-hard Tasks/Applications thanks to Phil Blunsom for slide AI-hard Tasks/Applications https://www.youtube.com/watch?v=wJjdu1bPJ04&feature=youtu.be Grounded Language Learning Practical • Experimenting with a dictionary deﬁnition model using Tensorﬂow • Microsoft Azure cloud resources available • Two 2-hour sessions, weeks 4 and 6 on Tuesday afternoons Assessment • One practical report (40%) • One take-home exam (60%) • See course web pages for details on when these are due Components of an End-to-End (Sentiment Analysis) System This ﬁlm is n’t great + — -VE binary classiﬁer Word Embeddings • Random initialization, learn as part of task objective • External initialization (eg Word2Vec), update as part of task objective • External initialization, keep ﬁxed This ﬁlm is n’t great Sentence Embeddings • Recurrent neural network (RNN, LSTM, Tree RNN) combines the word vectors • Could use a convolutional neural network (CNN), or a combination of RNN, CNN This ﬁlm is n’t great Why on Earth Does it Work? • Huge number parameters (so need lots of data) • Distributed representations share statistical strength • Optimization surface is highly non-convex, system is highly non-linear (but SGD works surprisingly well) This ﬁlm is n’t great Main Readings • Deep Learning, Goodfellow, Bengio and Courville www.deeplearningbook.org • A Primer on Neural Network Models for Natural Language Processing, Yoav Goldberg "
6,"School of Computer Science Probabilistic Graphical Models Generalized linear models Eric Xing Lecture 6, February 3, 2014 Reading: KF-chap 17 X1 X4 X2 X3 X4 X2 X3 X1 X1 X2 X1 X3 X1 X4 X2 X3 X4 X2 X3 X1 X1 X2 X1 X3 1 © Eric Xing @ CMU, 2005-2014 Parameterizing graphical models Bayesian network:    d i i i X P P : 1 ) | ( ) (  X X A B C a0 0.75 a1 0.25 b0 0.33 b1 0.67 a0b0 a0b1 a1b0 a1b1 c0 0.45 1 0.9 0.7 c1 0.55 0 0.1 0.3 Or A B C A~N(μa, Σa) B~N(μb, Σb) C~N(A+B, Σc) Or D i s c r e t e C o n t i n u o u s H y b r i d ? 2 © Eric Xing @ CMU, 2005-2014 Recall Linear Regression Let us assume that the target variable and the inputs are related by the equation: where ε is an error term of unmodeled effects or random noise Now assume that ε follows a Gaussian N(0,σ), then we have: We can use LMS algorithm, which is a gradient ascent/descent approach, to estimate the parameter i i T i y     x            2 2 2 2 1      ) ( exp ) ; | ( i T i i i y x y p x 3 © Eric Xing @ CMU, 2005-2014 Recall: Logistic Regression (sigmoid classifier, perceptron, etc.) The condition distribution: a Bernoulli where is a logistic function We can used the brute-force gradient method as in LR But we can also apply generic laws by observing the p(y|x) is an exponential family function, more specifically, a generalized linear model! y y x x x y p    1 1 )) ( ( ) ( ) | (   x T e x      1 1 ) ( 4 © Eric Xing @ CMU, 2005-2014 Parameterizing graphical models Markov random fields   ) ( exp 1 ) ( exp 1 ) ( x x x H Z Z p C c c c                        i i i N j i j i ij X X X Z X p i 0 , exp 1 ) (   5 © Eric Xing @ CMU, 2005-2014 hidden units visible units { } ∑ ∑ ∑ , , , ) ( - ) , ( + ) ( + ) ( exp = ) | , ( j j i j i j i j i j j j i i i i A h x h x h x p θ        Restricted Boltzmann Machines © Eric Xing @ CMU, 2005-2014 6 Conditional Random Fields         c c c c y x f x Z x y p ) , ( exp ) , ( 1 ) | (    A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... Y1 Y2 Y5 … X1 … Xn  Discriminative  Xi’s are assumed as features that are inter-dependent  When labeling Xi future observations are taken into account 7 © Eric Xing @ CMU, 2005-2014 Conditional Distribution  If the graph G = (V, E) of Y is a tree, the conditional distribution over the label sequence Y = y, given X = x, by the Hammersley Clifford theorem of random fields is: ─ x is a data sequence ─ y is a label sequence ─ v is a vertex from vertex set V = set of label random variables ─ e is an edge from edge set E over V ─ fk and gk are given and fixed. gk is a Boolean vertex feature; fk is a Boolean edge feature ─ k is the index number of the features ─ are parameters to be estimated ─ y|e is the set of components of y defined by edge e ─ y|v is the set of components of y defined by vertex v 1 2 1 2 ( , , , ; , , , ); and n n k k           Y1 Y2 Y5 … X1 … Xn (y| x) exp ( ,y| ,x) ( ,y| 1 (x) ,x)                k k e k k v e E,k v V ,k p f e g v Z 8 © Eric Xing @ CMU, 2005-2014 2-D Conditional Random Fields  Allow arbitrary dependencies on input  Clique dependencies on labels  Use approximate inference for general graphs         c c c c y x f x Z x y p ) , ( exp ) , ( ) | (    1 9 © Eric Xing @ CMU, 2005-2014 Exponential family, a basic building block For a numeric random variable X is an exponential family distribution with natural (canonical) parameter  Function T(x) is a sufficient statistic. Function A() = log Z() is the log normalizer. Examples: Bernoulli, multinomial, Gaussian, Poisson, gamma,...     ) ( exp ) ( ) ( ) ( ) ( exp ) ( ) | ( x T x h Z A x T x h x p T T      1    10 © Eric Xing @ CMU, 2005-2014 Example: Multivariate Gaussian Distribution For a continuous vector random variable XRk: Exponential family representation  Note: a k-dimensional Gaussian is a (d+d2)-parameter distribution with a (d+d2)- element vector of sufficient statistics (but because of symmetry and positivity, parameters are constrained and have lower degree of freedom)                                   log tr exp ) ( ) ( exp ) , ( / / /         1 2 1 1 1 2 1 2 1 2 1 2 2 1 2 1 2 1 T T T k T k x xx x x x p                 2 2 2 1 1 1 2 2 1 1 2 1 1 2 1 2 1 1 2 1 1 2 1 1 2 2 / ) ( log ) ( tr log ) ( vec ; ) ( and , vec , vec ; k T T T x h A xx x x T                                          Moment parameter Natural parameter 11 © Eric Xing @ CMU, 2005-2014 Example: Multinomial distribution For a binary vector random variable Exponential family representation ), | ( multi ~  x x          k k k x K x x x x p K      ln exp ) ( 1 1 2 1   1 1 0 1 1 1                                    ) ( ln ln ) ( ) ( ; ln x h e A x x T K k K k k K k k                                                                    1 1 1 1 1 1 1 1 1 1 1 1 1 ln 1 ln exp 1 ln 1 ln exp K k k K k k K k k k K k k K k K K k k k x x x      12 © Eric Xing @ CMU, 2005-2014 Why exponential family? Moment generating property       ) ( ) ( ) ( exp ) ( ) ( ) ( exp ) ( ) ( ) ( ) ( ) ( log x T E dx Z x T x h x T dx x T x h d d Z Z d d Z Z d d d dA T T                   1 1           ) ( ) ( ) ( ) ( ) ( ) ( ) ( exp ) ( ) ( ) ( ) ( exp ) ( ) ( x T Var x T E x T E Z d d Z dx Z x T x h x T dx Z x T x h x T d A d 2 T T        2 2 2 2 1         13 © Eric Xing @ CMU, 2005-2014 Moment estimation We can easily compute moments of any exponential family distribution by taking the derivatives of the log normalizer A(). The qth derivative gives the qth centered moment. When the sufficient statistic is a stacked vector, partial derivatives need to be considered.  variance ) ( mean ) (   2 2     d A d d dA 14 © Eric Xing @ CMU, 2005-2014 Moment vs canonical parameters The moment parameter µ can be derived from the natural (canonical) parameter A() is convex since Hence we can invert the relationship and infer the canonical parameter from the moment parameter (1-to-1):  A distribution in the exponential family can be parameterized not only by the canonical parameterization, but also by the moment parameterization.      def ) ( ) (   x T E d dA   0 2 2   ) ( ) ( x T Var d A d   ) ( def    4 8 -2 -1 0 1 2 4 8 -2 -1 0 1 2 A   15 © Eric Xing @ CMU, 2005-2014 MLE for Exponential Family For iid data, the log-likelihood is Take derivatives and set to zero: This amounts to moment matching. We can infer the canonical parameters using                 n n n T n n n T n NA x T x h A x T x h D ) ( ) ( ) ( log ) ( ) ( exp ) ( log ) ; (      l ) ( ) ( ) ( ) ( ) (                n n MLE n n n n x T N x T N A A N x T 1 1 0        l ) ( MLE MLE       16 © Eric Xing @ CMU, 2005-2014 Sufficiency For p(x|), T(x) is sufficient for if there is no information in X regarding beyond that in T(x).  We can throw away X for the purpose of inference w.r.t. .  Bayesian view  Frequentist view  The Neyman factorization theorem  T(x) is sufficient for if T(x)  X )) ( | ( ) ), ( | ( x T p x x T p    T(x)  X )) ( | ( ) ), ( | ( x T x p x T x p   T(x)  X )) ( , ( ) ), ( ( ) ), ( , ( x T x x T x T x p 2 1     )) ( , ( ) ), ( ( ) | ( x T x h x T g x p    17 © Eric Xing @ CMU, 2005-2014 Examples Gaussian: Multinomial: Poisson:           2 2 1 1 2 1 1 2 1 1 2 / ) ( log ) ( vec ; ) ( vec ; k T T x h A xx x x T                          n n n n MLE x N x T N 1 1 1 ) (   1 1 0 1 1 1                                    ) ( ln ln ) ( ) ( ; ln x h e A x x T K k K k k K k k          n n MLE x N 1  ! ) ( ) ( ) ( log x x h e A x x T 1              n n MLE x N 1  18 © Eric Xing @ CMU, 2005-2014 Bayesian est. © Eric Xing @ CMU, 2005-2014 19 Generalized Linear Models (GLIMs) The graphical model  Linear regression  Discriminative linear classification  Commonality: model Ep(Y)==f(TX)  What is p()? the cond. dist. of Y.  What is f()? the response function. GLIM  The observed input x is assumed to enter into the model via a linear combination of its elements  The conditional mean is represented as a function f() of , where f is known as the response function  The observed output y is assumed to be characterized by an exponential family distribution with conditional mean . Xn Yn N x T   20 © Eric Xing @ CMU, 2005-2014 GLIM, cont.  The choice of exp family is constrained by the nature of the data Y  Example: y is a continuous vector multivariate Gaussian y is a class label Bernoulli or multinomial  The choice of the response function  Following some mild constrains, e.g., [0,1]. Positivity …  Canonical response function:  In this case Tx directly corresponds to canonical parameter .     ) ( ) ( exp ) , ( ) , | ( 1       A y x y h y p T    ) (  1  f   ) ( ) ( exp ) ( ) | (    A y x y h y p T     f  x   y EXP 21 © Eric Xing @ CMU, 2005-2014 Example canonical response functions © Eric Xing @ CMU, 2005-2014 22 MLE for GLIMs with natural response Log-likelihood Derivative of Log-likelihood Online learning for canonical GLIMs  Stochastic gradient ascent = least mean squares (LMS) algorithm:        n n n n n T n A y x y h ) ( ) ( log   l   ) ( ) (                        y X x y d d d dA y x d d T n n n n n n n n n n l This is a fixed point function because is a function of   n t n n t t x y        1  size step a is and where    n T t t n x  23 © Eric Xing @ CMU, 2005-2014 Batch learning for canonical GLIMs The Hessian matrix where is the design matrix and which can be computed by calculating the 2nd derivative of A(n)   WX X x x d d x d d d d x d d x x y d d d d d H T n T n T n n n n n T n n n n n n T n n n n n n T T                              since l 2   T n x X           N N d d d d W     , , diag  1 1                          n x x x X    2 1              n y y y y   2 1 24 © Eric Xing @ CMU, 2005-2014 Recall LMS Cost function in matrix form: To minimize J(θ), take derivative and set to zero:    y y y J T n i i T i             X X x 2 1 2 1 1 2 ) ( ) (                          n x x x X    2 1              n y y y y   2 1       0 2 2 1 2 2 1 2 1                   y X X X y X X X X X y y X y X X y y X y y X X X J T T T T T T T T T T T T T T T                         tr tr tr tr y X X X T T     The normal equations   y X X X T T  1   *   25 © Eric Xing @ CMU, 2005-2014 Iteratively Reweighted Least Squares (IRLS) Recall Newton-Raphson methods with cost function J We now have Now:  where the adjusted response is This can be understood as solving the following "" Iteratively reweighted least squares "" problem J H t t         1 1 WX X H y X J T T      ) (        t t T t T t T t t T t T t t z W X X W X y X X W X X W X H 1 1 1 1            ) (     l   ) ( t t t t y W X z      1 ) ( ) ( min arg     X z W X z T t    1   y X X X T T  1   *  26 © Eric Xing @ CMU, 2005-2014 Example 1: logistic regression (sigmoid classifier) The condition distribution: a Bernoulli where is a logistic function p(y|x) is an exponential family function, with  mean:  and canonical response function IRLS y y x x x y p    1 1 )) ( ( ) ( ) | (   ) ( ) ( x e x     1 1   ) ( | x e x y E       1 1 x T                     ) ( ) ( ) ( N N W d d         1 1 1 1 1  27 © Eric Xing @ CMU, 2005-2014 Logistic regression: practical issues It is very common to use regularized maximum likelihood.  IRLS takes O(Nd3) per iteration, where N = number of training cases and d = dimension of input x.  Quasi-Newton methods, that approximate the Hessian, work faster.  Conjugate gradient takes O(Nd) per iteration, and usually works best in practice.  Stochastic gradient descent can also be used if N is large c.f. perceptron rule:               T n n T n T x y x y l I p x y e x y p T 2 0 1 1 1 1           ) ( log ) ( ) , ( Normal ~ ) ( ) ( ) , (           n n n T n x y x y ) ( 1 l 28 © Eric Xing @ CMU, 2005-2014 Example 2: linear regression The condition distribution: a Gaussian where is a linear function p(y|x) is an exponential family function, with  mean:  and canonical response function IRLS ) ( ) ( x x x T        x x y E T    | x T      1 I W d d  1         ) ( ) ( exp ) ( )) ( ( )) ( ( exp ) , , ( / /       A y x x h x y x y x y p T T k                    1 2 1 1 2 1 2 2 1 2 1         ) ( ) ( t T T t t t T T t t T t T t y X X X y X X X X z W X X W X                 1 1 1 1     t Y X X X T T 1 ) (    Steepest descent Normal equation Rescale 29 © Eric Xing @ CMU, 2005-2014 Classification Generative and discriminative approach Q X Q X Regression Linear, conditional mixture, nonparametric X Y Density estimation Parametric and nonparametric methods , X X Simple GMs are the building blocks of complex BNs 30 © Eric Xing @ CMU, 2005-2014 School of Computer Science An (incomplete) genealogy of graphical models The structures of most GMs (e.g., all listed here), are not learned from data, but designed by human. But such designs are useful and indeed favored because thereby human knowledge are put into good use … 31 © Eric Xing @ CMU, 2005-2014 MLE for general BNs If we assume the parameters for each CPD (a GLIM) are globally independent, and all nodes are fully observed, then the log-likelihood function decomposes into a sum of local terms, one per node: Therefore, MLE-based parameter estimation of GM reduces to local est. of each GLIM                    i n i n i n n i i n i n i i x p x p D p D ) , | ( log ) , | ( log ) | ( log ) ; ( , , , ,       x x l X2=1 X2=0 X5=0 X5=1 32 © Eric Xing @ CMU, 2005-2014 Earthquake Radio Burglary Alarm Call     M i i i x p p 1 ) | ( ) (  x x X Factorization: j i k i i x j k i x p    x x | ) | (  Local Distributions defined by, e.g., multinomial parameters: How to define parameter prior? Assumptions (Geiger & Heckerman 97,99):  Complete Model Equivalence  Global Parameter Independence  Local Parameter Independence  Likelihood and Prior Modularity ? ) | ( G p  33 © Eric Xing @ CMU, 2005-2014 Global Parameter Independence For every DAG model: Local Parameter Independence For every node:    M i i m G p G p 1 ) | ( ) | (   Earthquake Radio Burglary Alarm Call    i j i k i q j x i G p G p 1 | ) | ( ) | (    x independent of ) ( | YES Alarm Call P   ) ( | NO Alarm Call P   Global & Local Parameter Independence 34 © Eric Xing @ CMU, 2005-2014 Provided all variables are observed in all cases, we can perform Bayesian update each parameter independently !!! sample 1 sample 2  2|1 1 2|1 X1 X2 X1 X2 Global Parameter Independence Local Parameter Independence Parameter Independence, Graphical View 35 © Eric Xing @ CMU, 2005-2014 Which PDFs Satisfy Our Assumptions? (Geiger & Heckerman 97,99) Discrete DAG Models: Dirichlet prior: Gaussian DAG Models: Normal prior: Normal-Wishart prior:         k k k k k k k k k k C P 1 - 1 - ) ( ) ( ) ( ) (         ) ( Multi ~ |   j x i i x ) , ( Normal ~ |    j x i i x               ) ( )' ( exp | | ) ( ) , | ( / /        1 2 1 2 2 1 2 1 n p   . where , W tr exp W ) , ( ) , | W ( / ) ( / 1 2 1 2 2 1                W n w w w w n c p      , ) ( , Normal ) , , | ( 1   W W        p 36 © Eric Xing @ CMU, 2005-2014 Summary: Parameterizing GM For exponential family dist., MLE amounts to moment matching GLIM:  Natural response  Iteratively Reweighted Least Squares as a general algorithm GLIMs are building blocks of most GMs in practical use Parameter independence and appropriate priors 37 © Eric Xing @ CMU, 2005-2014 "
60,"Deep Learning for Natural Language Processing Stephen Clark University of Cambridge and DeepMind 2. Feedforward Neural Networks for NLP Stephen Clark University of Cambridge and DeepMind Named Entity Recognition (as a tagging task) Example tagset: {I-PER, I-ORG, I-LOC, I-TIME, O} NER as Ambiguity Resolution Discrete Features for NER curr_word=‘Somerset’ next_word=‘cricket’ next_next_word=‘club’ next_bigram=‘cricket club’ … prev_word=‘for’ prev_prev_word=‘bowled’ prev_bigram=‘bowled for’ … curr_pos=‘NNP’ next_pos=‘NN’ next_next_pos=‘NN’ next_bigram_pos=‘NN NN’ …. Botham bowled for Somerset cricket club in the 1980s NNP VBD IN NNP NN NN IN DT NNS Indicator Features for NER Botham bowled for Somerset cricket club in the 1980s NNP VBD IN NNP NN NN IN DT NNS fi(C, t) = ( 1 if word(C) = Somerset & t = I-ORG 0 otherwise word(C) = Somerset is a contextual predicate Log-Linear Classiﬁcation Model Botham bowled for Somerset cricket club in the 1980s NNP VBD IN NNP NN NN IN DT NNS Z(C) = X t2T exp n X i=1 λifi(C, t) ! where T is the tagset Decoding a Log-Linear Tagging Model Botham bowled for Somerset cricket club in the 1980s NNP VBD IN NNP NN NN IN DT NNS I-PER O O I-ORG I-ORG I-ORG O O I-TIME NER Feature Set I Taken from Curran and Clark (2003) NER Feature Set II Some Accuracy Numbers Taken from Curran and Clark (2003) What’s the Problem? • Huge number of features (potentially millions) • All features (esp. the combinations) deﬁned manually • All features are equally unlike each other (no ‘sharing of statistical strength’) • curr_word=‘Somerset’ no closer to curr_word=‘Kent’ than curr_word=‘Botham’ or curr_word=‘cat’ Dense Features for Classiﬁcation Botham bowled for Somerset cricket club in the 1980s … … words pos tags NNP VBD IN NNP NN NN IN DT NNS suﬃx caps hyphen MLP I-ORG The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … hidden layer … … aﬃne trans + non-linearity I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … hidden layer … … aﬃne trans + non-linearity I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … Word Embeddings Botham bowled for Somerset cricket club in the 1980s curr_word NNP VBD IN NNP NN NN IN DT NNS wi = f i E is the embedding for the ith word wi f i is the one-hot vector for the ith word E is the word embedding matrix (lookup table) wi 2 R1⇥n, f i 2 {0, 1}1⇥|V |, E 2 R|V |⇥n Word Embeddings one-hot input (learnable) embedding matrix word embedding The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … hidden layer … … aﬃne trans + non-linearity I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … xi = [ewi−bk/2c; . . . ; ewi−1; ewi; ewi+1; . . . ; ewi+bk/2c; swi; cwi; hwi] ; is concatenation, k is the window size (xi) The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … hidden layer … … aﬃne trans + non-linearity I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … The MLP Classiﬁer … … dense input hidden layer … … aﬃne trans + non-linearity hn i = S(hn−1 i Wn + bn) h1 i = S(xi W1 + b1) xi 2 R1⇥N, W1 2 RN⇥h1, b1 2 R1⇥h1 The MLP Classiﬁer one-hot input embedding matrix … … … … dense input … … … hidden layer … … aﬃne trans + non-linearity I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … The MLP Classiﬁer hidden layer I-ORG, I-LOC, I-PER, I-TIME, O softmax probabilistic output linear trans … y = hm i U U 2 Rhm⇥|L| m is the number of layers, L is the NER label set Lj = P(Lj) = eyj P k eyk hm i U y L Supervised Training bowled for Somerset cricket club I-ORG for Somerset cricket club in I-ORG Somerset cricket club in the I-ORG Botham bowled for Somerset cricket O cricket club in the 1980s O Ian Botham bowled for Somerset O • (Context, Label) pairs for training data Supervised Training • (Context, Label) pairs for training data • Maximize log-likelihood of the training data • Initialize all parameters • Calculate gradients (using a subset of the data) • Update parameters • Repeat Supervised Training … … … … … … … … … I-ORG, I-LOC, I-PER, I-TIME, O … Botham bowled for Somerset cricket club in the 1980s Backward pass (backprop) calculates gradients Forward pass calculates loss Gradients used for parameter updates Correct answer A State-of-the-Art System Taken from Chiu and Nichols (2016) Some State-of-the-Art Numbers Taken from Chiu and Nichols (2016) References • Language Independent NER using a Maximum Entropy Tagger, James R. Curran and Stephen Clark (2003) • Natural Language Processing (Almost) from Scratch, Collobert, Weston, et al. (2011) • Named Entity Recognition with Bidirectional LSTM-CNNs, Chiu and Nichols (2016) "
61,"Deep Learning for Natural Language Processing Stephen Clark University of Cambridge and DeepMind 3. Training and Optimization Stephen Clark University of Cambridge and DeepMind Supervised Training … … … … … … … … … I-ORG, I-LOC, I-PER, I-TIME, O … Botham bowled for Somerset cricket club in the 1980s Backward pass (backprop) calculates gradients Forward pass calculates loss Gradients used for parameter updates Correct answer An Aside on Reinforcement Learning • We’re seeing more work on RL in NLP (e.g. dialog) • RL problems typically have “trial-and-error search and delayed reward” An Aside on Unsupervised Learning • Important in deriving general-purpose representations The Loss • For language tasks, typically negative log-likelihood − m X i=1 log P(y(i)|x(i)) bowled for Somerset cricket club I-ORG <SOS> The − m X i=1 |y(i)| X j=1 log P(y(i) j |y(i) <j) <SOS> The dog <SOS> The dog sleeps <SOS> The dog sleeps before <SOS> The dog sleeps before chasing <SOS> The dog sleeps before chasing the <SOS> The dog sleeps before chasing the cat for Somerset cricket club in I-ORG Somerset cricket club in the I-ORG Botham bowled for Somerset cricket O Ian Botham bowled for Somerset O cricket club in the 1980s O Empirical Risk Minimization • What we’d ideally like to minimize: • What we minimize in practice: J⇤(✓) = E(x,y)⇠pdataL(f(x; ✓), y) J(✓) = E(x,y)⇠ˆ pdataL(f(x; ✓), y) = 1 m m X i=1 L(f(x(i); ✓), y(i)) ✓are the parameters, {(x(i), y(i))}m i=1 the training data, L the loss Regularization • Overﬁtting is a major problem with powerful deep models (esp. when data is scarce) • We’d like our models to perform well on unseen test data (possibly at the expense of training set performance) • Requiring good generalization is what separates machine learning from pure optimization problems It’s All About the Gradients r✓J(✓) = 1 m m X i=1 r✓L(x(i), y(i), ✓) J(✓) = Ex,y⇠ˆ pdataL(x, y, ✓) = 1 m m X i=1 L(x(i), y(i), ✓) where L(x, y, ✓) = −log p(y|x; ✓) It’s All About the Gradients r✓L(x(i), y(i), ✓) = ⇢@ @✓j L(x(i), y(i), ✓) "" j r✓J(✓) = 1 m m X i=1 r✓L(x(i), y(i), ✓) = ⌧@L @✓1 , @L @✓2 , @L @✓3 , . . . , @L @✓|✓| "" Gradient-based Optimization p.83 of the Deep Learning textbook Lots of Parameters … … … … … … … … … I-ORG, I-LOC, I-PER, I-TIME, O … Botham bowled for Somerset cricket club in the 1980s 0 B B @ −0.0537 0.0263 −0.0003 . . . 0.0099 0.0000 0.0986 −0.1023 . . . 0.0654 . . . 0.1020 −0.8750 0.0000 . . . −0.1200 1 C C A 0 B B @ −0.1063 −0.0234 −0.4013 . . . −0.1109 0.0011 0.1986 −0.0122 . . . 0.0876 . . . −0.9876 1.0092 −1.9650 . . . 1.1200 1 C C A ! 0.1043 −0.0135 −0.1113 . . . 0.1209 "" 0 B B @ 0.1063 −0.0214 −0.4013 0.9999 −0.1109 0.0211 0.0086 0.0102 0.2311 0.0876 . . . −0.1176 1.0012 1.2150 0.7610 1.1211 1 C C A All need to be learned (or updated) Gradient-based Optimization global optimum local optimum parameter space particular set of parameters gradient ⌧@L @✓1 , @L @✓2 , @L @✓3 , . . . , @L @✓|✓| "" ⌦ ✓1, ✓2, ✓3, . . . , ✓|✓| ↵ max ✓ L optimal set of parameters Gradient-based Optimization is Hard • Lots of local maxima for deep networks • In particular lots of saddle points • Exploding and vanishing gradients Approximate Minimization p.85 of the Deep Learning textbook Calculating Gradients • Back-propagation is an eﬃcient algorithm for computing gradients in a neural network: “Back-propagation is an algorithm that computes the chain rule [of calculus], with a speciﬁc order of operations that is highly eﬃcient.” • Another algorithm, such as SGD, uses those gradients to perform learning (optimization) • Back-propagation is not speciﬁc to MLPs, but in principle can compute derivatives of any function p.198 of the Deep Learning textbook Backprop for MLPs … … … … … … … … … … i j wij L = 1 2(t −y)2 where t is the target output, y is the actual output oj = σ( n X k=1 wkjok) Backprop for MLPs … … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij oj = σ(inpj) = σ( n X k=1 wkjok) Backprop for MLPs … … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij inpj = n X k=1 wkjok @inpj @wij = oi Backprop for MLPs … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij @σ(inpj) @inpj = σ(inpj)(1 −σ(inpj)) σ(z) = 1 1 + e−z dσ(z) dz = σ(z)(1 −σ(z)) = oj(1 −oj) Backprop for MLPs … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij Backprop for MLPs … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij oj = y @L @y = y −t Backprop for MLPs … … … … … … … … … i j wij L = 1 2(t −y)2 @L @wij = @L @oj @oj @inpj @inpj @wij @L @oj = X l ✓@L @ol @ol @inpl wjl ◆ Backprop for MLPs … … … … … … i j wij L = 1 2(t −y)2 @L @wij = oiδj … δj = ( (oj −t)oj(1 −oj) if j is an output neuron (P l δlwjl) oj(1 −oj) otherwise Backprop for MLPs … … … … … … … … … … j δj = ( (oj −tj)oj(1 −oj) if j is an output neuron (P l δlwjl) oj(1 −oj) otherwise l j δl Only compute this once and store in a DP table Backprop for MLPs … … … … … … … … … … i j wij δj = ( (oj −tj)oj(1 −oj) if j is an output neuron (P l δlwjl) oj(1 −oj) otherwise Backprop = chain rule + dynamic programming Backprop in Tensorﬂow Estimating Gradients Batch learning g = r✓J(✓) = 1 m m X i=1 r✓L(x(i), y(i), ✓) ˆ g = 1 |B| X (x(j),y(j))2B r✓L(x(j), y(j), ✓) Mini-batch learning In “pure” SGD, |B| = 1 (!) Selecting the Mini-batch • Larger batches give more accurate estimate of the gradient, but with less than linear returns (p.271 DL book) • Multicore architectures are usually underutilized by extremely small batch sizes (p.272 DL book) • Small batches can oﬀer a regularizing eﬀect, perhaps due to the noise they add to the learning process (p.272) • In practice, shuﬄe the training data, then cycle through the same mini-batches on each epoch Stochastic Gradient Descent (SGD) p.286 of the Deep Learning textbook ✏k = (1 −↵)✏0 + ↵✏⌧with ↵= max(k/⌧, 1) Lots of Other Alternatives ……. Optimization in Tensorﬂow from the Tensorﬂow RNN LM tutorial Parameter Initialization (matters) Imagine you’re trying to get to the top of Arthur’s Seat in the fog (haar); where you start will be crucial to success Parameter Initialization (matters) • Typically the weights are initialized randomly, biases are set to heuristically chosen constants • Random weights are drawn from Gaussian or Uniform • e.g. p.292 of the Deep Learning textbook U ✓ −1 pm, 1 pm ◆ where m is the number of inputs Regularization ˜ J(✓; x, y) = J(✓; x, y) + ↵⌦(✓) L2 parameter norm penalty (weight decay) ⌦(✓) = 1 2kwk2 2 = 1 2w>w Dropout References • How the backpropagation algorithm works, Michael Nielsen • colah’s blog http://neuralnetworksanddeeplearning.com/chap2.html http://colah.github.io/posts/2015-08-Backprop/ "
62,"Deep Learning for Natural Language Processing Stephen Clark et al… DeepMind and University of Cambridge 4. Word Embeddings Felix Hill DeepMind A potted history of……. In ancient times, hundreds of years before the dawn of history… In ancient times, hundreds of years before the dawn of history deep learning The meaning of meaning, (before the dawn of history) It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general-purpose functions. Our study provides strong evidence that language —indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study research The meaning of meaning, It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study research The meaning of meaning, It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 research The meaning of meaning, It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 1 1 0 0 research 0 0 0 0 1 The meaning of meaning, a_lot_of_different_words —————————-> study 1 0 1 0 0 0 0 2 0 0 7 6 0 0 0 2 0 3 0 4 0 0 0 4 0 1 0 1 1 0 0 0 1 0 0 1 research 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 2 0 0 0 0 5 0 0 0 4 0 5 0 0 0 6 0 0 7 distributional semantics The meaning of “research” http://aclweb.org/anthology/C/C65/C65-1010.pdf http://www.aclweb.org/anthology/C65-1003 1965: a great year for distributional semantics How can we improve this? It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 1 1 0 0 research 0 0 0 0 1 How can we improve this? It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 1 1 0 0 research 0 0 0 0 1 Change the size of this thing How can we improve this? It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 1 1 0 0 research 0 0 0 0 1 Use a parser to determine what “close” means How can we improve this? It has long been debated whether the mechanisms that underlie language are dedicated to this uniquely human capacity or whether in fact they serve more general- purpose functions. Our study provides strong evidence that language—indeed both ﬁrst and second language—is learned, in speciﬁc ways, by general-purpose neurocognitive mechanisms that preexist Homo sapiens. The results have broad implications. They elucidate both the ontogeny (development) and phylogeny (evolution) of language. Moreover, they suggest that our substantial knowledge of the general-purpose mechanisms, from both animal and human studies, may also apply to language. The study may thus lead to a research program that can generate a wide range of predictions about this critical domain. language evidence functions learn program …… study 1 1 1 0 0 research 0 0 0 0 1 put only certain words here…. a_lot_of_different_words —————————-> study 1 0 1 0 0 0 0 2 0 0 7 6 0 0 0 2 0 3 0 4 0 0 0 4 0 1 0 1 1 0 0 0 1 0 0 1 research 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 2 0 0 0 0 5 0 0 0 4 0 5 0 0 0 6 0 0 7 Do something fancy to these numbers… cf: Sparck-Jones and tf-idf https://en.wikipedia.org/wiki/Tf%E2%80%93idf How can we improve this? a_lot_of_different_words —————————-> study 1 0 1 0 0 0 0 2 0 0 7 6 0 0 0 2 0 3 0 4 0 0 0 4 0 1 0 1 1 0 0 0 1 0 0 1 research 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 2 0 0 0 0 5 0 0 0 4 0 5 0 0 0 6 0 0 7 a lot of other a lot of numbers……….. words too yay How can we improve this? matrix factorisation no obvious link to words… —————————-> study 4.12 3.81 -2.17 8.13 7.23 researc h a lot of other not so many numbers (zeros) words too yay a ‘better’ meaning of “research” A solution to Plato’s problem Latent Semantic Analysis Landauer & Dumais (1997) Plato a_lot_of_different_words —————————-> study 1 0 1 0 0 0 0 2 0 0 7 6 0 0 0 2 0 3 0 4 0 0 0 4 0 1 0 1 1 0 0 0 1 0 0 1 research 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 2 0 0 0 0 5 0 0 0 4 0 5 0 0 0 6 0 0 7 a lot of other a lot of numbers……….. words too yay matrix factorisation no obvious link to words… —————————-> study 4.12 3.81 -2.17 8.13 7.23 researc h a lot of other not so many numbers (zeros) words too yay a ‘better’ meaning of “research” A solution to Plato’s problem use SVD instead of PCA TOEFL questions Learning the meaning of words 1965-~2010 Word2Vec … … … … Word2Vec is a wonky MLP…. a word embedding a word embedding - 300 units 150,000 words x y Word2Vec Word2Vec Where do we get the input words and the outputs words? output word input word Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word Skipgram Word2Vec Where do we get the input words and the outputs words? output word input word CBOW Skipgram a word embedding - 300 units 150,000 words input we Where do we get the input words and the outputs words? window size = 3 Skipgram a word embedding - 300 units 150,000 words input get Where do we get the input words and the outputs words? Skipgram a word embedding - 300 units 150,000 words input Where do we get the input words and the outputs words? the Skipgram a word embedding - 300 units 150,000 words input Where do we get the input words and the outputs words? words Skipgram a word embedding - 300 units 150,000 words input Where do we get the input words and the outputs words? words etc!!! the words get and input CBOW a word embedding - 300 units 150,000 words Where do we get the input words and the outputs words? window size = 2 How many free parameters in this model? a word embedding - 300 units 150,000 words x y Computing the loss $$$ x y P(y | x) y A cheaper option… x σ(x) = a nice score function y “negative sampling” x σ(x) = a nice score function Associated words are close in vector space http://projector.tensorﬂow.org/ Anything more than that? Next thing you know… Anything more…? let’s efﬁciently predict anything from anything syntax videos images word net human behavior brain data locations tweets the real purpose… the words in the vocab … … … … … … … … = enlightenment = a prediction = a better prediction a low-resource language application References Natural language processing (almost) from scratch (Collobert et al. 2011, from 2008) Transfer learning with word-embeddings Eﬃcient estimation of word representations in vector space (Mikolov et al. 2013) Word2Vec - much faster and easier Evaluating semantic models with (genuine) similarity estimation (Hill et al. 2014) Similarity, not just association, in word embedding spaces Neural word embeddings as implicit matrix factorization (Levy & Goldberg, 2014) Equivalence between (old) count-based semantic spaces and word2vec "
63,"Deep Learning for Natural Language Processing Stephen Clark et al… DeepMind and University of Cambridge 5. Recurrent Neural Networks Felix Hill DeepMind What are neural nets for? What are neural nets for? How can you apply a neural net to language? “language does not naturally go here, ahem, but fortunately…..” How can you apply a neural net to language? “language does not naturally go here, ahem, but fortunately…..” what’s the issue here???? That’s the whole point!! What is James doing in the store room? searching for a book… What is that empty cup doing over there? err..being a cup? time flies like an arrow fruit flies like a banana The networks that are good at Go and Atari were ﬁrst developed for this reason! Finding structure in time - Elman, 1990 The simple recurrent network (now RNN) now now this this now is is now this a a now this is story story now this is a all all now this is a story about about now this is a story all how how now this is a story all about my my now this is a story all about how life life now this is a story all about how my got got now this is a story all about how my life flipped flipped now this is a story all about how my life got turned turned now this is a story all about how my life got flipped upside upside now this is a story all about how my life got flipped turned down down now this is a story all about how my life got flipped turned upside Suppose we have a vocabulary of 100k words. How many weights are there in Elman’s network? ht = tanh(Uht−1 + Wxt) yt = V ht A B C D E F G down what is represented here? now this is a story all about how my life got flipped turned upside what is represented here? now this is a story all about how my life got flipped turned upside dow Finding structure in time Finding more structure in time Any downsides? down now this is a story all about how my life got flipped turned upside now this is a story all about how my life got flipped turned = upside “Vanishing” gradients now this is a story all about how my life got flipped turned upside = a1 a2 wn wn−1 w1 c(f(x), y) y x = dC dw1 / σ0(z1) ⇥w2 ⇥σ0(z2) ⇥w3 · · · ⇥wn ⇥σ0(zn) ⇥dC dan where ai = σ(zi) an f “Vanishing” gradients now this is a story all about how my life got flipped turned upside = a1 a2 wn wn−1 w1 c(f(x), y) y x = an σ(x) σ0(x) dC dw1 / wn ⇥σ0(z1) ⇥· · · ⇥σ0(zn) ⇥dC dan (or exploding) small change, big consequences f in an RNN One ﬁnal thing… no output words….. BPTT But, more typically… http://www.cs.toronto.edu/~ilya/rnn.html References Finding structure in time (Elman, 1990) Description and analysis of a recurrent neural network, inference of structure in unsegmented sequences Connectionist Temporal Classiﬁcation: Labelling Unsegmented Sequence Data with Recurrent Neural Networks (Graves et al, 2006) Scales Elman up to the ML age Recurrent neural network-based language model (Mikolov et al. 2010) Scale Graves up to running text Learning to understand phrases by embedding the dictionary (Hill et al. 2015) Learns to predict words from dictionary deﬁnitions "
64,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 7. Tensorﬂow Stephen Clark University of Cambridge and DeepMind Borrowing from https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf Deep Learning Packages Theano PyTorch http://pytorch.org/about/ Tensorﬂow TensorFlow, as the name indicates, is a framework to deﬁne and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. (https://www.tensorﬂow.org/programmers_guide/tensors) ""When Graham Bell invented the telephone, he saw a missed call from Jeff Dean."" Tensors Mathematically 0.012 0.564 1.334 -1.22 -0.01 0.5654 1.422 -1.22 0.011 0.534 -0.33 -1.22 0.012 0.564 1.334 -1.22 3rd order tensor Tensors are higher-order generalizations of matrices (multi-linear algebra) Matrices can be thought of as 2-D tables of numbers, or linear maps; likewise Tensors are N-D tables of numbers, or multilinear maps: f : V1 ⇥· · · ⇥Vn ! W Matrixﬂow? This ﬁlm is n’t great Tensors This ﬁlm is n’t great This ﬁlm is good PAD batch dimension [B,W,E] Tensors This ﬁlm is n’t great batch dimension [B,W,E] [B,E] 0 B B @ 5 76 9998 5 17 4 65 543 0 0 . . . 5 435 4 765 0 1 C C A Padded Input Numpy vs. Tensorﬂow • Tensorﬂow shares many features with numpy • Values returned from a TF fetch are numpy values • Common programming paradigm is: • compute values using the TF graph (e.g. training a model); • fetch particular values from the graph (e.g. value of the loss function); • and then perform further computation in numpy/python (e.g. for evaluation) Programming in Tensorﬂow Tensorﬂow uses a declarative programming paradigm, and builds a computation graph statically, with python and C++ APIs Dataﬂow Graphs TensorFlow uses a dataﬂow graph to represent your computation in terms of the dependencies between individual operations. This leads to a low-level programming model in which you ﬁrst deﬁne the dataﬂow graph, then create a TensorFlow session to run parts of the graph (https://www.tensorﬂow.org/programmers_guide/graphs) Linear Regression Example Taken from https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf Linear Regression Example Taken from https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf Linear Regression Example Taken from https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf Tensorboard • Great for graph visualization A Simple Tensorﬂow Graph A Simple Tensorﬂow Graph Tensorﬂow Sessions Tensorﬂow Sessions Evaluation in Sessions Types in a TF Program Types in a TF Program The Default Graph Variables in Tensorﬂow Initializing Variables Initializing Variables Initializing Variables Updating Variables Importing Data Importing Data Importing Data Linear Regression Example Linear Regression Example Linear Regression Example Linear Regression Example "
65,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 8. Long Short Term Memory Felix Hill DeepMind RNNs: A recap ht = tanh(Uht−1 + Wxt) yt = V ht A B C D E F G What is the forward computation like? What is the forward computation like? ht+1 = tanh(Uht + V xt+1) What is the forward computation like? ht+1 = tanh(Uht + V xt+1) “Vanishing” gradients now this is a story all about how my life got flipped turned upside = a1 a2 wn wn−1 w1 c(f(x), y) y x = dC dw1 / σ0(z1) ⇥w2 ⇥σ0(z2) ⇥w3 · · · ⇥wn ⇥σ0(zn) ⇥dC dan where ai = σ(zi) an f “Vanishing” gradients now this is a story all about how my life got flipped turned upside = a1 a2 wn wn−1 w1 c(f(x), y) y x = an σ(x) σ0(x) dC dw1 / wn ⇥σ0(z1) ⇥· · · ⇥σ0(zn) ⇥dC dan (or exploding) small change, big consequences f in an RNN Long Short Term Memory (LSTM) Hochreiter and Schmidhuber, 1997 LSTM RNN Elman, 1990 xt h(t-1) ht X Thanks to http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Be careful with the past! Memory “cell” Ct Stuﬀ you (might) want to get out h(t+1) ht: hidden state C(t+1) Thanks to http://colah.github.io/posts/2015-08-Understanding-LSTMs/ Sigmoid “gates” ot = σ(Wohht−1 + Woxxt + bo) ft = σ(Wfhht−1 + Wfxxt + bf) it = σ(Wihht−1 + Wixxt + bi) New stuff to consider ot = σ(Wohht−1 + Woxxt + bo) ft = σ(Wfhht−1 + Wfxxt + bf) it = σ(Wihht−1 + Wixxt + bi) ˜ Ct = tanh(Wchht−1 + Wcxxt + bc) Look familiar? Carefully update your cell!! ot = σ(Wohht−1 + Woxxt + bo) ft = σ(Wfhht−1 + Wfxxt + bf) it = σ(Wihht−1 + Wixxt + bi) ˜ Ct = tanh(Wchht−1 + Wcxxt + bc) Ct = Ct−1 ⇤ft + ˜ Ct ⇤it forget some stuﬀ think some new stuﬀ Use your cell (if you want!!) ot = σ(Wohht−1 + Woxxt + bo) ft = σ(Wfhht−1 + Wfxxt + bf) it = σ(Wihht−1 + Wixxt + bi) ˜ Ct = tanh(Wchht−1 + Wcxxt + bc) Ct = Ct−1 ⇤ft + ˜ Ct ⇤it ht = ot ⇤tanh(Ct) Why does it work? ot = σ(Wohht−1 + Woxxt + bo) ft = σ(Wfhht−1 + Wfxxt + bf) it = σ(Wihht−1 + Wixxt + bi) ˜ Ct = tanh(Wchht−1 + Wcxxt + bc) Ct = Ct−1 ⇤ft + ˜ Ct ⇤it ht = ot ⇤tanh(Ct) VS. ht = tanh(Uht−1 + Wxt) plays the “role” of weights but always in (0,1) (can’t explode!) no tanh. activation = id new info - not dependent on c(t-1) How many weights in an LSTM? 1: 100,000 words in my vocab 2: layers C_t have 500 units Can we go deeper? xt h(t-1) ht X yt Understanding recurrent nets…. Thanks to http://karpathy.github.io/2015/05/21/rnn-eﬀectiveness/ Understanding recurrent nets…. Thanks to http://karpathy.github.io/2015/05/21/rnn-eﬀectiveness/ Understanding recurrent nets…. Thanks to http://karpathy.github.io/2015/05/21/rnn-eﬀectiveness/ Understanding recurrent nets…. Thanks to http://karpathy.github.io/2015/05/21/rnn-eﬀectiveness/ Why is it called Long Short Term Memory? NLP for the LSTM generation “Read” the sentence both ways and concatenate h_t at each time step “LSTM-based deep learning models for non-factoid answer selection.” Ming Tan et al. 2015 References A problem “Learning long-term dependencies with gradient descent is difficult"". Bengio et al. 1994 A solution “Long short-term memory”. Hochreiter and Schmidhuber, 1997 A(nother) cottage industry “LSTM-based deep learning models for non-factoid answer selection.” Ming Tan et al. 2015 A very clear explanation of LSTMs http://colah.github.io/posts/2015-08-Understanding-LSTMs/ "
66,"Conditional Language Modeling Chris Dyer A language model assigns probabilities to sequences of words, . w = (w1, w2, . . . , w`) p(w) = p(w1) ⇥p(w2 | w1) ⇥p(w3 | w1, w2) ⇥· · · ⇥ p(w` | w1, . . . , w`−1) = |w| Y t=1 p(wt | w1, . . . , wt−1) It is convenient to decompose this probability using the chain rule, as follows: This reduces the language modeling problem to modeling the probability of the next word, given the history of preceding words. Unconditional LMs Evaluating unconditional LMs How good is our unconditional language model? 1. Held-out per-word cross entropy or perplexity Same as training criterion. How uncertain is the model at each time position, an average? 2. Task-based evaluation Use in a task-model that uses a language model in place of some other language model. Does it improve? ppl = b− 1 |w| P|w| i=1 logb p(wi|w<i) H = −1 |w| |w| X i=1 log2 p(wi | w<i) (units: bits per word) (units: uncertainty per word) History-based LMs p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . A common strategy is to make a Markov assumption, which is a conditional independence assumption. History-based LMs Markov: forget the distant past. Is this valid for language? No… Is it practical? Often! p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . A common strategy is to make a Markov assumption, which is a conditional independence assumption. History-based LMs Markov: forget the distant past. Is this valid for language? No… Is it practical? Often! p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . A common strategy is to make a Markov assumption, which is a conditional independence assumption. Why RNNs are great for language: no more Markov assumptions! History-based LMs with RNNs h2 h1 h0 h3 h4 softmax w1 w2 w3 w4 w4 w3 w2 w1 p(W5|w1,w2,w3,w4) z }| { vector (word embedding) observed context word random variable RNN hidden state vector, length=|vocab| History-based LMs with RNNs h2 h1 h0 h3 h4 softmax w1 w2 w3 w4 w4 w3 w2 w1 p(W5|w1,w2,w3,w4) z }| { vector (word embedding) observed context word random variable RNN hidden state vector, length=|vocab| softmax h the a and cat dog horse runs says walked walks walking pig Lisbon sardines … u = Wh + b pi = exp ui P j exp uj Distributions over words Each dimension corresponds to a word in a closed vocabulary, V. The pi’s form a distribution, i.e. pi > 0 8i, X i pi = 1 Bridle. (1990) Probabilistic interpretation of feedforward classiﬁcation… softmax h the a and cat dog horse runs says walked walks walking pig Lisbon sardines … u = Wh + b pi = exp ui P j exp uj Distributions over words p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . softmax h the a and cat dog horse runs says walked walks walking pig Lisbon sardines … u = Wh + b pi = exp ui P j exp uj Distributions over words istories are sequences of words… h p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . softmax h the a and cat dog horse runs says walked walks walking pig Lisbon sardines … u = Wh + b pi = exp ui P j exp uj h 2 Rd |V | = 100, 000 What are the dimensions of ? b Distributions over words istories are sequences of words… h p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . softmax h the a and cat dog horse runs says walked walks walking pig Lisbon sardines … u = Wh + b pi = exp ui P j exp uj h 2 Rd |V | = 100, 000 What are the dimensions of ? Distributions over words istories are sequences of words… h p(w) = p(w1)⇥ p(w2 | w1)⇥ p(w3 | w1, w2)⇥ p(w4 | w1, w2, w3)⇥ . . . W RNN language models h1 h0 x1 <s> RNN language models softmax ˆ p1 h1 h0 x1 <s> RNN language models softmax ˆ p1 h1 h0 x1 <s> ⇠ tom p(tom | hsi) RNN language models softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom p(tom | hsi) RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom p(tom | hsi) RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom p(tom | hsi) ⇠ likes ⇥p(likes | hsi, tom) RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom p(tom | hsi) ⇠ likes ⇥p(likes | hsi, tom) x3 h3 softmax ⇠ beer ⇥p(beer | hsi, tom, likes) RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom p(tom | hsi) ⇠ likes ⇥p(likes | hsi, tom) x3 h3 softmax ⇠ beer ⇥p(beer | hsi, tom, likes) x4 h4 softmax ⇠ </s> ⇥p(h/si | hsi, tom, likes, beer) RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 ⇠ tom ⇠ likes x3 h3 softmax ⇠ beer x4 h4 softmax ⇠ </s> Training RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 tom likes x3 h3 softmax beer x4 h4 softmax </s> cost1 cost2 cost3 cost4 Training RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 tom likes x3 h3 softmax beer x4 h4 softmax </s> cost1 cost2 cost3 cost4 { log loss/ cross entropy Training RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 tom likes x3 h3 softmax beer x4 h4 softmax </s> cost1 cost2 cost3 cost4 F { log loss/ cross entropy Training RNN language models h2 softmax softmax ˆ p1 h1 h0 x1 <s> x2 tom likes x3 h3 softmax beer x4 h4 softmax </s> cost1 cost2 cost3 cost4 F { log loss/ cross entropy Training RNN language models Training RNN language models The cross-entropy objective seeks the maximum likelihood (MLE) objective. “Find the parameters that make the training data most likely.” Training RNN language models The cross-entropy objective seeks the maximum likelihood (MLE) objective. “Find the parameters that make the training data most likely.” You will overﬁt. 1. Stop training early, based on a validation set 2. Weight decay / other regularizers 3. “Dropout” during training. In contrast to count-based models, zeroes aren’t a problem. • Unlike Markov (n-gram) models, RNNs never forget • However, they don’t always remember so well (recall Felix’s lectures on RNNs vs. LSTMs) • Algorithms • Sample a sequence from the probability distribution deﬁned by the RNN • Train the RNN to minimize cross entropy (aka MLE) • What about: what is the most probable sequence? RNN language models How well do RNN LMs do? perplexity Word Error Rate (WER) order=5 Markov Kneser-Ney freq. est. 221 13.5 RNN 400 hidden 171 12.5 3xRNN interpolation 151 11.6 Mikolov et al. (2010 Interspeech) “Recurrent neural network based language model” How well do RNN LMs do? perplexity Word Error Rate (WER) order=5 Markov Kneser-Ney freq. est. 221 13.5 RNN 400 hidden 171 12.5 3xRNN interpolation 151 11.6 Mikolov et al. (2010 Interspeech) “Recurrent neural network based language model” A conditional language model assigns probabilities to sequences of words, , given some conditioning context, . w = (w1, w2, . . . , w`) Conditional LMs p(w | x) = ` Y t=1 p(wt | x, w1, w2, . . . , wt−1) As with unconditional models, it is again helpful to use the chain rule to decompose this probability: What is the probability of the next word, given the history of previously generated words and conditioning context ? x x Conditional LMs “input” “text output” An author A document written by that author A topic label An article about that topic {SPAM, NOT_SPAM} An email A sentence in French Its English translation A sentence in English Its French translation A sentence in English Its Chinese translation An image A text description of the image A document Its summary A document Its translation Meterological measurements A weather report Acoustic signal Transcription of speech Conversational history + database Dialogue system response A question + a document Its answer A question + an image Its answer x w Conditional LMs “input” “text output” An author A document written by that author A topic label An article about that topic {SPAM, NOT_SPAM} An email A sentence in French Its English translation A sentence in English Its French translation A sentence in English Its Chinese translation An image A text description of the image A document Its summary A document Its translation Meterological measurements A weather report Acoustic signal Transcription of speech Conversational history + database Dialogue system response A question + a document Its answer A question + an image Its answer x w Conditional LMs “input” “text output” An author A document written by that author A topic label An article about that topic {SPAM, NOT_SPAM} An email A sentence in French Its English translation A sentence in English Its French translation A sentence in English Its Chinese translation An image A text description of the image A document Its summary A document Its translation Meterological measurements A weather report Acoustic signal Transcription of speech Conversational history + database Dialogue system response A question + a document Its answer A question + an image Its answer x w Data for training conditional LMs To train conditional language models, we need paired samples, . Data availability varies. It’s easy to think of tasks that could be solved by conditional language models, but the data just doesn’t exist. Relatively large amounts of data for: Translation, summarisation, caption generation, speech recognition {(xi, wi)}N i=1 Evaluating conditional LMs How good is our conditional language model? These are language models, we can use cross-entropy or perplexity. Task-speciﬁc evaluation. Compare the model’s most likely output to human-generated expected output using a task-speciﬁc evaluation metric . w⇤= arg max w p(w | x) L L(w⇤, wref) Examples of : BLEU, METEOR, WER, ROUGE. Human evaluation. okay to implement, hard to interpret easy to implement, okay to interpret hard to implement, easy to interpret L Evaluating conditional LMs How good is our conditional language model? These are language models, we can use cross-entropy or perplexity. Task-speciﬁc evaluation. Compare the model’s most likely output to human-generated expected output using a task-speciﬁc evaluation metric . w⇤= arg max w p(w | x) L L(w⇤, wref) Examples of : BLEU, METEOR, WER, ROUGE. Human evaluation. okay to implement, hard to interpret easy to implement, okay to interpret hard to implement, easy to interpret L Lecture overview The rest of this lecture will look at “encoder-decoder” models that learn a function that maps into a ﬁxed-size vector and then uses a language model to “decode” that vector into a sequence of words, . w x Kunst kann nicht gelehrt werden… Artistry can’t be taught… x w encoder decoder representation Lecture overview The rest of this lecture will look at “encoder-decoder” models that learn a function that maps into a ﬁxed-size vector and then uses a language model to “decode” that vector into a sequence of words, . w x A dog is playing on the beach. x w encoder decoder representation • Two questions • How do we encode as a ﬁxed-size vector, ? • How do we condition on in the decoding model? Lecture overview x c c - Problem (or at least modality) speciﬁc - Think about assumptions - Less problem speciﬁc - We will review one standard solution: RNNs Kalchbrenner and Blunsom 2013 c = embed(x) s = Vc Encoder Kalchbrenner and Blunsom 2013 c = embed(x) s = Vc Encoder Recurrent decoder Source sentence Embedding of wt−1 Recurrent connection Learnt bias ht = g(W[ht−1; wt−1] + s + b]) ut = Pht + b0 p(Wt | x, w<t) = softmax(ut) Kalchbrenner and Blunsom 2013 c = embed(x) s = Vc Encoder Recurrent decoder Source sentence Embedding of wt−1 Recurrent connection Recall unconditional RNN ht = g(W[ht−1; wt−1] + b]) Learnt bias ht = g(W[ht−1; wt−1] + s + b]) ut = Pht + b0 p(Wt | x, w<t) = softmax(ut) K&B 2013: Encoder How should we deﬁne ? c = embed(x) The simplest model possible: What do you think of this model? x1 x1 x2 x3 x4 x5 x6 x2 x3 x4 x5 x6 c = X i xi K&B 2013: RNN Decoder c = embed(x) s = Vc Encoder Recurrent decoder Source sentence Embedding of wt−1 Recurrent connection Recall unconditional RNN ht = g(W[ht−1; wt−1] + b]) Learnt bias ht = g(W[ht−1; wt−1] + s + b]) ut = Pht + b0 p(Wt | x, w<t) = softmax(ut) h1 h0 x1 <s> s K&B 2013: RNN Decoder softmax ˆ p1 h1 h0 x1 <s> s K&B 2013: RNN Decoder softmax ˆ p1 h1 h0 x1 <s> s ⇠ tom p(tom | s, hsi) K&B 2013: RNN Decoder softmax ˆ p1 h1 h0 x1 <s> s ⇠ tom p(tom | s, hsi) K&B 2013: RNN Decoder h2 softmax x2 ⇠ likes ⇥p(likes | s, hsi, tom) softmax ˆ p1 h1 h0 x1 <s> s ⇠ tom p(tom | s, hsi) K&B 2013: RNN Decoder h2 softmax x2 ⇠ likes ⇥p(likes | s, hsi, tom) x3 h3 softmax ⇠ beer ⇥p(beer | s, hsi, tom, likes) softmax ˆ p1 h1 h0 x1 <s> s ⇠ tom p(tom | s, hsi) K&B 2013: RNN Decoder h2 softmax x2 ⇠ likes ⇥p(likes | s, hsi, tom) x3 h3 softmax ⇠ beer ⇥p(beer | s, hsi, tom, likes) x4 h4 softmax ⇠ </s> ⇥p(h\si | s, hsi, tom, likes, beer) A word about decoding w⇤= arg max w p(w | x) = arg max w |w| X t=1 log p(wt | x, w<t) In general, we want to ﬁnd the most probable (MAP) output given the input, i.e. A word about decoding w⇤= arg max w p(w | x) = arg max w |w| X t=1 log p(wt | x, w<t) In general, we want to ﬁnd the most probable (MAP) output given the input, i.e. This is, for general RNNs, a hard problem. We therefore approximate it with a greedy search: undecidable :( w⇤ 1 ⇡arg max w1 p(w1 | x) w⇤ 2 ⇡arg max w2 p(w2 | x, w⇤ 1) . . . w⇤ t ⇡arg max wt p(wt | x, w⇤ <t) A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 x = Bier trinke ich beer drink I A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 x = Bier trinke ich beer drink I logprob=-2.11 logprob=-1.82 beer I A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 x = Bier trinke ich beer drink I logprob=-2.11 logprob=-1.82 beer I logprob=-6.93 logprob=-5.80 drink I A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 x = Bier trinke ich beer drink I logprob=-2.11 logprob=-1.82 beer I logprob=-6.93 logprob=-5.80 drink I logprob=-8.66 logprob=-2.87 drink beer A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 logprob=-2.11 x = Bier trinke ich beer drink I logprob=-1.82 beer logprob=-8.66 logprob=-2.87 logprob=-6.93 logprob=-5.80 I drink I drink beer A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 logprob=-2.11 x = Bier trinke ich beer drink I logprob=-1.82 beer logprob=-8.66 logprob=-2.87 logprob=-6.93 logprob=-5.80 logprob=-3.04 logprob=-5.12 logprob=-6.28 logprob=-7.31 I drink I drink beer beer wine drink like A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 logprob=-2.11 x = Bier trinke ich beer drink I logprob=-1.82 beer logprob=-8.66 logprob=-2.87 logprob=-6.93 logprob=-5.80 logprob=-3.04 logprob=-5.12 logprob=-6.28 logprob=-7.31 I drink I drink beer beer wine drink like A word about decoding A slightly better approximation is to use a beam search with beam size b. Key idea: keep track of top b hypothesis. E.g., for b=2: w0 w1 w2 w3 hsi logprob=0 logprob=-2.11 x = Bier trinke ich beer drink I logprob=-1.82 beer logprob=-8.66 logprob=-2.87 logprob=-6.93 logprob=-5.80 logprob=-3.04 logprob=-5.12 logprob=-6.28 logprob=-7.31 I drink I drink beer beer wine drink like How well does this model do? perplexity (2011) perplexity (2012) order=5 Markov Kneser-Ney freq. est. 222 225 RNN LM 178 181 RNN LM + x 140 142 How well does this model do? How well does this model do? How well does this model do? How well does this model do? How well does this model do? How well does this model do? How well does this model do? (Literal: I will feel bad if you do not ﬁnd a solution.) How well does this model do? (Literal: I will feel bad if you do not ﬁnd a solution.) Summary • Conditional language modeling provides a convenient formulation for a lot of practical applications • Two big problems: • Model expressivity • Decoding difﬁculties • Next time • A better encoder for vector to sequence models • “Attention” for better learning • Lots of results on machine translation Questions? "
67,"Better Conditional Language Modeling Chris Dyer A conditional language model assigns probabilities to sequences of words, , given some conditioning context, . w = (w1, w2, . . . , w`) Conditional LMs p(w | x) = ` Y t=1 p(wt | x, w1, w2, . . . , wt−1) As with unconditional models, it is again helpful to use the chain rule to decompose this probability: What is the probability of the next word, given the history of previously generated words and conditioning context ? x x Kalchbrenner and Blunsom 2013 c = embed(x) s = Vc Encoder Recurrent decoder Source sentence Embedding of wt−1 Recurrent connection Recall unconditional RNN ht = g(W[ht−1; wt−1] + b]) Learnt bias ht = g(W[ht−1; wt−1] + s + b]) ut = Pht + b0 p(Wt | x, w<t) = softmax(ut) softmax ˆ p1 h1 h0 x1 <s> s ⇠ tom p(tom | s, hsi) K&B 2013: RNN Decoder h2 softmax x2 ⇠ likes ⇥p(likes | s, hsi, tom) x3 h3 softmax ⇠ beer ⇥p(beer | s, hsi, tom, likes) x4 h4 softmax ⇠ </s> ⇥p(h\si | s, hsi, tom, likes, beer) K&B 2013: Encoder How should we deﬁne ? c = embed(x) The simplest model possible: x1 x1 x2 x3 x4 x5 x6 x2 x3 x4 x5 x6 c = X i xi • The bag of words assumption is really bad (part 1) Alice saw Bob. Bob saw Alice. I would like some fresh bread with aged cheese. I would like some aged bread with fresh cheese. • We are putting a lot of information inside a single vector (part 2) K&B 2013: Problems Sutskever et al. (2014) LSTM encoder LSTM decoder (c0, h0) are parameters The encoding is where . w0 = hsi (ci, hi) = LSTM(xi, ci−1, hi−1) (ct+`, ht+`) = LSTM(wt−1, ct+`−1, ht+`−1) (c`, h`) ` = |x| ut = Pht+` + b p(Wt | x, w<t) = softmax(ut) Aller Anfang ist schwer STOP START c Sutskever et al. (2014) Aller Anfang ist schwer STOP START Beginnings c Sutskever et al. (2014) Aller Anfang ist schwer are STOP START Beginnings c Sutskever et al. (2014) Aller Anfang ist schwer are STOP START difﬁcult Beginnings c Sutskever et al. (2014) Aller Anfang ist schwer are STOP START difﬁcult STOP Beginnings c Sutskever et al. (2014) Sutskever et al. (2014) • Good • RNNs deal naturally with sequences of various lengths • LSTMs in principle can propagate gradients a long distance • Very simple architecture! • Bad • The hidden state has to remember a lot of information! Aller Anfang ist schwer are STOP START difﬁcult STOP Beginnings c Sutskever et al. (2014): Tricks Aller Anfang ist schwer are STOP START difﬁcult STOP Beginnings c Sutskever et al. (2014): Tricks Read the input sequence “backwards”: +4 BLEU Sutskever et al. (2014): Tricks Ensemble of 2 models: +3 BLEU Decoder: u(j) t = Ph(j) t + b(j) ut = 1 J J X j0=1 u(j0) p(Wt | x, w<t) = softmax(ut) Ensemble of 5 models: +4.5 BLEU Use an ensemble of J independently trained models. (c(j) t+`, h(j) t+`) = LSTM(j)(wt−1, c(j) t+`−1, h(j) t+`−1) Sutskever et al. (2014): Tricks Use beam search: +1 BLEU hsi logprob=0 logprob=-2.11 x = Bier trinke ich beer drink I logprob=-1.82 beer logprob=-8.66 logprob=-2.87 logprob=-6.93 logprob=-5.80 logprob=-3.04 logprob=-5.12 logprob=-6.28 logprob=-7.31 I drink I drink beer beer wine drink like w0 w1 w2 w3 Sutskever et al. (2014): Tricks Use beam search: +1 BLEU Make the beam really big: -1 BLEU (Koehn and Knowles, 2017) Conditioning with vectors We are compressing a lot of information in a ﬁnite-sized vector. Conditioning with vectors We are compressing a lot of information in a ﬁnite-sized vector. “You can't cram the meaning of a whole %&!$# sentence into a single $&!#* vector!” Prof. Ray Mooney We are compressing a lot of information in a ﬁnite-sized vector. Gradients have a long way to travel. Even LSTMs forget! Conditioning with vectors We are compressing a lot of information in a ﬁnite-sized vector. Gradients have a long way to travel. Even LSTMs forget! Conditioning with vectors What is to be done? • Represent a source sentence as a matrix • Generate a target sentence from a matrix • This will • Solve the capacity problem • Solve the gradient ﬂow problem Solving the vector bottleneck • Problem with the ﬁxed-size vector model • Sentences are of different sizes but vectors are of the same size • Solution: use matrices instead • Fixed number of rows, but number of columns depends on the number of words • Usually |f| = #cols Sentences as vectors matrices Ich m¨ ochte ein Bier Sentences as matrices Ich m¨ ochte ein Bier Mach’s gut Sentences as matrices Ich m¨ ochte ein Bier Mach’s gut Die Wahrheiten der Menschen sind die unwiderlegbaren Irrt¨ umer Sentences as matrices Ich m¨ ochte ein Bier Mach’s gut Die Wahrheiten der Menschen sind die unwiderlegbaren Irrt¨ umer Question: How do we build these matrices? Sentences as matrices • Each word type is represented by an n-dimensional vector • Take all of the vectors for the sentence and concatenate them into a matrix • Simplest possible model • So simple, no one has bothered to publish how well/badly it works! Sentences as matrices With concatenation Ich möchte ein Bier x1 x2 x3 x4 fi = xi Ich möchte ein Bier x1 x2 x3 x4 Ich m¨ ochte ein Bier fi = xi F 2 Rn⇥|f| Ich möchte ein Bier x1 x2 x3 x4 • By far the most widely used matrix representation, due to Bahdanau et al (2015) • One column per word • Each column (word) has two halves concatenated together: • a “forward representation”, i.e., a word and its left context • a “reverse representation”, i.e., a word and its right context • Implementation: bidirectional RNNs (GRUs or LSTMs) to read f from left to right and right to left, concatenate representations Sentences as matrices With bidirectional RNNs Ich möchte ein Bier x1 x2 x3 x4 Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 fi = [ − h i; − ! h i] Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 fi = [ − h i; − ! h i] Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 fi = [ − h i; − ! h i] Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 fi = [ − h i; − ! h i] Ich möchte ein Bier x1 x2 x3 x4 − ! h 1 − ! h 2 − ! h 3 − ! h 4 − h 1 − h 2 − h 3 − h 4 Ich m¨ ochte ein Bier F 2 R2n⇥|f| fi = [ − h i; − ! h i] • There are lots of ways to construct F • More exotic architectures coming out daily • Increasingly common goal: get rid of O(|f|) sequential processing steps, i.e., RNNs during training • syntactic information can help (Sennrich & Haddow, 2016; Nadejde et al., 2017), but many more integration strategies are possible • try something with phrase types instead of word types? Multi-word expressions are a pain in the neck . Sentences as matrices Where are we in 2018? • We have a matrix F representing the input, now we need to generate from it • Bahdanau et al. (2015) and Luong et al. (2015) concurrently proposed using attention for translating from matrix-encoded sentences • High-level idea • Generate the output sentence word by word using an RNN • At each output position t, the RNN receives two inputs (in addition to any recurrent inputs) • a ﬁxed-size vector embedding of the previously generated output symbol et-1 • a ﬁxed-size vector encoding a “view” of the input matrix • How do we get a ﬁxed-size vector from a matrix that changes over time? • Bahdanau et al: do a weighted sum of the columns of F (i.e., words) based on how important they are at the current time step. (i.e., just a matrix-vector product Fat) • The weighting of the input columns at each time-step (at) is called attention Conditioning on matrices Recall RNNs…  → Recall RNNs…  → Recall RNNs… I'd → Recall RNNs… I'd → I'd Recall RNNs… I'd → like I'd Recall RNNs…  →  → Ich m¨ ochte ein Bier  → Ich m¨ ochte ein Bier  → Ich m¨ ochte ein Bier Attention history: a> 1 a> 2 a> 3 a> 4 a> 5  → Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 c1 = Fa1  → Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 c1 = Fa1 I'd → Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 c1 = Fa1 I'd → Ich m¨ ochte ein Bier Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → Ich m¨ ochte ein Bier Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd I'd → Ich m¨ ochte ein Bier Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 c2 = Fa2 I'd I'd → like Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 c2 = Fa2 I'd I'd → like like a Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → like like a a beer Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → like like a a beer beer stop STOP Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 • How do we know what to attend to at each time- step? • That is, how do we compute ? at Attention • At each time step (one time step = one output word), we want to be able to “attend” to different words in the source sentence • We need a weight for every column: this is an |f|-length vector at • Here is a simpliﬁed version of Bahdanau et al.’s solution • Use an RNN to predict model output, call the hidden states • At time t compute the expected input embedding • Take the dot product with every column in the source matrix to compute the attention energy. • Exponentiate and normalize to 1: • Finally, the input source vector for time t is at = softmax(ut) ct = Fat (called in the paper) (called in the paper) st ut = F>rt rt = Vst−1 ( is a learned parameter) V et ↵t (Since F has |f| columns, has |f| rows) ut (st has a ﬁxed dimensionality, call it m) Computing attention • At each time step (one time step = one output word), we want to be able to “attend” to different words in the source sentence • We need a weight for every column: this is an |f|-length vector at • Here is a simpliﬁed version of Bahdanau et al.’s solution • Use an RNN to predict model output, call the hidden states • At time t compute the query key embedding • Take the dot product with every column in the source matrix to compute the attention energy. • Exponentiate and normalize to 1: • Finally, the input source vector for time t is at = softmax(ut) ct = Fat (called in the paper) (called in the paper) st ut = F>rt rt = Vst−1 ( is a learned parameter) V et ↵t (Since F has |f| columns, has |f| rows) ut (st has a ﬁxed dimensionality, call it m) Computing attention • At each time step (one time step = one output word), we want to be able to “attend” to different words in the source sentence • We need a weight for every column: this is an |f|-length vector at • Here is a simpliﬁed version of Bahdanau et al.’s solution • Use an RNN to predict model output, call the hidden states • At time t compute the query key embedding • Take the dot product with every column in the source matrix to compute the attention energy. • Exponentiate and normalize to 1: • Finally, the input source vector for time t is at = softmax(ut) ct = Fat (called in the paper) (called in the paper) st ut = F>rt rt = Vst−1 ( is a learned parameter) V et ↵t (Since F has |f| columns, has |f| rows) ut (st has a ﬁxed dimensionality, call it m) Computing attention • At each time step (one time step = one output word), we want to be able to “attend” to different words in the source sentence • We need a weight for every column: this is an |f|-length vector at • Here is a simpliﬁed version of Bahdanau et al.’s solution • Use an RNN to predict model output, call the hidden states • At time t compute the query key embedding • Take the dot product with every column in the source matrix to compute the attention energy. • Exponentiate and normalize to 1: • Finally, the input source vector for time t is at = softmax(ut) ct = Fat (called in the paper) (called in the paper) st ut = F>rt rt = Vst−1 ( is a learned parameter) V et ↵t (Since F has |f| columns, has |f| rows) ut (st has a ﬁxed dimensionality, call it m) Computing attention • At each time step (one time step = one output word), we want to be able to “attend” to different words in the source sentence • We need a weight for every column: this is an |f|-length vector at • Here is a simpliﬁed version of Bahdanau et al.’s solution • Use an RNN to predict model output, call the hidden states • At time t compute the query key embedding • Take the dot product with every column in the source matrix to compute the attention energy. • Exponentiate and normalize to 1: • Finally, the input source vector for time t is at = softmax(ut) ct = Fat (called in the paper) (called in the paper) st ut = F>rt rt = Vst−1 ( is a learned parameter) V et ↵t (Since F has |f| columns, has |f| rows) ut (st has a ﬁxed dimensionality, call it m) Computing attention • In the actual model, Bahdanau et al. replace the dot product between the columns of F and rt with an MLP: • Here, W and v are learned parameters of appropriate dimension and + “broadcasts” over the |f| columns in WF • This can learn more complex interactions • It is unclear if the added complexity is necessary for good performance ut = F>rt ut = tanh (WF + rt) v (simple model) (Bahdanau et al) Computing attention • In the actual model, Bahdanau et al. replace the dot product between the columns of F and rt with an MLP: • Here, W and v are learned parameters of appropriate dimension and + “broadcasts” over the |f| columns in WF • This can learn more complex interactions • It is unclear if the added complexity is necessary for good performance ut = F>rt (simple model) (Bahdanau et al) ut = v> tanh(WF + rt) Nonlinear additive attention model Computing attention • In the actual model, Bahdanau et al. replace the dot product between the columns of F and rt with an MLP: • Here, W and v are learned parameters of appropriate dimension and + “broadcasts” over the |f| columns in WF • This can learn more complex interactions • It is unclear if the added complexity is necessary for good performance ut = F>rt (simple model) (Bahdanau et al) ut = v> tanh(WF + rt) Nonlinear additive attention model Computing attention e0 = hsi while et 6= h/si : F = EncodeAsMatrix(f) (Part 1 of lecture) rt = Vst−1 s0 = w (Learned initial state; Bahdanau uses ) U − h 1 at = softmax(ut) ct = Fat st = RNN(st−1, [et−1; ct]) yt = softmax(Pst + b) et | e<t ⇠Categorical(yt) }(Compute attention; part 2 of lecture) t = 0 t = t + 1 ( is a learned embedding of ) et−1 et ( and are learned parameters) P b ut = v> tanh(WF + rt) Putting it all together e0 = hsi while et 6= h/si : F = EncodeAsMatrix(f) (Part 1 of lecture) rt = Vst−1 s0 = w (Learned initial state; Bahdanau uses ) U − h 1 at = softmax(ut) ct = Fat st = RNN(st−1, [et−1; ct]) yt = softmax(Pst + b) et | e<t ⇠Categorical(yt) }(Compute attention; part 2 of lecture) t = 0 t = t + 1 ( is a learned embedding of ) et−1 et ( and are learned parameters) P b doesn’t depend on output decisions ut = v> tanh(WF + rt) Putting it all together e0 = hsi while et 6= h/si : F = EncodeAsMatrix(f) (Part 1 of lecture) rt = Vst−1 s0 = w (Learned initial state; Bahdanau uses ) U − h 1 at = softmax(ut) ct = Fat st = RNN(st−1, [et−1; ct]) yt = softmax(Pst + b) et | e<t ⇠Categorical(yt) }(Compute attention; part 2 of lecture) t = 0 t = t + 1 ( is a learned embedding of ) et−1 et ( and are learned parameters) P b X = WF X ut = v> tanh(WF + rt) Putting it all together e0 = hsi while et 6= h/si : F = EncodeAsMatrix(f) (Part 1 of lecture) rt = Vst−1 s0 = w (Learned initial state; Bahdanau uses ) U − h 1 at = softmax(ut) ct = Fat st = RNN(st−1, [et−1; ct]) yt = softmax(Pst + b) et | e<t ⇠Categorical(yt) }(Compute attention; part 2 of lecture) t = 0 t = t + 1 ( is a learned embedding of ) et−1 et ( and are learned parameters) P b X = WF ut = v> tanh(X + rt) Putting it all together Add attention to seq2seq translation: +11 BLEU Attention in MT: Results A word about gradients I'd I'd → like like a a beer Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → like like a a beer Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 I'd I'd → like like a a beer Ich m¨ ochte ein Bier z }| { Attention history: a> 1 a> 2 a> 3 a> 4 a> 5 • Cho’s question: does a translator read and memorize the input sentence/document and then generate the output? • Compressing the entire input sentence into a vector basically says “memorize the sentence” • Common sense experience says translators refer back and forth to the input. (also backed up by eye- tracking studies) • Should humans be a model for machines? Attention and translation • Attention • provides the ability to establish information ﬂow directly from distant • closely related to “pooling” operations in convnets (and other architectures) • Traditional attention model seems to only cares about “content” • No obvious bias in favor of diagonals, short jumps, fertility, etc. • Some work has begun to add other “structural” biases (Luong et al., 2015; Cohn et al., 2016), but there are lots more opportunities • Factorization into keys and values (Miller et al., 2016; Ba et al., 2016, Gulcehre et al., 2016) • Attention weights provide interpretation you can look at Summary Questions? "
68,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 11. Machine Comprehension Edward Grefenstette DeepMind & UCL What is machine comprehension? • Too general a question to answer? <end of lecture> • Being able to retrieve information from a text? • Being able detect the type of relation between sentences? • We’ll explore a few of these in this lecture. What do we care about in Natural Language Understanding? • Machines that can understand us (👈 machine comp.?) • Machines that we can understand How do we test this? Reading Comprehension (RC) • Candidate reads a document • Candidate sees a question about the document • Candidate must select/generate an answer • Span selection • Multiple Choice • Free form answers RC Example: MCTest (MSR) James the Turtle was always getting in trouble. Sometimes he’d reach into the freezer and empty out all the food. Other times he’d sled on the deck and get a splinter. His aunt Jane tried as hard as she could to keep him out of trouble, but he was sneaky and got into lots of trouble behind her back. One day, James thought he would go into town and see what kind of trouble he could get into. He went to the grocery store and pulled all the pudding oﬀ the shelves and ate two jars. Then he walked to the fast food restaurant and ordered 15 bags of fries. He didn’t pay, and instead headed home. Where did James go after he went to the grocery store? 1. his deck 2. his freezer 3. a fast food restaurant 4. his room RC Example: MCTest (MSR) What’s great about it? • Real data • Relatively hard questions (many distractors) • Some need for paraphrase, coreference resolution What could be better? • Little data (<3k questions from <1k articles) • Multiple choice vs. free form. RC Example: bAbI QA (FAIR) John picked up the apple. John went to the oﬃce. John went to the kitchen. John dropped the apple. Q: Where was the apple before the kitchen? A: oﬃce RC Example: bAbI QA (FAIR) What’s so great about it? • Lots of data (can generate more) • Many sub-tasks based on modes of reasoning What could be better? • Synthetic data (better as a unit test) • Very predictable structure • Lack of grammatical or lexical diversity RC Example: CNN/DailyMail😱 (DeepMind) The BBC producer allegedly struck by Jeremy Clarkson will not press charges against the “Top Gear” host, his lawyer said Friday. Clarkson, who hosted one of the most-watched television shows in the world, was dropped by the BBC Wednesday after an internal investigation by the British broadcaster found he had subjected producer Oisin Tymon “to an unprovoked physical and verbal attack.” … Q: Producer X will not press charges against Jeremy Clarkson, his lawyer says. A: Oisin Tymon RC Example: CNN/DailyMail😱 (DeepMind) What’s so great about it? • Lots of data (easy to generate more) • Natural text (Cloze-questions from abstractive summaries) What could be better? • Semi-synthetic (lack of variety, unanswerable questions) • Answers basically are just pointers to entities RC Example: SQuAD (Stanford) In 1271, Kublai Khan imposed the name Great Yuan (Chinese: ⼤大元; pinyin: Dà Yuán; Wade–Giles: Ta-Yüan), establishing the Yuan dynasty. ""Dà Yuán"" (⼤大元) is from the sentence ""⼤大哉乾元"" (dà zai Qián Yuán / ""Great is Qián, the Primal"") in the Commentaries on the Classic of Changes (I Ching) section regarding Qián (乾). The counterpart in Mongolian language was Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus. In Mongolian, Dai Ön (Great Yuan) is often used in conjunction with the ""Yeke Mongghul Ulus"" (lit. ""Great Mongol State""), resulting in Dai Ön Yeke Mongghul Ulus (Mongolian script: ), meaning ""Great Yuan Great Mongol State"". The Yuan dynasty is also known as the ""Mongol dynasty"" or ""Mongol Dynasty of China"", similar to the names ""Manchu dynasty"" or ""Manchu Dynasty of China"" for the Qing dynasty. Furthermore, the Yuan is sometimes known as the ""Empire of the Great Khan"" or ""Khanate of the Great Khan"", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan. Nevertheless, both terms can also refer to the khanate within the Mongol Empire directly ruled by Great Khans before the actual establishment of the Yuan dynasty by Kublai Khan in 1271. Q: What writing inspired the name Great Yuan? A (spans): the Commentaries on the Classic of Changes (I Ching) the Commentaries on the Classic of Changes Q: What was the Yuan dynasty called in Mongolian? A (spans): Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus Dai Ön Ulus RC Example: SQuAD (Stanford) In 1271, Kublai Khan imposed the name Great Yuan (Chinese: ⼤大元; pinyin: Dà Yuán; Wade–Giles: Ta-Yüan), establishing the Yuan dynasty. ""Dà Yuán"" (⼤大元) is from the sentence ""⼤大哉乾元"" (dà zai Qián Yuán / ""Great is Qián, the Primal"") in the Commentaries on the Classic of Changes (I Ching) section regarding Qián (乾). The counterpart in Mongolian language was Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus. In Mongolian, Dai Ön (Great Yuan) is often used in conjunction with the ""Yeke Mongghul Ulus"" (lit. ""Great Mongol State""), resulting in Dai Ön Yeke Mongghul Ulus (Mongolian script: ), meaning ""Great Yuan Great Mongol State"". The Yuan dynasty is also known as the ""Mongol dynasty"" or ""Mongol Dynasty of China"", similar to the names ""Manchu dynasty"" or ""Manchu Dynasty of China"" for the Qing dynasty. Furthermore, the Yuan is sometimes known as the ""Empire of the Great Khan"" or ""Khanate of the Great Khan"", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan. Nevertheless, both terms can also refer to the khanate within the Mongol Empire directly ruled by Great Khans before the actual establishment of the Yuan dynasty by Kublai Khan in 1271. Q: What writing inspired the name Great Yuan? A (spans): the Commentaries on the Classic of Changes (I Ching) the Commentaries on the Classic of Changes Q: What was the Yuan dynasty called in Mongolian? A (spans): Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus Dai Ön Ulus RC Example: SQuAD (Stanford) In 1271, Kublai Khan imposed the name Great Yuan (Chinese: ⼤大元; pinyin: Dà Yuán; Wade–Giles: Ta-Yüan), establishing the Yuan dynasty. ""Dà Yuán"" (⼤大元) is from the sentence ""⼤大哉乾元"" (dà zai Qián Yuán / ""Great is Qián, the Primal"") in the Commentaries on the Classic of Changes (I Ching) section regarding Qián (乾). The counterpart in Mongolian language was Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus. In Mongolian, Dai Ön (Great Yuan) is often used in conjunction with the ""Yeke Mongghul Ulus"" (lit. ""Great Mongol State""), resulting in Dai Ön Yeke Mongghul Ulus (Mongolian script: ), meaning ""Great Yuan Great Mongol State"". The Yuan dynasty is also known as the ""Mongol dynasty"" or ""Mongol Dynasty of China"", similar to the names ""Manchu dynasty"" or ""Manchu Dynasty of China"" for the Qing dynasty. Furthermore, the Yuan is sometimes known as the ""Empire of the Great Khan"" or ""Khanate of the Great Khan"", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan. Nevertheless, both terms can also refer to the khanate within the Mongol Empire directly ruled by Great Khans before the actual establishment of the Yuan dynasty by Kublai Khan in 1271. Q: What writing inspired the name Great Yuan? A (spans): the Commentaries on the Classic of Changes (I Ching) the Commentaries on the Classic of Changes Q: What was the Yuan dynasty called in Mongolian? A (spans): Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus Dai Ön Ulus RC Example: SQuAD (Stanford) In 1271, Kublai Khan imposed the name Great Yuan (Chinese: ⼤大元; pinyin: Dà Yuán; Wade–Giles: Ta-Yüan), establishing the Yuan dynasty. ""Dà Yuán"" (⼤大元) is from the sentence ""⼤大哉乾元"" (dà zai Qián Yuán / ""Great is Qián, the Primal"") in the Commentaries on the Classic of Changes (I Ching) section regarding Qián (乾). The counterpart in Mongolian language was Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus. In Mongolian, Dai Ön (Great Yuan) is often used in conjunction with the ""Yeke Mongghul Ulus"" (lit. ""Great Mongol State""), resulting in Dai Ön Yeke Mongghul Ulus (Mongolian script: ), meaning ""Great Yuan Great Mongol State"". The Yuan dynasty is also known as the ""Mongol dynasty"" or ""Mongol Dynasty of China"", similar to the names ""Manchu dynasty"" or ""Manchu Dynasty of China"" for the Qing dynasty. Furthermore, the Yuan is sometimes known as the ""Empire of the Great Khan"" or ""Khanate of the Great Khan"", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan. Nevertheless, both terms can also refer to the khanate within the Mongol Empire directly ruled by Great Khans before the actual establishment of the Yuan dynasty by Kublai Khan in 1271. Q: What writing inspired the name Great Yuan? A (spans): the Commentaries on the Classic of Changes (I Ching) the Commentaries on the Classic of Changes Q: What was the Yuan dynasty called in Mongolian? A (spans): Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus Dai Ön Ulus RC Example: SQuAD (Stanford) In 1271, Kublai Khan imposed the name Great Yuan (Chinese: ⼤大元; pinyin: Dà Yuán; Wade–Giles: Ta-Yüan), establishing the Yuan dynasty. ""Dà Yuán"" (⼤大元) is from the sentence ""⼤大哉乾元"" (dà zai Qián Yuán / ""Great is Qián, the Primal"") in the Commentaries on the Classic of Changes (I Ching) section regarding Qián (乾). The counterpart in Mongolian language was Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus. In Mongolian, Dai Ön (Great Yuan) is often used in conjunction with the ""Yeke Mongghul Ulus"" (lit. ""Great Mongol State""), resulting in Dai Ön Yeke Mongghul Ulus (Mongolian script: ), meaning ""Great Yuan Great Mongol State"". The Yuan dynasty is also known as the ""Mongol dynasty"" or ""Mongol Dynasty of China"", similar to the names ""Manchu dynasty"" or ""Manchu Dynasty of China"" for the Qing dynasty. Furthermore, the Yuan is sometimes known as the ""Empire of the Great Khan"" or ""Khanate of the Great Khan"", which particularly appeared on some Yuan maps, since Yuan emperors held the nominal title of Great Khan. Nevertheless, both terms can also refer to the khanate within the Mongol Empire directly ruled by Great Khans before the actual establishment of the Yuan dynasty by Kublai Khan in 1271. Q: What writing inspired the name Great Yuan? A (spans): the Commentaries on the Classic of Changes (I Ching) the Commentaries on the Classic of Changes Q: What was the Yuan dynasty called in Mongolian? A (spans): Dai Ön Ulus, also rendered as Ikh Yuan Üls or Yekhe Yuan Ulus Dai Ön Ulus RC Example: SQuAD (Stanford) What’s so great about it? • Lots of data • Well-recognised (or well-marketed) benchmark • Human-produced questions What could be better? • Answers are contiguous spans, limiting sort of question • Overﬁtting is a massive issue What’s missing in datasets? • Lots of natural, human-written, question/answer pairs. • Questions that require linking information across diﬀerent parts of the text (i.e. beyond simple anaphora resolution) • More diverse questions. Not just who/what/where, but: - “how?”, “in what manner …?”. - Temporal questions. - Questions about abstract relations, narrative structure. Some recent work • Some recent datasets, e.g. SearchQA (Dunn et al. 2017) or NarrativeQA (Kočiský et al. 2017) try to incorporate the need to search/aggregate information, answer high-level questions. • Ultimately, tension between making task feasible (often over-simplifying) and realistic (often too hard). Deep Learning Methods Multiple-Choice: 1. Embed question given document (or vice versa). 2. Embed answers. 3. Compute similarity. Rank. Select. Deep Learning Methods Entity answers: 1. Embed document (or document words). 2. Embed question (possibly conditioned on doc). 3. Softmax over entity markers / attention / pointer net Deep Learning Methods Spans: 1. Enumerate spans. There are O(Length2). Typically only consider spans within sentences. Optionally further restrict e.g. using constituency parse. 2. Embed spans. 3. Embed question. 4. Compute similarity. Rank. Select. Cf. models at https://rajpurkar.github.io/SQuAD-explorer/ Deep Learning Methods Incorporating search Latent boolean decision to search or answer. No gradient info on this decision, or on search terms. Must use gradient estimator, e.g. REINFORCE (Williams 1992) to compute gradients of loss w.r.t. discrete decisions. Free form answers from large documents Output is just a conditional language model. Diﬃculty usually lies elsewhere, but objective is a good question. Minimise NLL, or use other metrics e.g. BLEU/ROUGE/etc? Textual Entailment (SNLI) A man is crowd surﬁng at a concert • The man is at a football game Contradiction • The man is drunk Neutral • The man is at a concert Entailment A wedding party is taking pictures • There is a funeral Contradiction • They are outside Neutral • Someone got married Entailment Learning to detect entailment Learning to detect entailment Learning to detect entailment Learning to detect entailment Learning to detect entailment Conclusions • Machine Comprehension can be tested using the same sort of test we use on humans (reading comprehension, reasoning exercises, etc). • Tasks are reasonably well speciﬁed, but frequently imperfect, have simplifying assumptions, not enough data, diversity, etc. • Many ways of approaching each problem (cf. literature), but no winner- takes-all. • Unsupervised learning is missing: Should we be training on the same sort of task we are testing on? Do we learn to understand language by answering reading comprehension examinations? Reading Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of EMNLP. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of NIPS. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP. Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2017. The NarrativeQA Reading Comprehension Challenge. Upcoming in TACL. Sam Bowman, Gabor Angeli, Chris Potts, and Christopher Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of ACL. Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, and Phil Blunsom. (2015). Reasoning about entailment with neural attention. In Proceedings of ICLR. "
69,"Convolutional Neural Networks Felix Hill Deep Learning for NLP — Felix Hill Convolutions 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 3 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 3 4 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 3 4 2 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 3 4 2 4 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 4 3 4 2 4 3 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 1 0 1 0 1 0 1 0 1 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 1 0 1 0 1 0 1 0 1 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 4 3 4 2 4 3 2 3 5 Convolutions Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 4 3 4 2 4 3 2 3 5 Convolutions - filter # 2 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 4 3 4 2 4 3 2 3 5 3 Convolutions - filter # 2 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 Convolutions - filter # 2 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 What do filters 'do'? 1 0 1 0 1 0 1 0 1 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 What do filters 'do'? 1 0 1 0 1 0 1 0 1 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 Convolutions - filter # 3 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Convolutions - filter # 4 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Subsampling / Max Pooling 4 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Subsampling / Max Pooling 4 4 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Subsampling / Max Pooling 4 4 4 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Subsampling / Max Pooling 4 4 4 5 Deep Learning for NLP — Felix Hill (Total) max pooling 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Deep Learning for NLP — Felix Hill 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 Max pooling 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 Deep Learning for NLP — Felix Hill Fully connected 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 Fully-connected final layer cat dog table chair grass car shoe apple ball sky Deep Learning for NLP — Felix Hill Fully connected 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 Fully-connected final layer cat dog table chair grass car shoe apple ball sky Deep Learning for NLP — Felix Hill Fully connected 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 Fully-connected final layer cat dog table chair grass car shoe apple ball sky leg eye hand cross Deep Learning for NLP — Felix Hill Fully connected 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 cat dog table chair grass car shoe apple ball sky How many weights does this network have?? Deep Learning for NLP — Felix Hill Fully connected 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 5 5 10 11 4 3 4 2 4 3 2 3 5 3 5 3 2 4 5 2 4 3 10 1 1 1 4 2 3 2 1 1 5 0 2 4 2 2 11 3 cat dog table chair grass car shoe apple ball sky How many weights does this network have?? Max pooling Deep Learning for NLP — Felix Hill For natural image, Conv Nets are 8+ layers deep Deep Learning for NLP — Felix Hill From Zeiler & Fergus, 2013 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 filter image 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 filter 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? See e.g Collobert & Weston 2007, Kalchbrenner & Blunsom 2014 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 2.95 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 2.95 5.43 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.97 0.86 0.70 0.35 0.28 0.26 0.15 0.28 0.18 0.84 0.94 0.09 0.61 0.93 0.21 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.07 0.99 0.17 0.08 0.80 0.67 0.87 0.08 0.04 0.13 0.79 0.16 0.81 0.53 0.55 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 Deep Learning for NLP — Felix Hill Can we use a conv net for language? 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 0.07 0.99 0.17 0.08 0.80 0.67 0.87 0.08 0.04 0.13 0.79 0.16 0.81 0.53 0.55 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 0.28 0.64 4.30 3.66 2.71 4.90 2.55 0.30 0.80 Deep Learning for NLP — Felix Hill Five 'independent' analyses of the sentence 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 0.28 0.64 4.30 3.66 2.71 4.90 2.55 0.30 0.80 4.51 3.84 1.63 1.71 1.67 3.51 4.69 4.01 3.55 4.68 0.68 2.43 4.51 4.30 1.69 0.26 3.52 1.67 3.27 2.96 2.68 2.43 4.51 0.30 3.69 0.26 3.52 2.67 4.27 2.96 Deep Learning for NLP — Felix Hill Five 'independent' analyses of the sentence.... 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 0.28 0.64 4.30 3.66 2.71 4.90 2.55 0.30 0.80 4.51 3.84 1.63 1.71 1.67 3.51 4.69 4.01 3.55 4.68 0.68 2.43 4.51 4.30 1.69 0.26 3.52 1.67 3.27 2.96 2.68 2.43 4.51 0.30 3.69 0.26 3.52 2.67 4.27 2.96 start over again...? Deep Learning for NLP — Felix Hill Or, max-pooling over time 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 0.28 0.64 4.30 3.66 2.71 4.90 2.55 0.30 0.80 4.51 3.84 1.63 1.71 1.67 3.51 4.69 4.01 3.55 4.68 0.68 2.43 4.51 4.30 1.69 0.26 3.52 1.67 3.27 2.96 2.68 2.43 3.51 0.30 3.69 0.26 3.52 2.67 4.27 2.96 4.32 4.30 4.51 4.51 3.69 Deep Learning for NLP — Felix Hill Fill in the gaps... 3.41 2.71 4.32 3.21 2.81 2.95 5.43 3.34 2.22 1.96 0.87 0.28 0.64 4.30 3.66 2.71 4.90 2.55 0.30 0.80 4.51 3.84 1.63 1.71 1.67 3.51 4.69 4.01 3.55 4.68 0.68 2.43 4.51 4.30 1.69 0.26 3.52 1.67 3.27 2.96 2.68 2.43 3.51 0.30 3.69 0.26 3.52 2.67 4.27 2.96 4.32 4.30 4.51 4.51 3.69 Deep Learning for NLP — Felix Hill Max Pooling gives us a 'syntactic' analysis From Kalchbrenner & Blunsom, 2014 Deep Learning for NLP — Felix Hill Analyse the first-layer feature-maps 0.04 0.97 0.01 0.65 0.85 0.14 0.65 0.42 0.64 0.61 0.61 0.56 0.80 0.74 0.30 0.29 0.72 0.04 0.74 0.01 0.11 0.85 0.30 0.61 0.40 0.30 0.61 0.20 0.08 0.53 0.50 0.95 0.18 0.14 0.26 0.00 0.77 0.63 0.30 0.95 Can we use a convnet for language ? 1.00 0.59 0.85 0.10 0.70 1.00 0.56 0.07 0.18 0.11 0.18 0.05 0.30 0.73 0.93 0.90 0.53 0.25 0.66 0.58 0.03 0.27 0.64 0.51 0.71 0.98 0.75 0.74 0.58 0.28 0.26 0.58 0.78 0.34 0.82 From Kalchbrenner & Blunsom, 2014 Deep Learning for NLP — Felix Hill Kim (2014) - pre-trained word embs + convnet Deep Learning for NLP — Felix Hill Kim (2014) - pre-trained word embs + convnet 1: Learned embeddings 2: Google w2v embeddings 3: Google w2v initialise 1 and 2 Deep Learning for NLP — Felix Hill Kim (2014) - pre-trained word embs + convnet 1: Learned embeddings 2: Google w2v embeddings 3: Google w2v initialise 1 and 2 Deep Learning for NLP — Felix Hill Summary ● Convolutions and convnets have applications in language processing ● They can model hierarchical processing ('syntax'?) Food for thought ● They are faster to compute than RNNs (why?) ● They are most commonly used in character-level models (why?, how?) Deep Learning for NLP — Felix Hill References ● Visualising and understanding convolutional networks (Zeiler & Fergus, 2013) ● A convolutional neural network for modelling sentences (Kalchbrenner & Blunsom, 2014) ● Convolutional Neural Networks for Sentence Classification (Kim, 2014) "
7,"School of Computer Science Probabilistic Graphical Models Parameter Est. in fully observed BNs Eric Xing Lecture 7, February 5, 2014 Reading: KF-chap 17 X1 X4 X2 X3 X4 X2 X3 X1 X1 X2 X1 X3 X1 X4 X2 X3 X4 X2 X3 X1 X1 X2 X1 X3 © Eric Xing @ CMU, 2005-2014 1 Learning Graphical Models The goal: Given set of independent samples (assignments of random variables), find the best (the most likely?) Bayesian Network (both DAG and CPDs) (B,E,A,C,R)=(T,F,F,T,F) (B,E,A,C,R)=(T,F,T,T,F) …….. (B,E,A,C,R)=(F,T,T,T,F) E R B A C 0.9 0.1 e b e 0.2 0.8 0.01 0.99 0.9 0.1 b e b b e B E P(A | E,B) E R B A C Structural learning Parameter learning © Eric Xing @ CMU, 2005-2014 2 Learning Graphical Models  Scenarios:  completely observed GMs  directed  undirected  partially or unobserved GMs  directed  undirected (an open research topic)  Estimation principles:  Maximal likelihood estimation (MLE)  Bayesian estimation  Maximal conditional likelihood  Maximal ""Margin""  Maximum entropy  We use learning as a name for the process of estimating the parameters, and in some cases, the topology of the network, from data. © Eric Xing @ CMU, 2005-2014 3 ML Structural Learning for completely observed GMs E R B A C Data ) , , ( ) ( ) ( 1 1 1 n x x  ) , , ( ) ( ) ( M n M x x  1  ) , , ( ) ( ) ( 2 2 1 n x x  © Eric Xing @ CMU, 2005-2014 4 Two “Optimal” approaches “Optimal” here means the employed algorithms guarantee to return a structure that maximizes the objectives (e.g., LogLik)  Many heuristics used to be popular, but they provide no guarantee on attaining optimality, interpretability, or even do not have an explicit objective  E.g.: structured EM, Module network, greedy structural search, etc. We will learn two classes of algorithms for guaranteed structure learning, which are likely to be the only known methods enjoying such guarantee, but they only apply to certain families of graphs:  Trees: The Chow-Liu algorithm (this lecture)  Pairwise MRFs: covariance selection, neighborhood-selection (later) © Eric Xing @ CMU, 2005-2014 5 Structural Search How many graphs over n nodes? How many trees over n nodes? But it turns out that we can find exact solution of an optimal tree (under MLE)!  Trick: MLE score decomposable to edge-related elements  Trick: in a tree each node has only one parent!  Chow-liu algorithm ) ( 2 2n O ) ! (n O © Eric Xing @ CMU, 2005-2014 6 Information Theoretic Interpretation of ML                                          i x G i G i G i i x G i G i G i i n G i G n i n n i G i G n i n G G G i i i i i G i i i i i i i i i x p x p M x p M x count M x p x p G D p D G ) ( ) ( , ) ( | ) ( ) ( , ) ( | ) ( ) ( ) ( | ) ( , , ) ( | ) ( , , ) , | ( log ) , ( ˆ ) , | ( log ) , ( ) , | ( log ) , | ( log ) , | ( log ) ; , (                   x x x x x x x x l From sum over data points to sum over count of variable states © Eric Xing @ CMU, 2005-2014 7 Information Theoretic Interpretation of ML (con'd)                                                 i i i G i i x i i i x i G G i G i G i i x i i G G i G i G i i x G i G i G i G G x H M x I M x p x p M x p p x p x p M x p x p p x p x p M x p x p M G D p D G i i G i i i i i i G i i i i i i G i i i i i ) ( ˆ ) , ( ˆ ) ( ˆ log ) ( ˆ ) ( ˆ ) ( ˆ ) , , ( ˆ log ) , ( ˆ ) ( ˆ ) ( ˆ ) ( ˆ ) , , ( ˆ log ) , ( ˆ ) , | ( ˆ log ) , ( ˆ ) , | ( ˆ log ) ; , ( ) ( , ) ( ) ( | ) ( ) ( , ) ( ) ( | ) ( ) ( , ) ( | ) ( ) ( ) ( ) ( ) (                     x x x x x x x x x x x x l Decomposable score and a function of the graph structure © Eric Xing @ CMU, 2005-2014 8 Chow-Liu tree learning algorithm Objection function: Chow-Liu:  For each pair of variable xi and xj  Compute empirical distribution:  Compute mutual information:  Define a graph with node x1,…, xn  Edge (I,j) gets weight      i i i G i G G x H M x I M G D p D G i ) ( ˆ ) , ( ˆ ) , | ( ˆ log ) ; , ( ) (    x l   i G i i x I M G C ) , ( ˆ ) ( ) (  x  M x x count X X p j i j i ) , ( ) , ( ˆ    j i x x j i j i j i j i x p x p x x p x x p X X I , ) ( ˆ ) ( ˆ ) , ( ˆ log ) , ( ˆ ) , ( ˆ ) , ( ˆ j i X X I © Eric Xing @ CMU, 2005-2014 9 Chow-Liu algorithm (con'd) Objection function: Chow-Liu: Optimal tree BN  Compute maximum weight spanning tree  Direction in BN: pick any node as root, do breadth-first-search to define directions  I-equivalence:      i i i G i G G x H M x I M G D p D G i ) ( ˆ ) , ( ˆ ) , | ( ˆ log ) ; , ( ) (    x l   i G i i x I M G C ) , ( ˆ ) ( ) (  x  A B C D E C A E B D E C D A B ) , ( ) , ( ) , ( ) , ( ) ( E C I D C I C A I B A I G C     © Eric Xing @ CMU, 2005-2014 10 Structure Learning for general graphs Theorem:  The problem of learning a BN structure with at most d parents is NP-hard for any (fixed) d≥2 Most structure learning approaches use heuristics  Exploit score decomposition  Two heuristics that exploit decomposition in different ways  Greedy search through space of node-orders  Local search of graph structures © Eric Xing @ CMU, 2005-2014 11 ML Parameter Est. for completely observed GMs of given structure Z X The data: { (z1,x1), (z2,x2), (z3,x3), ... (zN,xN)} © Eric Xing @ CMU, 2005-2014 12 Parameter Learning Assume G is known and fixed,  from expert design  from an intermediate outcome of iterative structure learning Goal: estimate from a dataset of N independent, identically distributed (iid) training cases D = {x1, . . . , xN}. In general, each training case xn=(xn,1, . . . , xn,M) is a vector of M values, one per node,  the model can be completely observable, i.e., every element in xn is known (no missing values, no hidden variables),  or, partially observable, i.e., i, s.t. xn,i is not observed. In this lecture we consider learning parameters for a BN with given structure and is completely observable                    i n i n i n n i i n i n i i x p x p D p D ) , | ( log ) , | ( log ) | ( log ) ; ( , , , ,       x x l © Eric Xing @ CMU, 2005-2014 13 Review of density estimation Can be viewed as single-node graphical models Instances of exponential family dist. Building blocks of general GM MLE and Bayesian estimate x1 x2 x3 xN … xi N GM: © Eric Xing @ CMU, 2005-2014 14 Bernoulli distribution: Ber(p) Multinomial distribution: Mult(1,)  Multinomial (indicator) variable: . , w.p. and ], , [ where , ∑ ∑ [1,...,6] ∈ [1,...,6] ∈ 1 1 1 1 0 6 5 4 3 2 1                          j j j j j j j X X X X X X X X X X     x k x k x T x G x C x A j j k T G C A j X P j x p                 ∏ } face - dice index the where , { )) ( ( 1 Discrete Distributions        1 0 1 x p x p x P for for ) ( x x p p x P    1 1 ) ( ) (  © Eric Xing @ CMU, 2005-2014 15 Multinomial distribution: Mult(n,)  Count variable: Discrete Distributions              j j K N n n n n where ,  1 n K n K n n K n n n N n n n N n p K     ! ! ! ! ! ! ! ! ) (    2 1 2 1 2 1 2 1   © Eric Xing @ CMU, 2005-2014 16 Example: multinomial model Data:  We observed N iid die rolls (K-sided): D={5, 1, K, …, 3} Representation: Unit basis vectors: Model: How to write the likelihood of a single observation xn? The likelihood of datasetD={x1, …,xN}:         K n n n x K x x k k n i n k x P x P , 2 , 1 , 2 1 , }) roll th the of side - die index the where , 1 ({ ) (                    N n k x k N n n N k n x P x x x P 1 1 2 1 , ) | ( ) | ,..., , (    1 and }, 1 , 0 { where , 1 , , , 2 , 1 ,                    K k k n k n K n n n n x x x x x x  1 and , w.p. 1 } 1 { ,     ,...K k k k k n X     K k x k k n 1 ,        k n k k x k k N n k n   1 , x1 x2 x3 xN … xi N GM: © Eric Xing @ CMU, 2005-2014 17 MLE: constrained optimization with Lagrange multipliers Objective function: We need to maximize this subject to the constrain Constrained cost function with a Lagrange multiplier Take derivatives wrt k Sufficient statistics  The counts, are sufficient statistics of data D      k k k k n k n D P D k     log log ) | ( log ) ; ( l 1 1    K k k               K k k k k k n 1 1    log l                    k k k k k k k k k N n n n 0 l N nk MLE k  ,     n k n MLE k x N , , 1   or Frequency as sample mean , ), , , ( , 1    n k n k K x n n n n   © Eric Xing @ CMU, 2005-2014 18 Bayesian estimation: Dirichlet distribution: Posterior distribution of :  Notice the isomorphism of the posterior to the prior,  such a prior is called a conjugate prior Posterior mean estimation:         k k k k k k k k k k C P 1 - 1 - ) ( ) ( ) ( ) (                 k n k k k k n k N N N k k k k x x p p x x p x x P 1 1 1 1 1 ) ,..., ( ) ( ) | ,..., ( ) ,..., | (                            N n d C d D p k k k n k k k k k k 1 ) | ( x1 x2 x3 xN … GM:  N  xi Dirichlet parameters can be understood as pseudo-counts © Eric Xing @ CMU, 2005-2014 19 More on Dirichlet Prior: Where is the normalize constant C() come from?  Integration by parts  () is the gamma function:  For inregers, Marginal likelihood: Posterior in closed-form: Posterior predictive rate:            k k k k K K d d C K          ) ( ) (    1 1 1 1 1 1       0 1 dt e t t  ) ( ! ) ( n n    1 ) ( ) ( ) | ( ) | ( ) | ( ) | } ,..., ({                         n C C d p n p n p x x p N 1       k n k N k k n C n p p n p x x P 1 1 ) ( ) | ( ) | ( ) | ( ) }, ,..., { | (                  ) ( Dir     n ) ( ) ( ) ( ) }, ,..., { | ( 1 1 1 N n i k n k N N x n C n C d n C x x i x p k k k k                                | | | |       n n i i © Eric Xing @ CMU, 2005-2014 20 Sequential Bayesian updating Start with Dirichlet prior Observe N ' samples with sufficient statistics . Posterior becomes: Observe another N "" samples with sufficient statistics . Posterior becomes: So sequentially absorbing data in any order is equivalent to batch update. ) : ( Dir ) | (          P ' n  ) ' : ( Dir ) ' , | ( n n P             "" n  ) "" ' : ( Dir ) "" , ' , | ( n n n n P                © Eric Xing @ CMU, 2005-2014 21 Hierarchical Bayesian Models are the parameters for the likelihood p(x|) are the parameters for the prior p(|) . We can have hyper-hyper-parameters, etc. We stop when the choice of hyper-parameters makes no difference to the marginal likelihood; typically make hyper- parameters constants. Where do we get the prior?  Intelligent guesses  Empirical Bayes (Type-II maximum likelihood) computing point estimates of : ) | ( max arg         n p MLE   © Eric Xing @ CMU, 2005-2014 22 Limitation of Dirichlet Prior:  N  xi © Eric Xing @ CMU, 2005-2014 23 - Log Partition Function - Normalization Constant      log log exp , ~ , ~                                1 1 1 1 1 1 1 0 K i K i i i K K K i i e C e N LN           μ Σ N  xi The Logistic Normal Prior Pro: co-variance structure Con: non-conjugate (we will discuss how to solve this later) © Eric Xing @ CMU, 2005-2014 24 Logistic Normal Densities © Eric Xing @ CMU, 2005-2014 25 Uniform Probability Density Function Normal (Gaussian) Probability Density Function  The distribution is symmetric, and is often illustrated as a bell-shaped curve.  Two parameters, (mean) and (standard deviation), determine the location and shape of the distribution.  The highest point on the normal curve is at the mean, which is also the median and mode.  The mean can be any numerical value: negative, zero, or positive. Multivariate Gaussian Continuous Distributions elsewhere for ) /( ) ( 0 1      b x a a b x p 2 2 2 2 1     / ) ( ) (    x e x p   x x f f( (x x) )   x x f f( (x x) )                            X X X p T n 1 2 1 2 2 1 2 1 exp ) , ; ( / / © Eric Xing @ CMU, 2005-2014 26 MLE for a multivariate-Gaussian It can be shown that the MLE for µ and Σ is where the scatter matrix is  The sufficient statistics are nxn and nxnxn T.  Note that XTX=nxnxn T may not be full rank (eg. if N <D), in which case ΣML is not invertible      S N x x N x N n T ML n ML n MLE n n MLE 1 1 1                 T ML ML n n T n n n T ML n ML n N x x x x S                           K n n n n x x x x , 2 , 1 ,                                   T N T T x x x X  2 1 © Eric Xing @ CMU, 2005-2014 27 Bayesian parameter estimation for a Gaussian There are various reasons to pursue a Bayesian approach  We would like to update our estimates sequentially over time.  We may have prior knowledge about the expected magnitude of the parameters.  The MLE for Σ may not be full rank if we don’t have enough data. We will restrict our attention to conjugate priors. We will consider various cases, in order of increasing complexity:  Known σ, unknown µ  Known µ, unknown σ  Unknown µ and σ © Eric Xing @ CMU, 2005-2014 28 Bayesian estimation: unknown µ, known σ Normal Prior: Joint probability: Posterior:     2 2 0 2 1 2 2 2      / ) ( exp ) ( /     P 1 2 2 2 0 2 2 2 2 2 2 1 1 1 1                         N N x N N ~ and , / / / / / / ~ where x1 x2 x3 xN … GM:  N  xi Sample mean         2 2 0 2 1 2 1 2 2 2 2 2 2 2 1 2         / ) ( exp exp ) , ( / /                 N n n N x x P     2 2 2 1 2 2 2       ~ / ) ~ ( exp ~ ) | ( /     x P © Eric Xing @ CMU, 2005-2014 29 Bayesian estimation: unknown µ, known σ  The posterior mean is a convex combination of the prior and the MLE, with weights proportional to the relative noise levels.  The precision of the posterior 1/σ2 N is the precision of the prior 1/σ2 0 plus one contribution of data precision 1/σ2 for each observed data point.  Sequentially updating the mean  µ∗= 0.8 (unknown), (σ2)∗= 0.1 (known)  Effect of single data point  Uninformative (vague/ flat) prior, σ2 0 →∞ 1 2 0 2 2 0 2 0 2 2 0 2 0 2 2 1 1 1 1                           N N x N N N ~ , / / / / / / 2 0 2 2 0 0 2 0 2 2 0 0 0 1                   ) ( ) ( x x x 0    N © Eric Xing @ CMU, 2005-2014 30 Other scenarios  Known µ, unknown λ = 1/σ2  The conjugate prior for λ is a Gamma with shape a0 and rate (inverse scale) b0  The conjugate prior for σ2 is Inverse-Gamma  Unknown µ and unknown σ2  The conjugate prior is Normal-Inverse-Gamma  Semi conjugate prior  Multivariate case:  The conjugate prior is Normal-Inverse-Wishart © Eric Xing @ CMU, 2005-2014 31 Estimation of conditional density Can be viewed as two-node graphical models Instances of GLIM Building blocks of general GM MLE and Bayesian estimate See supplementary slides Q X Q X © Eric Xing @ CMU, 2005-2014 32 MLE for general BNs If we assume the parameters for each CPD are globally independent, and all nodes are fully observed, then the log- likelihood function decomposes into a sum of local terms, one per node:                    i n i n i n n i i n i n i i x p x p D p D ) , | ( log ) , | ( log ) | ( log ) ; ( , , , ,       x x l X2=1 X2=0 X5=0 X5=1 © Eric Xing @ CMU, 2005-2014 33 A plate is a “macro” that allows subgraphs to be replicated  For iid (exchangeable) data, the likelihood is  We can represent this as a Bayes net with N nodes.  The rules of plates are simple: repeat every structure in a box a number of times given by the integer in the corner of the box (e.g. N), updating the plate index variable (e.g. n) as you go.  Duplicate every arrow going into the plate and every arrow leaving the plate by connecting the arrows to each copy of the structure. X1 X2 XN  …  Xn N  Plates   n n x p D p ) | ( ) | (   © Eric Xing @ CMU, 2005-2014 34 Consider the distribution defined by the directed acyclic GM: This is exactly like learning four separate small BNs, each of which consists of a node and its parents. Decomposable likelihood of a BN ) , , | ( ) , | ( ) , | ( ) | ( ) | ( 4 3 2 4 3 1 3 2 1 2 1 1      x x x p x x p x x p x p x p  X1 X4 X2 X3 X4 X2 X3 X1 X1 X2 X1 X3 © Eric Xing @ CMU, 2005-2014 35 MLE for BNs with tabular CPDs  Assume each CPD is represented as a table (multinomial) where  Note that in case of multiple parents, will have a composite state, and the CPD will be a high-dimensional table  The sufficient statistics are counts of family configurations  The log-likelihood is  Using a Lagrange multiplier to enforce , we get: ) | ( def k X j X p i i ijk      i  X   n k n j i n ijk i x x n  , , def     k j i ijk ijk k j i n ijk n D ijk , , , , log log ) ; (    l 1  j ijk    ' ' j k ij ijk ML ijk n n  © Eric Xing @ CMU, 2005-2014 36 Earthquake Radio Burglary Alarm Call     M i i i x p p 1 ) | ( ) (  x x X Factorization: j i k i i x j k i x p    x x | ) | (  Local Distributions defined by, e.g., multinomial parameters: How to define parameter prior? Assumptions (Geiger & Heckerman 97,99):  Complete Model Equivalence  Global Parameter Independence  Local Parameter Independence  Likelihood and Prior Modularity ? ) | ( G p  © Eric Xing @ CMU, 2005-2014 37 Global Parameter Independence For every DAG model: Local Parameter Independence For every node:    M i i m G p G p 1 ) | ( ) | (   Earthquake Radio Burglary Alarm Call    i j i k i q j x i G p G p 1 | ) | ( ) | (    x independent of ) ( | YES Alarm Call P   ) ( | NO Alarm Call P   Global & Local Parameter Independence © Eric Xing @ CMU, 2005-2014 38 Provided all variables are observed in all cases, we can perform Bayesian update each parameter independently !!! sample 1 sample 2  2|1 1 2|1 X1 X2 X1 X2 Global Parameter Independence Local Parameter Independence Parameter Independence, Graphical View © Eric Xing @ CMU, 2005-2014 39 Which PDFs Satisfy Our Assumptions? (Geiger & Heckerman 97,99) Discrete DAG Models: Dirichlet prior: Gaussian DAG Models: Normal prior: Normal-Wishart prior:         k k k k k k k k k k C P 1 - 1 - ) ( ) ( ) ( ) (         ) ( Multi ~ |   j x i i x ) , ( Normal ~ |    j x i i x               ) ( )' ( exp | | ) ( ) , | ( / /        1 2 1 2 2 1 2 1 n p   . where , W tr exp W ) , ( ) , | W ( / ) ( / 1 2 1 2 2 1                W n w w w w n c p      , ) ( , Normal ) , , | ( 1   W W        p © Eric Xing @ CMU, 2005-2014 40  Consider a time-invariant (stationary) 1st-order Markov model  Initial state probability vector:  State transition probability matrix:  The joint:  The log-likelihood:  Again, we optimize each parameter separately  is a multinomial frequency vector, and we've seen it before  What about A? Parameter sharing X1 X2 X3 XT … A  ) ( def 1 1   k k X p  ) | ( def 1 1 1     i t j t ij X X p A      T t t t t T X X p x p X p 2 2 1 1 1 ) | ( ) | ( ) | ( :         n T t t n t n n n A x x p x p D 2 1 1 ) , | ( log ) | ( log ) ; ( , , ,   l © Eric Xing @ CMU, 2005-2014 41 Learning a Markov chain transition matrix  A is a stochastic matrix:  Each row of A is multinomial distribution.  So MLE of Aij is the fraction of transitions from i to j  Application:  if the states Xt represent words, this is called a bigram language model  Sparse data problem:  If i j did not occur in data, we will have Aij =0, then any future sequence with word pair i j will have zero probability.  A standard hack: backoff smoothing or deleted interpolation 1  j ij A            n T t i t n j t n n T t i t n ML ij x x x i j i A 2 1 2 1 , , , ) ( # ) ( # ML i t i A A        ) ( ~   1 © Eric Xing @ CMU, 2005-2014 42 Bayesian language model  Global and local parameter independence  The posterior of Ai ∙and Ai' ∙is factorized despite v-structure on Xt, because Xt- 1 acts like a multiplexer  Assign a Dirichlet prior i to each row of the transition matrix:  We could consider more realistic priors, e.g., mixtures of Dirichlets to account for types of words (adjectives, verbs, etc.) X1 X2 X3 XT … Ai ·  Ai'·   … A ) ( # where , ) ( ) ( # ) ( # ) , , | ( ' , , def               i A i j i D i j p A i i i ML ij i k i i i k i i Bayes ij          1 © Eric Xing @ CMU, 2005-2014 43 Example: HMM: two scenarios  Supervised learning: estimation when the “right answer” is known  Examples: GIVEN: a genomic region x = x1…x1,000,000 where we have good (experimental) annotations of the CpG islands GIVEN: the casino player allows us to observe him one evening, as he changes dice and produces 10,000 rolls  Unsupervised learning: estimation when the “right answer” is unknown  Examples: GIVEN: the porcupine genome; we don’t know how frequent are the CpG islands there, neither do we know their composition GIVEN: 10,000 rolls of the casino player, but we don’t see when he changes dice  QUESTION: Update the parameters of the model to maximize P(x|) - -- Maximal likelihood (ML) estimation © Eric Xing @ CMU, 2005-2014 44 Recall definition of HMM  Transition probabilities between any two states or  Start probabilities  Emission probabilities associated with each state or in general: A A A A x2 x3 x1 xT y2 y3 y1 yT ... ... , ) | ( , j i i t j t a y y p     1 1 1   . , , , , l Multinomia ~ ) | ( , , , I     i a a a y y p M i i i i t t  2 1 1 1  . , , , l Multinomia ~ ) ( M y p     2 1 1   . , , , , l Multinomia ~ ) | ( , , , I    i b b b y x p K i i i i t t  2 1 1   . , | f ~ ) | ( I     i y x p i i t t  1 © Eric Xing @ CMU, 2005-2014 45 Supervised ML estimation  Given x = x1…xN for which the true state path y = y1…yN is known,  Define: Aij = # times state transition ij occurs in y Bik = # times state i in y emits k in x  We can show that the maximum likelihood parameters are:  What if x is continuous? We can treat as NT observations of, e.g., a Gaussian, and apply learning rules for Gaussian …              ' ' , , , ) ( # ) ( # j ij ij n T t i t n j t n n T t i t n ML ij A A y y y i j i a 2 1 2 1            ' ' , , , ) ( # ) ( # k ik ik n T t i t n k t n n T t i t n ML ik B B y x y i k i b 1 1     N n T t y x t n t n : , : : , , , 1 1   © Eric Xing @ CMU, 2005-2014 46 Supervised ML estimation, ctd.  Intuition:  When we know the underlying states, the best estimate of is the average frequency of transitions & emissions that occur in the training data  Drawback:  Given little data, there may be overfitting:  P(x|) is maximized, but is unreasonable 0 probabilities – VERY BAD  Example:  Given 10 casino rolls, we observe x = 2, 1, 5, 6, 1, 2, 3, 6, 2, 3 y = F, F, F, F, F, F, F, F, F, F  Then: aFF = 1; aFL = 0 bF1 = bF3 = .2; bF2 = .3; bF4 = 0; bF5 = bF6 = .1 © Eric Xing @ CMU, 2005-2014 47 Pseudocounts  Solution for small training sets:  Add pseudocounts Aij = # times state transition ij occurs in y + Rij Bik = # times state i in y emits k in x + Sik  Rij, Sij are pseudocounts representing our prior belief  Total pseudocounts: Ri = jRij , Si = kSik ,  --- ""strength"" of prior belief,  --- total number of imaginary instances in the prior  Larger total pseudocounts strong prior belief  Small total pseudocounts: just to avoid 0 probabilities --- smoothing  This is equivalent to Bayesian est. under a uniform prior with ""parameter strength"" equals to the pseudocounts © Eric Xing @ CMU, 2005-2014 48 Summary: Learning GM For fully observed BN, the log-likelihood function decomposes into a sum of local terms, one per node; thus learning is also factored  Structural learning  Chow liu  Neighborhood selection  Learning single-node GM – density estimation: exponential family dist.  Typical discrete distribution  Typical continuous distribution  Conjugate priors  Learning two-node BN: GLIM  Conditional Density Est.  Classification  Learning BN with more nodes  Local operations © Eric Xing @ CMU, 2005-2014 49 Supplemental review: © Eric Xing @ CMU, 2005-2014 50 Classification Generative and discriminative approaches Q X Q X Linear/Logistic Regression Two node fully observed BNs Conditional mixtures © Eric Xing @ CMU, 2005-2014 51 Classification: Goal: Wish to learn f: X Y Generative:  Modeling the joint distribution of all data Discriminative:  Modeling only points at the boundary © Eric Xing @ CMU, 2005-2014 52 Conditional Gaussian The data: Both nodes are observed:  Y is a class indicator vector  X is a conditional Gaussian variable with a class-specific mean GM: Yi Xi N    k y k n n k n y y p , ) : ( multi ) (     2 2 1 2 / 1 2 , ) - ( - exp ) 2 ( 1 ) , , 1 | ( 2 k n k n n x y x p                  n k y k n k n x N y x p , ) , : ( ) , , | (       ) , ( , ), , ( ), , ( ), , ( N N y x y x y x y x  3 3 2 2 1 1 © Eric Xing @ CMU, 2005-2014 53 Data log-likelihood MLE ) , , | ( ) | ( log ) , ( log ) ; (    n n n n n n n y x p y p y x p D     θ l ), ; ( max , , , N n N y D rg a k n k n MLE k MLE k          θ l the fraction of samples of class m k n n k n n k n n n k n MLE k MLE k n x y y x y D       , , , , , ), ; ( max arg     θ l the average of samples of class m MLE of conditional Gaussian GM: Yi Xi N © Eric Xing @ CMU, 2005-2014 54 Prior: Posterior mean (Bayesian est.) Bsyesian estimation of conditional Gaussian GM: Yi Xi N   ) : ( Dir ) | (          P ) , : ( Normal ) | (      k k P  K 1 2 2 2 2 2 2 2 2 2 1 1 1 1                          N n n n Bayes k ML k k k Bayes k and , / / / / / / , ,                  N n N N N k k k ML k Bayes k , ,  © Eric Xing @ CMU, 2005-2014 55 Classification Gaussian Discriminative Analysis:  The joint probability of a datum and it label is:  Given a datum xn, we predict its label using the conditional probability of the label given the datum: This is basic inference  introduce evidence, and then normalize   2 2 1 2 / 1 2 , , , ) - ( - exp ) 2 ( 1 ) , , 1 | ( ) 1 ( ) , | 1 , ( 2 k n k k n n k n k n n x y x p y p y x p                      ' 2 ' 2 1 2 / 1 2 ' 2 2 1 2 / 1 2 , ) - ( - exp ) 2 ( 1 ) - ( - exp ) 2 ( 1 ) , , | 1 ( 2 2 k k n k k n k n k n x x x y p           Yi Xi © Eric Xing @ CMU, 2005-2014 56 Transductive classification  Given Xn, what is its corresponding Yn when we know the answer for a set of training data?  Frequentist prediction:  we fit , and from data first, and then …  Bayesian:  we compute the posterior dist. of the parameters first … GM: Yi Xi N   K Yn Xn      ' ' ' , , ) , | , ( ) , | , ( ) , , | ( ) , , | , 1 ( ) , , , | 1 ( k k n k k n k n n k n n k n x N x N x p x y p x y p                © Eric Xing @ CMU, 2005-2014 57 Linear Regression  The data:  Both nodes are observed:  X is an input vector  Y is a response vector (we first consider y as a generic continuous response vector, then we consider the special case of classification where y is a discrete indicator)  A regression scheme can be used to model p(y|x) directly, rather than p(x,y) Yi Xi N   ) , ( , ), , ( ), , ( ), , ( N N y x y x y x y x  3 3 2 2 1 1 © Eric Xing @ CMU, 2005-2014 58 A discriminative probabilistic model Let us assume that the target variable and the inputs are related by the equation: where ε is an error term of unmodeled effects or random noise Now assume that ε follows a Gaussian N(0,σ), then we have: By independence assumption: i i T i y     x            2 2 2 2 1      ) ( exp ) ; | ( i T i i i y x y p x                       2 1 2 1 2 2 1       n i i T i n n i i i y x y p L ) ( exp ) ; | ( ) ( x © Eric Xing @ CMU, 2005-2014 59 Linear regression Hence the log-likelihood is: Do you recognize the last term? Yes it is: It is same as the MSE!     n i i T i y n l 1 2 2 2 1 1 2 1 ) ( log ) ( x          n i i T i y J 1 2 2 1 ) ( ) (   x © Eric Xing @ CMU, 2005-2014 60 A recap:  LMS update rule  Pros: on-line, low per-step cost  Cons: coordinate, maybe slow-converging  Steepest descent  Pros: fast-converging, easy to implement  Cons: a batch,  Normal equations  Pros: a single-shot algorithm! Easiest to implement.  Cons: need to compute pseudo-inverse (XTX)-1, expensive, numerical issues (e.g., matrix is singular ..) n t T n n t t y x x ) ( 1               n i n t T n n t t y 1 1 x x ) (       y X X X T T  1   *  © Eric Xing @ CMU, 2005-2014 61 Bayesian linear regression © Eric Xing @ CMU, 2005-2014 62 Classification Generative and discriminative approach Q X Q X Regression Linear, conditional mixture, nonparametric X Y Density estimation Parametric and nonparametric methods , X X Simple GMs are the building blocks of complex BNs © Eric Xing @ CMU, 2005-2014 63 "
70,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 13. Sentence Representations Edward Grefenstette DeepMind & UCL How do we represent sentence meaning? • Is it a logical expression? • Is it a vector? Many vectors? • Is there just one notion of meaning, or is it task/context- dependent? Montague Semantics • Sentence representations are logical expressions. • Sentence understanding is parsing and combining constituents to obtain logical form. • Syntax guides semantics. Montague Semantics Montague Semantics Montague Semantics Montague Semantics Pros: • Intuitive and interpretable(?) representations. • Leverage the power of predicate logic to model semantics. • Evaluate the truth of statements, derive conclusions, etc. Montague Semantics Cons: • Brittle, requires robust parsers. • Extensive logical model required for evaluation of clauses. • Extensive set of rules required to do anything useful. • Overall, an intractable (or unappealing) learning problem. Neural Networks as Models of Composition • Sentence Classiﬁcation: produce sentence representation, classify based on this representation • Seq2Seq: Produce sentence representation, generate another sentence conditioned on this representation What models of composition? What objectives? Algebraic Encoders Sequence Encoders in Sequence Classiﬁcation Sequence Encoders in Seq2Seq Sentence Matrices as Sentence Representations Les chiens aiment les os ||| Dogs love bones Dogs love bones </s> Source sequence Target sequence Les chiens aiment les os ||| Dogs love bones Dogs love bones </s> Source sequence Target sequence %&!$ Incorporating Structure Basic idea: use sampled or maximally likely parse tree to guide composition: Perhaps additionally condition on the production rule: f can be any diﬀerentiable function. There are RNN- style and LSTM-style updates in various papers. Figure due to Sam Bowman. Reproduced with author's permission. What if you don’t have parses at test time? [SHIFT, SHIFT, REDUCE, SHIFT, SHIFT, REDUCE, REDUCE] [SHIFT, SHIFT, SHIFT, SHIFT, REDUCE, REDUCE, REDUCE] [SHIFT, SHIFT, SHIFT, REDUCE, SHIFT, REDUCE, REDUCE] Figure due to Sam Bowman. Reproduced with author's permission. SPINN bu↵er down sat stack cat the composition tracking transition reduce down sat the cat composition tracking transition shift down sat the cat tracking Figure due to Sam Bowman. Reproduced with author's permission. SPINN bu↵er down sat stack cat the composition tracking transition reduce down sat the cat composition tracking transition shift down sat the cat tracking Figure due to Sam Bowman. Reproduced with author's permission. Word vectors start on buﬀer b (top: ﬁrst word in sentence). Shift moves word vectors from buﬀer to stack s. Reduce pops top two vectors oﬀ stack, applies a function fR to them, and pushes the result back on the stack. Tracker LSTM tracks parser/composer state, decides shift/reduce actions a, is supervised by both observed shift-reduce operations and end task: What if we don’t observe actions? x y x y z What if we don’t observe actions? SPINN+REINFORCE • Treat at ~ fA(ht) as a policy trained by REINFORCE. • Reward is negated loss of the end task, e.g. NLL of correct label. • Everything else is trained by backpropagation against the end task: tracker LSTM, representations, etc. receive gradient both from the supervised objective, and from Reinforce via the shift-reduce policy. a wo man wea ring sun glas ses is frow ning . a boy drag s his sled s thro ugh the sno w . What objective? • Seq2Seq? • Classiﬁcation? • Training against a speciﬁc objective will (trivially) produce representations that are useful for that objective? • Can we get something more general? Auto-Encoders • Reconstruction objective includes nothing about distance preservation in latent space. • Conversely, little incentive for similar latent codes to generate radically diﬀerent (but semantically equivalent) observations. • Generally, auto-encoders sparsely encode or densely compress information. No pressure to ensure similarity continuum amongst codes. Skip Thought • Similar to auto-encoding objective: encode sentence, but decode neighbouring sentences. • Pair of LSTM-based seq2seq models with share encoder, but alternative formulations are possible. • Conceptually similar to distributional semantics: a unit’s representation is a function of its neighbouring units, except units are sentence instead of words. Variational Auto-Encoders Prior on z enforces semantic continuum (e.g. no arbitrarily unrelated codes for similar data), but expectation is typically intractable to compute exactly, and Monte Carlo estimate of gradients will be high variance. z x N(0, I) Variational Auto-Encoders Goal is to estimate, by maximising p(x): • The parameters θ of a function modelling the part of the generative process pθ(x|z) given samples from a ﬁxed prior z ~ p(z). • The parameters φ of a proposal distribution qφ(z|x) approximating the true posterior p(z|x). Variational Auto-Encoders How do we do it? We maximise p(x) via a variational lower bound (VLB): Equivalently we can maximise the NLL(x): Deriving the VLB For right qφ(z|x) and p(z) (e.g. Gaussians) there is a closed form expression of DKL(qφ(z|x)||p(z)). Variational Auto-Encoders Estimating requires backpropagating through samples. For some choices of q (e.g. Gaussians) this can be done through the use of reparameterization tricks. Variational Auto-Encoders for Text Some issues • If decoder is powerful enough to model p(x) without exploiting the latent variable, the model will learn to ignore z and the VAE- encoder is useless. • This is what happens for LSTMs and other powerful autoregressive models. • Some “hacky” solutions (e.g. KL annealing) exist, but this is an open research problem. • In short, VAEs are promising, but not (yet) the solution for unsupervised sentence representation learning. Reading Montague, R. (1970). English as a formal language. Mitchell, J., & Lapata, M. (2008). Vector-based models of semantic composition. proceedings of ACL-08: HLT, 236-244. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems (pp. 3104-3112). Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing (pp. 1631-1642). Bowman, S. R., Gauthier, J., Rastogi, A., Gupta, R., Manning, C. D., & Potts, C. (2016). A fast uniﬁed model for parsing and sentence understanding. arXiv preprint arXiv:1603.06021. Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning (pp. 5-32). Springer, Boston, MA. Yogatama, D., Blunsom, P ., Dyer, C., Grefenstette, E., & Ling, W. (2016). Learning to compose words into sentences with reinforcement learning. ICLR 2017. Socher, R., Huang, E. H., Pennin, J., Manning, C. D., & Ng, A. Y. (2011). Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in neural information processing systems (pp. 801-809). Kingma, D. P ., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., & Bengio, S. (2015). Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. "
71,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 13. Sentence Representations Felix Hill DeepMind What does a sentence mean? Classical perspective • Sentence representations are logical expressions. • Sentence understanding is parsing and combining constituents to obtain logical form. • Syntax guides semantics. Classical perspective Classical perspective Classical perspective Pros: • Intuitive and interpretable(?) representations. • Leverage the power of predicate logic to model semantics • Evaluate the truth of statements, derive conclusions, etc. Classical perspective Thanks to Jay McClelland for examples Cons (1) - “John loves Mary”: loves(John, Mary) - “John loves ice cream” loves(John, ice cream) Cons (1) - “John loves Mary”: loves(John, Mary) - “John loves ice cream” loves(John, ice cream) All meaning is context-dependent Cons (2) - “the tiger threatens the giraﬀe”: threatens(tiger, giraffe) - “the protege threatens the master” threatens(protege, master) - “the scandal threatens the proﬁts” threatens(scandal, reputation) - “Dave pushed the button”: - “Dave pushed the trainees”: - “Dave pushed the agenda ” - “Dave pushed the drugs” Cons (2) Metaphoricity is the rule, not the exception Cons (3) - “the apple was in the container” - “the juice was in the container” Cons (3) - “the apple was in the container” - “the juice was in the container” Cons (3) - “the man cut his steak” Cons (3) - “the man cut his steak” Where did you come from? How many animals of each kind did Moses take on the Ark? “The haystack was important because the cloth ripped.” Cons (3) Cons (3) “The haystack was important because the cloth ripped.” Meaning is not in language, language indicates meaning Neural networks to the rescue • Nothing is an atom, everything a molecule (in theory) • Linguistic signal (e.g. words), perceptual clues (e.g. vision) and semantic knowledge all represented similarly • Representations of one information type constrain and interact with representations of others Sentence representations in neural nets Kalchbrenner & Blunsom, 2014 + / - / ? + / - / ? vector vector Can we improve on this? One approach.. [SHIFT, SHIFT, REDUCE, SHIFT, SHIFT, REDUCE, REDUCE] [SHIFT, SHIFT, SHIFT, SHIFT, REDUCE, REDUCE, REDUCE] [SHIFT, SHIFT, SHIFT, REDUCE, SHIFT, REDUCE, REDUCE] Figure due to Sam Bowman. Reproduced with author's permission. (1) the man inspects a painting in a museum (2) the man is sleeping CONTRADICTION Stanford NLI task bu↵er down sat stack cat the composition tracking transition reduce down sat the cat composition tracking transition shift down sat the cat tracking + $$$$$$$$$$ Bowman et al. 2015 (see also Socher et al. 2013) Wang and Jiang (2015) Parallel interactive processing wins perception and conceptual knowledge? Knowledge from stories “Skip-Thought Vectors” Kiros et al. 2015 Fast knowledge from stories “Learning distributed representations of sentences from unlabelled data” Hill et al. 2015 “Learning distributed representations of sentences from unlabelled data” Hill et al. 2015 Sequential Deniosing Auto-Encoder Knowledge from raw text “Learning distributed representations of sentences from unlabelled data” Hill et al. 2015 Knowledge from dictionaries “Learning distributed representations of sentences from unlabelled data” Hill et al. 2015 Knowledge from images? Next time: full “embodiment” Richer representation spaces Auto-encoding for representation learning http://kvfrans.com/variational-autoencoders-explained/ loss = pixel reconstruction loss Auto-encoding via a richer latent space http://kvfrans.com/variational-autoencoders-explained/ VAE: variational auto-encoder loss = pixel reconstruction loss + a unit normal distribution encoder predictions of mean and variance She went for a walk in the forest VAE for text Beneﬁts of VAE 1. Smooth(er) latent space of representations 2. Generate from the model Conclusions • The meaning of language is not in the language itself • Neural networks provide a model for combining the necessary information sources • Finding and using the right information is just as important as elaborate modelling Reading Formal semantics: Montague, R. (1970). English as a formal language. Meaning in context: McClelland, J. L. (1992). Can connectionist models discover the structure of natural language? Dictionary deﬁnitions to guide meaning: Hill, F , Cho, K and Korhonen, A. Learning to Understand Phrases by Embedding the Dictionary TACL. (2015). Skip-Thought Vectors: Kiros, R. et al. (NIPS 2015) Comparison of sentence representations (SDAE, FastSent): Hill, F , Cho, K and Korhonen, A. Learning Distributed Representations of Sentences from Unlabelled Data, NAACL. (2015). Variational AutoEncoder: Kingma, D. P ., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Variational AutoEncoder for sentences: Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., & Bengio, S. (2015). Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349. "
72,"Deep Learning for Natural Language Processing Stephen Clark et al. University of Cambridge and DeepMind 14. Image Captioning Stephen Clark University of Cambridge and DeepMind Image Captioning Vision Language Taken from Vinyals et al. 2015 Image Captioning Modiﬁed from Socher et al. 2011 Image Captioning quietly enters the historic church … Multimodal “meaning space” Image “Translation” Multilingual meaning space Image “Translation” Multimodal meaning space Some cycles and people outside the historic round church Caption Model Taken from Vinyals et al. 2015: Show and Tell: A Neural Image Caption Generator Model Optimization Taken from Vinyals et al. 2015 Optimized using stochastic gradient descent Is That It?! Yup, Pretty Much State-of-the-art CNN for object classiﬁcation; LSTM for the sentence generation Image is input only once to the LSTM at the beginning LSTM trained from scratch, only top layer of CNN retrained CNN pre-trained on object classiﬁcation; no pre-training of word embeddings Beam search (20) used to perform the argmax at test time (better than greedy) SGD with ﬁxed learning rate and no momentum Dropout and ensembles used to combat overﬁtting Datasets Taken from Vinyals et al. 2015 Results Taken from Vinyals et al. 2015 Results Taken from Vinyals et al. 2015 Generation Diversity Taken from Vinyals et al. 2015 Word Embeddings Taken from Vinyals et al. 2015 Example Output Taken from Vinyals et al. 2015 Captions with Attention • Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, Xu et al. 2015 • Nice demo at http://kelvinxu.github.io/projects/ capgen.html How Hard is the Task? How Hard is the Task? These examples from: http://www.cs.toronto.edu/~ﬁdler/slides/2017/CSC2539/Kaustav_slides.pdf How Hard is the Task? How Hard is the Task? How Hard is the Task? How Hard is the Task? How Hard is the Task? How Hard is the Task? Visual Question Answering What Does the Sign Say? Visual Question Answering What Does the Sign Say? Taken from Agrawal et al. 2016 Visual Question Answering What is the mustache made of? Visual Question Answering What is the mustache made of? Taken from Agrawal et al. 2016 Visual Question Answering Taken from Agrawal et al. 2016 Visual Question Answering Taken from Agrawal et al. 2016 CLEVR Dataset Taken from Johnson et al. 2016: CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning Clever Hans Taken from: https://en.wikipedia.org/wiki/Clever_Hans CLEVR Taken from Johnson et al. 2016 CLEVR A functional program is used to generate the questions and answers, given a randomly generated image; see Johnson et al. 2016 for details Systems Tested • Question LSTM without looking at the image (46.8% acc) • CNN (image) + Bag-of-words (question) (48.4%) • CNN + LSTM (52.3%) • CNN + LSTM + sophisticated pooling (51.4%) • CNN + LSTM + spatial attention (68.5%) Relation Networks Taken from: Santoro, Raposo et al. 2017: A simple neural network module for relational reasoning Relation Networks Taken from: Santoro, Raposo et al. 2017: A simple neural network module for relational reasoning Relation Networks Taken from: Santoro, Raposo et al. 2017: A simple neural network module for relational reasoning Superhuman Performance! Taken from: Santoro, Raposo et al. 2017: A simple neural network module for relational reasoning References • Show and Tell: A Neural Image Caption Generator, Vinyals et al. 2015 • Visual Question Answering, Agrawal et al. 2016 • CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning, Johnson et al. 2016 • A Simple Neural Network Module for Relational Reasoning, Santoro, Raposo, et al. 2017 "
73,"Situated Language Learning with Policy Gradients L14. Felix Hill DeepMind This feels like a long shot.... ""Ahem....don't you think this bedroom is a bit of a pigsty?"" ?????????? Mnih et al, 2015 - General-purpose learning algorithm - Works for many different problems - Teaches us about the game Reinforcement learning for games Supervised learning M(x, θ) x y = [ 0, 0, 0, 1, 0, 0] A neural network Supervised learning M(x, θ) x y = [ 0, 0, 0, 1, 0, 0] where Find weights θ to minimize e.g. A dataset D of (x,y) input-output pairs Supervised learning M(x, θ) x y = [ 0, 0, 0, 1, 0, 0] where Find theta to minimize e.g. A dataset D of (x,y) input-output pairs = C(x, y, θ) The ""cost"" function Improve M (θ values) by gradient descent Reinforcement learning π(x1, θ) π(x2, θ) π(x3, θ) [0.12, 0.64, 0.07, 0.21] UP [0.03, 0.24, 0.47, 0.22] RIGHT [0.92, 0.14, 0.27, 0.11] PICK r1 = 0 r2 = 0 r3 = 1 x 1 x 2 x 3 Supervised learning M(x, θ) x y = [ 0, 0, 0, 1, 0, 0] Reinforcement learning π(x1, θ) π(x2, θ) π(x3, θ) [0.12, 0.64, 0.07, 0.21] UP [0.03, 0.24, 0.47, 0.22] RIGHT [0.92, 0.14, 0.27, 0.11] PICK r1 = 0 r2 = 0 r3 = 1 x 1 x 2 x 3 An environment that gives us observations x Reinforcement learning π(x1, θ) π(x2, θ) π(x3, θ) [0.12, 0.64, 0.07, 0.21] UP [0.03, 0.24, 0.47, 0.22] RIGHT [0.92, 0.14, 0.27, 0.11] PICK r1 = 0 r2 = 0 r3 = 1 x 1 x 2 x 3 An environment that gives us observations x And scalar rewards r Reinforcement learning π(x1, θ) π(x2, θ) π(x3, θ) [0.12, 0.64, 0.07, 0.21] UP [0.03, 0.24, 0.47, 0.22] RIGHT [0.92, 0.14, 0.27, 0.11] PICK r1 = 0 r2 = 0 r3 = 1 x 1 x 2 x 3 An environment that gives us observations x And scalar rewards r The ""policy"" π predicts action probabilities given observations - Like the supervised M Reinforcement learning π(x1, θ) π(x2, θ) π(x3, θ) [0.12, 0.64, 0.07, 0.21] UP [0.03, 0.24, 0.47, 0.22] RIGHT [0.92, 0.14, 0.27, 0.11] PICK r1 = 0 r2 = 0 r3 = 1 x 1 x 2 x 3 An environment that gives us observations x And scalar rewards r Only rewards to learn from - no guarantee 'actions' are correct The ""policy"" π predicts actions given observations Gradient methods for Reinforcement Learning? We want to optimise Find the policy weights that give us the highest expected return where All the reward I got from the environment Gradient methods for Reinforcement Learning? We want to optimise where If we knew we could just do gradient ascent! Trajectory s 1 s 2 s 3 We want to optimise where Gradient methods for Reinforcement Learning? Trajectory s 1 s 2 s 3 Return For a stationary environment: A given trajectory -> unique well-defined return We want to optimise where Gradient methods for Reinforcement Learning? Trajectory s 1 s 2 s 3 Return We want to optimise where So condition on trajectories Gradient methods for Reinforcement Learning? Estimating a policy gradient in an environment Estimating a policy gradient in an environment Definition of expectation Space of all possible trajectories Estimating a policy gradient in an environment Definition of expectation Only P depends on theta Estimating a policy gradient in an environment Definition of expectation Only P depends on theta x 1 Estimating a policy gradient in an environment Definition of expectation Only P depends on theta x 1 By chain rule Estimating a policy gradient in an environment Definition of expectation Only P depends on theta x 1 By chain rule Definition of expectation Estimating a policy gradient in an environment Definition of expectation Only P depends on theta x 1 By chain rule Definition of expectation The gradient of the objective wrt. policy weights A quantity that I can compute by following a trajectory The REINFORCE algorithm To estimate the gradient of our objective wrt. the parameters of the policy function. We can estimate Notice also that The log probability of following a trajectory So - act in the environment (follow trajectories). - At each time step, remember: - After each trajectory (episode), compute : The REINFORCE algorithm Initialise θ randomly: For episodes Compute For t = 1 .. T: The gradient of the policy network, evaluated at a particular input / output pair If action choices led to good rewards, move weights to follow gradient (scaled by R) Reinforcement learning Supervised learning ● Something went well (high R), you know what you did - can reinforce ○ Not sure if you could have done better ● Something went badly (low R) - no idea what you should have done ● Something went well (high LL) AND you know how to make it even better ● Something went badly (low LL) - you know how to fix it Learning language in RL environments Language can refer to the visual world - Similar to image captioning / VQA Language can refer to actions and / or policies - Like a lot of natural language does! Where does reward come from? DeepMind Lab Beattie et al. DeepMind Lab. arXiv 2016. (https://github.com/deepmind/lab) DeepMind Lab Beattie et al. DeepMind Lab. arXiv 2016. (https://github.com/deepmind/lab) ""language"" Deep RL alone (A3C) not enough A 2-layer LSTM V 3-layer CNN at L 1-layer LSTM “ladder” M Valt vt lt Start off small (or large)... Colour words... Shape words... Language in DeepMind Lab: The Lexicon Shapes (40) tv, ball, balloon, cake, can, cassette, chair, guitar, hairbrush, hat, ice lolly, ladder, mug, pencil, suitcase, toothbrush, key, bottle, car, cherries, fork, fridge, hammer, knife, spoon, apple, banana, cow, flower, jug, pig, pincer, plant, saxophone, shoe, tennis racket, tomato, tree, wine glass, zebra. Colours (13) red, blue, white, grey, cyan, pink, orange, black, green, magenta, brown, purple, yellow. Patterns (9) plain, chequered, crosses, stripes, discs, hex, pinstripe, spots, swirls. Shades (3) light, dark, neutral. Sizes (3) small, large, medium. Hermann and Hill et al. Grounded Language Learning in a Simulated 3D World. arXiv 2017. Auxiliary objectives A 2-layer LSTM V 3-layer CNN at L 1-layer LSTM “ladder” M Valt vt lt tAE LP RP VR Unsupervised learning makes word learning possible A 2-layer LSTM V 3-layer CNN at L 1-layer LSTM “ladder” M Valt vt lt tAE LP RP VR A3C agent A3C agent +RP + VR A3C agent +RP +VR +LP A3C agent +RP +VR +tAE A3C agent +RP +VR +tAE +LP And provides insight into agents' 'thoughts'.... Combining exploration and language X X X X Top-down view of the level 1 X X X X two object words and room descriptors 2 X X X X 3 4 X X X X X X X X single-room layout two room layout two room layout two room layout two object words and room descriptors medium object word / room descriptor vocabulary full object word / room descriptor vocabulary Agent trained from scratch Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 3 Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Curriculum is critical 1 X X X X two object words and room descriptors 2 X X X X 3 4 X X X X X X X X single-room layout two room layout two room layout two room layout two object words and room descriptors medium object word / room descriptor vocabulary full object word / room descriptor vocabulary Agent trained from scratch Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 3 Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Curriculum is critical 1 X X X X two object words and room descriptors 2 X X X X 3 4 X X X X X X X X single-room layout two room layout two room layout two room layout two object words and room descriptors medium object word / room descriptor vocabulary full object word / room descriptor vocabulary Agent trained from scratch Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Agent previously trained on level 3 Agent previously trained on level 2 Agent previously trained on level 1 Agent trained from scratch Curriculum is critical Isn't this all a bit convoluted? Environment Agent lang + world AI researcher lang + world Training ""ladder"" ""mug"" ""pencil"" ""suitcase"" ""toothbrush"" ""red"" ""green"" ""blue"" ""pink"" ""red ladder"" ""green mug"" ""blue pencil"" Test ""pink ladder"" ""yellow mug"" ""green pencil"" x 13 x 40 x 400 x 120 Agents naturally generalise word composition... Performance on training set Performance on test set Training ""red ladder"" ""green mug"" ""blue pencil"" Test ""pink ladder"" ""yellow mug"" ""green pencil"" x 400 x 120 Decompose before re-compose Performance on training set Performance on test set Training ""larger"" (ball) ""smaller"" (ball) ""larger"" (mug) ""smaller"" (mug) Test ""larger"" (pencil) ""smaller"" (pencil) x 30 x 10 Apply modifiers and predicates to novel objects Performance on training set Performance on test set Generalisation (zero-shot etc...) Environment Agent lang + world AI researcher lang + world Isn't learning slow? A3C agent A3C agent +RP + VR A3C agent +RP +VR +LP A3C agent +RP +VR +tAE A3C agent +RP +VR +tAE +LP 500,000!!!!!  Agent that already knows 20 words outside of training set Agent that already knows 2 words outside of training set Agent trained from scratch Word learning gets quicker the more the agent 'knows' Much like little people Much like little people  Agent that already knows 20 words outside of training set Agent that already knows 2 words outside of training set Agent trained from scratch Much like little people vocabulary 'spurt' conceptual 'bootstrapping' How does the agent represent its knowledge? Layerwise attention A LSTM V 3-layer CNN at L 1-layer LSTM “ladder” M Valt vt lt Layerwise attention A LSTM V 3-layer CNN at L 1-layer LSTM “ladder” M Valt vt lt v3 L M v2 v1 ""language"" Convolutional layers Processing colour words Agent visual input Attention distribution Visualise attention layer 3 Visualise attention layer 2 Visualise attention layer 1 Language channel instruction  Agent visual input Attention distribution Visualise attention layer 3 Visualise attention layer 2 Visualise attention layer 1 Language channel instruction Processing shape words Conclusions ● Using RL we can ground language in vision, actions and policies ● Neural network policies enable natural generalisation and composition ● Ongoing work to scale approaches to full sentences and natural commands ○ What aspects of language are not covered by this approach? ○ What challenges do we face extending this? "
74,"Introduction to Natural Language Processing CMSC 470 Marine Carpuat “Language is the ability to acquire and use complex systems of communication, particularly the human ability to do so, and a language is any specific example of such a system. The scientific study of language is called linguistics.” From Wikipedia Computational Linguistics (CL) • The science of doing what linguists do with language, but using computers Natural Language Processing (NLP) • The engineering discipline of doing what people do with language, but using computers Speech/Language/Text processing Human Language Technology NLP today What does an NLP system need to “know”? • Language consists of many levels of structure • Humans fluently integrate all of these in producing and understanding language • Ideally, so would a computer! Example from Nathan Schneider Why is NLP hard? Ambiguity At the word level • Part of speech • [V Duck]! • [N Duck] is delicious for dinner. • Word sense • I went to the bank to deposit my check. • I went to the bank to look out at the river Ambiguity At the syntactic level • PP Attachment ambiguity • I saw the man on the hill with the telescope • Structural ambiguity • I cooked her duck • Visiting relatives can be annoying • Time flies like an arrow Ambiguity • Quantifier scope • Everyone on the island speaks two languages. • Hard cases require world knowledge, understanding of speaker goals • The city council denied the demonstrators the permit because they advocated violence • The city council denied the demonstrators the permit because they feared violence Ambiguity • NLP challenge: how can we model ambiguity, and choose the correct analysis in context? • Approach: learn from data Word counts • Most frequent words in the English Europarl corpus • (out of 24M word tokens) Word counts • But also, out of the 93,638 distinct words (word types), 36,231 occur only once Plotting word frequencies Plotting word frequencies (with log-log axes) Zipf’s law Zipf’s law: implications • Even in a very large corpus, there will be a lot of infrequent words • The same holds for many other levels of linguistic structure • Core NLP challenge: we need to estimate probabilities or to be able to make predictions for things we have rarely or never seen Variation and Expressivity • The same meaning can be expressed with different forms • I saw the man • The man was seen by me • She needed to make a quick decision in that situation • The scenario required her to make a split-second judgment 6,800 living languages 600 with written tradition 100 spoken by 95% of population Social Impact • NLP experiments and applications can have a direct effect on individual users’ lives • Some issues • Privacy • Exclusion • Overgeneralization • Dual-use problems [Hovy & Spruit ACL 2016] Today • Levels of linguistic analysis in NLP • Morphology, syntax, semantics, discourse • Why is NLP hard? • Ambiguity • Sparse data • Zipf’s law, corpus, word types and tokens • Variation and expressivity • Social Impact This semester • Words and their meanings • Distributional semantics and word sense disambiguation • Fundamentals of supervised classification • Sequences • N-gram and neural language models • Sequence labeling tasks • Structured prediction and search algorithms • Application: Machine Translation • Trees • Syntax and grammars • Parsing Course Logistics http://www.cs.umd.edu/class/fall2018/cmsc470/ Exam dates • 9/27 2pm-3:15pm Midterm #1 • 11/1 2pm-3:15pm Midterm #2 • 12/15 10:30am-12:30pm Final Exam Before next class • Read the syllabus • Make sure you have access to piazza • Get started on homework 1 – due Wednesday Sep 5 by 11:59pm. "
75,"Words & their Meaning: Distributional Semantics CMSC 470 Marine Carpuat Slides credit: Dan Jurafsky Reminders • Read the syllabus • Make sure you have access to piazza • Get started on homework 1 – due Wed Sep 5 by 11:59pm. • Only available to students who are officially registered • If you have conflicts with exam dates, send me private message on piazza by tomorrow Aug 31 Words & their Meaning 2 core issues from an NLP perspective • Semantic similarity: given two words, how similar are they in meaning? • Word sense disambiguation: given a word that has more than one meaning, which one is used in a specific context? Word similarity for question answering “fast” is similar to “rapid” “tall” is similar to “height” Question answering: Q: “How tall is Mt. Everest?” Candidate A: “The official height of Mount Everest is 29029 feet” Word similarity for plagiarism detection Word similarity for historical linguistics: semantic change over time Kulkarni, Al-Rfou, Perozzi, Skiena 2015 Distributional models of meaning aka vector-space models of meaning aka vector semantics Vector Semantics Intuition Zellig Harris (1954): • “oculist and eye-doctor … occur in almost the same environments” • “If A and B have almost identical environments we say that they are synonyms.” Firth (1957): • “You shall know a word by the company it keeps!” tesgüino A bottle of tesgüino is on the table Everybody likes tesgüino Tesgüino makes you drunk We make tesgüino out of corn. Intuition: two words are similar if they have similar word contexts. Vector Semantics • Model the meaning of a word by “embedding” in a vector space. • The meaning of a word is a vector of numbers • Vector models are also called “embeddings”. • Contrast: word represented by a vocabulary index (“word number 545”) Many varieties of vector models Sparse vector representations 1. Mutual-information weighted word co-occurrence matrices Dense vector representations: 2. Singular value decomposition (and Latent Semantic Analysis) 3. Neural-network-inspired models (word2vec, skip-grams, CBOW) As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 Term-document matrix • Each cell: count of term t in a document d: tft,d • Each document is a count vector in ℕv: a column below Term-document matrix • Two documents are similar if their vectors are similar As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The words in a term-document matrix • Each word is a count vector in ℕD: a row below As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The words in a term-document matrix • Two words are similar if their vectors are similar As You Like It Twelfth Night Julius Caesar Henry V battle 1 1 8 15 soldier 2 2 12 36 fool 37 58 1 5 clown 6 117 0 0 The word-word or word-context matrix • Instead of entire documents, use smaller contexts • Paragraph • Window of ± 4 words • A word is now defined by a vector over counts of context words • Instead of each vector being of length D • Each vector is now of length |V| • The word-word matrix is |V|x|V| Word-word matrix Sample contexts ± 7 words aardvark computer data pinch result sugar … apricot 0 0 0 1 0 1 pineapple 0 0 0 1 0 1 digital 0 2 1 0 1 0 information 0 1 6 0 4 0 … … Word-word matrix • The |V|x|V| matrix is very sparse (most values are 0) • The size of windows depends on representation goals • The shorter the windows , the more syntactic the representation ± 1-3 very syntacticy • The longer the windows, the more semantic the representation ± 4-10 more semanticy Positive Pointwise Mutual Information (PPMI) Vector Semantics Problem with raw counts • Raw word frequency is not a great measure of association between words • We’d rather have a measure that asks whether a context word is particularly informative about the target word. • Positive Pointwise Mutual Information (PPMI) Pointwise Mutual Information Pointwise mutual information: Do events x and y co-occur more than if they were independent? PMI between two words: (Church & Hanks 1989) Do words x and y co-occur more than if they were independent? PMI 𝑤𝑜𝑟𝑑1, 𝑤𝑜𝑟𝑑2 = log2 𝑃(𝑤𝑜𝑟𝑑1,𝑤𝑜𝑟𝑑2) 𝑃𝑤𝑜𝑟𝑑1 𝑃(𝑤𝑜𝑟𝑑2) PMI(X,Y) = log2 P(x,y) P(x)P(y) Positive Pointwise Mutual Information • PMI ranges from −∞to + ∞ • But the negative values are problematic • Things are co-occurring less than we expect by chance • Unreliable without enormous corpora • So we just replace negative PMI values by 0 • Positive PMI (PPMI) between word1 and word2: PPMI 𝑤𝑜𝑟𝑑1, 𝑤𝑜𝑟𝑑2 = max log2 𝑃(𝑤𝑜𝑟𝑑1, 𝑤𝑜𝑟𝑑2) 𝑃𝑤𝑜𝑟𝑑1 𝑃(𝑤𝑜𝑟𝑑2) , 0 Computing PPMI on a term-context matrix • Matrix F with W rows (words) and C columns (contexts) • fij is # of times wi occurs in context cj pij = fij fij j=1 C å i=1 W å pi* = fij j=1 C å fij j=1 C å i=1 W å p* j = fij i=1 W å fij j=1 C å i=1 W å pmiij = log2 pij pi*p* j ppmiij = pmiij if pmiij > 0 0 otherwise ì í ï î ï p(w=information,c=data) = p(w=information) = p(c=data) = p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 = .32 6/19 11/19 = .58 7/19 = .37 pij = fij fij j=1 C å i=1 W å p(wi) = fij j=1 C å N p(cj) = fij i=1 W å N pmiij = log2 pij pi*p* j p(w,context) p(w) computer data pinch result sugar apricot 0.00 0.00 0.05 0.00 0.05 0.11 pineapple 0.00 0.00 0.05 0.00 0.05 0.11 digital 0.11 0.05 0.00 0.05 0.00 0.21 information 0.05 0.32 0.00 0.21 0.00 0.58 p(context) 0.16 0.37 0.11 0.26 0.11 PPMI(w,context) computer data pinch result sugar apricot - - 2.25 - 2.25 pineapple - - 2.25 - 2.25 digital 1.66 0.00 - 0.00 - information 0.00 0.57 - 0.47 - Weighting PMI • PMI is biased toward infrequent events • Very rare words have very high PMI values • Two solutions: • Give rare words slightly higher probabilities • Use add-k smoothing (which has a similar effect) Weighting PMI: Giving rare context words slightly higher probability • Raise the context probabilities to 𝛼= 0.75: • Consider two events, P(a) = .99 and P(b)=.01 𝑃 𝛼𝑎= .99.75 .99.75+.01.75 = .97 𝑃 𝛼𝑏= .01.75 .01.75+.01.75 = .03 Add-2 smoothing Add-2 Smoothed Count(w,context) computer data pinch result sugar apricot 2 2 3 2 3 pineapple 2 2 3 2 3 digital 4 3 2 3 2 information 3 8 2 6 2 PPMI vs add-2 smoothed PPMI PPMI(w,context) [add-2] computer data pinch result sugar apricot 0.00 0.00 0.56 0.00 0.56 pineapple 0.00 0.00 0.56 0.00 0.56 digital 0.62 0.00 0.00 0.00 0.00 information 0.00 0.58 0.00 0.37 0.00 PPMI(w,context) computer data pinch result sugar apricot - - 2.25 - 2.25 pineapple - - 2.25 - 2.25 digital 1.66 0.00 - 0.00 - information 0.00 0.57 - 0.47 - tf.idf: an alternative to PPMI for measuring association • The combination of two factors • TF: Term frequency (Luhn 1957): frequency of the word • IDF: Inverse document frequency (Sparck Jones 1972) • N is the total number of documents • dfi = “document frequency of word i” = # of documents with word i • wij = word i in document j wij=tfij idfi idfi = log N dfi æ è ç ç ö ø ÷ ÷ Measuring similarity: the cosine Vector Semantics Cosine for computing similarity cos(v,w) = v ·w v w = v v · w w = viwi i=1 N å vi 2 i=1 N å wi 2 i=1 N å Dot product Unit vectors vi is the PPMI value for word v in context i wi is the PPMI value for word w in context i. Cos(v,w) is the cosine similarity of v and w Sec. 6.3 Other possible similarity measures Evaluating similarity Vector Semantics Evaluating similarity • Extrinsic (task-based, end-to-end) Evaluation: • Question Answering • Spell Checking • Essay grading • Intrinsic Evaluation: • Correlation between algorithm and human word similarity ratings • Wordsim353: 353 noun pairs rated 0-10. sim(plane,car)=5.77 • Taking TOEFL multiple-choice vocabulary tests • Levied is closest in meaning to: imposed, believed, requested, correlated Words & their Meaning • Semantic similarity: given two words, how similar are they in meaning? • Distributional semantics • Meaning of a word as defined by its contexts • Implemented as vector space model • Vector space models can be induced from raw text • Different ways of defining context • Different metrics for computing association between word & context • Different similarity metrics Words & their Meaning: Distributional Semantics CMSC 470 Marine Carpuat Slides credit: Dan Jurafsky "
76,"Words & their Meaning: Word Sense Disambiguation CMSC 470 Marine Carpuat Slides credit: Dan Jurafsky Today: Word Meaning 2 core issues from an NLP perspective • Semantic similarity: given two words, how similar are they in meaning? • Word sense disambiguation: given a word that has more than one meaning, which one is used in a specific context? “Big rig carrying fruit crashes on 210 Freeway, creates jam” http://articles.latimes.com/2013/may/20/local/la-me-ln-big-rig-crash-20130520 How do we know that a word (lemma) has distinct senses? • Linguists often design tests for this purpose • e.g., zeugma combines distinct senses in an uncomfortable way Which ﬂight serves breakfast? Which ﬂights serve BWI? *Which ﬂights serve breakfast and BWI? Word Senses • “Word sense” = distinct meaning of a word • Same word, different senses • Homonyms (homonymy): unrelated senses; identical orthographic form is coincidental • E.g., financial bank vs. river bank • Polysemes (polysemy): related, but distinct senses • E.g., Financial bank vs. blood bank vs. tree bank • Metonyms (metonymy): “stand in”, technically, a sub-case of polysemy • E.g., use “Washington” in place of “the US government” • Different word, same sense • Synonyms (synonymy) • Homophones: same pronunciation, different orthography, different meaning • Examples: would/wood, to/too/two • Homographs: distinct senses, same orthographic form, different pronunciation • Examples: bass (fish) vs. bass (instrument) Relationship Between Senses • IS-A relationships • From specific to general (up): hypernym (hypernymy) • From general to specific (down): hyponym (hyponymy) • Part-Whole relationships • wheel is a meronym of car (meronymy) • car is a holonym of wheel (holonymy) WordNet: a lexical database for English https://wordnet.princeton.edu/ • Includes most English nouns, verbs, adjectives, adverbs • Electronic format makes it amenable to automatic manipulation: used in many NLP applications • “WordNets” generically refers to similar resources in other languages Synonymy in WordNet • WordNet is organized in terms of “synsets” • Unordered set of (roughly) synonymous “words” (or multi-word phrases) • Each synset expresses a distinct meaning/concept WordNet: Example Noun {pipe, tobacco pipe} (a tube with a small bowl at one end; used for smoking tobacco) {pipe, pipage, piping} (a long tube made of metal or plastic that is used to carry water or oil or gas etc.) {pipe, tube} (a hollow cylindrical shape) {pipe} (a tubular wind instrument) {organ pipe, pipe, pipework} (the flues and stops on a pipe organ) Verb {shriek, shrill, pipe up, pipe} (utter a shrill cry) {pipe} (transport by pipeline) “pipe oil, water, and gas into the desert” {pipe} (play on a pipe) “pipe a tune” {pipe} (trim with piping) “pipe the skirt” WordNet 3.0: Size Part of speech Word form Synsets Noun 117,798 82,115 Verb 11,529 13,767 Adjective 21,479 18,156 Adverb 4,481 3,621 Total 155,287 117,659 http://wordnet.princeton.edu/ Word Sense Disambiguation • Task: automatically select the correct sense of a word • Input: a word in context • Output: sense of the word • Motivated by many applications: • Information retrieval • Machine translation • … How big is the problem? • Most words in English have only one sense • 62% in Longman’s Dictionary of Contemporary English • 79% in WordNet • But the others tend to have several senses • Average of 3.83 in LDOCE • Average of 2.96 in WordNet • Ambiguous words are more frequently used • In the British National Corpus, 84% of instances have more than one sense • Some senses are more frequent than others Baseline Performance • Baseline: most frequent sense • Equivalent to “take first sense” in WordNet • Does surprisingly well! 62% accuracy in this case! Upper Bound Performance • Upper bound • Fine-grained WordNet sense: 75-80% human agreement • Coarser-grained inventories: 90% human agreement possible Simplest WSD algorithm: Lesk’s Algorithm • Intuition: note word overlap between context and dictionary entries • Unsupervised, but knowledge rich The bank can guarantee deposits will eventually cover future tuition costs because it invests in adjustable-rate mortgage securities. WordNet Lesk’s Algorithm • Simplest implementation: • Count overlapping content words between glosses and context • Lots of variants: • Include the examples in dictionary definitions • Include hypernyms and hyponyms • Give more weight to larger overlaps (e.g., bigrams) • Give extra weight to infrequent words • … Alternative: WSD as Supervised Classification label1 label2 label3 label4 Classifier supervised machine learning algorithm ? unlabeled document label1? label2? label3? label4? Testing Training training data Feature Functions Existing Corpora • Lexical sample • line-hard-serve corpus (4k sense-tagged examples) • interest corpus (2,369 sense-tagged examples) • … • All-words • SemCor (234k words, subset of Brown Corpus) • Senseval/SemEval (2081 tagged content words from 5k total words) • … Word Meaning 2 core issues from an NLP perspective • Semantic similarity: given two words, how similar are they in meaning? • Key concepts: vector semantics, PPMI and its variants, cosine similarity • Word sense disambiguation: given a word that has more than one meaning, which one is used in a specific context? • Key concepts: word sense, WordNet and sense inventories, unsupervised disambiguation (Lesk), supervised disambiguation "
77,"Classification, Linear Models, Naïve Bayes CMSC 470 Marine Carpuat Slides credit: Dan Jurafsky & James Martin, Jacob Eisenstein Today • Text classification problems • and their evaluation • Linear classifiers • Features & Weights • Bag of words • Naïve Bayes Classification problems Multiclass Classification label1 label2 label3 label4 Classifier supervised machine learning algorithm ? unlabeled document label1? label2? label3? label4? Testing Training training data Feature Functions Is this spam? From: ""Fabian Starr“ <Patrick_Freeman@pamietaniepeerelu.pl> Subject: Hey! Sofware for the funny prices! Get the great discounts on popular software today for PC and Macintosh http://iiled.org/Cj4Lmx 70-90% Discounts from retail price!!! All sofware is instantly available to download - No Need Wait! What is the subject of this article? • Antogonists and Inhibitors • Blood Supply • Chemistry • Drug Therapy • Embryology • Epidemiology • … MeSH Subject Category Hierarchy ? MEDLINE Article Text Classification • Assigning subject categories, topics, or genres • Spam detection • Authorship identification • Age/gender identification • Language Identification • Sentiment analysis • … Text Classification: definition • Input: • a document d • a fixed set of classes Y = {y1, y2,…, yJ} • Output: a predicted class y Y Classification Methods: Supervised Machine Learning • Input • a document d • a fixed set of classes Y = {y1, y2,…, yJ} • a training set of m hand-labeled documents (d1,y1),....,(dm,ym) • Output • a learned classifier d y Aside: getting examples for supervised learning • Human annotation • By experts or non-experts (crowdsourcing) • Found data • How do we know how good a classifier is? • Compare classifier predictions with human annotation • On held out test examples • Evaluation metrics: accuracy, precision, recall The 2-by-2 contingency table correct not correct selected tp fp not selected fn tn Precision and recall • Precision: % of selected items that are correct Recall: % of correct items that are selected correct not correct selected tp fp not selected fn tn A combined measure: F • A combined measure that assesses the P/R tradeoff is F measure (weighted harmonic mean): • People usually use balanced F1 measure • i.e., with = 1 (that is, = ½): F = 2PR/(P+R) R P PR R P F + + = - + = 2 2 ) 1 ( 1 ) 1 ( 1 1 b b a a Linear Models for Multiclass Classification Linear Models for Classification Feature function representation Weights Defining features: Bag of words Defining features Linear Classification Linear Models for Classification Feature function representation Weights How can we learn weights? • By hand • Probability • e.g.,Naïve Bayes • Discriminative training • e.g., perceptron, support vector machines Naïve Bayes Models for Text Classification Generative Story for Multinomial Naïve Bayes • A hypothetical stochastic process describing how training examples are generated Prediction with Naïve Bayes Score(x,y) Definition of conditional probability Generative story assumptions This is a linear model! Prediction with Naïve Bayes Score(x,y) Definition of conditional probability Generative story assumptions This is a linear model! Prediction with Naïve Bayes Score(x,y) Definition of conditional probability Generative story assumptions This is a linear model! Parameter Estimation • “count and normalize” • Parameters of a multinomial distribution • Relative frequency estimator • Formally: this is the maximum likelihood estimate • See CIML for derivation Smoothing (add alpha) Naïve Bayes recap Why is this model called “Naïve Bayes”? Another view of the same model 𝑦 = 𝑎𝑟𝑔𝑚𝑎𝑥𝑦𝑃𝑌= 𝑦𝑋= 𝑥) = 𝑎𝑟𝑔𝑚𝑎𝑥𝑦𝑃(𝑌= 𝑦)𝑃𝑋= 𝑥𝑌= 𝑦) = 𝑎𝑟𝑔𝑚𝑎𝑥𝑦𝑃(𝑌= 𝑦) 𝑖=1 𝑑 𝑃𝑋𝑖= 𝑥𝑖𝑌= 𝑦) Bayes rule + Conditional independence assumption Today • Text classification problems • and their evaluation • Linear classifiers • Features & Weights • Bag of words • Naïve Bayes "
78,"Linear Models: Naïve Bayes, Perceptron CMSC 470 Marine Carpuat Slides credit: Jacob Eisenstein Linear Models for Multiclass Classification Feature function representation Weights Naïve Bayes recap Prediction with Naïve Bayes Score(x,y) Definition of conditional probability Generative story assumptions This is a linear model! • Naïve Bayes worked example on board The perceptron • A linear model for classification • Prediction rule • An algorithm to learn feature weights given labeled data • online algorithm • error-driven Multiclass perceptron Online vs batch learning algorithms • In an online algorithm, parameter values are updated after every example • E.g., perceptron • In a batch algorithm, parameter values are set after observing the entire training set • E.g., naïve Bayes Multiclass perceptron: a simple algorithm with some theoretical guarantees Theorem: If the data is linearly separable, then the perceptron algorithm will find a separator (Novikoff, 1962) Practical considerations • In which order should we select instances? • Shuffling before learning to randomize order helps • How do we decide when to stop? • When the weight values don’t change much • E.g., norm of the difference between previous and current weight vectors falls below some threshold • When the accuracy on held out data starts to decrease • Early stopping ML fundamentals aside: overfitting/underfitting/generalization Training error is not sufficient • We care about generalization to new examples • A classifier can classify training data perfectly, yet classify new examples incorrectly • Because training examples are only a sample of data distribution • a feature might correlate with class by coincidence • Because training examples could be noisy • e.g., accident in labeling Overfitting • Consider a model 𝜃and its: • Error rate over training data 𝑒𝑟𝑟𝑜𝑟 𝑡𝑟𝑎𝑖𝑛(𝜃) • True error rate over all data 𝑒𝑟𝑟𝑜𝑟 𝑡𝑟𝑢𝑒𝜃 • We say ℎoverfits the training data if 𝑒𝑟𝑟𝑜𝑟 𝑡𝑟𝑎𝑖𝑛𝜃< 𝑒𝑟𝑟𝑜𝑟 𝑡𝑟𝑢𝑒𝜃 Evaluating on test data • Problem: we don’t know 𝑒𝑟𝑟𝑜𝑟𝑡𝑟𝑢𝑒𝜃! • Solution: • we set aside a test set • some examples that will be used for evaluation • we don’t look at them during training! • after learning a classifier 𝜃, we calculate 𝑒𝑟𝑟𝑜𝑟 𝑡𝑒𝑠𝑡𝜃 Overfitting • Another way of putting it • A classifier 𝜃is said to overfit the training data, if there is another hypothesis 𝜃′, such that • 𝜃has a smaller error than 𝜃′ on the training data • but 𝜃has larger error on the test data than 𝜃′. Underfitting/Overfitting • Underfitting • Learning algorithm had the opportunity to learn more from training data, but didn’t • Overfitting • Learning algorithm paid too much attention to idiosyncracies of the training data; the resulting classifier doesn’t generalize Back to the Perceptron Averaged Perceptron improves generalization Properties of Linear Models we’ve seen so far Naïve Bayes • Batch learning • Generative model p(x,y) • Grounded in probability • Assumes features are independent given class • Learning = find parameters that maximize likelihood of training data Perceptron • Online learning • Discriminative model score(y|x), Guaranteed to converge if data is linearly separable • But might overfit the training set • Error-driven learning What you should know about linear models • Their properties, strengths and weaknesses (see previous slides) • How to make a prediction given a model • How to train a model given a dataset "
79,"Linear Models: Perceptron, Logistic Regression CMSC 470 Marine Carpuat Slides credit: Jacob Eisenstein Linear Models for Multiclass Classification Feature function representation Weights Multiclass perceptron Properties of Linear Models we’ve seen so far Naïve Bayes • Batch learning • Generative model p(x,y) • Grounded in probability • Assumes features are independent given class • Learning = find parameters that maximize likelihood of training data Perceptron • Online learning • Discriminative model score(y|x) • Guaranteed to converge if data is linearly separable • But might overfit the training set • Error-driven learning Averaged Perceptron improves generalization Differential Calculus Refresher • Derivatives • Chain rule • Convex functions • Gradients Logistic Regression for Binary Classification Examples & illustrations: Graham Neubig Perceptron & Probabilities • What if we want a probability p(y|x)? • The perceptron gives us a prediction y • Let’s illustrate this with binary classification Illustrations: Graham Neubig The logistic function • “Softer” function than in perceptron • Can account for uncertainty • Differentiable Logistic regression: how to train? • Train based on conditional likelihood • Find parameters w that maximize conditional likelihood of all answers 𝑦𝑖given examples 𝑥𝑖 Stochastic gradient ascent (or descent) • Online training algorithm • Update weights for every training example • Move in direction given by gradient • Size of update step scaled by learning rate Gradient of the logistic function Example: Person/not-person classification problem Given an introductory sentence in Wikipedia predict whether the article is about a person Example: initial update Example: second update How to set the learning rate? • Various strategies • decay over time 𝛼= 1 𝐶+ 𝑡 • Use held-out test set, increase learning rate when likelihood increases Parameter Number of samples What you should know about linear models • Standard supervised learning set-up for text classification • Difference between train vs. test data • How to evaluate • 3 examples of linear classifiers • Naïve Bayes, Perceptron, Logistic Regression • How to make predictions, how to train, strengths and weaknesses • Learning as optimization: loss functions and their properties • Difference between generative vs. discriminative classifiers • General machine learning concepts • Smoothing, overfitting, underfitting, regularization "
8,"School of Computer Science Probabilistic Graphical Models Maximum likelihood learning of undirected GM Eric Xing Lecture 8, February 10, 2014 Reading: MJ Chap 9, and 11 1 © Eric Xing @ CMU, 2005-2014 Why? Sometimes an UNDIRECTED association graph makes more sense and/or is more informative  gene expressions may be influenced by unobserved factor that are post- transcriptionally regulated  The unavailability of the state of B results in a constrain over A and C B A C B A C B A C Undirected Graphical Models 2 © Eric Xing @ CMU, 2005-2014 ML Structural Learning via Neighborhood Selection for completely observed MRF Data ) , , ( ) ( ) ( 1 1 1 n x x  ) , , ( ) ( ) ( M n M x x  1  ) , , ( ) ( ) ( 2 2 1 n x x  3 © Eric Xing @ CMU, 2005-2014 Gaussian Graphical Models Multivariate Gaussian density: WOLG: let We can view this as a continuous Markov Random Field with potentials defined on every node and edge:   ) - ( ) - ( - exp ) ( ) , | ( / /     x x x 1 2 1 2 1 2 2 1      T n p              j i j i ij i i ii n p x x q x q Q Q x x x p 2 2 1 2 / 2 / 1 2 1 - exp ) 2 ( ) , 0 | , , , (    4 © Eric Xing @ CMU, 2005-2014 Pairwise MRF (e.g., Ising Model) Assuming the nodes are discrete, and edges are weighted, then for a sample xd, we have 5 © Eric Xing @ CMU, 2005-2014 The covariance and the precision matrices Covariance matrix  Graphical model interpretation? Precision matrix  Graphical model interpretation? 6 © Eric Xing @ CMU, 2005-2014 Sparse precision vs. sparse covariance in GGM 2 1 3 5 4                   5 9 0 0 0 9 4 8 0 0 0 8 3 7 0 0 0 7 2 6 0 0 0 6 1 1                             08 0 07 0 12 0 03 0 15 0 07 0 04 0 07 0 01 0 08 0 12 0 07 0 10 0 02 0 13 0 03 0 01 0 02 0 03 0 15 0 15 0 08 0 13 0 15 0 10 0 . . . . . . . . . . . . . . . . . . . . . . . . . ) 5 ( or ) 1 ( 5 1 1 15 0 nbrs nbrs X X X     0 15 5 1    X X  7 © Eric Xing @ CMU, 2005-2014 Another example How to estimate this MRF? What if p >> n  MLE does not exist in general!  What about only learning a “sparse” graphical model?  This is possible when s=o(n)  Very often it is the structure of the GM that is more interesting … 8 © Eric Xing @ CMU, 2005-2014 Recall lasso 9 © Eric Xing @ CMU, 2005-2014 Graph Regression Lasso: Neighborhood selection 10 © Eric Xing @ CMU, 2005-2014 Graph Regression 11 © Eric Xing @ CMU, 2005-2014 Graph Regression It can be shown that: given iid samples, and under several technical conditions (e.g., ""irrepresentable""), the recovered structured is ""sparsistent"" even when p >> n 12 © Eric Xing @ CMU, 2005-2014 Learning Ising Model (i.e. pairwise MRF) Assuming the nodes are discrete, and edges are weighted, then for a sample xd, we have It can be shown following the same logic that we can use L_1 regularized logistic regression to obtain a sparse estimate of the neighborhood of each variable in the discrete case. 13 © Eric Xing @ CMU, 2005-2014 Consistency Theorem: for the graphical regression algorithm, under certain verifiable conditions (omitted here for simplicity): Note the from this theorem one should see that the regularizer is not actually used to introduce an “artificial” sparsity bias, but a devise to ensure consistency under finite data and high dimension condition. 14 © Eric Xing @ CMU, 2005-2014 ML Parameter Est. for completely observed MRFs of given structure The data: { (z1,x1), (z2,x2), (z3,x3), ... (zN,xN)} X1 X4 X3 X2 15 © Eric Xing @ CMU, 2005-2014 Recap: MLE for BNs  Assuming the parameters for each CPD are globally independent, and all nodes are fully observed, then the log-likelihood function decomposes into a sum of local terms, one per node:                    i n i i n n i i i n i i x p x p D p D ) , | ( log ) , | ( log ) | ( log ) ; ( , ,       x x l   k j i k ij ijk ML ijk n n , ' , '  16 © Eric Xing @ CMU, 2005-2014 MLE for undirected graphical models For directed graphical models, the log-likelihood decomposes into a sum of terms, one per family (node plus parents). For undirected graphical models, the log-likelihood does not decompose, because the normalization constant Z is a function of all the parameters In general, we will need to do inference (i.e., marginalization) to learn parameters for undirected models, even in the fully observed case.    C c c c n Z x x P ) ( ) , , ( x  1 1     n x x C c c c Z , , ) (  1 x  17 © Eric Xing @ CMU, 2005-2014 Log Likelihood for UGMs with tabular clique potentials Sufficient statistics: for a UGM (V,E), the number of times that a configuration x (i.e., XV=x) is observed in a dataset D={x1,…,xN} can be represented as follows: In terms of the counts, the log likelihood is given by: There is a nasty log Z in the likelihood     c V m m m c n n \ count) (clique ) ( ) ( and , count) (total ) , ( ) ( def def x x x x x x  Z N m Z m p p D p p D p c c c c c c c n n n n n c n log ) ( log ) ( ) ( 1 log ) ( ) | ( log ) , ( ) | ( log ) , ( ) ( log ) | ( ) ( ) , (                     x x x x x x x x x x x x x x x x x x           l 18 © Eric Xing @ CMU, 2005-2014 Z N m D p c c c c c log ) ( log ) ( ) ( log   x x x   Log Likelihood for UGMs with tabular clique potentials Sufficient statistics: for a UGM (V,E), the number of times that a configuration x (i.e., XV=x) is observed in a dataset D={x1,…,xN} can be represented as follows: In terms of the counts, the log likelihood is given by: There is a nasty log Z in the likelihood     c V m m m c n n \ count) (clique ) ( ) ( and , count) (total ) , ( ) ( def def x x x x x x  19 © Eric Xing @ CMU, 2005-2014 Derivative of log Likelihood Log-likelihood: First term: Second term: Z N m c c c c c log ) ( log ) (   x x x  l ) ( ) ( ) ( c c c c c m x x x     1 l ) ( ) ( ) ~ ( ) , ~ ( ) ( ) ~ ( ) ~ ( ) , ~ ( ) ~ ( ) ( ) , ~ ( ) ~ ( ) ( ) ( log ~ ~ ~ ~ c c c c c c c d d d c c c c d d d c c c c d d d c c c c p p Z Z Z Z x x x x x x x x x x x x x x x x x x x x x                                              1 1 1 1 1 Set the value of variables to x x 20 © Eric Xing @ CMU, 2005-2014 Conditions on Clique Marginals Derivative of log-likelihood Hence, for the maximum likelihood parameters, we know that: In other words, at the maximum likelihood setting of the parameters, for each clique, the model marginals must be equal to the observed marginals (empirical counts). This doesn’t tell us how to get the ML parameters, it just gives us a condition that must be satisfied when we have them. ) ( ) ( ) ( ) ( ) ( c c c c c c c c p N m x x x x x       l ) ( ~ ) ( ) ( def * c c c MLE p N m p x x x   21 © Eric Xing @ CMU, 2005-2014 MLE for undirected graphical models Is the graph decomposable (triangulated)? Are all the clique potentials defined on maximal cliques (not sub-cliques)? e.g., 123, 234 not 12, 23, … Are the clique potentials full tables (or Gaussians), or parameterized more compactly, e.g. ? X1 X4 X3 X2 X1 X4 X3 X2     c c k k c c f ) ( exp ) ( x x   22 © Eric Xing @ CMU, 2005-2014 Properties on MLE of clique potentials For decomposable models, where potentials are defined on maximal cliques, the MLE of clique potentials equate to the empirical marginals (or conditionals) of the corresponding clique. Thus the MLE can be solved by inspection!! If the graph is non-decomposable, and or the potentials are defined on non-maximal cliques (e.g., 12, 34), we could not equate MLE of cliques potentials to empirical marginals (or conditionals).  Potential expressed as a tabular form: IPF  Feature-based potentials: GIS 23 © Eric Xing @ CMU, 2005-2014 MLE for decomposable undirected models Decomposable models:  G is decomposable G is triangulated G has a junction tree  Potential based representation: Consider a chain X1 − X2 − X3. The cliques are (X1,X2 ) and (X2,X3); the separator is X2  The empirical marginals must equal the model marginals. Let us guess that  We can verify that such a guess satisfies the conditions: and similarly ) ( ~ ) , ( ~ ) , ( ~ ) , , ( 2 3 2 2 1 3 2 1 x p x x p x x p MLE x x x p   ) , ( ~ ) , ( ~ ) | ( ~ ) , , ( ) , ( 2 1 3 2 2 1 3 2 1 2 1 3 3 x x p x x p x x p x x x p x x p x x MLE MLE        ) , ( ~ ) , ( 3 2 3 2 x x p x x pMLE      s s s c c c p ) ( ) ( ) ( x x x   24 © Eric Xing @ CMU, 2005-2014 MLE for decomposable undirected models (cont.) Let us guess that To compute the clique potentials, just equate them to the empirical marginals (or conditionals), i.e., the separator must be divided into one of its neighbors. Then Z = 1. One more example: X1 X4 X3 X2 ) , ( ~ ) , , ( ~ ) , , ( ~ ) , , , ( 3 2 4 3 2 3 2 1 4 3 2 1 x x p x x x p x x x p x x x x pMLE   ) , | ( ~ ) , ( ~ ) , , ( ~ ) , , ( 3 2 1 3 2 3 2 1 3 2 1 123 x x x p x x p x x x p x x x MLE     ) , , ( ~ ) , , ( 4 3 2 4 3 2 234 x x x p x x x MLE    ) ( ~ ) , ( ~ ) , ( ~ ) , , ( 2 3 2 2 1 3 2 1 x p x x p x x p MLE x x x p   ) , ( ~ ) , ( 2 1 2 1 12 x x p x x MLE    ) | ( ~ ) ( ~ ) , ( ~ ) , ( 3 2 2 3 2 3 2 23 x x p x p x x p x x MLE     25 © Eric Xing @ CMU, 2005-2014 Non-decomposable and/or with non-maximal clique potentials If the graph is non-decomposable, and or the potentials are defined on non-maximal cliques (e.g., 12, 34), we could not equate empirical marginals (or conditionals) to MLE of cliques potentials. X1 X4 X3 X2 X1 X4 X3 X2   } , { ) , ( ) , , , ( j i j i ij x x x x x x p  4 3 2 1        ) ( ~ / ) , ( ~ ) ( ~ / ) , ( ~ ) , ( ~ ) , ( s.t. ) , ( MLE j j i i j i j i j i ij x p x x p x p x x p x x p x x j i  Homework! 26 © Eric Xing @ CMU, 2005-2014 MLE for undirected graphical models Is the graph decomposable (triangulated)? Are all the clique potentials defined on maximal cliques (not sub-cliques)? e.g., 123, 234 not 12, 23, … Are the clique potentials full tables (or Gaussians), or parameterized more compactly, e.g. ? X1 X4 X3 X2 X1 X4 X3 X2     c c k k c c f ) ( exp ) ( x x   Decomposable? Max clique? Tabular? Method    Direct - -  IPF - - - Gradient - - - GIS 27 © Eric Xing @ CMU, 2005-2014 Iterative Proportional Fitting (IPF)  From the derivative of the likelihood:  we can derive another relationship: in which c appears implicitly in the model marginal p(xc).  This is therefore a fixed-point equation for c.  Solving c in closed-form is hard, because it appears on both sides of this implicit nonlinear equation.  The idea of IPF is to hold c fixed on the right hand side (both in the numerator and denominator) and solve for it on the left hand side. We cycle through all cliques, then iterate: ) ( ) ( ) ( ) ( ) ( c c c c c c c c p N m x x x x x       l ) ( ) ( ) ( ) ( ~ c c c c c c p p x x x x    ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( c t c c t c c t c p p x x x x    1 Need to do inference here 28 © Eric Xing @ CMU, 2005-2014 Properties of IPF Updates IPF iterates a set of fixed-point equations: However, we can prove it is also a coordinate ascent algorithm (coordinates = parameters of clique potentials). Hence at each step, it will increase the log-likelihood, and it will converge to a global maximum. I-projection: finding a distribution with the correct marginals that has the maximal entropy ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( c t c c t c c t c p p x x x x    1 29 © Eric Xing @ CMU, 2005-2014 KL Divergence View IPF can be seen as coordinate ascent in the likelihood using the way of expressing likelihoods using KL divergences. We can show that maximizing the log likelihood is equivalent to minimizing the KL divergence (cross entropy) from the observed distribution to the model distribution: Using a property of KL divergence based on the conditional chain rule: p(x) = p(xa)p(xb|xa):     x x p x p x p x p x p ) | ( ) ( ~ log ) ( ~ ) | ( || ) ( ~ min max   KL l                a b a b a b a x a b a b a a a x x a b a b a b a x x a a a b a x x a b a a b a a b a b a b a x x p x x q x q x p x q x x p x x q x x q x q x p x q x x q x q x x p x p x x q x q x x q x q x x p x x q ) | ( || ) | ( ) ( ) ( || ) ( ) | ( ) | ( log ) | ( ) ( ) ( ) ( log ) | ( ) ( ) | ( ) ( ) | ( ) ( log ) | ( ) ( ) , ( || ) , ( , , , KL KL KL 30 © Eric Xing @ CMU, 2005-2014 IPF minimizes KL divergence Putting things together, we have It can be shown that changing the clique potential c has no effect on the conditional distribution, so the second term in unaffected. To minimize the first term, we set the marginal to the observed marginal, just as in IPF.  Note that this is only good when the model is decomposable ! We can interpret IPF updates as retaining the “old” conditional probabilities p(t)(x-c|xc) while replacing the “old” marginal probability p(t)(xc) with the observed marginal .            a x c c c c c c c p p p p p p p ) | ( || ) | ( ~ ) ( ~ ) | ( || ) ( ~ ) | ( || ) ( ~ x x x x x x x x x KL KL KL   ) ( ~ c p x 31 © Eric Xing @ CMU, 2005-2014 MLE for undirected graphical models Is the graph decomposable (triangulated)? Are all the clique potentials defined on maximal cliques (not sub-cliques)? e.g., 123, 234 not 12, 23, … Are the clique potentials full tables (or Gaussians), or parameterized more compactly, e.g. ? X1 X4 X3 X2 X1 X4 X3 X2     c c k k c c f ) ( exp ) ( x x   Decomposable? Max clique? Tabular? Method    Direct - -  IPF - - - Gradient - - - GIS 32 © Eric Xing @ CMU, 2005-2014 Feature-based Clique Potentials So far we have discussed the most general form of an undirected graphical model in which cliques are parameterized by general “tabular” potential functions c(xc). But for large cliques these general potentials are exponentially costly for inference and have exponential numbers of parameters that we must learn from limited data. One solution: change the graphical model to make cliques smaller. But this changes the dependencies, and may force us to make more independence assumptions than we would like. Another solution: keep the same graphical model, but use a less general parameterization of the clique potentials. This is the idea behind feature-based models. 33 © Eric Xing @ CMU, 2005-2014 Features Consider a clique xc of random variables in a UGM, e.g. three consecutive characters c1c2c3 in a string of English text. How would we build a model of p(c1c2c3)?  If we use a single clique function over c1c2c3, the full joint clique potential would be huge: 263−1 parameters.  However, we often know that some particular joint settings of the variables in a clique are quite likely or quite unlikely. e.g. ing, ate, ion, ?ed, qu?, jkx, zzz,... A “feature” is a function which is vacuous over all joint settings except a few particular ones on which it is high or low.  For example, we might have fing(c1c2c3) which is 1 if the string is ’ing’ and 0 otherwise, and similar features for ’?ed’, etc. We can also define features when the inputs are continuous. Then the idea of a cell on which it is active disappears, but we might still have a compact parameterization of the feature. 34 © Eric Xing @ CMU, 2005-2014 Features as Micropotentials By exponentiating them, each feature function can be made into a “micropotential”. We can multiply these micropotentials together to get a clique potential. Example: a clique potential (c1c2c3) could be expressed as: This is still a potential over 263 possible settings, but only uses K parameters if there are K features.  By having one indicator function per combination of xc, we recover the standard tabular potential.             K k k k f f c c c c f e e c c c 1 3 2 1 3 2 1 ) , , ( exp ) , , ( ?ed ?ed ing ing      35 © Eric Xing @ CMU, 2005-2014 Combining Features Each feature has a weight k which represents the numerical strength of the feature and whether it increases or decreases the probability of the clique. The marginal over the clique is a generalized exponential family distribution, actually, a GLIM: In general, the features may be overlapping, unconstrained indicators or any function of any subset of the clique variables: How can we combine feature into a probability model?                 ) , , ( ) , , ( ) , , ( ) , , ( exp ) , , ( zzz zzz qu? qu? ?ed ?ed ing ing 3 2 1 3 2 1 3 2 1 3 2 1 3 2 1 c c c f c c c f c c c f c c c f c c c p             c i I i c k k c c f ) ( exp ) ( def x x   36 © Eric Xing @ CMU, 2005-2014 Feature Based Model We can multiply these clique potentials as usual: However, in general we can forget about associating features with cliques and just use a simplified form: This is just our friend the exponential family model, with the features as sufficient statistics! Learning: recall that in IPF, we have  Not obvious how to use this rule to update the weights and features individually !!!            c I i c k k c c c c i f Z Z p ) ( exp ) ( ) ( ) ( ) ( x x x     1 1         i c i i i f Z p ) ( exp ) ( ) ( x x   1 ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( c t c c t c c t c p p x x x x    1 37 © Eric Xing @ CMU, 2005-2014 MLE of Feature Based UGMs Scaled likelihood function Instead of optimizing this objective directly, we attack its lower bound  The logarithm has a linear upper bound …  This bound holds for all , in particular, for  Thus we have      x n n x p x p x p N N D D ) | ( log ) ( ~ ) | ( log / ) ; ( ) ; ( ~     1 l l     x i i i Z x f x p ) ( log ) ( ) ( ~   1        log ) ( ) ( log Z Z ) ( ) (t Z   1   1      x t t i i i Z Z Z x f x p D ) ( log ) ( ) ( ) ( ) ( ~ ) ; ( ~ ) ( ) (      l 38 © Eric Xing @ CMU, 2005-2014 Generalized Iterative Scaling (GIS) Lower bound of scaled loglikelihood Define Relax again  Assume  Convexity of exponential: We have: 1      x t t i i i Z Z Z x f x p D ) ( log ) ( ) ( ) ( ) ( ~ ) ; ( ~ ) ( ) (      l ) ( def ) ( t i i t i       1 1 1 1 1                                                  ) ( log ) ( exp ) | ( ) ( ) ( ~ ) ( log ) ( exp ) ( exp ) ( ) ( ) ( ~ ) ( log ) ( exp ) ( ) ( ) ( ~ ) ; ( ~ ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( t i i t i x t i i x i t i i t i i i t i x t i i x i x t i i i x t i i i Z x f x p x f x p Z x f x f Z x f x p Z x f Z x f x p D               l        i i i i i i x x exp exp     ) ( ) ( log exp ) ( ) | ( ) ( ) ( ~ ) ; ( ~ def ) ( ) ( ) (                 1 t x i t i i t i i x i Z x f x p x f x p D l 1 0   i i i x f x f ) ( , ) ( 39 © Eric Xing @ CMU, 2005-2014 GIS Lower bound of scaled loglikelihood Take derivative: Set to zero  where p(t)(x) is the unnormalized version of p(x|(t)) Update   ) ( ) ( log exp ) ( ) | ( ) ( ) ( ~ ) ; ( ~ def ) ( ) ( ) (                 1 t x i t i i t i i x i Z x f x p x f x p D l          x i t t i i x i x f x p x f x p ) ( ) | ( exp ) ( ) ( ~ ) ( ) (    ) ( ) ( ) ( ) ( ) ( ~ ) ( ) | ( ) ( ) ( ~ ) ( ) ( ) ( ) ( t x i t i x x i t i x Z x f x p x f x p x f x p x f x p e t i                    i x f t t t i t i t i i t i e x p x p ) ( ) ( ) 1 ( ) ( ) ( ) 1 ( ) ( ) ( ) (                                        i x f x f x p x f x p t i x f t x f x f x p x f x p t t i x f t x f x p x f x p t t t i x i t i x i i i x i t i x i x i t i x x p Z Z x p Z Z x p x p ) ( ) ( ) ( ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) (     1 ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( c t c c t c c t c p p x x x x    1 Recall IPF: 40 © Eric Xing @ CMU, 2005-2014 Summary  IPF is a general algorithm for finding MLE of UGMs.  a fixed-point equation for c over single cliques, coordinate ascent  I-projection in the clique marginal space  Requires the potential to be fully parameterized  The clique described by the potentials do not have to be max-clique  For fully decomposable model, reduces to a single step iteration  GIS  Iterative scaling on general UGM with feature-based potentials  IPF is a special case of GIS which the clique potential is built on features defined as an indicator function of clique configurations.            i x f x f x p x f x p t t i x i t i x x p x p ) ( ) ( ) ( ) ( ) ( ~ ) ( ) 1 ( ) ( ) ( ) ( ) ( ) ( ~ ) ( ) ( ) ( ) ( ) ( c t c c t c c t c p p x x x x    1 IPF: GIS:            x i t i x x f x p x f x p t i t i ) ( ) ( ) ( ) ( ~ ) ( ) 1 ( ) ( log   41 © Eric Xing @ CMU, 2005-2014 Where does the exponential form come from? Review: Maximum Likelihood for exponential family i.e., At ML estimate, the expectations of the sufficient statistics under the model must match empirical feature average.                 x i i i x i i i x Z N x f x m Z x f x m x p x m D ) ( log ) ( ) ( ) ( log ) ( ) ( ) | ( log ) ( ) ; (       l            x i x i x i i i x f x p N x f x m Z N x f x m D ) ( ) | ( ) ( ) ( ) ( log ) ( ) ( ) ; (     l       x i x i x i x f x p x f N x m x f x p ) ( ) | ( ~ ) ( ) ( ) ( ) | (   42 © Eric Xing @ CMU, 2005-2014 Maximum Entropy We can approach the modeling problem from an entirely different point of view. Begin with some fixed feature expectations: Assuming expectations are consistent, there may exist many distributions which satisfy them. Which one should we select?  The most uncertain or flexible one, i.e., the one with maximum entropy. This yields a new optimization problem: i i x x f x p    ) ( ) (          x i i x x p x p x f x p x p x p x p 1 ) ( ) ( ) ( s.t. ) ( log ) ( ) ( H max  This is a variational definition of a distribution! 43 © Eric Xing @ CMU, 2005-2014 Solution to the MaxEnt Problem To solve the MaxEnt problem, we use Lagrange multipliers: So feature constraints + MaxEnt exponential family. Problem is strictly convex w.r.t. p, so solution is unique.                       x i i i x i x x p x f x p x p x p L 1 ) ( ) ( ) ( ) ( log ) (                                         i i i x x i i i i i i i i i x f Z x p x p x f e Z x f e x p x f x p x p L ) ( exp ) ( ) ( ) ) ( (since ) ( exp ) ( ) ( exp ) ( ) ( ) ( log ) ( * *           1 1 1 1 1 44 © Eric Xing @ CMU, 2005-2014 A more general MaxEnt problem             x i i x x x p x p x f x p x h x p p x h x p x p x h x p 1 ) ( ) ( ) ( s.t. ) ( log ) ( ) ( H ) ( ) ( log ) ( ) ( || ) ( KL min def           i i i x f x h Z x p ) ( exp ) ( ) ( ) (    1 45 © Eric Xing @ CMU, 2005-2014 Constraints from Data Where do the constraints i come from? Just as before, measure the empirical counts on the training data: This also ensures consistency automatically. Known as the “method of moments”. (c.f. law of large numbers) We have seen a case of convex duality:  In one case, we assume exponential family and show that ML implies model expectations must match empirical expectations.  In the other case, we assume model expectations must match empirical feature counts and show that MaxEnt implies exponential family distribution.  No duality gap yield the same value of the objective ) ( ) ( ~ ) ( ) ( x x x x i x x i N m i f p f      46 © Eric Xing @ CMU, 2005-2014 Geometric interpretation All exponential family distribution: All distributions satisfying moment constraints Pythagorean theorem                i i i x f x h Z x p x p ) ( exp ) ( ) ( ) ( : ) (    1 E           ) ( ) ( ~ ) ( ) ( : ) ( x f x p x f x p x p i x i x M       p p p q p q M M || KL || KL || KL           || KL || KL || KL s.t. || KL min : MaxEnt h p p q h q q h q M M p   M         || KL || ~ KL || ~ KL s.t. || ~ KL min : MaxLik p p p p p p q p p M M p   E 47 © Eric Xing @ CMU, 2005-2014 Summary Exponential family distribution can be viewed as the solution to an variational expression --- the maximum entropy! The max-entropy principle to parameterization offers a dual perspective to the MLE. 48 © Eric Xing @ CMU, 2005-2014 "
80,"From Logistic Regression to Neural Networks CMSC 470 Marine Carpuat Slides credit: Jacob Eisenstein The logistic function • “Softer” function than in perceptron • Can account for uncertainty • Differentiable Logistic regression: how to train? • Train based on conditional likelihood • Find parameters w that maximize conditional likelihood of all answers 𝑦𝑖given examples 𝑥𝑖 Stochastic gradient ascent (or descent) • Online training algorithm • Update weights for every training example • Move in direction given by gradient • Size of update step scaled by learning rate Gradient of the logistic function How to set the learning rate? • Various strategies • decay over time 𝛼= 1 𝐶+ 𝑡 • Use held-out test set, increase learning rate when likelihood increases Parameter Number of samples Logistic Regression for Multiclass Classification Logistic Regression: Prediction • Find y that maximizes Logistic Regression: Training • Find parameters that • maximize the conditional likelihood • of a training dataset Logistic Regression: Gradient Expected feature counts under the current model Observed feature counts Learning as optimization: Loss Functions • Loss function scores how bad a model predictions are on a training set (or on a single example) • Each of the linear models we’ve seen so far optimize a different loss function • Logistic regression minimizes the logistic loss Learning as optimization: Loss Functions • Naïve Bayes loss • Zero-one loss Learning as optimization: Loss Functions • Naïve bayes loss • can suffer infinite loss on a single example • But the optimization problem has a closed form solution • Zero-one loss • most closely related to error rate • but non-convex and not continuous • Logistic loss • Never zero: the objective can always be improved by assigning higher confidence to the correct label • Convex and continuous Regularization Some models are better then others… • Consider these 2 examples • Which of the 2 models below is better? Classifier 2 will probably generalize better! It does not include irrelevant information => Smaller model is better Regularization • Encodes a preference towards simpler models to avoid overfitting • By augmenting the loss with a penalty on adding extra weights • L2 regularization: • big penalty on large weights • small penalty on small weights • L1 regularization: • Uniform increase when large or small • Will cause many weights to become zero 𝑤2 𝑤1 What you should know about linear models • Standard supervised learning set-up for text classification • Difference between train vs. test data • How to evaluate • 3 examples of linear classifiers • Naïve Bayes, Perceptron, Logistic Regression • How to make predictions, how to train, strengths and weaknesses • Learning as optimization: loss functions and their properties • Difference between generative vs. discriminative classifiers • General machine learning concepts • Smoothing, regularization, overfitting, underfitting Neural Networks “Machines” that learn combinations of features Let’s go back to our Binary Classification Problem Given an introductory sentence in Wikipedia predict whether the article is about a person Example & figures by Graham Neubig Binary Classification with the Perceptron Making Predictions with the Perceptron sign 𝑖=1 𝐼 𝑤𝑖⋅ϕ𝑖𝑥 φ“A” = 1 φ“site” = 1 φ“,” = 2 φ“located” = 1 φ“in” = 1 φ“Maizuru”= 1 φ“Kyoto” = 1 φ“priest” = 0 φ“black” = 0 0 -3 0 0 0 0 0 2 0 -1 The Perceptron: Geometric interpretation O X O X O X Limitation of perceptron ●can only find linear separations between positive and negative examples X O O X Binary Classification with a Multi-layer Perceptron φ“A” = 1 φ“site” = 1 φ“,” = 2 φ“located” = 1 φ“in” = 1 φ“Maizuru”= 1 φ“Kyoto” = 1 φ“priest” = 0 φ“black” = 0 -1 Multi-layer Perceptrons are a kind of “Neural Network” (NN) φ“A” = 1 φ“site” = 1 φ“,” = 2 φ“located” = 1 φ“in” = 1 φ“Maizuru”= 1 φ“Kyoto” = 1 φ“priest” = 0 φ“black” = 0 -1 • Input (aka features) • Output • Nodes • Layers • Hidden layers • Activation function (non-linear) Example: binary classification with a NN ●Create two classifiers X O O X φ0(x2) = {1, 1} φ0(x1) = {-1, 1} φ0(x4) = {1, -1} φ0(x3) = {-1, -1} sign sign φ0[0] φ0[1] 1 1 1 -1 -1 -1 -1 φ0[0] φ0[1] φ1[0] φ0[0] φ0[1] 1 w0,0 b0,0 φ1[1] w0,1 b0,1 Example: binary classification with a NN ●These classifiers map to a new space X O O X φ0(x2) = {1, 1} φ0(x1) = {-1, 1} φ0(x4) = {1, -1} φ0(x3) = {-1, -1} 1 1 -1 -1 -1 -1 φ1 φ2 φ1[1] φ1[0] φ1[0] φ1[1] φ1(x1) = {-1, -1} X O φ1(x2) = {1, -1} O φ1(x3) = {-1, 1} φ1(x4) = {-1, -1} Example: binary classification with a NN X O O X φ0(x2) = {1, 1} φ0(x1) = {-1, 1} φ0(x4) = {1, -1} φ0(x3) = {-1, -1} 1 1 -1 -1 -1 -1 φ0[0] φ0[1] φ1[1] φ1[0] φ1[0] φ1[1] φ1(x1) = {-1, -1} X O φ1(x2) = {1, -1} O φ1(x3) = {-1, 1} φ1(x4) = {-1, -1} 1 1 1 φ2[0] = y Example: the Final Net tanh tanh φ0[0] φ0[1] 1 φ0[0] φ0[1] 1 1 1 -1 -1 -1 -1 1 1 1 1 tanh φ1[0] φ1[1] φ2[0] Replace “sign” with smoother non-linear function (e.g. tanh, sigmoid) "
81,"Neural Networks, Computation Graphs CMSC 470 Marine Carpuat Binary Classification with a Multi-layer Perceptron φ“A” = 1 φ“site” = 1 φ“,” = 2 φ“located” = 1 φ“in” = 1 φ“Maizuru”= 1 φ“Kyoto” = 1 φ“priest” = 0 φ“black” = 0 -1 Example: binary classification with a NN X O O X φ0(x2) = {1, 1} φ0(x1) = {-1, 1} φ0(x4) = {1, -1} φ0(x3) = {-1, -1} 1 1 -1 -1 -1 -1 φ0[0] φ0[1] φ1[1] φ1[0] φ1[0] φ1[1] φ1(x1) = {-1, -1} X O φ1(x2) = {1, -1} O φ1(x3) = {-1, 1} φ1(x4) = {-1, -1} 1 1 1 φ2[0] = y Example: the Final Net tanh tanh φ0[0] φ0[1] 1 φ0[0] φ0[1] 1 1 1 -1 -1 -1 -1 1 1 1 1 tanh φ1[0] φ1[1] φ2[0] Replace “sign” with smoother non-linear function (e.g. tanh, sigmoid) Multi-layer Perceptrons are a kind of “Neural Network” (NN) φ“A” = 1 φ“site” = 1 φ“,” = 2 φ“located” = 1 φ“in” = 1 φ“Maizuru”= 1 φ“Kyoto” = 1 φ“priest” = 0 φ“black” = 0 -1 • Input (aka features) • Output • Nodes (aka neuron) • Layers • Hidden layers • Activation function (non-linear) Neural Networks as Computation Graphs Example & figures by Philipp Koehn Computation Graphs Make Prediction Easy: Forward Propagation Computation Graphs Make Prediction Easy: Forward Propagation Neural Networks as Computation Graphs • Decomposes computation into simple operations over matrices and vectors • Forward propagation algorithm • Produces network output given an output • By traversing the computation graph in topological order Neural Networks for Multiclass Classification Multiclass Classification ●The softmax function Exact same function as in multiclass logistic regression 𝑃𝑦∣𝑥= 𝑒𝐰⋅ϕ 𝑥,𝑦 𝑦𝑒𝐰⋅ϕ 𝑥, 𝑦 Current class Sum of other classes Example: A feedforward Neural Network for 3-way Classification Sigmoid function Softmax function (as in multi-class logistic reg) From Eisenstein p66 Designing Neural Networks: Activation functions • Hidden layer can be viewed as set of hidden features • The output of the hidden layer indicates the extent to which each hidden feature is “activated” by a given input • The activation function is a non- linear function that determines range of hidden feature values Designing Neural Networks: Network structure • 2 key decisions: • Width (number of nodes per layer) • Depth (number of hidden layers) • More parameters means that the network can learn more complex functions of the input Neural Networks so far • Powerful non-linear models for classification • Predictions are made as a sequence of simple operations • matrix-vector operations • non-linear activation functions • Choices in network structure • Width and depth • Choice of activation function • Feedforward networks (no loop) • Next: how to train? Training Neural Networks How do we estimate the parameters (aka “train”) a neural net? For training, we need: • Data: (a large number of) examples paired with their correct class (x,y) • Loss/error function: quantify how bad our prediction y is compared to the truth t • Let’s use squared error: Stochastic Gradient Descent • We view the error as a function of the trainable parameters, on a given dataset • We want to find parameters that minimize the error w = 0 for I iterations for each labeled pair x, y in the data w = w −μ 𝑑error(w, x, y) 𝑑w Start with some initial parameter values Go through the training data one example at a time Take a step down the gradient Computation Graphs Make Training Easy: Computing Error Computation Graphs Make Training Easy: Computing Gradients Computation Graphs Make Training Easy: Given forward pass + derivatives for each node Computation Graphs Make Training Easy: Computing Gradients Computation Graphs Make Training Easy: Computing Gradients Computation Graphs Make Training Easy: Updating Parameters Computation Graph: A Powerful Abstraction • To build a system, we only need to: • Define network structure • Define loss • Provide data • (and set a few more hyperparameters to control training) • Given network structure • Prediction is done by forward pass through graph (forward propagation) • Training is done by backward pass through graph (back propagation) • Based on simple matrix vector operations • Forms the basis of neural network libraries • Tensorflow, Pytorch, mxnet, etc. Neural Networks • Powerful non-linear models for classification • Predictions are made as a sequence of simple operations • matrix-vector operations • non-linear activation functions • Choices in network structure • Width and depth • Choice of activation function • Feedforward networks (no loop) • Training with the back-propagation algorithm • Requires defining a loss/error function • Gradient descent + chain rule • Easy to implement on top of computation graphs "
82,"N-gram Language Models CMSC 470 Marine Carpuat Slides credit: Jurasky & Martin Roadmap • Language Models • Our first example of modeling sequences • n-gram language models • How to estimate them? • How to evaluate them? • Neural models Probabilistic Language Models • Goal: assign a probability to a sentence • Why? • Machine Translation: • P(high winds tonite) > P(large winds tonite) • Spell Correction • The office is about fifteen minuets from my house • P(about fifteen minutes from) > P(about fifteen minuets from) • Speech Recognition • P(I saw a van) >> P(eyes awe of an) • + Summarization, question-answering, etc., etc.!! Probabilistic Language Modeling • Goal: compute the probability of a sentence or sequence of words P(W) = P(w1,w2,w3,w4,w5…wn) • Related task: probability of an upcoming word P(w5|w1,w2,w3,w4) • A model that computes either of these: P(W) or P(wn|w1,w2…wn-1) is called a language model. How to compute P(W) • How to compute this joint probability: • P(its, water, is, so, transparent, that) • Intuition: let’s rely on the Chain Rule of Probability • George Kingsley Zipf (1902-1950) observed the following relation between frequency and rank • Example • the 50th most common word should occur three times more often than the 150th most common word Recall: Zipf’s Law c r f   or r c f  f = frequency r = rank c = constant Recall: Zipf’s Law Graph illustrating Zipf’s Law for the Brown corpus from Manning and Shütze Reminder: The Chain Rule • Recall the definition of conditional probabilities p(B|A) = P(A,B)/P(A) Rewriting: P(A,B) = P(A)P(B|A) • More variables: P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C) • The Chain Rule in General P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1) The Chain Rule applied to compute joint probability of words in sentence P(“its water is so transparent”) = P(its) × P(water|its) × P(is|its water) × P(so|its water is) × P(transparent|its water is so) P(w1w2… wn) = P(wi | w1w2… wi-1) i Õ … … How to estimate these probabilities • Could we just count and divide? • No! Too many possible sentences! • We’ll never see enough data for estimating these P(the |its water is so transparent that) = Count(its water is so transparent that the) Count(its water is so transparent that) Markov Assumption • Simplifying assumption: • Or maybe P(the |its water is so transparent that) » P(the |that) P(the |its water is so transparent that) » P(the |transparent that) Andrei Markov Markov Assumption • In other words, we approximate each component in the product P(w1w2… wn) » P(wi | wi-k… wi-1) i Õ P(wi | w1w2… wi-1) » P(wi | wi-k… wi-1) … … … … Unigram model (1-gram) fifth, an, of, futures, the, an, incorporated, a, a, the, inflation, most, dollars, quarter, in, is, mass thrift, did, eighty, said, hard, 'm, july, bullish that, or, limited, the Some automatically generated sentences from a unigram model P(w1w2… wn) » P(wi) i Õ … Condition on the previous word: Bigram model (2-gram) texaco, rose, one, in, this, issue, is, pursuing, growth, in, a, boiler, house, said, mr., gurria, mexico, 's, motion, control, proposal, without, permission, from, five, hundred, fifty, five, yen outside, new, car, parking, lot, of, the, agreement, reached this, would, be, a, record, november P(wi | w1w2… wi-1) » P(wi | wi-1) … N-gram models • We can extend to 3-grams (“trigrams”), 4-grams, 5-grams • In general this is an insufficient model of language • because language has long-distance dependencies: “The computer which I had just put into the machine room on the ground floor crashed.” • But we can often get away with N-gram models Estimating bigram probabilities • The Maximum Likelihood Estimate P(wi | wi-1) = count(wi-1,wi) count(wi-1) P(wi | wi-1) = c(wi-1,wi) c(wi-1) Example 1: Estimating bigram probabilities on toy corpus <s> I am Sam </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s> P(wi | wi-1) = c(wi-1,wi) c(wi-1) Example 2: Estimating bigram probabilities on Berkeley Restaurant Project sentences 9222 sentences in total Examples • can you tell me about any good cantonese restaurants close by • mid priced thai food is what i’m looking for • tell me about chez panisse • can you give me a listing of the kinds of food that are available • i’m looking for a good place to eat breakfast • when is caffe venezia open during the day Raw bigram counts • Out of 9222 sentences Raw bigram probabilities • Normalize by unigrams: • Result: Using bigram model to compute sentence probabilities P(<s> I want english food </s>) = P(I|<s>) × P(want|I) × P(english|want) × P(food|english) × P(</s>|food) = .000031 What kinds of knowledge? • P(english|want) = .0011 • P(chinese|want) = .0065 • P(to|want) = .66 • P(eat | to) = .28 • P(food | to) = 0 • P(want | spend) = 0 • P (i | <s>) = .25 Google N-Gram Release, August 2006 … Google N-Gram Release • serve as the incoming 92 • serve as the incubator 99 • serve as the independent 794 • serve as the index 223 • serve as the indication 72 • serve as the indicator 120 • serve as the indicators 45 • serve as the indispensable 111 • serve as the indispensible 40 • serve as the individual 234 http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html Problem: Zeros • Training set: … denied the allegations … denied the reports … denied the claims … denied the request P(“offer” | denied the) = 0 • Test set … denied the offer … denied the loan Smoothing: the intuition • When we have sparse statistics: • Steal probability mass to generalize better P(w | denied the) 3 allegations 2 reports 1 claims 1 request 7 total P(w | denied the) 2.5 allegations 1.5 reports 0.5 claims 0.5 request 2 other 7 total allegations reports claims attack request man outcome … allegations attack man outcome … allegations reports claims request From Dan Klein Add-one estimation • Also called Laplace smoothing • Pretend we saw each word one more time than we did (i.e. just add one to all the counts) • MLE estimate: • Add-1 estimate: P MLE(wi | wi-1) = c(wi-1,wi) c(wi-1) P Add-1(wi | wi-1) = c(wi-1,wi)+1 c(wi-1)+V Berkeley Restaurant Corpus: Laplace smoothed bigram counts Laplace-smoothed bigrams Reconstituted counts Reconstituted vs. raw bigram counts Add-1 estimation is a blunt instrument • So add-1 isn’t used for N-grams • Typically use back-off and interpolation instead • But add-1 is used to smooth other NLP models • E.g., Naïve Bayes for text classification • in domains where the number of zeros isn’t so huge. Backoff • Sometimes it helps to use less context • Condition on less context for contexts you haven’t learned much about • Backoff: • use trigram if you have good evidence, • otherwise bigram, otherwise unigram Smoothing for web-scale N-grams • “Stupid backoff” (Brants et al. 2007) • No discounting, just use relative frequencies S(wi | wi-k+1 i-1 ) = count(wi-k+1 i ) count(wi-k+1 i-1 ) if count(wi-k+1 i ) > 0 0.4S(wi | wi-k+2 i-1 ) otherwise ì í ï ï î ï ï S(wi) = count(wi) N Unknown words: Open vocabulary vs. closed vocabulary tasks • If we know all the words in advanced • Vocabulary V is fixed • Closed vocabulary task • Often we don’t know this • Out Of Vocabulary = OOV words • Open vocabulary task Unknown words: Open vocabulary model with UNK token • Define an unknown word token <UNK> • Training of <UNK> probabilities • Create a fixed lexicon L of size V • Any training word not in L changed to <UNK> • Train language model probabilities as if <UNK> were a normal word • At decoding time • Use <UNK> probabilities for any word not in training Language Modeling Toolkits • SRILM • http://www.speech.sri.com/projects/srilm/ • KenLM • https://kheafield.com/code/kenlm/ Roadmap • Language Models • Our first example of modeling sequences • n-gram language models • How to estimate them? • How to evaluate them? • Neural models "
83,"Language Models (2) CMSC 470 Marine Carpuat Slides credit: Jurasky & Martin Roadmap • Language Models • Our first example of modeling sequences • n-gram language models • How to estimate them? • How to evaluate them? • Neural models Pros and cons of n-gram models • Really easy to build, can train on billions and billions of words • Smoothing helps generalize to new data • Only work well for word prediction if the test corpus looks like the training corpus • Only capture short distance context Evaluation: How good is our model? • Does our language model prefer good sentences to bad ones? • Assign higher probability to “real” or “frequently observed” sentences • Than “ungrammatical” or “rarely observed” sentences? • Extrinsic vs intrinsic evaluation Intrinsic evaluation: intuition • The Shannon Game: • How well can we predict the next word? • Unigrams are terrible at this game. (Why?) • A better model of a text assigns a higher probability to the word that actually occurs I always order pizza with cheese and ____ The 33rd President of the US was ____ I saw a ____ mushrooms 0.1 pepperoni 0.1 anchovies 0.01 …. fried rice 0.0001 …. and 1e-100 Intrinsic evaluation metric: perplexity Perplexity is the inverse probability of the test set, normalized by the number of words: Chain rule: For bigrams: Minimizing perplexity is the same as maximizing probability The best language model is one that best predicts an unseen test set • Gives the highest P(sentence) PP(W) = P(w1w2...wN ) - 1 N = 1 P(w1w2...wN ) N Perplexity as branching factor • Let’s suppose a sentence consisting of random digits • What is the perplexity of this sentence according to a model that assign P=1/10 to each digit? Lower perplexity = better model • Training 38 million words, test 1.5 million words, WSJ N-gram Order Unigram Bigram Trigram Perplexity 962 170 109 The perils of overfitting • N-grams only work well for word prediction if the test corpus looks like the training corpus • In real life, it often doesn’t! • We need to train robust models that generalize • Smoothing is important • Choose n carefully Roadmap • Language Models • Our first example of modeling sequences • n-gram language models • How to estimate them? • How to evaluate them? • Neural models Toward a Neural Language Model Figures by Philipp Koehn (JHU) Representing Words • “one hot vector” dog = [ 0, 0, 0, 0, 1, 0, 0, 0 …] cat = [ 0, 0, 0, 0, 0, 0, 1, 0 …] eat = [ 0, 1, 0, 0, 0, 0, 0, 0 …] • That’s a large vector! practical solutions: • limit to most frequent words (e.g., top 20000) • cluster words into classes • break up rare words into subword units Language Modeling with Feedforward Neural Networks Map each word into a lower-dimensional real-valued space using shared weight matrix Embedding layer Bengio et al. 2003 Example: Prediction with a Feedforward LM Example: Prediction with a Feedforward LM Note: bias omitted in figure Estimating Model Parameters • Intuition: a model is good if it gives high probability to existing word sequences • Training examples: • sequences of words in the language of interest • Error/loss: negative log likelihood • At the corpus level error 𝜆= − 𝐸in corpus log 𝑃λ(𝐸) • At the word level error 𝜆= −log 𝑃λ(𝑒𝑡|𝑒1 … 𝑒𝑡−1) Example: Parameter Estimation Loss function at each position t Parameter update rule Word Embeddings: a useful by-product of neural LMs • Words that occurs in similar contexts tend to have similar embeddings • Embeddings capture many usage regularities • Useful features for many NLP tasks Word Embeddings Word Embeddings Word Embeddings Capture Useful Regularities Morpho-Syntactic • Adjectives: base form vs. comparative • Nouns: singular vs. plural • Verbs: present tense vs. past tense [Mikolov et al. 2013] Semantic • Word similarity/relatedness • Semantic relations • But tends to fail at distinguishing • Synonyms vs. antonyms • Multiple senses of a word Language Modeling with Feedforward Neural Networks Bengio et al. 2003 Count-based n-gram models vs. feedforward neural networks • Pros of feedforward neural LM • Word embeddings capture generalizations across word typesq • Cons of feedforward neural LM • Closed vocabulary • Training/testing is more computationally expensive • Weaknesses of both types of model • Only work well for word prediction if the test corpus looks like the training corpus • Only capture short distance context Roadmap • Language Models • Our first example of modeling sequences • n-gram language models • How to estimate them? • How to evaluate them? • Neural models • Feedfworward neural networks • Recurrent neural networks "
84,"Dense Word Embeddings CMSC 470 Marine Carpuat Slides credit: Jurasky & Martin How to generate vector embeddings? One approach: feedforward neural language models Training a neural language model just to get word embeddings is expensive! Is there a faster/cheaper way to get word embeddings if we don’t need the language model? Roadmap • Dense vs. sparse word embeddings • Generating word embeddings with Word2vec • Skip-gram model • Training • Evaluating word embeddings • Word similarity • Word relations • Analysis of biases Word embedding methods we’ve seen so far yield sparse representations tf-idf and PPMI vectors are •long (length |V|= 20,000 to 50,000) •sparse (most elements are zero) Alternative: dense vectors vectors which are • short (length 50-1000) • dense (most elements are non-zero) 5 Why short dense vectors? • Short vectors may be easier to use as features in machine learning (fewer weights to tune) • Dense vectors may generalize better than storing explicit counts • They may do better at capturing synonymy: • car and automobile are synonyms; but are distinct dimensions • a word with car as a neighbor and a word with automobile as a neighbor should be similar, but aren't • In practice, they work better 6 Dense embeddings you can download! Word2vec https://code.google.com/archive/p/word2vec/ Fasttext http://www.fasttext.cc/ Glove http://nlp.stanford.edu/projects/glove/ Word2vec •Popular embedding method •Very fast to train •Code available on the web •Key idea: predict rather than count Word2vec Approach: •Instead of counting how often each word w occurs near ""apricot“ •Train a classifier on a binary prediction task: Is w likely to show up near ""apricot""? Note: we don’t actually care about this task! But we'll take the learned classifier weights as the word embeddings Insight: running text provides implicitly supervised training data! • A word s near apricot • Acts as gold ‘correct answer’ to the question • “Is word w likely to show up near apricot?” • No need for hand-labeled supervision • The idea comes from neural language modeling • Bengio et al. (2003) • Collobert et al. (2011) Word2Vec: Skip-Gram Task • Word2vec provides a variety of options. Let's do • ""skip-gram with negative sampling"" (SGNS) Skip-gram algorithm 1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples 3. Use logistic regression to train a classifier to distinguish those two cases 4. Use the weights as the embeddings Skip-Gram Task •Given a tuple (t,c) = target, context (apricot, jam) (apricot, aardvark) •Return probability that c is a real context word: •P(+|t,c) •P(−|t,c) = 1−P(+|t,c) Skip-Gram Training Data • Assume context words are those in +/- 2 word window • Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 target c3 c4 How to compute p(+|t,c)? • Intuition: • Words are likely to appear near similar words • Model similarity with dot-product! • Similarity(t,c) ∝t ∙ c • Problem: • Dot product is not a probability! • (Neither is cosine) Turning dot product into a probability • The sigmoid lies between 0 and 1: Turning dot product into a probability This is a logistic regression model! For all the context words: • Assume all context words are independent Skip-Gram Training Data • Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 • Training data: input/output pairs centering on apricot • Asssume a +/- 2 word window Skip-Gram Training • Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 •For each positive example, we'll create k negative examples. •Using noise words •Any random word that isn't t Skip-Gram Training • Training sentence: ... lemon, a tablespoon of apricot jam a pinch ... c1 c2 t c3 c4 k=2 Choosing noise words • Could pick w according to their unigram frequency P(w) • More common to chosen then according to pα(w) • α= ¾ works well because it gives rare noise words slightly higher probability • imagine two events p(a)=.99 and p(b) = .01: Skip-gram: training set-up • Let's represent words as vectors of some length (say 300), randomly initialized. • So we start with 300 * V random parameters and use gradient descent to update these parameters • We need to define a loss function / training objective Skip-gram: training objective • Motivation: Over the entire training set, we’d like to adjust those word vectors such that we • Maximize the similarity of the positive target word, context word pairs (t,c) • Minimize the similarity of the negative (t,c) pairs • Objective: we want to maximize • Maximize the + label for the pairs from the positive training data, and the – label for the pairs sample from the negative data. Skip-gram: training objective • Focusing on one target word t Skip-gram illustrated Summary: How to learn word2vec (skip-gram) embeddings • Choose the embedding dimension, e.g., d=300 • Start with V random 300-dimensional vectors as initial embeddings • Take a corpus and take pairs of words that co-occur as positive examples • Construct negative examples • Train a logistic regression classifier to distinguish positive from negative examples • Throw away the classifier and keep the embeddings! Evaluating embeddings • We can use the same evaluations as for other distributional semantic models (see lecture 2) • Compare to human scores on word similarity-type tasks: • WordSim-353 (Finkelstein et al., 2002) • SimLex-999 (Hill et al., 2015) • Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) • TOEFL dataset: Levied is closest in meaning to: imposed, believed, requested, correlated Analogy: Embeddings capture relational meaning! vector(‘king’) - vector(‘man’) + vector(‘woman’) ≈vector(‘queen’) vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) ≈vector(‘Rome’) Word embeddings are a very useful tool • Can be used as features in classifiers • Capture generalizations across word types • Can be used to analyze language usage patterns in large corpora • E.g., to study change in word meaning 1900 1950 2000 vs. Word vectors for 1920 Word vectors 1990 “dog” 1920 word vector “dog” 1990 word vector Yet word embeddings are not perfect models of word meaning • Limitations include • One vector per word (even if the word has multiple senses) • Cosine similarity not sufficient to distinguish antonyms from synonyms • Embeddings reflect cultural bias implicit in training text Embeddings reflect cultural bias • Ask “Paris : France :: Tokyo : x” • x = Japan • Ask “father : doctor :: mother : x” • x = nurse • Ask “man : computer programmer :: woman : x” • x = homemaker Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. ""Man is to computer programmer as woman is to homemaker? debiasing word embeddings."" In Advances in Neural Information Processing Systems, pp. 4349-4357. 2016. Embeddings reflect cultural bias • Implicit Association test (Greenwald et al 1998): How associated are • concepts (flowers, insects) & attributes (pleasantness, unpleasantness)? • Studied by measuring timing latencies for categorization. • Psychological findings on US participants: • African-American names are associated with unpleasant words (more than European-American names) • Male names associated more with math, female names with arts • Old people's names with unpleasant words, young people with pleasant words. • Caliskan et al. replication with embeddings: • African-American names had a higher cosine with unpleasant words • European American names had a higher cosine with pleasant words • Embeddings reflect and replicate all sorts of pernicious biases. Caliskan, Aylin, Joanna J. Bruson and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science 356:6334, 183-186. So what can we do about bias? • Attempt to remove or decrease bias by “debiasing” for embeddings • Bolukbasi, Tolga, Chang, Kai-Wei, Zou, James Y., Saligrama, Venkatesh, and Kalai, Adam T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Infor- mation Processing Systems, pp. 4349–4357. • Use embeddings as a historical tool to study bias • Garg, Nikhil, Schiebinger, Londa, Jurafsky, Dan, and Zou, James (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635–E3644 Roadmap • Dense vs. sparse word embeddings • Generating word embeddings with Word2vec • Skip-gram model • Training • Evaluating word embeddings • Word similarity • Word relations • Analysis of biases "
85,"Recurrent Language Models CMSC 470 Marine Carpuat Toward a Neural Language Model Figures by Philipp Koehn (JHU) Count-based n-gram models vs. feedforward neural networks • Pros of feedforward neural LM • Word embeddings capture generalizations across word typesq • Cons of feedforward neural LM • Closed vocabulary • Training/testing is more computationally expensive • Weaknesses of both types of model • Only work well for word prediction if the test corpus looks like the training corpus • Only capture short distance context Language Modeling with Recurrent Neural Networks Figure by Philipp Koehn Recurrent Neural Networks (RNN) The hidden layer includes a recurrent connection as part of its input Unrolling the RNN over the time sequence as a feed-forward network Figures from Jurafsky & Martin The hidden layer from the previous time step plays the role of memory, remembering earlier context Unrolled RNN illustrated weights U, V, W are shared across all timesteps Prediction/Inference with RNNs For language modeling, f = softmax function to provide normalized probability distribution over possible output classes Training RNNs with backpropagation • Training goal: estimate parameter values for U, V, W • Use same loss as for feedforward language models • Given unrolled network, run forward and backpropagation algorithms as usual Training RNNs with backpropagation Practical Training Issues: minibatch • Compute parameter updates based on a “minibatch” of examples • instead of using one example at a time • More efficient • matrix-matrix operations as opposed to multiple matrix-vector operations) • Can lead to better model parameters • middle ground between online and batch training Figure by Graham Neubig Practical Training Issues: vanishing/exploding gradients Figure by Graham Neubig Multiple ways to work around this problem: - ReLU activations help - Dedicated RNN architecture (Long Short Term Memory Networks) Aside: Long Short Term Memory Networks What do Recurrent Language Models Learn? Figure from Karpathy 2015 What do Recurrent Language Models Learn? Figure from Karpathy 2015 What do Recurrent Language Models Learn? • Parameters are hard to interpret, so we can gain insights by analyzing their output behavior instead • Can capture (some) long-distance dependencies After much economic progress over the years, the country has… The country, which has made much economic progress over the years, still has… Recurrent neural network language models • Have all the strengths of feedforward language model • And do a better job at modeling long distance context • However • Training is trickier due to vanishing/exploding gradients • Performance on test sets is still sensitive to distance from training data "
86,"POS Tagging & Sequence Labeling Tasks CMSC 470 Marine Carpuat Parts of Speech • “Equivalence class” of linguistic entities • “Categories” or “types” of words • Study dates back to the ancient Greeks • Dionysius Thrax of Alexandria (c. 100 BC) • 8 parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, article • Remarkably enduring list! 3 How can we define POS? • By meaning? • Verbs are actions • Adjectives are properties • Nouns are things • By the syntactic environment • What occurs nearby? • What does it act as? • By what morphological processes affect it • What affixes does it take? • Typically combination of syntactic+morphology Parts of Speech • Open class • Impossible to completely enumerate • New words continuously being invented, borrowed, etc. • Closed class • Closed, fixed membership • Reasonably easy to enumerate • Generally, short function words that “structure” sentences Open Class POS • Four major open classes in English • Nouns • Verbs • Adjectives • Adverbs • All languages have nouns and verbs... but may not have the other two Nouns • Open class • New inventions all the time: muggle, webinar, ... • Semantics: • Generally, words for people, places, things • But not always (bandwidth, energy, ...) • Syntactic environment: • Occurring with determiners • Pluralizable, possessivizable • Other characteristics: • Mass vs. count nouns Verbs • Open class • New inventions all the time: google, tweet, ... • Semantics • Generally, denote actions, processes, etc. • Syntactic environment • E.g., Intransitive, transitive • Other characteristics • Main vs. auxiliary verbs • Gerunds (verbs behaving like nouns) • Participles (verbs behaving like adjectives) Adjectives and Adverbs • Adjectives • Generally modify nouns, e.g., tall building • Adverbs • A semantic and formal hodge-podge… • Sometimes modify verbs, e.g., sang beautifully • Sometimes modify adjectives, e.g., extremely cold Closed Class POS • Prepositions • In English, occurring before noun phrases • Specifying some type of relation (spatial, temporal, …) • Examples: on the shelf, before noon • Particles • Resembles a preposition, but used with a verb (“phrasal verbs”) • Examples: find out, turn over, go on Particle vs. Prepositions He came by the office in a hurry He came by his fortune honestly We ran up the phone bill We ran up the small hill He lived down the block He never lived down the nicknames (by = preposition) (by = particle) (up = particle) (up = preposition) (down = preposition) (down = particle) More Closed Class POS • Determiners • Establish reference for a noun • Examples: a, an, the (articles), that, this, many, such, … • Pronouns • Refer to person or entities: he, she, it • Possessive pronouns: his, her, its • Wh-pronouns: what, who Closed Class POS: Conjunctions • Coordinating conjunctions • Join two elements of “equal status” • Examples: cats and dogs, salad or soup • Subordinating conjunctions • Join two elements of “unequal status” • Examples: We’ll leave after you finish eating. While I was waiting in line, I saw my friend. • Complementizers are a special case: I think that you should finish your assignment Beyond English… Chinese No verb/adjective distinction! Riau Indonesian/Malay No Articles No Tense Marking 3rd person pronouns neutral to both gender and number No features distinguishing verbs from nouns 漂亮: beautiful/to be beautiful Ayam (chicken) Makan (eat) The chicken is eating The chicken ate The chicken will eat The chicken is being eaten Where the chicken is eating How the chicken is eating Somebody is eating the chicken The chicken that is eating POS TAGGING POS Tagging: What’s the task? • Process of assigning part-of-speech tags to words • But what tags are we going to assign? • Coarse grained: noun, verb, adjective, adverb, … • Fine grained: {proper, common} noun • Even finer-grained: {proper, common} noun animate • Important issues to remember • Choice of tags encodes certain distinctions/non-distinctions • Tagsets will differ across languages! • For English, Penn Treebank is the most common tagset Penn Treebank Tagset: 45 Tags https://web.stanford.edu/~jurafsky/slp3/8.pdf Penn Treebank Tagset: Choices • Example: • The/DT grand/JJ jury/NN commmented/VBD on/IN a/DT number/NN of/IN other/JJ topics/NNS ./. • Distinctions and non-distinctions • Prepositions and subordinating conjunctions are tagged “IN” (“Although/IN I/PRP..”) • Except the preposition/complementizer “to” is tagged “TO” Why do POS tagging? • One of the most basic NLP tasks • Nicely illustrates principles of statistical NLP • Useful for higher-level analysis • Needed for syntactic analysis • Needed for semantic analysis • Sample applications that require POS tagging • Machine translation • Information extraction • Lots more… Try your hand at tagging… • The back door • On my back • Win the voters back • Promised to back the bill Try your hand at tagging… • I hope that she wins • That day was nice • You can go that far Why is POS tagging hard? • Ambiguity! • Ambiguity in English • 11.5% of word types ambiguous in Brown corpus • 40% of word tokens ambiguous in Brown corpus • Annotator disagreement in Penn Treebank: 3.5% POS tagging: how to do it? • Given Penn Treebank, how would you build a system that can POS tag new text? • Baseline: pick most frequent tag for each word type • 90% accuracy if train+test sets are drawn from Penn Treebank • How can we do better? We can view POS tagging as a classification task POS tagging Sequence labeling with the perceptron Sequence labeling problem • Input: • sequence of tokens x = [x1 … xL] • Variable length L • Output (aka label): • sequence of tags y = [y1 … yL] • # tags = K • Size of output space? Structured Perceptron • Perceptron algorithm can be used for sequence labeling • But there are challenges • How to compute argmax efficiently? • What are appropriate features? • Approach: leverage structure of output space Feature functions for sequence labeling • Example features? • Number of times “monsters” is tagged as noun • Number of times noun is followed by verb • Number of times tasty as tagged as verb • Number of times two verbs are adjacent • … Example from CIML chapter 17 Feature functions for sequence labeling • Standard features of POS tagging • Unary features: # times word w has been labeled with tag l for all words w and all tags l • Markov features: # times tag l is adjacent to tag l’ in output for all tags l and l’ • Size of feature representation is constant wrt input length Example from CIML chapter 17 Solving the argmax problem for sequences • Trellis sequence labeling • Any path represents a labeling of input sentence • Gold standard path in red • Each edge receives a weight such that adding weights along the path corresponds to score for input/ouput configuration • Any max-weight path algorithm can find the argmax • e.g. Viterbi algorithm O(LK2) Solving the argmax problem for sequences with dynamic programming • Efficient algorithms possible if the feature function decomposes over the input • This holds for unary and markov features used for POS tagging POS tagging • An example of sequence labeling tasks • Requires a predefined set of POS tags • Penn Treebank commonly used for English • Encodes some distinctions and not others • Given annotated examples, we can address sequence labeling with multiclass perceptron • but computing the argmax naively is expensive • constraints on the feature definition make efficient algorithms possible "
87,"Sequence Labeling with the Structured Perceptron CMSC 470 Marine Carpuat POS tagging Sequence labeling with the perceptron Sequence labeling problem • Input: • sequence of tokens x = [x1 … xL] • Variable length L • Output (aka label): • sequence of tags y = [y1 … yL] • # tags = K • Size of output space? Structured Perceptron • Perceptron algorithm can be used for sequence labeling • But there are challenges • How to compute argmax efficiently? • What are appropriate features? • Approach: leverage structure of output space Perceptron algorithm remains the same as for multiclass classification Note: CIML denotes - the weight vector as 𝑤instead of 𝜃 - The feature function as Φ(𝑥, 𝑦) instead of 𝑓(𝑥, 𝑦) Feature functions for sequence labeling • Standard features of POS tagging • Unary features: # times word w has been labeled with tag l for all words w and all tags l • Markov features: # times tag l is adjacent to tag l’ in output for all tags l and l’ • Size of feature representation is constant wrt input length Example from CIML chapter 17 Solving the argmax problem for sequences with dynamic programming • Efficient algorithms possible if the feature function decomposes over the input • This holds for unary and markov features used for POS tagging Decomposition of structure • Features decompose over the input if • If features decompose over the input, structures (x,y) can be scored incrementally Feature function that only includes features about position l Decomposition of structure: Lattice/trellis representation • Trellis sequence labeling • Any path represents a labeling of input sentence • Gold standard path in red • Each edge receives a weight such that adding weights along the path corresponds to score for input/ouput configuration • Any max-weight path algorithm can find the argmax • We’ll describe the Viterbi algorithm Dynamic programming solution relies on recursively computing prefix scores 𝛼𝑙,𝑘 Score of best possible output prefix, up to and including position l, that labels the l-th word as label k Sequence of labels of length l-1 Sequence of length l obtained by adding k at the end. Features for sequence starting at position 1 up to and including position l Computing prefix scores 𝛼𝑙,𝑘 Example Let’s compute 𝛼3,𝐴given • Prefix scores for length 2 𝛼2,𝑁= 2, 𝛼2,𝑉= 9, 𝛼2,𝐴= −1 • Unary feature weights 𝑤𝑡𝑎𝑠𝑡𝑦/𝐴= 1.2 • Markov feature weights 𝑤𝑁,𝐴= −5, 𝑤𝑉,𝐴= 2.5, 𝑤𝐴,𝐴= 2.2 Dynamic programming solution relies on recursively computing prefix scores 𝛼𝑙,𝑘 Derivation on board + CIML ch17 Backpointer to the label that achieves the above maximum Score of best possible output prefix, up to and including position l+1, that labels the (l+1)-th word as label k Viterbi algorithm Assumptions: - Unary features - Markov features based on 2 adjacent labels Runtime: 𝑂(𝐿𝐾2) Exercise: Impact of feature definitions • Consider a structured perceptron with the following features • # times word w has been labeled with tag l for all words w and all tags l • # times word w has been labeled with tag l when it follows word w’ for all words w, w’ and all tags l • # times tag l occurs in the sequence (l’,l’’,l) in the output for all tags l, l’, l’’ • What is the dimension of the perceptron weight vector? • Can we use dynamic programming to compute the argmax? Recap: POS tagging • An example of sequence labeling tasks • Requires a predefined set of POS tags • Penn Treebank commonly used for English • Encodes some distinctions and not others • Given annotated examples, we can address sequence labeling with multiclass perceptron • but computing the argmax naively is expensive • constraints on the feature definition make efficient algorithms possible • Viterbi algorithm for unary and markov features "
88,"Sequence Labeling: more tasks, more methods CMSC 470 Marine Carpuat Recap: We know how to perform POS tagging with structured perceptron • An example of sequence labeling tasks • Requires a predefined set of POS tags • Penn Treebank commonly used for English • Encodes some distinctions and not others • Given annotated examples, we can address sequence labeling with multiclass perceptron • but computing the argmax naively is expensive • constraints on the feature definition make efficient algorithms possible • Viterbi algorithm for unary and markov features Sequence labeling tasks Beyond POS tagging Many NLP tasks can be framed as sequence labeling • Information Extraction: detecting named entities • E.g., names of people, organizations, locations “Brendan Iribe, a co-founder of Oculus VR and a prominent University of Maryland donor, is leaving Facebook four years after it purchased his company.” http://www.dbknews.com/2018/10/24/brendan-iribe-facebook-leaves-oculus-vr-umd-computer- science/ Many NLP tasks can be framed as sequence labeling x = [Brendan, Iribe, “,”, a, co-founder, of, Oculus, VR, and, a, prominent, University, of, Maryland, donor, “,”, is, leaving, Facebook, four, years, after, it, purchased, his, company, “.”] y = [B-PER, I-PER, O, O, O, O, B-ORG, I-ORG, O, O, O,B-ORG, I-ORG, I- ORG, O, O, O,B-ORG, O, O, O, O, O, O, O, O] “BIO” labeling scheme for named entity recognition Many NLP tasks can be framed as sequence labeling • The same kind of BIO scheme can be used to tag other spans of text • Syntactic analysis: detecting noun phrase and verb phrases • Semantic roles: detecting semantic roles (who did what to whom) Many NLP tasks can be framed as sequence labeling • Other sequence labeling tasks • Language identification in code-switched text “Ulikuwa ukiongea a lot of nonsense.” (Swahili/English) • Metaphor detection “he swam in a sea of diamonds” “authority is a chair, it needs legs to stand” “in Washington, people change dance partners frequently, but not the dance” • … Other algorithms for solving the argmax problem Structured perceptron can be used for other structures than sequences • The Viterbi algorithm we’ve seen is specific to sequences • Other argmax algorithms necessary for other structures (e.g. trees) • Integer Linear Programming provides a general framework for solving the argmax problem Argmax problem as an Integer Linear Program • An integer linear program (ILP) is an optimization problem of the form • For a fixed vector a • Example of integer constraint: • Well-engineered solvers exist • e.g, Gurobi • Useful for prototyping • But general not as efficient as dynamic programming Casting sequence labeling with Markov features as an ILP • Step 1: Define variables z as binary indicator variables which encode an output sequence y • Step 2: Construct the linear objective function Casting sequence labeling with Markov features as an ILP • Step 3: Define constraints to ensure a well-formed solution • Z’s should be binary: for all l, k’, k • For a given position l, there is exactly one active z • The z’s are internally consistent Loss-augmented structured prediction In default structured perceptron, all bad output sequences are equally bad • Consider • 𝑦1 = 𝐴, 𝐴, 𝐴, 𝐴 • 𝑦2 = [𝑁, 𝑉, 𝑁, 𝑁] • With 0-1 loss 𝑙0−1 (𝑦, 𝑦1) = 𝑙0−1 𝑦, 𝑦2 = 1 • An alternative • Hamming Loss gives a more nuanced evaluation of output than 0–1 loss Loss functions for structured prediction • Recall learning as optimization for multiclass classification • e.g., • Let’s define a structure-aware optimization objective • e.g., Structured hinge loss • 0 if true output beats score of every imposter output • Otherwise: scales linearly as function of score diff between most confusing imposter and true output Optimization: stochastic subgradient descent • Subgradients of structured hinge loss? Optimization: stochastic subgradient descent • subgradients of structured hinge loss Optimization: stochastic subgradient descent Resulting training algorithm Only 2 differences compared to structured perceptron! Loss-augmented inference/search Recall dynamic programming solution without Hamming loss Loss-augmented inference/search Dynamic programming with Hamming loss We can use Viterbi algorithm as before as long as the loss function decomposes over the input consistently with features! Sequence labeling • Structured perceptron • A general algorithm for structured prediction problems such as sequence labeling • The Argmax problem • Efficient argmax for sequences with Viterbi algorithm, given some assumptions on feature structure • A more general solution: Integer Linear Programming • Loss-augmented structured prediction • Training algorithm • Loss-augmented argmax "
89,"Machine Translation History & Evaluation CMSC 470 Marine Carpuat T oday’s topics Machine Translation • Context: Historical Background • Machine Translation is an old idea • Machine Translation Evaluation 1947 When I look at an article in Russian, I say to myself: This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode. Warren Weaver 1950s-1960s • 1954 Georgetown-IBM experiment • 250 words, 6 grammar rules • 1966 ALPAC report • Skeptical in research progress • Led to decreased US government funding for MT Rule based systems • Approach • Build dictionaries • Write transformation rules • Refine, refine, refine • Meteo system for weather forecasts (1976) • Systran (1968), … 1988 More about the IBM story: 20 years of bitext workshop Statistical Machine Translation • 1990s: increased research • Mid 2000s: phrase-based MT • (Moses, Google Translate) • Around 2010: commercial viability • Since mid 2010s: neural network models MT History: Hype vs. Reality How Good is Machine Translation Today? March 14 2018: “Microsoft reaches a historic milestone, using AI to match human performance in translating news from Chinese to English” https://techcrunch.com/2018/03/14/mi crosoft-announces-breakthrough-in- chinese-to-english-machine-translation/ But also https://www.haaretz.com/israel-news/palestinian-arrested-over-mistranslated-good- morning-facebook-post-1.5459427 How Good is Machine Translation T oday? Output of Research Systems at WMT18 上周，古装剧《美人私房菜》 临时停播，意外引发了关于国 产剧收视率造假的热烈讨论。 Last week, the vintage drama ""Beauty private dishes"" was temporarily suspended, accidentally sparking a heated discussion about the fake ratings of domestic dramas. 民权团体针对密苏里州发出旅 行警告 Civil rights groups issue travel warnings against Missouri http://matrix.statmt.org The Vauquois Triangle Challenges: word translation ambiguity • What is the best translation? • Solution intuition: use counts in parallel corpus (aka bitext) • Here European Parliament corpus Challenges: word order • Problem: different languages organize words in different order to express the same idea En: The red house Fr: La maison rouge • Solution intuition: language modeling! Challenges: output language fluency • What is most fluent? • Solution intuition: a language modeling problem! Word Alignment Phrase-based Models • Input segmented in phrases • Each phrase is translated in output language • Phrases are reordered Neural MT T oday’s topics Machine Translation • Context: Historical Background • Machine Translation is an old idea • Machine Translation Evaluation How good is a translation? Problem: no single right answer Evaluation • How good is a given machine translation system? • Many different translations acceptable • Evaluation metrics • Subjective judgments by human evaluators • Automatic evaluation metrics • Task-based evaluation Adequacy and Fluency • Human judgment • Given: machine translation output • Given: input and/or reference translation • Task: assess quality of MT output • Metrics • Adequacy: does the output convey the meaning of the input sentence? Is part of the message lost, added, or distorted? • Fluency: is the output fluent? Involves both grammatical correctness and idiomatic word choices. Fluency and Adequacy: Scales Let’s try: rate fluency & adequacy on 1-5 scale Challenges in MT evaluation • No single correct answer • Human evaluators disagree Automatic Evaluation Metrics • Goal: computer program that computes quality of translations • Advantages: low cost, optimizable, consistent • Basic strategy • Given: MT output • Given: human reference translation • Task: compute similarity between them Precision and Recall of Words Precision and Recall of Words BLEU Bilingual Evaluation Understudy Multiple Reference Translations BLEU examples Some metrics use more linguistic insights in matching references and hypotheses Drawbacks of Automatic Metrics • All words are treated as equally relevant • Operate on local level • Scores are meaningless (absolute value not informative) • Human translators score low on BLEU Yet automatic metrics such as BLEU correlate with human judgement Caveats: bias toward statistical systems Automatic metrics • Essential tool for system development • Use with caution: not suited to rank systems of different types • Still an open area of research • Connects with semantic analysis T ask-Based Evaluation Post-Editing Machine Translation T ask-Based Evaluation Content Understanding T ests T oday’s topics Machine Translation • Historical Background • Machine Translation is an old idea • Machine Translation Today • Use cases and method • Machine Translation Evaluation What you should know • Context: Historical Background • Machine Translation is an old idea • Difference between hype and reality! • Machine Translation Evaluation • What are adequacy and fluency • Pros and cons of human vs automatic evaluation • How to compute automatic scores: Precision/Recall and BLEU "
9,"School of Computer Science Probabilistic Graphical Models Learning Partially Observed GM: the Expectation-Maximization algorithm Eric Xing Lecture 9, February 12, 2014 Reading: MJ Chap 9, and 11 1 © Eric Xing @ CMU, 2005-2014 Partially observed GMs Speech recognition A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... 2 © Eric Xing @ CMU, 2005-2014 Partially observed GM Biological Evolution A G A G A C 3 © Eric Xing @ CMU, 2005-2014 Mixture Models 4 © Eric Xing @ CMU, 2005-2014 Mixture Models, con'd A density model p(x) may be multi-modal. We may be able to model it as a mixture of uni-modal distributions (e.g., Gaussians). Each mode may correspond to a different sub-population (e.g., male and female).  5 © Eric Xing @ CMU, 2005-2014 Unobserved Variables A variable can be unobserved (latent) because:  it is an imaginary quantity meant to provide some simplified and abstractive view of the data generation process  e.g., speech recognition models, mixture models …  it is a real-world object and/or phenomena, but difficult or impossible to measure  e.g., the temperature of a star, causes of a disease, evolutionary ancestors …  it is a real-world object and/or phenomena, but sometimes wasn’t measured, because of faulty sensors, etc. Discrete latent variables can be used to partition/cluster data into sub-groups. Continuous latent variables (factors) can be used for dimensionality reduction (factor analysis, etc). 6 © Eric Xing @ CMU, 2005-2014 Gaussian Mixture Models (GMMs) Consider a mixture of K Gaussian components: This model can be used for unsupervised clustering.  This model (fit by AutoClass) has been used to discover new kinds of stars in astronomical data, etc.     k k k k n x N x p ) , | , ( ) , (    mixture proportion mixture component 7 © Eric Xing @ CMU, 2005-2014 Gaussian Mixture Models (GMMs) Consider a mixture of K Gaussian components:  Z is a latent class indicator vector:  X is a conditional Gaussian variable with a class-specific mean/covariance  The likelihood of a sample:      k z k n n k n z z p  ) : ( multi ) (   ) - ( ) - ( - exp ) ( ) , , | ( / / k n k T k n k m k n n x x z x p     1 2 1 2 1 2 2 1 1                      k k k k z k z k k n z k k k k n x N x N z x p z p x p n k n k n ) , | , ( ) , : ( ) , , | , ( ) | ( ) , (        1 1 mixture proportion mixture component Z X 8 © Eric Xing @ CMU, 2005-2014 Why is Learning Harder? In fully observed iid settings, the log likelihood decomposes into a sum of local terms (at least for directed models). With latent variables, all the parameters become coupled together via marginalization ) , | ( log ) | ( log ) | , ( log ) ; ( x z c z x p z p z x p D        l     z x z z c z x p z p z x p D ) , | ( ) | ( log ) | , ( log ) ; (     l 9 © Eric Xing @ CMU, 2005-2014 Recall MLE for completely observed data Data log-likelihood MLE What if we do not know zn? C x z z x N z x p z p x z p D n k k n k n n k k k n n k z k n n k z k n n n n n n n k n k n                2 2 1 ) - ( log ) , ; ( log log ) , , | ( ) | ( log ) , ( log ) ; ( 2          θ l Toward the EM algorithm zi xi N ), ; ( max arg ˆ , D MLE k θ l    ) ; ( max arg ˆ , D MLE k θ l    ) ; ( max arg ˆ , D MLE k θ l        n k n n n k n MLE k z x z , ˆ  10 © Eric Xing @ CMU, 2005-2014 Question “ … We solve problem X using Expectation-Maximization …”  What does it mean? E  What do we take expectation with?  What do we take expectation over? M  What do we maximize?  What do we maximize with respect to? 11 © Eric Xing @ CMU, 2005-2014 Recall: K-means ) ( ) ( max arg ) ( ) ( ) ( ) ( t k n t k T t k n k t n x x z       1     n t n n n t n t k k z x k z ) , ( ) , ( ) ( ) ( ) (    1 12 © Eric Xing @ CMU, 2005-2014 Expectation-Maximization Start:  ""Guess"" the centroid k and coveriance k of each of the K clusters Loop 13 © Eric Xing @ CMU, 2005-2014 Example: Gaussian mixture model  A mixture of K Gaussians:  Z is a latent class indicator vector  X is a conditional Gaussian variable with class-specific mean/covariance  The likelihood of a sample:  The expected complete log likelihood Zn Xn N      k z k n n k n z z p  ) : ( multi ) (   ) - ( ) - ( - exp ) ( ) , , | ( / / k n k T k n k m k n n x x z x p     1 2 1 2 1 2 2 1 1                      k k k k z k z k k n z k k k k n x N x N z x p z p x p n k n k n ) , | , ( ) , : ( ) , , | , ( ) | ( ) , (        1 1                   n k k k n k T k n k n n k k k n n x z p n n n x z p n c C x x z z z x p z p z x log ) ( ) ( 2 1 log ) , , | ( log ) | ( log ) , ; ( 1 ) | ( ) | (      θ l 14 © Eric Xing @ CMU, 2005-2014 We maximize iteratively using the following iterative procedure: ─Expectation step: computing the expected value of the sufficient statistics of the hidden variables (i.e., z) given current est. of the parameters (i.e., and ). Here we are essentially doing inference         i t i t i n t i t k t k n t k t t k n q k n t k n x N x N x z p z t ) , | , ( ) , | , ( ) , , | 1 ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) (       ) (θ c l E-step 15 © Eric Xing @ CMU, 2005-2014 We maximize iteratively using the following iterative procudure: ─Maximization step: compute the parameters under current results of the expected value of the hidden variables  This is isomorphic to MLE except that the variables that are hidden are replaced by their expectations (in general they will by replaced by their corresponding ""sufficient statistics"") ) (θ c l M-step 1 s.t. , , 0 ) ( , ) ( max arg ) ( * k * ) ( N n N N z k l l k n t k n n q k n k k c c k t k                    θ θ       n t k n n n t k n t k k x l ) ( ) ( ) 1 ( * , ) ( max arg     θ             n t k n n T t k n t k n t k n t k k x x l ) ( ) 1 ( ) 1 ( ) ( ) 1 ( * ) )( ( , ) ( max arg     θ T T T xx x x         A A A A A log : Fact 1 1 16 © Eric Xing @ CMU, 2005-2014 Compare: K-means and EM  K-means  In the K-means “E-step” we do hard assignment:  In the K-means “M-step” we update the means as the weighted sum of the data, but now the weights are 0 or 1:  EM  E-step  M-step ) ( ) ( max arg ) ( ) ( ) ( ) ( t k n t k T t k n k t n x x z       1     n t n n n t n t k k z x k z ) , ( ) , ( ) ( ) ( ) (    1 The EM algorithm for mixtures of Gaussians is like a ""soft version"" of the K-means algorithm.         i t i t i n t i t k t k n t k t t k n q k n t k n x N x N x z p z t ) , | , ( ) , | , ( ) , , | 1 ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) (           n t k n n n t k n t k x ) ( ) ( ) 1 (    17 © Eric Xing @ CMU, 2005-2014 Theory underlying EM What are we doing? Recall that according to MLE, we intend to learn the model parameter that would have maximize the likelihood of the data. But we do not observe z, so computing is difficult! What shall we do?     z x z z c z x p z p z x p D ) , | ( ) | ( log ) | , ( log ) ; (     l 18 © Eric Xing @ CMU, 2005-2014 Complete & Incomplete Log Likelihoods  Complete log likelihood Let X denote the observable variable(s), and Z denote the latent variable(s). If Z could be observed, then  Usually, optimizing lc() given both z and x is straightforward (c.f. MLE for fully observed models).  Recalled that in this case the objective for, e.g., MLE, decomposes into a sum of factors, the parameter for each factor can be estimated separately.  But given that Z is not observed, lc() is a random quantity, cannot be maximized directly. Incomplete log likelihood With z unobserved, our objective becomes the log of a marginal probability:  This objective won't decouple ) | , ( log ) , ; ( def   z x p z x c  l    z c z x p x p x ) | , ( log ) | ( log ) ; (    l 19 © Eric Xing @ CMU, 2005-2014 Expected Complete Log Likelihood   z q c z x p x z q z x ) | , ( log ) , | ( ) , ; ( def    l        z z z x z q z x p x z q x z q z x p x z q z x p x p x ) | ( ) | , ( log ) | ( ) | ( ) | , ( ) | ( log ) | , ( log ) | ( log ) ; (      l q q c H z x x    ) , ; ( ) ; (   l l For any distribution q(z), define expected complete log likelihood: A deterministic function of  Linear in lc() --- inherit its factorizabiility Does maximizing this surrogate yield a maximizer of the likelihood? Jensen’s inequality 20 © Eric Xing @ CMU, 2005-2014 Lower Bounds and Free Energy For fixed data x, define a functional called the free energy: The EM algorithm is coordinate-ascent on F :  E-step:  M-step: ) ; ( ) | ( ) | , ( log ) | ( ) , ( def x x z q z x p x z q q F z    l   ) , ( max arg t q t q F q   1 ) , ( max arg t t t q F    1 1   21 © Eric Xing @ CMU, 2005-2014 E-step: maximization of expected lc w.r.t. q Claim:  This is the posterior distribution over the latent variables given the data and the parameters. Often we need this at test time anyway (e.g. to perform classification). Proof (easy): this setting attains the bound l(;x)F(q,) Can also show this result using variational calculus or the fact that ) , | ( ) , ( max arg t t q t x z p q F q     1 ) ; ( ) | ( log ) | ( log ) | ( ) , ( ) | , ( log ) , ( ) ), , ( ( x x p x p x z q x z p z x p x z p x z p F t t z t z t t t t t         l         ) , | ( || KL ) , ( ) ; (    x z p q q F x   l 22 © Eric Xing @ CMU, 2005-2014 E-step plug in posterior expectation of latent variables Without loss of generality: assume that p(x,z|) is a generalized exponential family distribution:  Special cases: if p(X|Z) are GLIMs, then The expected complete log likelihood under is ) ( ) , ( ) ( ) | , ( log ) , | ( ) , ; ( ) , | (        A z x f A z x p x z q z x i x z q i t i z t t q t c t t       1 l         i i i z x f z x h Z z x p ) , ( exp ) , ( ) ( ) , (    1 ) ( ) ( ) , ( x z z x f i T i i    ) , | ( t t x z p q   1 ) ( ) ( ) ( ) , | ( GLIM ~      A x z i i x z q i t i p t   23 © Eric Xing @ CMU, 2005-2014 M-step: maximization of expected lc w.r.t.  Note that the free energy breaks into two terms:  The first term is the expected complete log likelihood (energy) and the second term, which does not depend on , is the entropy. Thus, in the M-step, maximizing with respect to for fixed q we only need to consider the first term:  Under optimal qt+1, this is equivalent to solving a standard MLE of fully observed model p(x,z|), with the sufficient statistics involving z replaced by their expectations w.r.t. p(z|x,). q q c z z z H z x x z q x z q z x p x z q x z q z x p x z q q F         ) , ; ( ) | ( log ) | ( ) | , ( log ) | ( ) | ( ) | , ( log ) | ( ) , (     l      z q c t z x p x z q z x t ) | , ( log ) | ( max arg ) , ; ( max arg      1 1 l 24 © Eric Xing @ CMU, 2005-2014 Example: HMM  Supervised learning: estimation when the “right answer” is known  Examples: GIVEN: a genomic region x = x1…x1,000,000 where we have good (experimental) annotations of the CpG islands GIVEN: the casino player allows us to observe him one evening, as he changes dice and produces 10,000 rolls  Unsupervised learning: estimation when the “right answer” is unknown  Examples: GIVEN: the porcupine genome; we don’t know how frequent are the CpG islands there, neither do we know their composition GIVEN: 10,000 rolls of the casino player, but we don’t see when he changes dice  QUESTION: Update the parameters of the model to maximize P(x|) - -- Maximal likelihood (ML) estimation 25 © Eric Xing @ CMU, 2005-2014 Hidden Markov Model: from static to dynamic mixture models Dynamic mixture A A A A X2 X3 X1 XT Y2 Y3 Y1 YT ... ... Static mixture A X1 Y1 N The sequence: The underlying source: Phonemes, Speech signal, sequence of rolls, dice, 26 © Eric Xing @ CMU, 2005-2014 The Baum Welch algorithm The complete log likelihood The expected complete log likelihood EM  The E step  The M step (""symbolically"" identical to MLE)                 n T t t n t n T t t n t n n c x x p y y p y p p 1 2 1 1 ) | ( ) | ( ) ( log ) , ( log ) , ; ( , , , , , y x y x θ l                             n T t k i y p i t n k t n n T t j i y y p j t n i t n n i y p i n c b y x a y y y n t n n t n t n n n 1 2 1 1 1 1 , ) | ( , , , ) | , ( , , ) | ( , log log log ) , ; ( , , , , x x x y x θ  l ) | ( , , , n i t n i t n i t n y p y x 1     ) | , ( , , , , , , n j t n i t n j t n i t n j i t n y y p y y x 1 1 1 1              n T t i t n n T t j i t n ML ij a 1 1 2 , , ,         n T t i t n k t n n T t i t n ML ik x b 1 1 1 , , ,   N n i n ML i   1 ,   27 © Eric Xing @ CMU, 2005-2014 Unsupervised ML estimation  Given x = x1…xN for which the true state path y = y1…yN is unknown,  EXPECTATION MAXIMIZATION 0. Starting with our best guess of a model M, parameters : 1. Estimate Aij , Bik in the training data  How? , , 2. Update according to Aij , Bik  Now a ""supervised learning"" problem 3. Repeat 1 & 2, until convergence This is called the Baum-Welch Algorithm We can get to a provably more (or equally) likely parameter set each iteration k t n t n i t n ik x y B , , ,      t n j t n i t n ij y y A , , , 1 28 © Eric Xing @ CMU, 2005-2014 EM for general BNs while not converged % E-step for each node i ESSi = 0 % reset expected sufficient statistics for each data sample n do inference with Xn,H for each node i % M-step for each node i i := MLE(ESSi ) ) | ( , , , , ) , ( H n H n i x x p n i n i i x x SS ESS     29 © Eric Xing @ CMU, 2005-2014 Summary: EM Algorithm  A way of maximizing likelihood function for latent variable models. Finds MLE of parameters when the original (hard) problem can be broken up into two (easy) pieces: 1. Estimate some “missing” or “unobserved” data from observed data and current parameters. 2. Using this “complete” data, find the maximum likelihood parameter estimates.  Alternate between filling in the latent variables using the best guess (posterior) and updating the parameters based on this guess:  E-step:  M-step:  In the M-step we optimize a lower bound on the likelihood. In the E- step we close the gap, making bound=likelihood. ) , ( max arg t q t q F q   1 ) , ( max arg t t t q F    1 1   30 © Eric Xing @ CMU, 2005-2014 Conditional mixture model: Mixture of experts  We will model p(Y |X) using different experts, each responsible for different regions of the input space.  Latent variable Z chooses expert using softmax gating function:  Each expert can be a linear regression model:  The posterior expert responsibilities are   x x z P T k  Softmax ) (  1   2 1 k T k k x y z x y P   , ; ) , ( N        j j j j j k k k k k x y p x z p x y p x z p y x z P ) , , ( ) ( ) , , ( ) ( ) , , ( 2 2 1 1 1      31 © Eric Xing @ CMU, 2005-2014 EM for conditional mixture model Model: The objective function EM:  E-step:  M-step:  using the normal equation for standard LR , but with the data re-weighted by (homework)  IRLS and/or weighted IRLS algorithm to update {kkk} based on data pair (xn,yn), with weights (homework?) ) , , , | ( ) , | ( ) (    i k k k x z y p x z p x y P 1 1          j j j n n j n j n k k n n k n k n n n k n t k n x y p x z p x y p x z p y x z P ) , , ( ) ( ) , , ( ) ( ) , , ( ) ( 2 2 1 1 1      θ                     n k k k n T k n k n n k n T k k n n y x z p n n n n y x z p n n c C x y z x z z x y p x z p z y x 2 2 ) , | ( ) , | ( log ) - ( 2 1 ) softmax( log ) , , , | ( log ) , | ( log ) , , ; (        θ l Y X X X T T 1   ) (  ) (t k n  32 © Eric Xing @ CMU, 2005-2014 Hierarchical mixture of experts  This is like a soft version of a depth-2 classification/regression tree.  P(Y |X,G1,G2) can be modeled as a GLIM, with parameters dependent on the values of G1 and G2 (which specify a ""conditional path"" to a given leaf in the tree). 33 © Eric Xing @ CMU, 2005-2014 Mixture of overlapping experts By removing the X Z arc, we can make the partitions independent of the input, thus allowing overlap. This is a mixture of linear regressors; each subpopulation has a different conditional mean.      j j j j j k k k k k x y p z p x y p z p y x z P ) , , ( ) ( ) , , ( ) ( ) , , ( 2 2 1 1 1      34 © Eric Xing @ CMU, 2005-2014 Partially Hidden Data Of course, we can learn when there are missing (hidden) variables on some cases and not on others. In this case the cost function is:  Note that Ym do not have to be the same in each case --- the data can have different missing values in each different sample Now you can think of this in a new way: in the E-step we estimate the hidden variables on the incomplete cases only. The M-step optimizes the log likelihood on the complete data plus the expected likelihood on the incomplete data using the E-step.        Missing Complete ) | , ( log ) | , ( log ) ; ( m y m m n n n c m y x p y x p D    l 35 © Eric Xing @ CMU, 2005-2014 EM Variants Sparse EM: Do not re-compute exactly the posterior probability on each data point under all models, because it is almost zero. Instead keep an “active list” which you update every once in a while. Generalized (Incomplete) EM: It might be hard to find the ML parameters in the M-step, even given the completed data. We can still make progress by doing an M-step that improves the likelihood a bit (e.g. gradient step). Recall the IRLS step in the mixture of experts model. 36 © Eric Xing @ CMU, 2005-2014 A Report Card for EM Some good things about EM:  no learning rate (step-size) parameter  automatically enforces parameter constraints  very fast for low dimensions  each iteration guaranteed to improve likelihood Some bad things about EM:  can get stuck in local minima  can be slower than conjugate gradient (especially near convergence)  requires expensive inference step  is a maximum likelihood/MAP method 37 © Eric Xing @ CMU, 2005-2014 "
90,"Neural sequence-to-sequence models for machine translation CMSC 470 Marine Carpuat Machine Translation • Translation system • Input: source sentence F • Output: target sentence E • Can be viewed as a function • Modern machine translation systems • 3 problems • Modeling • how to define P(.)? • Training/Learning • how to estimate parameters from parallel corpora? • Search • How to solve argmax efficiently? Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • Training • Practical tricks • Sequence to sequence models for other NLP tasks A feedforward neural 3-gram model A recurrent language model A recurrent language model Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • Practical tricks • Sequence to sequence models for other NLP tasks P(E|F) as an encoder-decoder model The Encoder models the input/source entence F=(f1,….f|F|) The Decoder models the output/target sentence E=(e1,….e|E|). The decoder hidden state is initialized with the last hidden state of the encoder P(E|F) as an encoder-decoder model Generating Output • We have a model P(E|F), how can we generate translations? • 2 methods • Sampling: generate a random sentence according to probability distribution • Argmax: generate sentence with highest probability Ancestral Sampling • Randomly generate words one by one • Until end of sentence symbol • Done! While 𝑒 𝑗−1! = </s> 𝑒 𝑗~ P(𝑒 𝑗|F,𝑒1, … , 𝑒 𝑗−1) Greedy search • One by one, pick single highest probability word • Problems • Often generates easy words first • Often prefers multiple common words to rare words While 𝑒 𝑗−1! = </s> 𝑒 𝑗= argmax P(𝑒 𝑗|F,𝑒1, … , 𝑒 𝑗−1) Greedy Search Example Consider this complete search graph for a model with vocabulary {a,b,</s>} What sequence does greedy search produces? What is the best scoring sequence in the search space? Greedy Search Example Consider this complete search graph for a model with vocabulary {a,b,</s>} Here greedy search fails to discover the best scoring output! Beam Search Idea: consider b top hypotheses at each time step At each time step: - Expand the b hypotheses for all words in the vocabulary - Prune down to the top b hypotheses - Move to next step Example beam search with b = 2 Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • Training • Practical tricks • Sequence to sequence models for other NLP tasks "
91,"Neural sequence-to-sequence models for machine translation CMSC 470 Marine Carpuat Machine Translation • Translation system • Input: source sentence F • Output: target sentence E • Can be viewed as a function • Modern machine translation systems • 3 problems • Modeling • how to define P(.)? • Training/Learning • how to estimate parameters from parallel corpora? • Search • How to solve argmax efficiently? P(E|F) as an encoder-decoder model The Encoder models the input/source entence F=(f1,….f|F|) The Decoder models the output/target sentence E=(e1,….e|E|). The decoder hidden state is initialized with the last hidden state of the encoder Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • How to train? • Model variants and practical tricks • Attention mechanism Training • Same as for RNN language modeling • Intuition: a good model assigns high probability to training examples • Loss function • Negative log-likelihood of training data • Also called cross-entropy loss • Total loss for one example (sentence pair) = sum of loss at each time step (word) • Backpropagation • Gradient of loss at time step t is propagated through the network all the way back to 1st time step Aside: why don’t we use BLEU as training loss? Training in practice: online Training in practice: batch Training in practice: minibatch • Compromise between online and batch • Computational advantages • Can leverage vector processing instructions in modern hardware • By processing multiple examples simultaneously Problem with minibatches: examples have varying length • 3 tricks (same as for language modeling) • Padding • Add </s> symbol to make all sentences same length • Masking • Multiply loss function calculated over padded symbols by zero • + sort sentences by length Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • How to train? • Model variants and practical tricks • Attention mechanism Other encoder structures: RNN variants • LSTMs • Aim to address vanishing/exploding gradient issue • Stacked RNNs • … Other encoder structures: Bidirectional encoder • Motivation: - Help bootstrap learning - By shortening length of dependencies Motivation: - Take 2 hidden vectors from source encoder - Combine them into a vector of size required by decoder A few more tricks: ensembling • Combine predictions from multiple models • Methods • Linear or log-linear interpolation • Parameter averaging Tricks: addressing length bias • Default models tend to generate short sentences • Solutions: • Prior probability on sentence length • Normalize by sentence length Neural Machine Translation • Neural language models review • Sequence to sequence models for MT • Encoder-Decoder • Sampling and search (greedy vs beam search) • How to train? • Model variants and practical tricks • Attention mechanism "
92,"Attention models & Current topics in Neural MT CMSC 470 Marine Carpuat P(E|F) as an encoder-decoder model The Encoder models the input/source entence F=(f1,….f|F|) The Decoder models the output/target sentence E=(e1,….e|E|). The decoder hidden state is initialized with the last hidden state of the encoder P(E|F) as an encoder-decoder model Problem with previous encoder-decoder model • Long-distance dependencies remain a problem • A single vector represents the entire source sentence • No matter its length • Solution: attention mechanism • An example of incorporating inductive bias in model architecture Attention model intuition • Encode each word in source sentence into a vector • When decoding, perform a linear combination of these vectors, weighted by “attention weights” • Use this combination when predicting next word [Bahdanau et al. 2015] Attention model Source word representations • We can use representations from bidirectional RNN encoder • And concatenate them in a matrix Attention model Create a source context vector • Attention vector: • Entries between 0 and 1 • Interpreted as weight given to each source word when generating output at time step t Attention vector Context vector Attention model Illustrating attention weights Attention model How to calculate attention scores Attention model Various ways of calculating attention score • Dot product • Bilinear function • Multi-layer perceptron (original formulation in Bahdanau et al.) Advantages of attention • Helps illustrate/interpret translation decisions • Can help insert translations for out-of-vocabulary words • By copying or look up in external dictionary • Can incorporate linguistically motivated priors in model Attention extensions Bidirectional constraints (Cohn et al. 2015) • Intuition: attention should be similar in forward and backward translation directions • Method: train so that we get a bonus based on the trace of matrix product for training in both directions Attention extensions An active area of research • Attend to multiple sentences (Zoph et al. 2015) • Attend to a sentence and an image (Huang et al. 2016) • Incorporate bias from alignment models Issue with Neural MT: it only works well in high- resource settings Ongoing research • Learn from other sources of supervision than pairs (E,F) • Monolingual text • Multiple languages • Incorporate linguistic knowledge • As additional embeddings • As prior on network structure or parameters • To make better use of training data [Koehn & Knowles 2017] The Google Multilingual NMT System [Johnson et al. 2017] The Google Multilingual NMT System [Johnson et al. 2017] • A simple idea • Train on sentence pairs in all languages • Add token to mark target language • Helps most for low-resources languages • Enables zero-shot translation • Can handle code-switched input Issue with NMT: Exposure Bias • Mismatch between contexts seen at training and test time • During training, model produces outputs based on sequence prefix from reference translation • This is sometimes called teacher forcing • During decoding, the model produce outputs based on its own previous predictions • This is called the exposure bias problem • Idea: expose model to its own predictions during training • Challenges: model predictions are very noisy, esp. during early training stages • Challenges: • Currently addressed using imitation learning and reinforcement learning algorithms Issue with Neural MT: sentences are translated out-of-context Example from Rico Sennrich (2018) Issue with Neural MT: sentences are translated out-of-context Example from Rico Sennrich (2018) Issue with Neural MT: sentences are translated out-of-context Example from Rico Sennrich (2018) Idea: Translate documents, not sentences! State-of-the-art neural MT models are very powerful, but still make many errors https://www.youtube.com/watch?v=3-rfBsWmo0M Beyond MT: Encoder-Decoder can be used as Conditioned Language Models to generate text Y according to some specification X Neural Machine Translation What you should know • How to formulate machine translation as a sequence-to-sequence transformation task • How to model P(E|F) using RNN encoder-decoder models, with and without attention • Algorithms for producing translations • Ancestral sampling, greedy search, beam search • How to train models • loss functions, parameter update rules, batch vs online vs minibatch training • Examples of weaknesses of neural MT models and how to address them • Bidirectional encoder, length bias, multilingual models • Determine whether a NLP task should be addressed with neural sequence- to-sequence models "
93,"Syntax, Grammars & Parsing CMSC 470 Marine Carpuat Fig credits: Joakim Nivre, Dan Jurafsky & James Martin Syntax & Grammar • Syntax • From Greek syntaxis, meaning “setting out together” • refers to the way words are arranged together. • Grammar • Set of structural rules governing composition of clauses, phrases, and words in any given natural language • Descriptive, not prescriptive • Panini’s grammar of Sanskrit ~2000 years ago Syntax and Grammar • Goal of syntactic theory • “explain how people combine words to form sentences and how children attain knowledge of sentence structure” • Grammar • implicit knowledge of a native speaker • acquired without explicit instruction • minimally able to generate all and only the possible sentences of the language [Philips, 2003] Syntax in NLP • Syntactic analysis can be useful in many NLP applications • Grammar checkers • Dialogue systems • Question answering • Information extraction • Machine translation • … • Sequence models can go a long way but syntactic analysis is particularly useful • In low resource settings • In tasks where precise output structure matters Two views of syntactic structure • Constituency (phrase structure) • Phrase structure organizes words in nested constituents • Dependency structure • Shows which words depend on (modify or are arguments of) which on other words Constituency • Basic idea: groups of words act as a single unit • Constituents form coherent classes that behave similarly • With respect to their internal structure: e.g., at the core of a noun phrase is a noun • With respect to other constituents: e.g., noun phrases generally occur before verbs Constituency: Example • The following are all noun phrases in English... • Why? • They can all precede verbs • They can all be preposed/postposed • … Grammars and Constituency • For a particular language: • What are the “right” set of constituents? • What rules govern how they combine? • Answer: not obvious and difficult • There are many different theories of grammar and competing analyses of the same data! An Example Context-Free Grammar Parse Tree: Example Note: equivalence between parse trees and bracket notation Dependency Grammars • Context-Free Grammars focus on constituents • Non-terminals don’t actually appear in the sentence • In dependency grammar, a parse is a graph (usually a tree) where: • Nodes represent words • Edges represent dependency relations between words (typed or untyped, directed or undirected) Example Dependency Parse They hid the letter on the shelf Compare with constituent parse… What’s the relation? Dependency Grammars • Syntactic structure = lexical items linked by binary asymmetrical relations called dependencies Example Dependency Parse They hid the letter on the shelf Compare with constituent parse… What’s the relation? Dependencies form a tree: - Connected - Acyclic - Single-head Dependency Relations Universal Dependencies project • Set of dependency relations that are • Linguistically motivated • Computationally useful • Cross-linguistically applicable • [Nivre et al. 2016] • Universaldependencies.org Outline • Syntax & Grammar • Two views of syntactic structures • Context-Free Grammars • Dependency grammars • Can be used to capture various facts about the structure of language (but not all!) • Dependency Parsing Data-driven dependency parsing Goal: learn a good predictor of dependency graphs Input: sentence Output: dependency graph/tree G = (V,A) Can be framed as a structured prediction task - very large output space - with interdependent labels 2 dominant approaches: transition-based parsing and graph-based parsing Transition-based dependency parsing • Builds on shift-reduce parsing [Aho & Ullman, 1972] • Configuration • Stack • Input buffer of words • Set of dependency relations • Goal of parsing • find a final configuration where • all words accounted for • Relations form dependency tree Defining Transitions • Transitions • Are functions that produce a new configuration given current configuration • Parsing is the task of finding a sequence of transition that leads from start state to desired goal state • Start state • Stack initialized with ROOT node • Input buffer initialized with words in sentence • Dependency relation set = empty • End state • Stack and word lists are empty • Set of dependency relations = final parse Arc Standard Transition System defines 3 transition operators [Covington, 2001; Nivre 2003] LEFT-ARC • create head-dependent relation between word at top of stack and 2nd word (under top) • remove 2nd word from stack RIGHT-ARC • Create head-dependent relation between word on 2nd word on stack and word on top • Remove word at top of stack SHIFT • Remove word at head of input buffer • Push it on the stack Arc standard transition systems • Preconditions • ROOT cannot have incoming arcs • LEFT-ARC cannot be applied when ROOT is the 2nd element in stack • LEFT-ARC and RIGHT-ARC require 2 elements in stack to be applied Transition-based Dependency Parser Properties of this algorithm: - Linear in sentence length - A greedy algorithm - Output quality depends on oracle Let’s parse this sentence Transition-Based Parsing Illustrated Outline • Syntax & Grammar • Two views of syntactic structures • Context-Free Grammars • Dependency grammars • Can be used to capture various facts about the structure of language (but not all!) • Dependency Parsing • Transition-based parser Where do we get an oracle? • Multiclass classification problem • Input: current parsing state (e.g., current and previous configurations) • Output: one transition among all possible transitions • Q: size of output space? • Supervised classifiers can be used • E.g., perceptron • Open questions • What are good features for this task? • Where do we get training examples? "
94,"Dependency Parsing (2) CMSC 470 Marine Carpuat Fig credits: Joakim Nivre, Dan Jurafsky & James Martin Transition-based Dependency Parser Properties of this algorithm: - Linear in sentence length - A greedy algorithm - Output quality depends on oracle Where do we get an oracle? • Multiclass classification problem • Input: current parsing state (e.g., current and previous configurations) • Output: one transition among all possible transitions • Q: size of output space? • 3 for unlabeled parsing (LEFT-ARC, RIGHT-ARC, SHIFT) • 1 + 2L for labeled parsing where L is the number of labeled dependencies • Any supervised classifier can be used • E.g., perceptron, neural network • Open questions • What are good features for this task? • Where do we get training examples? Generating Training Examples • What we have in a treebank • What we need to train an oracle • Pairs of configurations and predicted parsing action Generating training examples • Approach: simulate parsing to generate reference tree • Given • A current config with stack S, dependency relations Rc • A reference parse (V,Rp) • Do Additional condition on RightArc makes sure a word is not removed from stack before its been attached to all its dependent Let’s try it out How can we define classifier features? • What makes a good feature? • Captures useful correlations between patterns in input and predicted class • Avoid sparsity, encourage generalization • Here input is parser configuration • consists of stack, buffer, current set of relations • Typical features • Features focus on top level of stack • Use word forms, POS, and their location in stack and buffer • Use dependency relations found so far Features example • Given configuration • Example of useful features Features example Research highlight: Dependency parsing with stack-LSTMs • From Dyer et al. 2015: http://www.aclweb.org/anthology/P15-1033 • Idea • Instead of hand-crafted feature • Predict next transition using recurrent neural networks to learn representation of stack, buffer, sequence of transitions Research highlight: Dependency parsing with stack-LSTMs Research highlight: Dependency parsing with stack-LSTMs An Alternative to the Arc- Standard Transition System A weakness of arc-standard parsing Right dependents cannot be attached to their head until all their dependents have been attached Arc Eager Parsing • LEFT-ARC • Create head-dependent rel. between word at front of buffer and word at top of stack • pop the stack • RIGHT-ARC • Create head-dependent rel. between word on top of stack and word at front of buffer • Shift buffer head to stack • SHIFT • Remove word at head of input buffer • Push it on the stack • REDUCE • Pop the stack Arc Eager Parsing Example Properties of transition-based parsing algorithms Trees & Forests • A dependency tree is a graph satisfying the following conditions • Root • Single head • No cycles • Connectedness • A dependency forest is a dependency graph satisfying • Root • Single head • No cycles • but not Connectedness Properties of this transition-based parsing algorithm - Correctness - For every complete transition sequence, the resulting graph is a projective dependency forest (soundness) - For every projective dependency forest G, there is a transition sequence that generates G (completeness) - Trick: forest can be turned into tree by adding links to ROOT0 Projectivity • Arc from head to dependent is projective • If there is a path from head to every word between head and dependent • Dependency tree is projective • If all arcs are projective • Or equivalently, if it can be drawn with no crossing edges • Projective trees make computation easier • But most theoretical frameworks do not assume projectivity • Need to capture long-distance dependencies, free word order Arc-standard parsing can’t produce non- projective trees How frequent are non-projective structures? • Statistics from CoNLL shared task • NPD = non projective dependencies • NPS = non projective sentences How to deal with non-projectivity? (1) change the transition system • Intuition: • Add new transitions • That apply to 2nd word of the stack • Top word of stack is treated as context [Attardi 2006] How to deal with non-projectivity? (2) pseudo-projective parsing Solution: • “projectivize” a non-projective tree by creating new projective arcs • That can be transformed back into non-projective arcs in a post-processing step How to deal with non-projectivity? (2) pseudo-projective parsing Solution: • “projectivize” a non-projective tree by creating new projective arcs • That can be transformed back into non-projective arcs in a post-processing step Dependency Parsing: what you should know • Transition-based dependency parsing • Shift-reduce parsing • Transition systems: arc standard, arc eager • Oracle algorithm: how to obtain a transition sequence given a tree • How to construct a multiclass classifier to predict parsing actions • What transition-based parsers can and cannot do • That transition-based parsers provide a flexible framework that allows many extensions • such as RNNs vs feature engineering, non-projectivity (but I don’t expect you to memorize these algorithms) • Next: Graph-based dependency parsing "
95,"Dependency Parsing (3) CMSC 470 Marine Carpuat Fig credits: Joakim Nivre, Dan Jurafsky & James Martin Dependency Parsing: what you should know • Transition-based dependency parsing • Shift-reduce parsing • Transition systems: arc standard, arc eager • Oracle algorithm: how to obtain a transition sequence given a tree • How to construct a multiclass classifier to predict parsing actions • What transition-based parsers can and cannot do • That transition-based parsers provide a flexible framework that allows many extensions • such as RNNs vs feature engineering, non-projectivity (but I don’t expect you to memorize these algorithms) • Next: Graph-based dependency parsing Generating Training Examples • What we have in a treebank • What we need to train an oracle • Pairs of configurations and predicted parsing action Generating training examples • Approach: simulate parsing to generate reference tree • Given • A current config with stack S, dependency relations Rc • A reference parse (V,Rp) • Do Additional condition on RightArc makes sure a word is not removed from stack before its been attached to all its dependent Graph-based Dependency Parsing Directed Spanning Trees Dependency Parsing as Finding the Maximum Spanning Tree • Views parsing as finding the best directed spanning tree • of multi-digraph that captures all possible dependencies in a sentence • needs a score that quantifies how good a tree is • Assume we have an arc factored model i.e. weight of graph can be factored as sum or product of weights of its arcs • Chu-Liu-Edmonds algorithm can find the maximum spanning tree for us • Recursive algorithm • Naïve implementation: O(n^3) Chu-Liu-Edmonds illustrated (for unlabeled dependency parsing) Chu-Liu-Edmonds illustrated Chu-Liu-Edmonds illustrated Chu-Liu-Edmonds illustrated Chu-Liu-Edmonds illustrated Arc weights as linear classifiers Weight of arc from head i to dependent j, with label k Example of classifier features Typical classifier features • Word forms, lemmas, and parts of speech of the headword and its dependent • Corresponding features derived from the contexts before, after and between the words • Word embeddings • The dependency relation itself • The direction of the relation (to the right or left) • The distance from the head to the dependent • … How to score a graph G using features? Arc-factored model assumption By definition of arc weights as linear classifiers Learning parameters with the Structured Perceptron Dependency parsing algorithms Transition-based • Locally trained • Use greedy search algorithms • Deﬁne features over a rich history of parsing decisions Graph-based • Globally trained • Use exact (or near exact) search algorithms • Define features over a limited history of parsing decisions Dependency Parsing: what you should know • Interpreting dependency trees • Transition-based dependency parsing • Shift-reduce parsing • Transition system: arc standard, arc eager • Oracle • Learning/predicting parsing actions • Graph-based dependency parsing • A flexible framework that allows many extensions • RNNs vs feature engineering, non-projectivity "
96,"Introduction to Natural Language Processing CMSC 470 Marine Carpuat Where we started on the 1st day of class • Levels of linguistic analysis in NLP • Morphology, syntax, semantics, discourse • Why is NLP hard? • Ambiguity • Sparse data • Zipf’s law, corpus, word types and tokens • Variation and expressivity • Social Impact Topics • Words and their meanings • Distributional semantics and word sense disambiguation • Fundamentals of supervised classification • Sequences • N-gram and neural language models • Sequence labeling tasks • Structured prediction and search algorithms • Application: Machine Translation • Trees • Syntax and grammars • Parsing Ambiguity and Sparsity • What are examples of NLP challenges due to ambiguity/sparsity? • What are techniques for addressing ambiguity/sparsity in NLP systems? Linguistic Knowledge • How is linguistic knowledge incorporated in NLP systems? • Attention model as an example NLP tasks often require predicting structured outputs • What kind of output structures? • Why is predicting structures challenging from a ML perspective? • What techniques have we learned for addressing these challenges? Structured prediction trade-offs in dependency parsing Transition-based • Locally trained • Use greedy search algorithms • Deﬁne features over a rich history of parsing decisions Graph-based • Globally trained • Use exact (or near exact) search algorithms • Define features over a limited history of parsing decisions Structured prediction trade-offs in sequence labeling Multiclass Classification at each time step • Locally trained • Make predictions greedily • Can define features over history of tag predictions Sequence labeling with structured perceptron • Globally trained • Use exact search algorithms • Define features over a limited history of predictions Consider this new NLP task • Goal: verify information using evidence from Wikipedia. • Input: a factual claim involving one or more entities (resolvable to Wikipedia pages) • Outputs: • the system must extract textual evidence (sets of sentences from Wikipedia pages) that support or refute the claim. • Using this evidence, label the claim as Supported, Refuted given the evidence or NotEnoughInfo. How would you build a system for this task? This is the shared task of the Fact Extraction and Verification (FEVER) workshop You can see what solutions researchers came up with here: http://fever.ai/task.html Social Impact • NLP experiments and applications can have a direct effect on individual users’ lives • Some issues • Privacy • Exclusion • Overgeneralization • Dual-use problems • What are examples of each of these issues in NLP systems? [Hovy & Spruit ACL 2016] Last few items • Course • Project and final • Keep learning • CLIP talks (Wed 11am) http://go.umd.edu/cliptalks • Language Science Center http://lsc.umd.edu • Podcasts: • NLP Highlights covers recent papers and trends in NLP research • Lingthusiam covers a very wide range of linguistic topics https://lingthusiasm.com/ • Talking Machines: “Human Conversations about Machine Learning” https://www.thetalkingmachines.com "
